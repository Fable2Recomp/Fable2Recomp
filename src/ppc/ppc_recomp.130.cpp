#include "ppc_recomp_shared.h"

PPC_FUNC_IMPL(__imp__sub_82CDCFE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82cdd020
	if (cr6.eq) goto loc_82CDD020;
	// bl 0x82cdcc08
	sub_82CDCC08(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82CDD020:
	// bl 0x82cdc850
	sub_82CDC850(ctx, base);
	// lis r11,-32768
	r11.s64 = -2147483648;
	// ori r10,r11,16388
	ctx.r10.u64 = r11.u64 | 16388;
	// cmpw cr6,r3,r10
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, xer);
	// beq cr6,0x82cdd03c
	if (cr6.eq) goto loc_82CDD03C;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82cdd074
	if (cr6.lt) goto loc_82CDD074;
loc_82CDD03C:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(76) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,48(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(48) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82cdd074
	if (cr6.lt) goto loc_82CDD074;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce6098
	sub_82CE6098(ctx, base);
loc_82CDD074:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDCFE8) {
	__imp__sub_82CDCFE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ce59d0
	sub_82CE59D0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82cdd0dc
	if (cr6.lt) goto loc_82CDD0DC;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// addi r3,r11,-8
	ctx.r3.s64 = r11.s64 + -8;
	// bne cr6,0x82cdd0c4
	if (!cr6.eq) goto loc_82CDD0C4;
	// li r3,0
	ctx.r3.s64 = 0;
loc_82CDD0C4:
	// li r5,3
	ctx.r5.s64 = 3;
	// addi r4,r31,52
	ctx.r4.s64 = r31.s64 + 52;
	// bl 0x82cd7eb8
	sub_82CD7EB8(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82cdd5d8
	sub_82CDD5D8(ctx, base);
loc_82CDD0DC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD088) {
	__imp__sub_82CDD088(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD0F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// li r10,8
	ctx.r10.s64 = 8;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82CDD108:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82cdd108
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82CDD108;
	// stb r9,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r9.u8);
	// lis r11,0
	r11.s64 = 0;
	// lbz r10,1(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// stb r10,1(r4)
	PPC_STORE_U8(ctx.r4.u32 + 1, ctx.r10.u8);
	// ori r9,r11,48000
	ctx.r9.u64 = r11.u64 | 48000;
	// stw r9,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r9.u32);
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// stw r8,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r8.u32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stw r7,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r7.u32);
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// stw r6,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r6.u32);
	// lbz r11,8(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82cdd154
	if (!cr6.eq) goto loc_82CDD154;
	// li r11,1
	r11.s64 = 1;
loc_82CDD154:
	// stb r11,24(r4)
	PPC_STORE_U8(ctx.r4.u32 + 24, r11.u8);
	// lbz r11,9(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 9);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82cdd168
	if (!cr6.eq) goto loc_82CDD168;
	// li r11,6
	r11.s64 = 6;
loc_82CDD168:
	// stb r11,25(r4)
	PPC_STORE_U8(ctx.r4.u32 + 25, r11.u8);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD0F8) {
	__imp__sub_82CDD0F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD178) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,1(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// lis r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// ori r8,r10,48000
	ctx.r8.u64 = ctx.r10.u64 | 48000;
	// stb r9,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r9.u8);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r11.u8);
	// bl 0x82ce50d0
	sub_82CE50D0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82cdd1d0
	if (cr6.lt) goto loc_82CDD1D0;
	// li r5,3
	ctx.r5.s64 = 3;
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r4,r31,52
	ctx.r4.s64 = r31.s64 + 52;
	// bl 0x82cd7fe0
	sub_82CD7FE0(ctx, base);
loc_82CDD1D0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD178) {
	__imp__sub_82CDD178(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD1F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r10,r11,16448
	ctx.r10.s64 = r11.s64 + 16448;
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// bl 0x82ce62c0
	sub_82CE62C0(ctx, base);
	// lis r9,-31949
	ctx.r9.s64 = -2093809664;
	// lbz r8,76(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 76);
	// mulli r10,r8,44
	ctx.r10.s64 = ctx.r8.s64 * 44;
	// lwz r11,28900(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(28900) );
	// lwz r11,124(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(124) );
	// add r27,r10,r11
	r27.u64 = ctx.r10.u64 + r11.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdd258
	if (cr6.eq) goto loc_82CDD258;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r8
	cr6.compare<uint32_t>(r29.u32, ctx.r8.u32, xer);
	// beq cr6,0x82cdd270
	if (cr6.eq) goto loc_82CDD270;
loc_82CDD258:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
loc_82CDD270:
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x82cdd2b0
	if (cr6.eq) goto loc_82CDD2B0;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDD2B0:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82cdd2f4
	if (cr6.eq) goto loc_82CDD2F4;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82cdd2f4
	if (!cr6.eq) goto loc_82CDD2F4;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdd2f4
	if (!cr0.eq) goto loc_82CDD2F4;
	// li r11,0
	r11.s64 = 0;
	// lbz r29,12(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CDD2F4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ce54a8
	sub_82CE54A8(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CDD1F0) {
	__imp__sub_82CDD1F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD308) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r11,61(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 61);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD308) {
	__imp__sub_82CDD308(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD318) {
	PPC_FUNC_PROLOGUE();
	// b 0x82ce5548
	sub_82CE5548(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CDD318) {
	__imp__sub_82CDD318(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD320) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x82cdd0f8
	sub_82CDD0F8(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ce51d8
	sub_82CE51D8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// blt cr6,0x82cdd36c
	if (cr6.lt) goto loc_82CDD36C;
	// li r5,3
	ctx.r5.s64 = 3;
	// lwz r3,32(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82cd7fe0
	sub_82CD7FE0(ctx, base);
loc_82CDD36C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD320) {
	__imp__sub_82CDD320(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD388) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82ce5090
	sub_82CE5090(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r9,-31949
	ctx.r9.s64 = -2093809664;
	// addi r8,r11,16448
	ctx.r8.s64 = r11.s64 + 16448;
	// stw r8,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r8.u32);
	// lbz r7,10(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 10);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// stb r7,76(r29)
	PPC_STORE_U8(r29.u32 + 76, ctx.r7.u8);
	// mulli r10,r6,44
	ctx.r10.s64 = ctx.r6.s64 * 44;
	// lwz r11,28900(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(28900) );
	// lwz r11,124(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(124) );
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r28,r13
	r28.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdd400
	if (cr6.eq) goto loc_82CDD400;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// beq cr6,0x82cdd414
	if (cr6.eq) goto loc_82CDD414;
loc_82CDD400:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
	// stb r27,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r27.u8);
loc_82CDD414:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdd488
	if (cr6.eq) goto loc_82CDD488;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdd488
	if (!cr6.eq) goto loc_82CDD488;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdd488
	if (!cr0.eq) goto loc_82CDD488;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CDD488:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CDD388) {
	__imp__sub_82CDD388(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD498) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82cdd1f0
	sub_82CDD1F0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD498) {
	__imp__sub_82CDD498(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD4C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r4,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r4.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// bl 0x82cdd0f8
	sub_82CDD0F8(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ce5120
	sub_82CE5120(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82cdd508
	if (cr6.lt) goto loc_82CDD508;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// addi r10,r11,80
	ctx.r10.s64 = r11.s64 + 80;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
loc_82CDD508:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82cdd5ac
	if (cr6.lt) goto loc_82CDD5AC;
	// lis r3,24962
	ctx.r3.s64 = 1635909632;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// addi r5,r1,204
	ctx.r5.s64 = ctx.r1.s64 + 204;
	// ori r3,r3,6
	ctx.r3.u64 = ctx.r3.u64 | 6;
	// bl 0x82ce4fe0
	sub_82CE4FE0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82cdd5ac
	if (cr6.lt) goto loc_82CDD5AC;
	// lwz r3,204(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(204) );
	// li r4,80
	ctx.r4.s64 = 80;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82cdd56c
	if (cr6.eq) goto loc_82CDD56C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,204(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(204) );
	// bl 0x82cdd388
	sub_82CDD388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82cdd578
	if (!cr6.eq) goto loc_82CDD578;
loc_82CDD56C:
	// lis r30,-32761
	r30.s64 = -2147024896;
	// ori r30,r30,14
	r30.u64 = r30.u64 | 14;
	// b 0x82cdd5ac
	goto loc_82CDD5AC;
loc_82CDD578:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cdd320
	sub_82CDD320(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82cdd598
	if (cr6.lt) goto loc_82CDD598;
	// stw r31,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r31.u32);
	// b 0x82cdd5ac
	goto loc_82CDD5AC;
loc_82CDD598:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CDD5AC:
	// lwz r3,204(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(204) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82cdd5c8
	if (cr6.eq) goto loc_82CDD5C8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CDD5C8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CDD4C8) {
	__imp__sub_82CDD4C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD5D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	// lis r11,-31949
	r11.s64 = -2093809664;
	// rlwinm r10,r3,2,0,29
	ctx.r10.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,28904
	r11.s64 = r11.s64 + 28904;
	// add r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 + r11.u64;
loc_82CDD5E8:
	// mfmsr r7
	// mtmsrd r13,1
	// lwarx r9,0,r6
	reserved.u32 = *(uint32_t*)(base + ctx.r6.u32);
	ctx.r9.u64 = __builtin_bswap32(reserved.u32);
	// add r8,r4,r9
	ctx.r8.u64 = ctx.r4.u64 + ctx.r9.u64;
	// stwcx. r8,0,r6
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r6.u32), reserved.s32, __builtin_bswap32(ctx.r8.s32));
	cr0.so = xer.so;
	// mtmsrd r7,1
	// bne 0x82cdd5e8
	if (!cr0.eq) goto loc_82CDD5E8;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD5D8) {
	__imp__sub_82CDD5D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD610) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mftb r10
	ctx.r10.u64 = read_timestamp_counter();
	// lis r11,-31949
	r11.s64 = -2093809664;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// addi r31,r11,28904
	r31.s64 = r11.s64 + 28904;
	// li r30,0
	r30.s64 = 0;
	// sradi r26,r10,32
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0xFFFFFFFF) != 0);
	r26.s64 = ctx.r10.s64 >> 32;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// lwz r11,28904(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28904) );
	// std r30,0(r28)
	PPC_STORE_U64(r28.u32 + 0, r30.u64);
	// lis r3,22593
	ctx.r3.s64 = 1480654848;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r5,40
	ctx.r5.s64 = 40;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// ori r3,r3,30052
	ctx.r3.u64 = ctx.r3.u64 | 30052;
	// lwz r7,16(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// lwz r29,24(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// std r30,8(r28)
	PPC_STORE_U64(r28.u32 + 8, r30.u64);
	// std r30,16(r28)
	PPC_STORE_U64(r28.u32 + 16, r30.u64);
	// std r30,24(r28)
	PPC_STORE_U64(r28.u32 + 24, r30.u64);
	// std r30,32(r28)
	PPC_STORE_U64(r28.u32 + 32, r30.u64);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// stw r7,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r7.u32);
	// stw r6,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r6.u32);
	// stw r29,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r29.u32);
	// bl 0x83004a90
	sub_83004A90(ctx, base);
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82CDD6A4:
	// lwzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// addi r8,r31,28
	ctx.r8.s64 = r31.s64 + 28;
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// stwx r30,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, r30.u32);
	// stwx r9,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpwi cr6,r11,28
	cr6.compare<int32_t>(r11.s32, 28, xer);
	// blt cr6,0x82cdd6a4
	if (cr6.lt) goto loc_82CDD6A4;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CDD610) {
	__imp__sub_82CDD610(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD6D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// li r11,8
	r11.s64 = 8;
	// lfs f0,16564(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 16564);
	f0.f64 = double(temp.f32);
	// li r10,5
	ctx.r10.s64 = 5;
	// lfs f13,16572(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16572);
	ctx.f13.f64 = double(temp.f32);
	// li r8,6
	ctx.r8.s64 = 6;
	// li r9,4
	ctx.r9.s64 = 4;
	// stfs f0,56(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 56, temp.u32);
	// stfs f0,60(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 60, temp.u32);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stfs f0,64(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 64, temp.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// stfs f13,76(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 76, temp.u32);
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// lfs f0,16560(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16560);
	f0.f64 = double(temp.f32);
	// stw r8,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r8.u32);
	// lfs f13,16564(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r11.u32);
	// lfs f12,16568(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16568);
	ctx.f12.f64 = double(temp.f32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// lfs f11,16572(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 16572);
	ctx.f11.f64 = double(temp.f32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stfs f0,52(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// stw r9,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r9.u32);
	// stfs f13,68(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 68, temp.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// stfs f12,72(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 72, temp.u32);
	// stw r9,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r9.u32);
	// stfs f11,80(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 80, temp.u32);
	// stw r10,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r10.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD6D0) {
	__imp__sub_82CDD6D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD758) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r12,r1,-16
	r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82ca7500
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f0,12(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 12);
	f0.f64 = double(temp.f32);
	// lfs f13,8(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fadds f31,f0,f13
	f31.f64 = double(float(f0.f64 + ctx.f13.f64));
	// lfs f26,16564(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16564);
	f26.f64 = double(temp.f32);
	// lfs f0,16568(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16568);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f26
	cr6.compare(f31.f64, f26.f64);
	// bge cr6,0x82cdd7a0
	if (!cr6.lt) goto loc_82CDD7A0;
	// fdivs f28,f13,f31
	f28.f64 = double(float(ctx.f13.f64 / f31.f64));
	// b 0x82cdd7a4
	goto loc_82CDD7A4;
loc_82CDD7A0:
	// fmr f28,f0
	ctx.fpscr.disableFlushMode();
	f28.f64 = f0.f64;
loc_82CDD7A4:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fsubs f13,f0,f28
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = static_cast<float>(f0.f64 - f28.f64);
	// lfs f12,4(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f11,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfd f0,19800(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 19800);
	// fmul f10,f12,f0
	ctx.f10.f64 = ctx.f12.f64 * f0.f64;
	// lfd f30,3384(r10)
	f30.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3384);
	// lfs f0,3052(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3052);
	f0.f64 = double(temp.f32);
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// fmuls f9,f13,f31
	ctx.f9.f64 = double(float(ctx.f13.f64 * f31.f64));
	// fdiv f8,f10,f11
	ctx.f8.f64 = ctx.f10.f64 / ctx.f11.f64;
	// fmuls f2,f9,f0
	ctx.f2.f64 = double(float(ctx.f9.f64 * f0.f64));
	// frsp f27,f8
	f27.f64 = double(float(ctx.f8.f64));
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// lfd f29,3368(r8)
	f29.u64 = PPC_LOAD_U64(ctx.r8.u32 + 3368);
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// fmuls f7,f28,f31
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(f28.f64 * f31.f64));
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// frsp f31,f1
	f31.f64 = double(float(ctx.f1.f64));
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// lfs f0,2736(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2736);
	f0.f64 = double(temp.f32);
	// fmuls f2,f7,f0
	ctx.f2.f64 = double(float(ctx.f7.f64 * f0.f64));
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// stfs f1,24(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r31.u32 + 24, temp.u32);
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// stfs f6,24(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r31.u32 + 24, temp.u32);
	// lfs f0,19792(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 19792);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// bge cr6,0x82cdd89c
	if (!cr6.lt) goto loc_82CDD89C;
	// fmr f1,f27
	ctx.f1.f64 = f27.f64;
	// bl 0x82239e88
	sub_82239E88(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fmuls f11,f31,f31
	ctx.f11.f64 = double(float(f31.f64 * f31.f64));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfd f0,3248(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 3248);
	// lfd f13,3552(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3552);
	// fsub f10,f0,f31
	ctx.f10.f64 = f0.f64 - f31.f64;
	// fmuls f9,f12,f12
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// fsub f8,f0,f12
	ctx.f8.f64 = f0.f64 - ctx.f12.f64;
	// fmuls f7,f12,f31
	ctx.f7.f64 = double(float(ctx.f12.f64 * f31.f64));
	// frsp f6,f10
	ctx.f6.f64 = double(float(ctx.f10.f64));
	// fsub f5,f0,f9
	ctx.f5.f64 = f0.f64 - ctx.f9.f64;
	// fmul f4,f8,f31
	ctx.f4.f64 = ctx.f8.f64 * f31.f64;
	// fsub f3,f0,f7
	ctx.f3.f64 = f0.f64 - ctx.f7.f64;
	// fmul f2,f5,f11
	ctx.f2.f64 = ctx.f5.f64 * ctx.f11.f64;
	// fmsub f1,f4,f13,f2
	ctx.f1.f64 = ctx.f4.f64 * ctx.f13.f64 - ctx.f2.f64;
	// fsqrt f0,f1
	f0.f64 = simd::sqrt_f64(ctx.f1.f64);
	// fsub f13,f3,f0
	ctx.f13.f64 = ctx.f3.f64 - f0.f64;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fdivs f11,f12,f6
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f6.f64));
	// stfs f11,28(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 28, temp.u32);
	// b 0x82cdd8a0
	goto loc_82CDD8A0;
loc_82CDD89C:
	// stfs f26,28(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f26.f64);
	PPC_STORE_U32(r31.u32 + 28, temp.u32);
loc_82CDD8A0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// addi r12,r1,-16
	r12.s64 = ctx.r1.s64 + -16;
	// bl 0x82ca754c
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD758) {
	__imp__sub_82CDD758(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD8C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,2048
	ctx.r10.s64 = 2048;
loc_82CDD8C8:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cdd8c8
	if (!cr0.eq) goto loc_82CDD8C8;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD8C0) {
	__imp__sub_82CDD8C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD920) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,64
	ctx.r10.s64 = 64;
loc_82CDD928:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cdd928
	if (!cr0.eq) goto loc_82CDD928;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD920) {
	__imp__sub_82CDD920(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD980) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,16
	ctx.r10.s64 = 16;
loc_82CDD988:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cdd988
	if (!cr0.eq) goto loc_82CDD988;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD980) {
	__imp__sub_82CDD980(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDD9E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,256
	ctx.r10.s64 = 256;
loc_82CDD9E8:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cdd9e8
	if (!cr0.eq) goto loc_82CDD9E8;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDD9E0) {
	__imp__sub_82CDD9E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDDA40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,32
	ctx.r10.s64 = 32;
loc_82CDDA48:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cdda48
	if (!cr0.eq) goto loc_82CDDA48;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDDA40) {
	__imp__sub_82CDDA40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDDAA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,128
	ctx.r10.s64 = 128;
loc_82CDDAA8:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cddaa8
	if (!cr0.eq) goto loc_82CDDAA8;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDDAA0) {
	__imp__sub_82CDDAA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDDB00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// li r10,512
	ctx.r10.s64 = 512;
loc_82CDDB08:
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stfs f0,-4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,4(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,12(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f9,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,16(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// lfs f8,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,20(r11)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// lfs f7,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,24(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// bne 0x82cddb08
	if (!cr0.eq) goto loc_82CDDB08;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDDB00) {
	__imp__sub_82CDDB00(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDDB60) {
	PPC_FUNC_PROLOGUE();
	// stw r4,100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 100, ctx.r4.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDDB60) {
	__imp__sub_82CDDB60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDDB68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f0,26348(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 26348);
	f0.f64 = double(temp.f32);
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
	// stfs f0,92(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 92, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDDB68) {
	__imp__sub_82CDDB68(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDDB80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f14{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x82ca74d0
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,84
	ctx.r5.s64 = 84;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lis r10,-32252
	ctx.r10.s64 = -2113667072;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lfs f14,-16936(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16936);
	f14.f64 = double(temp.f32);
	// stfs f14,96(r1)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// bne cr6,0x82cddbc8
	if (!cr6.eq) goto loc_82CDDBC8;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82cddbf4
	goto loc_82CDDBF4;
loc_82CDDBC8:
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lfs f0,88(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 88);
	f0.f64 = double(temp.f32);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fmuls f9,f10,f14
	ctx.f9.f64 = double(float(ctx.f10.f64 * f14.f64));
	// fctidz f8,f9
	ctx.f8.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f8.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
loc_82CDDBF4:
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,4095
	cr6.compare<uint32_t>(ctx.r3.u32, 4095, xer);
	// ble cr6,0x82cddc04
	if (!cr6.gt) goto loc_82CDDC04;
	// li r3,4095
	ctx.r3.s64 = 4095;
loc_82CDDC04:
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lis r11,2
	r11.s64 = 131072;
	// lis r9,3
	ctx.r9.s64 = 196608;
	// lis r8,4
	ctx.r8.s64 = 262144;
	// ori r6,r10,61380
	ctx.r6.u64 = ctx.r10.u64 | 61380;
	// ori r7,r11,40712
	ctx.r7.u64 = r11.u64 | 40712;
	// ori r5,r9,41056
	ctx.r5.u64 = ctx.r9.u64 | 41056;
	// ori r4,r8,20736
	ctx.r4.u64 = ctx.r8.u64 | 20736;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stwx r3,r30,r6
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, ctx.r3.u32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// stwx r3,r30,r7
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, ctx.r3.u32);
	// stwx r3,r30,r5
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, ctx.r3.u32);
	// stwx r3,r30,r4
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, ctx.r3.u32);
	// lfs f0,80(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 80);
	f0.f64 = double(temp.f32);
	// lfs f11,2792(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2792);
	ctx.f11.f64 = double(temp.f32);
	// addi r31,r11,16584
	r31.s64 = r11.s64 + 16584;
	// lfs f15,16568(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 16568);
	f15.f64 = double(temp.f32);
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// stfs f15,80(r1)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// blt cr6,0x82cddc88
	if (cr6.lt) goto loc_82CDDC88;
	// lis r11,-32254
	r11.s64 = -2113798144;
	// lfs f12,88(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f13,26348(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 26348);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f0,f13
	ctx.f10.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfs f13,19908(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 19908);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,2848(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2848);
	f0.f64 = double(temp.f32);
	// fmuls f9,f10,f12
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// fmuls f25,f9,f13
	f25.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// b 0x82cddcc0
	goto loc_82CDDCC0;
loc_82CDDC88:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f10,88(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f12,2740(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2740);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f9,f0,f13,f12
	ctx.f9.f64 = double(std::fma(float(f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// lfs f13,26348(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 26348);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,19908(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 19908);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,2848(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2848);
	f0.f64 = double(temp.f32);
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f10
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// fmuls f25,f7,f12
	f25.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
loc_82CDDCC0:
	// fcmpu cr6,f25,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(f25.f64, f0.f64);
	// bge cr6,0x82cddccc
	if (!cr6.lt) goto loc_82CDDCCC;
	// fmr f25,f0
	f25.f64 = f0.f64;
loc_82CDDCCC:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f13,3140(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f30,f25,f13
	f30.f64 = double(float(f25.f64 * ctx.f13.f64));
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// bge cr6,0x82cddce8
	if (!cr6.lt) goto loc_82CDDCE8;
	// fmr f30,f0
	f30.f64 = f0.f64;
	// b 0x82cddcf4
	goto loc_82CDDCF4;
loc_82CDDCE8:
	// fcmpu cr6,f30,f15
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f15.f64);
	// ble cr6,0x82cddcf4
	if (!cr6.gt) goto loc_82CDDCF4;
	// fmr f30,f15
	f30.f64 = f15.f64;
loc_82CDDCF4:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,76(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 76);
	f0.f64 = double(temp.f32);
	// lfs f13,19904(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19904);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x82cddd28
	if (cr6.lt) goto loc_82CDDD28;
	// fsubs f13,f0,f13
	ctx.f13.f64 = static_cast<float>(f0.f64 - ctx.f13.f64);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fmr f29,f15
	f29.f64 = f15.f64;
	// fmr f28,f15
	f28.f64 = f15.f64;
	// fmr f27,f15
	f27.f64 = f15.f64;
	// lfs f0,-20492(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -20492);
	f0.f64 = double(temp.f32);
	// fmuls f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x82cddd90
	goto loc_82CDDD90;
loc_82CDDD28:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// blt cr6,0x82cddd54
	if (cr6.lt) goto loc_82CDDD54;
	// fsubs f13,f0,f11
	ctx.f13.f64 = static_cast<float>(f0.f64 - ctx.f11.f64);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,-20492(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -20492);
	f0.f64 = double(temp.f32);
	// fmr f28,f15
	f28.f64 = f15.f64;
	// fmr f27,f15
	f27.f64 = f15.f64;
	// lfs f31,16564(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16564);
	f31.f64 = double(temp.f32);
	// fmuls f29,f13,f0
	f29.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x82cddd90
	goto loc_82CDDD90;
loc_82CDDD54:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,19900(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19900);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// lfs f31,16564(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16564);
	f31.f64 = double(temp.f32);
	// fmr f29,f31
	f29.f64 = f31.f64;
	// blt cr6,0x82cddd84
	if (cr6.lt) goto loc_82CDDD84;
	// fsubs f13,f0,f13
	ctx.f13.f64 = static_cast<float>(f0.f64 - ctx.f13.f64);
	// lfs f0,-20492(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -20492);
	f0.f64 = double(temp.f32);
	// fmr f27,f15
	f27.f64 = f15.f64;
	// fmuls f28,f13,f0
	f28.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x82cddd90
	goto loc_82CDDD90;
loc_82CDDD84:
	// lfs f13,-20492(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -20492);
	ctx.f13.f64 = double(temp.f32);
	// fmr f28,f31
	f28.f64 = f31.f64;
	// fmuls f27,f0,f13
	f27.f64 = double(float(f0.f64 * ctx.f13.f64));
loc_82CDDD90:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,19896(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19896);
	f0.f64 = double(temp.f32);
	// fmuls f0,f30,f0
	f0.f64 = double(float(f30.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r28,92(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// ori r8,r10,240
	ctx.r8.u64 = ctx.r10.u64 | 240;
	// ori r7,r9,252
	ctx.r7.u64 = ctx.r9.u64 | 252;
	// addi r6,r3,-1
	ctx.r6.s64 = ctx.r3.s64 + -1;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// stwx r3,r30,r8
	PPC_STORE_U32(r30.u32 + ctx.r8.u32, ctx.r3.u32);
	// stwx r6,r30,r7
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, ctx.r6.u32);
	// lfs f0,19892(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 19892);
	f0.f64 = double(temp.f32);
	// fmuls f12,f30,f0
	ctx.f12.f64 = double(float(f30.f64 * f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// lis r11,2
	r11.s64 = 131072;
	// ori r10,r4,2868
	ctx.r10.u64 = ctx.r4.u64 | 2868;
	// ori r9,r11,2880
	ctx.r9.u64 = r11.u64 | 2880;
	// addi r8,r3,-1
	ctx.r8.s64 = ctx.r3.s64 + -1;
	// stwx r3,r30,r10
	PPC_STORE_U32(r30.u32 + ctx.r10.u32, ctx.r3.u32);
	// stwx r8,r30,r9
	PPC_STORE_U32(r30.u32 + ctx.r9.u32, ctx.r8.u32);
	// lwz r7,4(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// subfic r6,r7,0
	xer.ca = ctx.r7.u32 <= 0;
	ctx.r6.s64 = 0 - ctx.r7.s64;
	// subfe r5,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + xer.ca < xer.ca);
	ctx.r5.u64 = ~ctx.r6.u64 + ctx.r6.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r5,0,31,29
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// rlwinm r11,r11,0,28,21
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFC0F;
	// addi r11,r11,1020
	r11.s64 = r11.s64 + 1020;
	// clrldi r4,r11,32
	ctx.r4.u64 = r11.u64 & 0xFFFFFFFF;
	// std r4,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r4.u64);
	// lfd f10,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fmuls f7,f8,f30
	ctx.f7.f64 = double(float(ctx.f8.f64 * f30.f64));
	// fctidz f6,f7
	ctx.f6.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// stfd f6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f6.u64);
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r11,2
	r11.s64 = 131072;
	// ori r10,r11,14224
	ctx.r10.u64 = r11.u64 | 14224;
	// stwx r3,r30,r10
	PPC_STORE_U32(r30.u32 + ctx.r10.u32, ctx.r3.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r9,2
	ctx.r9.s64 = 131072;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// ori r7,r9,18360
	ctx.r7.u64 = ctx.r9.u64 | 18360;
	// ori r6,r8,18372
	ctx.r6.u64 = ctx.r8.u64 | 18372;
	// addi r5,r3,-1
	ctx.r5.s64 = ctx.r3.s64 + -1;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// stwx r3,r30,r7
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, ctx.r3.u32);
	// stwx r5,r30,r6
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, ctx.r5.u32);
	// lfs f0,19888(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 19888);
	f0.f64 = double(temp.f32);
	// fmuls f5,f30,f0
	ctx.f5.f64 = double(float(f30.f64 * f0.f64));
	// fctidz f4,f5
	ctx.f4.s64 = (ctx.f5.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f5.f64);
	// stfd f4,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f4.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// ori r8,r10,20988
	ctx.r8.u64 = ctx.r10.u64 | 20988;
	// ori r7,r9,21000
	ctx.r7.u64 = ctx.r9.u64 | 21000;
	// addi r6,r11,-1
	ctx.r6.s64 = r11.s64 + -1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stwx r11,r30,r8
	PPC_STORE_U32(r30.u32 + ctx.r8.u32, r11.u32);
	// stwx r6,r30,r7
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, ctx.r6.u32);
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// fmuls f26,f31,f30
	ctx.fpscr.disableFlushMode();
	f26.f64 = double(float(f31.f64 * f30.f64));
	// ori r11,r5,32344
	r11.u64 = ctx.r5.u64 | 32344;
	// lfs f0,19884(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 19884);
	f0.f64 = double(temp.f32);
	// stwx r3,r30,r11
	PPC_STORE_U32(r30.u32 + r11.u32, ctx.r3.u32);
	// fmuls f3,f26,f0
	ctx.f3.f64 = double(float(f26.f64 * f0.f64));
	// fctidz f2,f3
	ctx.f2.s64 = (ctx.f3.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f3.f64);
	// stfd f2,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f2.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cddef4
	if (!cr6.eq) goto loc_82CDDEF4;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDDEF4:
	// addis r16,r30,3
	r16.s64 = r30.s64 + 196608;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r16,r16,-29064
	r16.s64 = r16.s64 + -29064;
	// addis r15,r30,3
	r15.s64 = r30.s64 + 196608;
	// addi r15,r15,-26992
	r15.s64 = r15.s64 + -26992;
	// lfs f0,19880(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19880);
	f0.f64 = double(temp.f32);
	// stw r3,0(r16)
	PPC_STORE_U32(r16.u32 + 0, ctx.r3.u32);
	// fmuls f0,f30,f0
	f0.f64 = double(float(f30.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// fmuls f12,f31,f25
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(f31.f64 * f25.f64));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stw r3,0(r15)
	PPC_STORE_U32(r15.u32 + 0, ctx.r3.u32);
	// lfs f0,19876(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 19876);
	f0.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f10.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cddf54
	if (!cr6.eq) goto loc_82CDDF54;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDDF54:
	// addis r14,r30,3
	r14.s64 = r30.s64 + 196608;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r14,r14,-24836
	r14.s64 = r14.s64 + -24836;
	// lfs f0,19872(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19872);
	f0.f64 = double(temp.f32);
	// stw r3,0(r14)
	PPC_STORE_U32(r14.u32 + 0, ctx.r3.u32);
	// fmuls f0,f26,f0
	f0.f64 = double(float(f26.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cddf88
	if (!cr6.eq) goto loc_82CDDF88;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDDF88:
	// addis r18,r30,3
	r18.s64 = r30.s64 + 196608;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r18,r18,-8396
	r18.s64 = r18.s64 + -8396;
	// addis r17,r30,3
	r17.s64 = r30.s64 + 196608;
	// addi r17,r17,-6324
	r17.s64 = r17.s64 + -6324;
	// lfs f0,19868(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19868);
	f0.f64 = double(temp.f32);
	// stw r3,0(r18)
	PPC_STORE_U32(r18.u32 + 0, ctx.r3.u32);
	// fmuls f0,f30,f0
	f0.f64 = double(float(f30.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// fmuls f12,f29,f25
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(f29.f64 * f25.f64));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stw r3,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r3.u32);
	// lfs f0,19864(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 19864);
	f0.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f10.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cddfe8
	if (!cr6.eq) goto loc_82CDDFE8;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDDFE8:
	// addis r19,r30,3
	r19.s64 = r30.s64 + 196608;
	// fmuls f28,f28,f30
	ctx.fpscr.disableFlushMode();
	f28.f64 = double(float(f28.f64 * f30.f64));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r19,r19,-4168
	r19.s64 = r19.s64 + -4168;
	// lfs f0,19860(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19860);
	f0.f64 = double(temp.f32);
	// stw r3,0(r19)
	PPC_STORE_U32(r19.u32 + 0, ctx.r3.u32);
	// fmuls f0,f28,f0
	f0.f64 = double(float(f28.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde020
	if (!cr6.eq) goto loc_82CDE020;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE020:
	// fmuls f31,f29,f30
	ctx.fpscr.disableFlushMode();
	f31.f64 = double(float(f29.f64 * f30.f64));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addis r20,r30,3
	r20.s64 = r30.s64 + 196608;
	// addi r20,r20,12272
	r20.s64 = r20.s64 + 12272;
	// lfs f0,19856(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19856);
	f0.f64 = double(temp.f32);
	// stw r3,0(r20)
	PPC_STORE_U32(r20.u32 + 0, ctx.r3.u32);
	// fmuls f0,f31,f0
	f0.f64 = double(float(f31.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde058
	if (!cr6.eq) goto loc_82CDE058;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE058:
	// addis r22,r30,3
	r22.s64 = r30.s64 + 196608;
	// addi r10,r3,-1
	ctx.r10.s64 = ctx.r3.s64 + -1;
	// addi r22,r22,20488
	r22.s64 = r22.s64 + 20488;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addis r21,r30,3
	r21.s64 = r30.s64 + 196608;
	// addi r21,r21,28708
	r21.s64 = r21.s64 + 28708;
	// stw r10,0(r22)
	PPC_STORE_U32(r22.u32 + 0, ctx.r10.u32);
	// lfs f0,19852(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19852);
	f0.f64 = double(temp.f32);
	// fmuls f0,f30,f0
	f0.f64 = double(float(f30.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stw r3,0(r21)
	PPC_STORE_U32(r21.u32 + 0, ctx.r3.u32);
	// lfs f0,19848(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 19848);
	f0.f64 = double(temp.f32);
	// fmuls f12,f26,f0
	ctx.f12.f64 = double(float(f26.f64 * f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde0b8
	if (!cr6.eq) goto loc_82CDE0B8;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE0B8:
	// fmuls f13,f27,f25
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f27.f64 * f25.f64));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addis r25,r30,4
	r25.s64 = r30.s64 + 262144;
	// addi r25,r25,-28612
	r25.s64 = r25.s64 + -28612;
	// lfs f0,19844(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19844);
	f0.f64 = double(temp.f32);
	// stw r3,0(r25)
	PPC_STORE_U32(r25.u32 + 0, ctx.r3.u32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde0f0
	if (!cr6.eq) goto loc_82CDE0F0;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE0F0:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addis r23,r30,4
	r23.s64 = r30.s64 + 262144;
	// addi r23,r23,-24492
	r23.s64 = r23.s64 + -24492;
	// lfs f0,19840(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19840);
	f0.f64 = double(temp.f32);
	// fmuls f0,f28,f0
	f0.f64 = double(float(f28.f64 * f0.f64));
	// stw r3,0(r23)
	PPC_STORE_U32(r23.u32 + 0, ctx.r3.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde124
	if (!cr6.eq) goto loc_82CDE124;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE124:
	// addis r24,r30,4
	r24.s64 = r30.s64 + 262144;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r24,r24,-8048
	r24.s64 = r24.s64 + -8048;
	// lfs f0,19836(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19836);
	f0.f64 = double(temp.f32);
	// stw r3,0(r24)
	PPC_STORE_U32(r24.u32 + 0, ctx.r3.u32);
	// fmuls f0,f31,f0
	f0.f64 = double(float(f31.f64 * f0.f64));
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde158
	if (!cr6.eq) goto loc_82CDE158;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE158:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addis r26,r30,4
	r26.s64 = r30.s64 + 262144;
	// addi r26,r26,168
	r26.s64 = r26.s64 + 168;
	// lfs f0,19832(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19832);
	f0.f64 = double(temp.f32);
	// fmuls f0,f26,f0
	f0.f64 = double(float(f26.f64 * f0.f64));
	// stw r3,0(r26)
	PPC_STORE_U32(r26.u32 + 0, ctx.r3.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde18c
	if (!cr6.eq) goto loc_82CDE18C;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE18C:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addis r28,r30,4
	r28.s64 = r30.s64 + 262144;
	// addis r27,r30,4
	r27.s64 = r30.s64 + 262144;
	// addi r28,r28,8388
	r28.s64 = r28.s64 + 8388;
	// addi r27,r27,16604
	r27.s64 = r27.s64 + 16604;
	// lfs f0,19828(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19828);
	f0.f64 = double(temp.f32);
	// fmuls f0,f26,f0
	f0.f64 = double(float(f26.f64 * f0.f64));
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stw r3,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r3.u32);
	// lfs f0,19824(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 19824);
	f0.f64 = double(temp.f32);
	// fmuls f12,f25,f0
	ctx.f12.f64 = double(float(f25.f64 * f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f11.u64);
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// bl 0x82ce6820
	sub_82CE6820(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cde1e8
	if (!cr6.eq) goto loc_82CDE1E8;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CDE1E8:
	// lwz r10,0(r17)
	ctx.r10.u64 = PPC_LOAD_U32(r17.u32 + int32_t(0) );
	// lis r8,4
	ctx.r8.s64 = 262144;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(0) );
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// lwz r9,0(r18)
	ctx.r9.u64 = PPC_LOAD_U32(r18.u32 + int32_t(0) );
	// ori r24,r8,20724
	r24.u64 = ctx.r8.u64 | 20724;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// lwz r8,0(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lis r28,-32255
	r28.s64 = -2113863680;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,0(r23)
	ctx.r9.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// lwz r7,0(r14)
	ctx.r7.u64 = PPC_LOAD_U32(r14.u32 + int32_t(0) );
	// lfd f30,3384(r4)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3384);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// lwz r6,0(r21)
	ctx.r6.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// lfs f13,88(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lwz r8,0(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + int32_t(0) );
	// lwz r5,0(r15)
	ctx.r5.u64 = PPC_LOAD_U32(r15.u32 + int32_t(0) );
	// lfd f0,19816(r28)
	f0.u64 = PPC_LOAD_U64(r28.u32 + 19816);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,0(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// lwz r4,0(r16)
	ctx.r4.u64 = PPC_LOAD_U32(r16.u32 + int32_t(0) );
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// lwz r7,0(r20)
	ctx.r7.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// lwz r28,0(r19)
	r28.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwx r3,r30,r24
	PPC_STORE_U32(r30.u32 + r24.u32, ctx.r3.u32);
	// lfs f12,72(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 72);
	ctx.f12.f64 = double(temp.f32);
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// add r3,r11,r28
	ctx.r3.u64 = r11.u64 + r28.u64;
	// rlwinm r11,r3,31,1,31
	r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x7FFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 / ctx.f13.f64));
	// fdiv f7,f12,f8
	ctx.f7.f64 = ctx.f12.f64 / ctx.f8.f64;
	// fdiv f2,f0,f7
	ctx.f2.f64 = f0.f64 / ctx.f7.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r9,16(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lwz r8,8(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lwz r6,12(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + int32_t(12) );
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// rlwinm r3,r9,2,0,29
	ctx.r3.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,3368(r10)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3368);
	// rlwinm r28,r8,2,0,29
	r28.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// fmul f6,f1,f0
	ctx.f6.f64 = ctx.f1.f64 * f0.f64;
	// lfs f0,440(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 440);
	f0.f64 = double(temp.f32);
	// lfs f13,2920(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2920);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f11,3056(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 3056);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r31,888
	r11.s64 = r31.s64 + 888;
	// addi r10,r31,376
	ctx.r10.s64 = r31.s64 + 376;
	// addi r9,r31,192
	ctx.r9.s64 = r31.s64 + 192;
	// addi r8,r31,504
	ctx.r8.s64 = r31.s64 + 504;
	// addi r7,r31,632
	ctx.r7.s64 = r31.s64 + 632;
	// addi r5,r31,760
	ctx.r5.s64 = r31.s64 + 760;
	// lfsx f28,r3,r11
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + r11.u32);
	f28.f64 = double(temp.f32);
	// addi r4,r31,504
	ctx.r4.s64 = r31.s64 + 504;
	// lfsx f5,r3,r10
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	ctx.f5.f64 = double(temp.f32);
	// lfsx f4,r3,r9
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r9.u32);
	ctx.f4.f64 = double(temp.f32);
	// fsqrts f12,f6
	ctx.f12.f64 = double(simd::sqrt_f32(float(ctx.f6.f64)));
	// lfsx f1,r28,r8
	temp.u32 = PPC_LOAD_U32(r28.u32 + ctx.r8.u32);
	ctx.f1.f64 = double(temp.f32);
	// lfsx f25,r6,r7
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
	f25.f64 = double(temp.f32);
	// lfsx f24,r28,r5
	temp.u32 = PPC_LOAD_U32(r28.u32 + ctx.r5.u32);
	f24.f64 = double(temp.f32);
	// lfsx f23,r6,r4
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r4.u32);
	f23.f64 = double(temp.f32);
	// fnmsubs f13,f12,f0,f13
	ctx.f13.f64 = -double(std::fma(float(ctx.f12.f64), float(f0.f64), -float(ctx.f13.f64)));
	// fmuls f11,f12,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// addi r7,r31,376
	ctx.r7.s64 = r31.s64 + 376;
	// lwz r3,20(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(20) );
	// addi r24,r31,192
	r24.s64 = r31.s64 + 192;
	// lwz r4,40(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r5,r3,2,0,29
	ctx.r5.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,44(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(44) );
	// rlwinm r3,r4,4,0,27
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r8,36(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + int32_t(36) );
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// fmuls f8,f5,f13
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// subf r9,r4,r3
	ctx.r9.s64 = ctx.r3.s64 - ctx.r4.s64;
	// lwz r26,24(r29)
	r26.u64 = PPC_LOAD_U32(r29.u32 + int32_t(24) );
	// rlwinm r11,r11,1,0,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,28(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + int32_t(28) );
	// add r3,r9,r10
	ctx.r3.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lfsx f10,r5,r7
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r7.u32);
	ctx.f10.f64 = double(temp.f32);
	// add r9,r11,r8
	ctx.r9.u64 = r11.u64 + ctx.r8.u64;
	// fmuls f7,f10,f13
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lfsx f9,r5,r24
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + r24.u32);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r31,888
	ctx.r7.s64 = r31.s64 + 888;
	// fmuls f21,f4,f13
	f21.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// addi r23,r31,760
	r23.s64 = r31.s64 + 760;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// fneg f6,f11
	ctx.f6.u64 = ctx.f11.u64 ^ 0x8000000000000000;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f17,f9,f13
	f17.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f0,19808(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 19808);
	f0.f64 = double(temp.f32);
	// rlwinm r8,r3,2,0,29
	ctx.r8.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f27,r5,r7
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r7.u32);
	f27.f64 = double(temp.f32);
	// addi r7,r31,632
	ctx.r7.s64 = r31.s64 + 632;
	// lfsx f10,r6,r23
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + r23.u32);
	ctx.f10.f64 = double(temp.f32);
	// addi r5,r31,1016
	ctx.r5.s64 = r31.s64 + 1016;
	// addi r3,r31,316
	ctx.r3.s64 = r31.s64 + 316;
	// fmuls f22,f28,f0
	f22.f64 = double(float(f28.f64 * f0.f64));
	// addi r6,r31,1536
	ctx.r6.s64 = r31.s64 + 1536;
	// fmuls f20,f8,f0
	f20.f64 = double(float(ctx.f8.f64 * f0.f64));
	// addi r24,r31,2056
	r24.s64 = r31.s64 + 2056;
	// fmuls f19,f7,f0
	f19.f64 = double(float(ctx.f7.f64 * f0.f64));
	// lfsx f16,r28,r7
	temp.u32 = PPC_LOAD_U32(r28.u32 + ctx.r7.u32);
	f16.f64 = double(temp.f32);
	// addi r23,r31,2600
	r23.s64 = r31.s64 + 2600;
	// lfsx f5,r9,r5
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	ctx.f5.f64 = double(temp.f32);
	// addi r22,r31,132
	r22.s64 = r31.s64 + 132;
	// lfsx f4,r10,r3
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	ctx.f4.f64 = double(temp.f32);
	// addi r7,r31,3144
	ctx.r7.s64 = r31.s64 + 3144;
	// lfsx f3,r9,r6
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	ctx.f3.f64 = double(temp.f32);
	// addi r5,r31,3144
	ctx.r5.s64 = r31.s64 + 3144;
	// rlwinm r3,r26,2,0,29
	ctx.r3.u64 = rotl64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// fmuls f18,f27,f0
	f18.f64 = double(float(f27.f64 * f0.f64));
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f2,r8,r24
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r24.u32);
	ctx.f2.f64 = double(temp.f32);
	// lfsx f29,r8,r23
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r23.u32);
	f29.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lfsx f26,r10,r22
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + r22.u32);
	f26.f64 = double(temp.f32);
	// stfs f10,88(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lfsx f0,r3,r7
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r7.u32);
	f0.f64 = double(temp.f32);
	// lfsx f31,r9,r5
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r5.u32);
	f31.f64 = double(temp.f32);
	// bne cr6,0x82cde428
	if (!cr6.eq) goto loc_82CDE428;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x82cde454
	goto loc_82CDE454;
loc_82CDE428:
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lfs f13,88(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lfd f10,104(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f10,f9
	ctx.f10.f64 = double(float(ctx.f9.f64));
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f13,f9,f14
	ctx.f13.f64 = double(float(ctx.f9.f64 * f14.f64));
	// fctidz f10,f13
	ctx.f10.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f13.f64);
	// stfd f10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.f10.u64);
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(108) );
loc_82CDE454:
	// addi r11,r30,104
	r11.s64 = r30.s64 + 104;
	// lwz r9,116(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(116) );
	// lwz r8,112(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(112) );
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// bne cr6,0x82cde474
	if (!cr6.eq) goto loc_82CDE474;
	// stfs f15,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_82CDE474:
	// addis r11,r30,1
	r11.s64 = r30.s64 + 65536;
	// addi r11,r11,136
	r11.s64 = r11.s64 + 136;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x82cde498
	if (!cr6.eq) goto loc_82CDE498;
	// stfs f15,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
loc_82CDE498:
	// lis r11,2
	r11.s64 = 131072;
	// lfs f13,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lfs f10,4(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// ori r5,r11,244
	ctx.r5.u64 = r11.u64 | 244;
	// fneg f15,f0
	f15.u64 = f0.u64 ^ 0x8000000000000000;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// lfs f9,20(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 20);
	ctx.f9.f64 = double(temp.f32);
	// addis r11,r30,2
	r11.s64 = r30.s64 + 131072;
	// fneg f14,f31
	f14.u64 = f31.u64 ^ 0x8000000000000000;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// ori r3,r9,268
	ctx.r3.u64 = ctx.r9.u64 | 268;
	// ori r4,r10,256
	ctx.r4.u64 = ctx.r10.u64 | 256;
	// stfsx f13,r30,r5
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, temp.u32);
	// addi r11,r11,2332
	r11.s64 = r11.s64 + 2332;
	// ori r9,r7,2884
	ctx.r9.u64 = ctx.r7.u64 | 2884;
	// lis r8,2
	ctx.r8.s64 = 131072;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfsx f1,r30,r3
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r30.u32 + ctx.r3.u32, temp.u32);
	// ori r10,r8,2872
	ctx.r10.u64 = ctx.r8.u64 | 2872;
	// stfsx f10,r30,r4
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, temp.u32);
	// ori r8,r6,2896
	ctx.r8.u64 = ctx.r6.u64 | 2896;
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f15,4(r11)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// stfsx f9,r30,r9
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r30.u32 + ctx.r9.u32, temp.u32);
	// addis r9,r30,2
	ctx.r9.s64 = r30.s64 + 131072;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// addi r9,r9,11104
	ctx.r9.s64 = ctx.r9.s64 + 11104;
	// stfsx f13,r30,r10
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + ctx.r10.u32, temp.u32);
	// stfsx f22,r30,r8
	temp.f32 = float(f22.f64);
	PPC_STORE_U32(r30.u32 + ctx.r8.u32, temp.u32);
	// addis r8,r30,2
	ctx.r8.s64 = r30.s64 + 131072;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// addi r8,r8,12152
	ctx.r8.s64 = ctx.r8.s64 + 12152;
	// ori r5,r5,232
	ctx.r5.u64 = ctx.r5.u64 | 232;
	// stfs f0,8(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// stfs f15,4(r9)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// lis r3,2
	ctx.r3.s64 = 131072;
	// lis r11,2
	r11.s64 = 131072;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// stfs f0,8(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// ori r7,r7,14228
	ctx.r7.u64 = ctx.r7.u64 | 14228;
	// stfs f15,4(r8)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// ori r6,r6,14240
	ctx.r6.u64 = ctx.r6.u64 | 14240;
	// stfsx f25,r30,r5
	temp.f32 = float(f25.f64);
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, temp.u32);
	// addis r9,r30,2
	ctx.r9.s64 = r30.s64 + 131072;
	// lis r28,2
	r28.s64 = 131072;
	// ori r4,r4,236
	ctx.r4.u64 = ctx.r4.u64 | 236;
	// ori r3,r3,18364
	ctx.r3.u64 = ctx.r3.u64 | 18364;
	// stfsx f21,r30,r7
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, temp.u32);
	// ori r11,r11,18376
	r11.u64 = r11.u64 | 18376;
	// stfsx f8,r30,r6
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, temp.u32);
	// ori r10,r10,18388
	ctx.r10.u64 = ctx.r10.u64 | 18388;
	// addi r9,r9,20452
	ctx.r9.s64 = ctx.r9.s64 + 20452;
	// ori r8,r28,20992
	ctx.r8.u64 = r28.u64 | 20992;
	// stfsx f24,r30,r4
	temp.f32 = float(f24.f64);
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, temp.u32);
	// lis r26,2
	r26.s64 = 131072;
	// stfsx f13,r30,r3
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + ctx.r3.u32, temp.u32);
	// lis r24,2
	r24.s64 = 131072;
	// stfsx f10,r30,r11
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r30.u32 + r11.u32, temp.u32);
	// addis r5,r30,2
	ctx.r5.s64 = r30.s64 + 131072;
	// stfsx f23,r30,r10
	temp.f32 = float(f23.f64);
	PPC_STORE_U32(r30.u32 + ctx.r10.u32, temp.u32);
	// ori r7,r26,21004
	ctx.r7.u64 = r26.u64 | 21004;
	// stfs f0,8(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// ori r6,r24,21016
	ctx.r6.u64 = r24.u64 | 21016;
	// stfs f15,4(r9)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// addi r5,r5,29224
	ctx.r5.s64 = ctx.r5.s64 + 29224;
	// stfsx f13,r30,r8
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + ctx.r8.u32, temp.u32);
	// lis r4,2
	ctx.r4.s64 = 131072;
	// addis r8,r30,2
	ctx.r8.s64 = r30.s64 + 131072;
	// stfsx f9,r30,r7
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, temp.u32);
	// lis r3,2
	ctx.r3.s64 = 131072;
	// stfsx f18,r30,r6
	temp.f32 = float(f18.f64);
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, temp.u32);
	// lis r11,2
	r11.s64 = 131072;
	// stfs f0,8(r5)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// stfs f15,4(r5)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lis r9,2
	ctx.r9.s64 = 131072;
	// addi r8,r8,30272
	ctx.r8.s64 = ctx.r8.s64 + 30272;
	// ori r7,r4,32348
	ctx.r7.u64 = ctx.r4.u64 | 32348;
	// ori r5,r11,18352
	ctx.r5.u64 = r11.u64 | 18352;
	// stfs f0,8(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// ori r6,r3,32360
	ctx.r6.u64 = ctx.r3.u64 | 32360;
	// stfs f15,4(r8)
	temp.f32 = float(f15.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// ori r4,r10,18356
	ctx.r4.u64 = ctx.r10.u64 | 18356;
	// stfsx f17,r30,r7
	temp.f32 = float(f17.f64);
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, temp.u32);
	// lis r11,2
	r11.s64 = 131072;
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	f0.f64 = double(temp.f32);
	// ori r3,r9,40620
	ctx.r3.u64 = ctx.r9.u64 | 40620;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// stfsx f16,r30,r5
	temp.f32 = float(f16.f64);
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, temp.u32);
	// lis r9,2
	ctx.r9.s64 = 131072;
	// ori r5,r11,40644
	ctx.r5.u64 = r11.u64 | 40644;
	// stfsx f7,r30,r6
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, temp.u32);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// stfsx f0,r30,r4
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, temp.u32);
	// lis r8,2
	ctx.r8.s64 = 131072;
	// stfs f31,8(r16)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r16.u32 + 8, temp.u32);
	// ori r11,r9,40652
	r11.u64 = ctx.r9.u64 | 40652;
	// stfs f14,4(r16)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(r16.u32 + 4, temp.u32);
	// lis r7,2
	ctx.r7.s64 = 131072;
	// stfs f31,8(r15)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r15.u32 + 8, temp.u32);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfs f14,4(r15)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(r15.u32 + 4, temp.u32);
	// ori r4,r10,40648
	ctx.r4.u64 = ctx.r10.u64 | 40648;
	// stfsx f4,r30,r5
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, temp.u32);
	// ori r10,r8,40680
	ctx.r10.u64 = ctx.r8.u64 | 40680;
	// stfsx f5,r30,r3
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r30.u32 + ctx.r3.u32, temp.u32);
	// ori r9,r7,40684
	ctx.r9.u64 = ctx.r7.u64 | 40684;
	// stfsx f2,r30,r11
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(r30.u32 + r11.u32, temp.u32);
	// ori r8,r6,57120
	ctx.r8.u64 = ctx.r6.u64 | 57120;
	// lis r7,2
	ctx.r7.s64 = 131072;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stfsx f3,r30,r4
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, temp.u32);
	// lis r5,2
	ctx.r5.s64 = 131072;
	// stfsx f29,r30,r10
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r30.u32 + ctx.r10.u32, temp.u32);
	// ori r7,r7,61288
	ctx.r7.u64 = ctx.r7.u64 | 61288;
	// stfsx f26,r30,r9
	temp.f32 = float(f26.f64);
	PPC_STORE_U32(r30.u32 + ctx.r9.u32, temp.u32);
	// ori r6,r6,61312
	ctx.r6.u64 = ctx.r6.u64 | 61312;
	// stfsx f12,r30,r8
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r30.u32 + ctx.r8.u32, temp.u32);
	// lis r3,2
	ctx.r3.s64 = 131072;
	// stfs f31,8(r18)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r18.u32 + 8, temp.u32);
	// lis r11,2
	r11.s64 = 131072;
	// stfs f14,4(r18)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(r18.u32 + 4, temp.u32);
	// ori r5,r5,61316
	ctx.r5.u64 = ctx.r5.u64 | 61316;
	// stfs f31,8(r17)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r17.u32 + 8, temp.u32);
	// ori r3,r3,61320
	ctx.r3.u64 = ctx.r3.u64 | 61320;
	// stfs f14,4(r17)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(r17.u32 + 4, temp.u32);
	// ori r11,r11,61348
	r11.u64 = r11.u64 | 61348;
	// stfsx f5,r30,r7
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r30.u32 + ctx.r7.u32, temp.u32);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// stfsx f4,r30,r6
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, temp.u32);
	// lis r9,3
	ctx.r9.s64 = 196608;
	// lis r8,3
	ctx.r8.s64 = 196608;
	// stfsx f3,r30,r5
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, temp.u32);
	// lis r4,3
	ctx.r4.s64 = 196608;
	// stfsx f2,r30,r3
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(r30.u32 + ctx.r3.u32, temp.u32);
	// ori r10,r10,61352
	ctx.r10.u64 = ctx.r10.u64 | 61352;
	// stfsx f29,r30,r11
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r30.u32 + r11.u32, temp.u32);
	// ori r9,r9,12252
	ctx.r9.u64 = ctx.r9.u64 | 12252;
	// lis r7,4
	ctx.r7.s64 = 262144;
	// lis r6,4
	ctx.r6.s64 = 262144;
	// ori r5,r8,57468
	ctx.r5.u64 = ctx.r8.u64 | 57468;
	// ori r4,r4,57472
	ctx.r4.u64 = ctx.r4.u64 | 57472;
	// stfsx f26,r30,r10
	temp.f32 = float(f26.f64);
	PPC_STORE_U32(r30.u32 + ctx.r10.u32, temp.u32);
	// ori r3,r7,37148
	ctx.r3.u64 = ctx.r7.u64 | 37148;
	// stfsx f12,r30,r9
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r30.u32 + ctx.r9.u32, temp.u32);
	// ori r11,r6,37152
	r11.u64 = ctx.r6.u64 | 37152;
	// stfs f31,8(r25)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r25.u32 + 8, temp.u32);
	// stfs f14,4(r25)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(r25.u32 + 4, temp.u32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stfsx f11,r30,r5
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, temp.u32);
	// stfsx f11,r30,r4
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, temp.u32);
	// stfs f31,8(r27)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r27.u32 + 8, temp.u32);
	// stfs f14,4(r27)
	temp.f32 = float(f14.f64);
	PPC_STORE_U32(r27.u32 + 4, temp.u32);
	// stfsx f6,r30,r3
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r30.u32 + ctx.r3.u32, temp.u32);
	// stfsx f6,r30,r11
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r30.u32 + r11.u32, temp.u32);
	// lfd f29,-8776(r10)
	f29.u64 = PPC_LOAD_U64(ctx.r10.u32 + -8776);
	// lfs f13,68(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 68);
	ctx.f13.f64 = double(temp.f32);
	// fmul f2,f13,f29
	ctx.f2.f64 = ctx.f13.f64 * f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lis r9,2
	ctx.r9.s64 = 131072;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// ori r8,r9,40716
	ctx.r8.u64 = ctx.r9.u64 | 40716;
	// fmuls f11,f12,f28
	ctx.f11.f64 = double(float(ctx.f12.f64 * f28.f64));
	// stfsx f11,r30,r8
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r30.u32 + ctx.r8.u32, temp.u32);
	// lfs f10,68(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 68);
	ctx.f10.f64 = double(temp.f32);
	// fmul f2,f10,f29
	ctx.f2.f64 = ctx.f10.f64 * f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f9,f1
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f1.f64));
	// lis r7,2
	ctx.r7.s64 = 131072;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// ori r6,r7,61384
	ctx.r6.u64 = ctx.r7.u64 | 61384;
	// fmuls f8,f9,f27
	ctx.f8.f64 = double(float(ctx.f9.f64 * f27.f64));
	// stfsx f8,r30,r6
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r30.u32 + ctx.r6.u32, temp.u32);
	// lfs f7,68(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 68);
	ctx.f7.f64 = double(temp.f32);
	// fmul f2,f7,f29
	ctx.f2.f64 = ctx.f7.f64 * f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// lis r5,3
	ctx.r5.s64 = 196608;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// ori r4,r5,41060
	ctx.r4.u64 = ctx.r5.u64 | 41060;
	// fmuls f5,f6,f20
	ctx.f5.f64 = double(float(ctx.f6.f64 * f20.f64));
	// stfsx f5,r30,r4
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r30.u32 + ctx.r4.u32, temp.u32);
	// lfs f4,68(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 68);
	ctx.f4.f64 = double(temp.f32);
	// fmul f2,f4,f29
	ctx.f2.f64 = ctx.f4.f64 * f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f3,f1
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = double(float(ctx.f1.f64));
	// lis r3,4
	ctx.r3.s64 = 262144;
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// ori r11,r3,20740
	r11.u64 = ctx.r3.u64 | 20740;
	// fmuls f2,f3,f19
	ctx.f2.f64 = double(float(ctx.f3.f64 * f19.f64));
	// stfsx f2,r30,r11
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(r30.u32 + r11.u32, temp.u32);
	// lfs f0,64(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 64);
	f0.f64 = double(temp.f32);
	// fmul f2,f0,f29
	ctx.f2.f64 = f0.f64 * f29.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// lfs f0,20(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 20);
	f0.f64 = double(temp.f32);
	// fmuls f12,f31,f0
	ctx.f12.f64 = double(float(f31.f64 * f0.f64));
	// addis r10,r30,5
	ctx.r10.s64 = r30.s64 + 327680;
	// frsp f13,f1
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// stfs f13,96(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 96, temp.u32);
	// addi r10,r10,-28368
	ctx.r10.s64 = ctx.r10.s64 + -28368;
	// addis r9,r30,5
	ctx.r9.s64 = r30.s64 + 327680;
	// addis r8,r30,5
	ctx.r8.s64 = r30.s64 + 327680;
	// addis r7,r30,5
	ctx.r7.s64 = r30.s64 + 327680;
	// addi r9,r9,-26296
	ctx.r9.s64 = ctx.r9.s64 + -26296;
	// addi r8,r8,-24224
	ctx.r8.s64 = ctx.r8.s64 + -24224;
	// stfs f12,8(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 8, temp.u32);
	// addis r31,r30,2
	r31.s64 = r30.s64 + 131072;
	// addi r7,r7,-22152
	ctx.r7.s64 = ctx.r7.s64 + -22152;
	// fneg f11,f12
	ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f11,4(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r31,r31,168
	r31.s64 = r31.s64 + 168;
	// stfs f12,8(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// stfs f11,4(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f12,8(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stfs f11,4(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// stfs f11,4(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f12,8(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// lfs f10,52(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,4(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// lfs f9,56(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 56);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,8(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 8, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// lfs f8,60(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 60);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,12(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// addis r31,r30,2
	r31.s64 = r30.s64 + 131072;
	// lfs f7,52(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 52);
	ctx.f7.f64 = double(temp.f32);
	// addi r31,r31,200
	r31.s64 = r31.s64 + 200;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stfs f7,4(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// lfs f6,56(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 56);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,8(r31)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r31.u32 + 8, temp.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// lfs f5,60(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 60);
	ctx.f5.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stfs f5,12(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// lwz r11,48(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(48) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82cde888
	if (!cr6.eq) goto loc_82CDE888;
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x82cde8b8
	goto loc_82CDE8B8;
loc_82CDE888:
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lfs f0,88(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 88);
	f0.f64 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lfd f12,104(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.f7.u64);
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(108) );
loc_82CDE8B8:
	// addis r11,r30,5
	r11.s64 = r30.s64 + 327680;
	// addi r11,r11,-20080
	r11.s64 = r11.s64 + -20080;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x82cde8e0
	if (!cr6.eq) goto loc_82CDE8E0;
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
loc_82CDE8E0:
	// addis r11,r30,5
	r11.s64 = r30.s64 + 327680;
	// addi r11,r11,-19024
	r11.s64 = r11.s64 + -19024;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x82cde908
	if (!cr6.eq) goto loc_82CDE908;
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
loc_82CDE908:
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x82ca751c
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82CDDB80) {
	__imp__sub_82CDDB80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDE918) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc8
	// addi r12,r1,-104
	r12.s64 = ctx.r1.s64 + -104;
	// bl 0x82ca74dc
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lis r24,-32255
	r24.s64 = -2113863680;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r23,-32255
	r23.s64 = -2113863680;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addi r6,r11,15788
	ctx.r6.s64 = r11.s64 + 15788;
	// lfs f13,16572(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 16572);
	ctx.f13.f64 = double(temp.f32);
	// lis r22,-32255
	r22.s64 = -2113863680;
	// lfs f0,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// stw r6,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r6.u32);
	// li r11,8
	r11.s64 = 8;
	// li r10,5
	ctx.r10.s64 = 5;
	// lfs f12,16568(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 16568);
	ctx.f12.f64 = double(temp.f32);
	// li r8,6
	ctx.r8.s64 = 6;
	// stfs f0,60(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 60, temp.u32);
	// li r9,4
	ctx.r9.s64 = 4;
	// stfs f0,64(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 64, temp.u32);
	// stfs f0,68(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 68, temp.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stfs f13,80(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 80, temp.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// lfs f0,16560(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16560);
	f0.f64 = double(temp.f32);
	// stw r8,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r8.u32);
	// lfs f13,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// stw r8,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r8.u32);
	// lfs f11,16572(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 16572);
	ctx.f11.f64 = double(temp.f32);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stfs f12,76(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 76, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lfs f12,16576(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 16576);
	ctx.f12.f64 = double(temp.f32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// lfs f10,16568(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 16568);
	ctx.f10.f64 = double(temp.f32);
	// stw r9,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r9.u32);
	// stfs f0,56(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 56, temp.u32);
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// stfs f13,72(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 72, temp.u32);
	// stw r9,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r9.u32);
	// stfs f11,84(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 84, temp.u32);
	// stw r10,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r10.u32);
	// stfs f12,88(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 88, temp.u32);
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// stfs f10,92(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 92, temp.u32);
	// lis r3,-32256
	ctx.r3.s64 = -2113929216;
	// lfd f29,3376(r4)
	f29.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3376);
	// lfd f28,3384(r3)
	f28.u64 = PPC_LOAD_U64(ctx.r3.u32 + 3384);
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// li r30,0
	r30.s64 = 0;
	// stfs f0,96(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 96, temp.u32);
	// lfs f0,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// li r29,40
	r29.s64 = 40;
	// stw r30,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r30.u32);
	// stfs f0,108(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 108, temp.u32);
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// lfs f0,16576(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 16576);
	f0.f64 = double(temp.f32);
	// stfs f13,124(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 124, temp.u32);
	// stw r29,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r29.u32);
	// stfs f0,104(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 104, temp.u32);
	// stw r29,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r29.u32);
	// stw r29,120(r31)
	PPC_STORE_U32(r31.u32 + 120, r29.u32);
	// stfs f13,132(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 132, temp.u32);
	// stw r30,128(r31)
	PPC_STORE_U32(r31.u32 + 128, r30.u32);
	// addi r11,r31,104
	r11.s64 = r31.s64 + 104;
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// bl 0x82cdd8c0
	sub_82CDD8C0(ctx, base);
	// addis r11,r31,1
	r11.s64 = r31.s64 + 65536;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,136
	r11.s64 = r11.s64 + 136;
	// lfs f0,16576(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 16576);
	f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r29,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r29.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r29,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r29.u32);
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stw r29,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r29.u32);
	// stfs f13,28(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// bl 0x82cdd8c0
	sub_82CDD8C0(ctx, base);
	// lis r21,-32256
	r21.s64 = -2113929216;
	// addis r3,r31,2
	ctx.r3.s64 = r31.s64 + 131072;
	// lfs f12,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r3,r3,168
	ctx.r3.s64 = ctx.r3.s64 + 168;
	// lfs f0,3056(r21)
	temp.u32 = PPC_LOAD_U32(r21.u32 + 3056);
	f0.f64 = double(temp.f32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,12(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f0,24(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f13,8(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f12,0(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f11,4(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// stfs f0,28(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// addis r3,r31,2
	ctx.r3.s64 = r31.s64 + 131072;
	// lfs f10,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// addi r3,r3,200
	ctx.r3.s64 = ctx.r3.s64 + 200;
	// lfs f0,3056(r21)
	temp.u32 = PPC_LOAD_U32(r21.u32 + 3056);
	f0.f64 = double(temp.f32);
	// lfs f13,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,12(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 12, temp.u32);
	// stfs f0,24(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// stfs f13,8(r3)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// stfs f10,0(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// stfs f9,4(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// stfs f0,28(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 28, temp.u32);
	// bl 0x82cdd758
	sub_82CDD758(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r20,r11,16584
	r20.s64 = r11.s64 + 16584;
	// lis r9,2
	ctx.r9.s64 = 131072;
	// ori r8,r10,232
	ctx.r8.u64 = ctx.r10.u64 | 232;
	// lfs f30,16584(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16584);
	f30.f64 = double(temp.f32);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// ori r7,r9,236
	ctx.r7.u64 = ctx.r9.u64 | 236;
	// addi r11,r11,240
	r11.s64 = r11.s64 + 240;
	// lfs f26,44(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 44);
	f26.f64 = double(temp.f32);
	// li r25,509
	r25.s64 = 509;
	// lfs f25,48(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 48);
	f25.f64 = double(temp.f32);
	// li r26,508
	r26.s64 = 508;
	// stfsx f26,r31,r8
	temp.f32 = float(f26.f64);
	PPC_STORE_U32(r31.u32 + ctx.r8.u32, temp.u32);
	// li r29,1
	r29.s64 = 1;
	// lfs f24,4(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 4);
	f24.f64 = double(temp.f32);
	// stfsx f25,r31,r7
	temp.f32 = float(f25.f64);
	PPC_STORE_U32(r31.u32 + ctx.r7.u32, temp.u32);
	// addi r3,r11,36
	ctx.r3.s64 = r11.s64 + 36;
	// lfs f23,8(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 8);
	f23.f64 = double(temp.f32);
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r26,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r26.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r29,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r29.u32);
	// stfs f0,32(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 32, temp.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f24,16(r11)
	temp.f32 = float(f24.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stfs f23,28(r11)
	temp.f32 = float(f23.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r30.u32);
	// stfs f0,40(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 40, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,2332
	r11.s64 = r11.s64 + 2332;
	// lfs f21,12(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 12);
	f21.f64 = double(temp.f32);
	// li r5,83
	ctx.r5.s64 = 83;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// lfs f22,19924(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 19924);
	f22.f64 = double(temp.f32);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// stfs f22,4(r11)
	temp.f32 = float(f22.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f21,8(r11)
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd980
	sub_82CDD980(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// addi r11,r11,2868
	r11.s64 = r11.s64 + 2868;
	// li r3,2039
	ctx.r3.s64 = 2039;
	// lfs f19,2736(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2736);
	f19.f64 = double(temp.f32);
	// li r10,2038
	ctx.r10.s64 = 2038;
	// lfs f20,20(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 20);
	f20.f64 = double(temp.f32);
	// li r27,600
	r27.s64 = 600;
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r27,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r27.u32);
	// stfs f0,32(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 32, temp.u32);
	// addi r3,r11,36
	ctx.r3.s64 = r11.s64 + 36;
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f20,16(r11)
	temp.f32 = float(f20.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stfs f19,28(r11)
	temp.f32 = float(f19.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r30.u32);
	// stfs f0,40(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 40, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,11104
	r11.s64 = r11.s64 + 11104;
	// lfs f27,3056(r21)
	temp.u32 = PPC_LOAD_U32(r21.u32 + 3056);
	f27.f64 = double(temp.f32);
	// li r8,211
	ctx.r8.s64 = 211;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// lfs f31,3204(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3204);
	f31.f64 = double(temp.f32);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdda40
	sub_82CDDA40(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// li r7,311
	ctx.r7.s64 = 311;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,12152
	r11.s64 = r11.s64 + 12152;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// li r28,1020
	r28.s64 = 1020;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,14224
	r11.s64 = r11.s64 + 14224;
	// lfs f18,36(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 36);
	f18.f64 = double(temp.f32);
	// lfs f17,40(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 40);
	f17.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r29,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r29.u32);
	// stfs f18,4(r11)
	temp.f32 = float(f18.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f17,16(r11)
	temp.f32 = float(f17.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stfs f0,28(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// bl 0x82cddaa0
	sub_82CDDAA0(ctx, base);
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lis r5,2
	ctx.r5.s64 = 131072;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// ori r4,r6,18352
	ctx.r4.u64 = ctx.r6.u64 | 18352;
	// ori r10,r5,18356
	ctx.r10.u64 = ctx.r5.u64 | 18356;
	// addi r11,r11,18360
	r11.s64 = r11.s64 + 18360;
	// addi r3,r11,36
	ctx.r3.s64 = r11.s64 + 36;
	// stfsx f26,r31,r4
	temp.f32 = float(f26.f64);
	PPC_STORE_U32(r31.u32 + ctx.r4.u32, temp.u32);
	// stfsx f25,r31,r10
	temp.f32 = float(f25.f64);
	PPC_STORE_U32(r31.u32 + ctx.r10.u32, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r26,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r26.u32);
	// stfs f0,32(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 32, temp.u32);
	// stw r29,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r29.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f24,16(r11)
	temp.f32 = float(f24.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stfs f23,28(r11)
	temp.f32 = float(f23.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r30.u32);
	// stfs f0,40(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 40, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// li r9,97
	ctx.r9.s64 = 97;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,20452
	r11.s64 = r11.s64 + 20452;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stfs f22,4(r11)
	temp.f32 = float(f22.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f21,8(r11)
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd980
	sub_82CDD980(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// li r8,1297
	ctx.r8.s64 = 1297;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,20988
	r11.s64 = r11.s64 + 20988;
	// li r7,1296
	ctx.r7.s64 = 1296;
	// addi r3,r11,36
	ctx.r3.s64 = r11.s64 + 36;
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r7,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r7.u32);
	// stfs f0,32(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 32, temp.u32);
	// stw r27,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r27.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f20,16(r11)
	temp.f32 = float(f20.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stfs f19,28(r11)
	temp.f32 = float(f19.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,36(r11)
	PPC_STORE_U32(r11.u32 + 36, r30.u32);
	// stfs f0,40(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 40, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// li r6,223
	ctx.r6.s64 = 223;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,29224
	r11.s64 = r11.s64 + 29224;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdda40
	sub_82CDDA40(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// li r5,293
	ctx.r5.s64 = 293;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,30272
	r11.s64 = r11.s64 + 30272;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,2
	r11.s64 = r31.s64 + 131072;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,32344
	r11.s64 = r11.s64 + 32344;
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r28,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r28.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r29,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r29.u32);
	// stfs f18,4(r11)
	temp.f32 = float(f18.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f17,16(r11)
	temp.f32 = float(f17.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stfs f0,28(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// bl 0x82cddaa0
	sub_82CDDAA0(ctx, base);
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-29064
	r11.s64 = r11.s64 + -29064;
	// lfs f0,76(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 76);
	f0.f64 = double(temp.f32);
	// li r10,409
	ctx.r10.s64 = 409;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// lfs f30,19920(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 19920);
	f30.f64 = double(temp.f32);
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// li r9,257
	ctx.r9.s64 = 257;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,-26992
	r11.s64 = r11.s64 + -26992;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r8,r31,3
	ctx.r8.s64 = r31.s64 + 196608;
	// addis r7,r31,3
	ctx.r7.s64 = r31.s64 + 196608;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r8,-24920
	ctx.r8.s64 = ctx.r8.s64 + -24920;
	// lfs f0,52(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 52);
	f0.f64 = double(temp.f32);
	// addis r6,r31,3
	ctx.r6.s64 = r31.s64 + 196608;
	// lfs f12,56(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 56);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r7,-24896
	ctx.r7.s64 = ctx.r7.s64 + -24896;
	// lfs f11,60(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 60);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r6,-24864
	ctx.r6.s64 = ctx.r6.s64 + -24864;
	// lfs f10,64(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 64);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,68(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 68);
	ctx.f9.f64 = double(temp.f32);
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// stfs f0,4(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r29.u32);
	// stfs f13,8(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,12(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// stfs f0,16(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 16, temp.u32);
	// stfs f0,20(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 20, temp.u32);
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stfs f0,16(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 16, temp.u32);
	// stfs f12,4(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// stfs f11,8(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// stfs f10,12(r7)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + 12, temp.u32);
	// stfs f0,20(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 20, temp.u32);
	// stfs f0,24(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 24, temp.u32);
	// stfs f0,28(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 28, temp.u32);
	// stw r29,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r29.u32);
	// stfs f0,4(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// lfs f0,72(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 72);
	f0.f64 = double(temp.f32);
	// stfs f0,12(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 12, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f9,8(r6)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r6.u32 + 8, temp.u32);
	// stfs f0,20(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 20, temp.u32);
	// stfs f13,16(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// stfs f0,24(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 24, temp.u32);
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f8,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f1.f64));
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// lfs f12,84(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// li r10,3432
	ctx.r10.s64 = 3432;
	// lfs f13,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-24836
	r11.s64 = r11.s64 + -24836;
	// lfs f0,16568(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 16568);
	f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// fmuls f7,f8,f12
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f7,16(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stfs f13,28(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// bl 0x82cddb00
	sub_82CDDB00(ctx, base);
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// addis r5,r31,3
	ctx.r5.s64 = r31.s64 + 196608;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-8396
	r11.s64 = r11.s64 + -8396;
	// lfs f12,76(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 76);
	ctx.f12.f64 = double(temp.f32);
	// addi r5,r5,-8420
	ctx.r5.s64 = ctx.r5.s64 + -8420;
	// fmr f0,f20
	f0.f64 = f20.f64;
	// li r4,383
	ctx.r4.s64 = 383;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,4(r5)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// stw r29,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r29.u32);
	// stfs f13,8(r5)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,12(r5)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// stfs f0,16(r5)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// stfs f0,20(r5)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f12,8(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// li r10,233
	ctx.r10.s64 = 233;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-6324
	r11.s64 = r11.s64 + -6324;
	// fmr f0,f27
	f0.f64 = f27.f64;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r9,r31,3
	ctx.r9.s64 = r31.s64 + 196608;
	// addis r8,r31,3
	ctx.r8.s64 = r31.s64 + 196608;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,-4252
	ctx.r9.s64 = ctx.r9.s64 + -4252;
	// lfs f0,52(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 52);
	f0.f64 = double(temp.f32);
	// addi r8,r8,-4228
	ctx.r8.s64 = ctx.r8.s64 + -4228;
	// lfs f12,64(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 64);
	ctx.f12.f64 = double(temp.f32);
	// addis r7,r31,3
	ctx.r7.s64 = r31.s64 + 196608;
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// addi r7,r7,-4196
	ctx.r7.s64 = ctx.r7.s64 + -4196;
	// stfs f0,4(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r29.u32);
	// stfs f13,8(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,12(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// stfs f0,16(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// stfs f0,20(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r29.u32);
	// stfs f0,16(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 16, temp.u32);
	// lfs f0,56(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 56);
	f0.f64 = double(temp.f32);
	// stfs f0,4(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// lfs f13,60(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 60);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,8(r8)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r8.u32 + 8, temp.u32);
	// stfs f12,12(r8)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r8.u32 + 12, temp.u32);
	// lfs f0,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// stfs f0,20(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 20, temp.u32);
	// stfs f0,24(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 24, temp.u32);
	// stfs f0,28(r8)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r8.u32 + 28, temp.u32);
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stfs f0,4(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// lfs f0,68(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 68);
	f0.f64 = double(temp.f32);
	// stfs f0,8(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// lfs f0,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// lfs f13,72(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 72);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// stfs f13,12(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 12, temp.u32);
	// stfs f0,20(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 20, temp.u32);
	// stfs f12,16(r7)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r7.u32 + 16, temp.u32);
	// stfs f0,24(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 24, temp.u32);
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f6,f1
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f1.f64));
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// lfs f12,84(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// li r10,3820
	ctx.r10.s64 = 3820;
	// lfs f13,16564(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-4168
	r11.s64 = r11.s64 + -4168;
	// lfs f0,16568(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 16568);
	f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// fmuls f5,f6,f12
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f5,16(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stfs f13,28(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// bl 0x82cddb00
	sub_82CDDB00(ctx, base);
	// addis r6,r31,3
	ctx.r6.s64 = r31.s64 + 196608;
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r6,12248
	ctx.r6.s64 = ctx.r6.s64 + 12248;
	// fmr f0,f20
	f0.f64 = f20.f64;
	// addi r11,r11,12272
	r11.s64 = r11.s64 + 12272;
	// lfs f27,76(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 76);
	f27.f64 = double(temp.f32);
	// li r5,1511
	ctx.r5.s64 = 1511;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,4(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// stw r29,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, r29.u32);
	// stfs f13,8(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 8, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,16(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// stfs f0,20(r6)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r6.u32 + 20, temp.u32);
	// stfs f13,12(r6)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r6.u32 + 12, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// li r4,1061
	ctx.r4.s64 = 1061;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,20488
	r11.s64 = r11.s64 + 20488;
	// addi r3,r11,20
	ctx.r3.s64 = r11.s64 + 20;
	// stfs f0,16(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stfs f27,4(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f30,12(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r30,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r30.u32);
	// stfs f0,24(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,3
	r11.s64 = r31.s64 + 196608;
	// li r10,853
	ctx.r10.s64 = 853;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,28708
	r11.s64 = r11.s64 + 28708;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// li r9,541
	ctx.r9.s64 = 541;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-28612
	r11.s64 = r11.s64 + -28612;
	// lfs f0,3056(r21)
	temp.u32 = PPC_LOAD_U32(r21.u32 + 3056);
	f0.f64 = double(temp.f32);
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cddaa0
	sub_82CDDAA0(ctx, base);
	// fmr f2,f29
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f29.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// li r10,1510
	ctx.r10.s64 = 1510;
	// lfs f0,16568(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 16568);
	f0.f64 = double(temp.f32);
	// addi r11,r11,-24492
	r11.s64 = r11.s64 + -24492;
	// lfs f12,84(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// frsp f4,f1
	ctx.f4.f64 = double(float(ctx.f1.f64));
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// fmuls f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stfs f3,16(r11)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f0,28(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// bl 0x82cddb00
	sub_82CDDB00(ctx, base);
	// addis r7,r31,4
	ctx.r7.s64 = r31.s64 + 262144;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r7,r7,-8076
	ctx.r7.s64 = ctx.r7.s64 + -8076;
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// li r6,1657
	ctx.r6.s64 = 1657;
	// addi r11,r11,-8048
	r11.s64 = r11.s64 + -8048;
	// lfs f0,2848(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2848);
	f0.f64 = double(temp.f32);
	// stfs f0,8(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 8, temp.u32);
	// stw r29,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r29.u32);
	// stfs f0,12(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 12, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f13,4(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 4, temp.u32);
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,16(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 16, temp.u32);
	// stfs f0,20(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 20, temp.u32);
	// stfs f0,24(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 24, temp.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// li r5,1103
	ctx.r5.s64 = 1103;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,168
	r11.s64 = r11.s64 + 168;
	// addi r3,r11,20
	ctx.r3.s64 = r11.s64 + 20;
	// stfs f0,16(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// stfs f27,4(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f30,12(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r30,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r30.u32);
	// stfs f0,24(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// li r4,887
	ctx.r4.s64 = 887;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,8388
	r11.s64 = r11.s64 + 8388;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stfs f30,4(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f27,8(r11)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd9e0
	sub_82CDD9E0(ctx, base);
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// li r10,491
	ctx.r10.s64 = 491;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,16604
	r11.s64 = r11.s64 + 16604;
	// lfs f0,3056(r21)
	temp.u32 = PPC_LOAD_U32(r21.u32 + 3056);
	f0.f64 = double(temp.f32);
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,8(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f13,12(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cddaa0
	sub_82CDDAA0(ctx, base);
	// fmr f2,f29
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f29.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// addis r11,r31,4
	r11.s64 = r31.s64 + 262144;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,84(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// frsp f2,f1
	ctx.f2.f64 = double(float(ctx.f1.f64));
	// addi r11,r11,20724
	r11.s64 = r11.s64 + 20724;
	// lfs f0,16568(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 16568);
	f0.f64 = double(temp.f32);
	// li r10,1438
	ctx.r10.s64 = 1438;
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// fmuls f1,f2,f12
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f13,8(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stfs f0,4(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// stfs f1,16(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r11.u32 + 16, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stfs f13,28(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// bl 0x82cddb00
	sub_82CDDB00(ctx, base);
	// addis r9,r31,5
	ctx.r9.s64 = r31.s64 + 327680;
	// addis r11,r31,5
	r11.s64 = r31.s64 + 327680;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r9,-28396
	ctx.r9.s64 = ctx.r9.s64 + -28396;
	// lfs f0,120(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 120);
	f0.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lfs f30,124(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 124);
	f30.f64 = double(temp.f32);
	// addi r11,r11,-28368
	r11.s64 = r11.s64 + -28368;
	// li r7,131
	ctx.r7.s64 = 131;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,8(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r29.u32);
	// stfs f0,12(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
	// stfs f13,4(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// stfs f0,16(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 16, temp.u32);
	// stfs f0,20(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 20, temp.u32);
	// stfs f0,24(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 24, temp.u32);
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// lfs f31,19916(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 19916);
	f31.f64 = double(temp.f32);
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f30,8(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,5
	r11.s64 = r31.s64 + 327680;
	// li r6,113
	ctx.r6.s64 = 113;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,-26296
	r11.s64 = r11.s64 + -26296;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f30,8(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,5
	r11.s64 = r31.s64 + 327680;
	// li r5,107
	ctx.r5.s64 = 107;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,-24224
	r11.s64 = r11.s64 + -24224;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f30,8(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,5
	r11.s64 = r31.s64 + 327680;
	// li r4,127
	ctx.r4.s64 = 127;
	// lfs f0,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	f0.f64 = double(temp.f32);
	// addi r11,r11,-22152
	r11.s64 = r11.s64 + -22152;
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// stfs f0,12(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stfs f31,4(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stfs f30,8(r11)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 8, temp.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// bl 0x82cdd920
	sub_82CDD920(ctx, base);
	// addis r11,r31,5
	r11.s64 = r31.s64 + 327680;
	// li r29,20
	r29.s64 = 20;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-20080
	r11.s64 = r11.s64 + -20080;
	// lfs f0,16576(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 16576);
	f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r29,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r29.u32);
	// stw r29,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r29.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r29,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r29.u32);
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// stfs f13,28(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// bl 0x82cdda40
	sub_82CDDA40(ctx, base);
	// addis r11,r31,5
	r11.s64 = r31.s64 + 327680;
	// lfs f13,16564(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 16564);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-19024
	r11.s64 = r11.s64 + -19024;
	// lfs f0,16576(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 16576);
	f0.f64 = double(temp.f32);
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// stfs f13,4(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// stw r29,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r29.u32);
	// stfs f13,20(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
	// stw r29,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r29.u32);
	// stfs f0,0(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stw r29,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r29.u32);
	// stfs f13,28(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stw r30,24(r11)
	PPC_STORE_U32(r11.u32 + 24, r30.u32);
	// bl 0x82cdda40
	sub_82CDDA40(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-104
	r12.s64 = ctx.r1.s64 + -104;
	// bl 0x82ca7528
	// b 0x82ca2c18
	return;
}

PPC_WEAK_FUNC(sub_82CDE918) {
	__imp__sub_82CDE918(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDF4F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lwz r11,16556(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16556) );
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// li r29,4
	r29.s64 = 4;
	// li r5,6
	ctx.r5.s64 = 6;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// lwz r11,16548(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(16548) );
	// lfs f31,26348(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 26348);
	f31.f64 = double(temp.f32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lfs f30,3080(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3080);
	f30.f64 = double(temp.f32);
	// lwz r11,16548(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(16548) );
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// lwz r11,16552(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(16552) );
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// lwz r11,16552(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(16552) );
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lfs f0,16572(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16572);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 80, temp.u32);
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// extsw r3,r4
	ctx.r3.s64 = ctx.r4.s32;
	// std r3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r3.u64);
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * f31.f64));
	// stfs f11,56(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 56, temp.u32);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// extsw r10,r11
	ctx.r10.s64 = r11.s32;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f10,80(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// stw r29,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r29.u32);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// stw r5,44(r31)
	PPC_STORE_U32(r31.u32 + 44, ctx.r5.u32);
	// li r11,8
	r11.s64 = 8;
	// fmuls f7,f8,f31
	ctx.f7.f64 = double(float(ctx.f8.f64 * f31.f64));
	// stfs f7,60(r31)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r31.u32 + 60, temp.u32);
	// lfs f6,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// fcmpu cr6,f6,f30
	cr6.compare(ctx.f6.f64, f30.f64);
	// blt cr6,0x82cdf62c
	if (cr6.lt) goto loc_82CDF62C;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// lfs f1,16(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821b1580
	sub_821B1580(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfd f0,19928(r10)
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + 19928);
	// fmul f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f11,f12
	ctx.f11.u64 = uint64_t(int32_t(std::trunc(ctx.f12.f64)));
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// cmpwi cr6,r11,-8
	cr6.compare<int32_t>(r11.s32, -8, xer);
	// bge cr6,0x82cdf600
	if (!cr6.lt) goto loc_82CDF600;
	// li r11,-8
	r11.s64 = -8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// b 0x82cdf614
	goto loc_82CDF614;
loc_82CDF600:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82cdf610
	if (!cr6.lt) goto loc_82CDF610;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// b 0x82cdf614
	goto loc_82CDF614;
loc_82CDF610:
	// li r11,8
	r11.s64 = 8;
loc_82CDF614:
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lfs f13,16(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f12,72(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 72, temp.u32);
	// b 0x82cdf688
	goto loc_82CDF688;
loc_82CDF62C:
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lfs f1,16(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821b1580
	sub_821B1580(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfd f0,3656(r10)
	f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3656);
	// fmul f12,f13,f0
	ctx.f12.f64 = ctx.f13.f64 * f0.f64;
	// fctiwz f11,f12
	ctx.f11.u64 = uint64_t(int32_t(std::trunc(ctx.f12.f64)));
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// cmpwi cr6,r11,-8
	cr6.compare<int32_t>(r11.s32, -8, xer);
	// bge cr6,0x82cdf668
	if (!cr6.lt) goto loc_82CDF668;
	// li r11,-8
	r11.s64 = -8;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// b 0x82cdf67c
	goto loc_82CDF67C;
loc_82CDF668:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bge cr6,0x82cdf678
	if (!cr6.lt) goto loc_82CDF678;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// b 0x82cdf67c
	goto loc_82CDF67C;
loc_82CDF678:
	// li r11,8
	r11.s64 = 8;
loc_82CDF67C:
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// lfs f0,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,72(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 72, temp.u32);
loc_82CDF688:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// extsw r8,r11
	ctx.r8.s64 = r11.s32;
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f12,-30920(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -30920);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f11,f31
	ctx.f10.f64 = double(float(ctx.f11.f64 * f31.f64));
	// stfs f10,64(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 64, temp.u32);
	// lfs f9,24(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 24);
	ctx.f9.f64 = double(temp.f32);
	// lwz r7,16540(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(16540) );
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f8,80(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fmuls f0,f9,f12
	f0.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f13,f7
	ctx.f13.f64 = double(float(ctx.f7.f64));
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x82cdf6dc
	if (cr6.lt) goto loc_82CDF6DC;
	// fsubs f0,f13,f30
	f0.f64 = static_cast<float>(ctx.f13.f64 - f30.f64);
loc_82CDF6DC:
	// fcmpu cr6,f0,f30
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f30.f64);
	// bgt cr6,0x82cdf6e8
	if (cr6.gt) goto loc_82CDF6E8;
	// fmr f0,f30
	f0.f64 = f30.f64;
loc_82CDF6E8:
	// fctidz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfiwx f0,0,r31
	PPC_STORE_U32(r31.u32, f0.u32);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lwz r10,28(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f13
	ctx.f11.f64 = double(ctx.f13.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f31
	ctx.f9.f64 = double(float(ctx.f10.f64 * f31.f64));
	// stfs f9,68(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 68, temp.u32);
	// lwz r8,16544(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16544) );
	// lfs f8,32(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 32);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f0,f8,f12
	f0.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f7,80(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f13,f6
	ctx.f13.f64 = double(float(ctx.f6.f64));
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// blt cr6,0x82cdf73c
	if (cr6.lt) goto loc_82CDF73C;
	// fsubs f0,f13,f30
	f0.f64 = static_cast<float>(ctx.f13.f64 - f30.f64);
loc_82CDF73C:
	// fctidz f0,f0
	ctx.fpscr.disableFlushMode();
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfiwx f0,r31,r29
	PPC_STORE_U32(r31.u32 + r29.u32, f0.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,3124(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3124);
	f0.f64 = double(temp.f32);
	// lfs f13,36(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 36);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// lfs f10,40(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 40);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,76(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 76, temp.u32);
	// lfs f9,44(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 44);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,52(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 52, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-48(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CDF4F8) {
	__imp__sub_82CDF4F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDF788) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r22{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// cmplwi cr6,r4,12
	cr6.compare<uint32_t>(ctx.r4.u32, 12, xer);
	// bgt cr6,0x82cdf894
	if (cr6.gt) goto loc_82CDF894;
	// lis r12,-32050
	r12.s64 = -2100428800;
	// addi r12,r12,-2112
	r12.s64 = r12.s64 + -2112;
	// rlwinm r0,r4,2,0,29
	r0.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r4.u64) {
	case 0:
		goto loc_82CDF7F4;
	case 1:
		goto loc_82CDF808;
	case 2:
		goto loc_82CDF814;
	case 3:
		goto loc_82CDF820;
	case 4:
		goto loc_82CDF82C;
	case 5:
		goto loc_82CDF838;
	case 6:
		goto loc_82CDF844;
	case 7:
		goto loc_82CDF850;
	case 8:
		goto loc_82CDF85C;
	case 9:
		goto loc_82CDF868;
	case 10:
		goto loc_82CDF874;
	case 11:
		goto loc_82CDF880;
	case 12:
		goto loc_82CDF88C;
	default:
		__builtin_unreachable();
	}
	// lwz r22,-2060(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-2060) );
	// lwz r22,-2040(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-2040) );
	// lwz r22,-2028(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-2028) );
	// lwz r22,-2016(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-2016) );
	// lwz r22,-2004(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-2004) );
	// lwz r22,-1992(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1992) );
	// lwz r22,-1980(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1980) );
	// lwz r22,-1968(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1968) );
	// lwz r22,-1956(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1956) );
	// lwz r22,-1944(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1944) );
	// lwz r22,-1932(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1932) );
	// lwz r22,-1920(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1920) );
	// lwz r22,-1908(r13)
	r22.u64 = PPC_LOAD_U32(ctx.r13.u32 + int32_t(-1908) );
loc_82CDF7F4:
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// li r5,48
	ctx.r5.s64 = 48;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF808:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF814:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF820:
	// lfs f0,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	f0.f64 = double(temp.f32);
	// stfs f0,12(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF82C:
	// lfs f0,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	f0.f64 = double(temp.f32);
	// stfs f0,16(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 16, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF838:
	// lfs f0,16(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	f0.f64 = double(temp.f32);
	// stfs f0,20(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 20, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF844:
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF850:
	// lfs f0,24(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24);
	f0.f64 = double(temp.f32);
	// stfs f0,28(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 28, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF85C:
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF868:
	// lfs f0,32(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 32);
	f0.f64 = double(temp.f32);
	// stfs f0,36(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 36, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF874:
	// lfs f0,36(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,40(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 40, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF880:
	// lfs f0,40(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	f0.f64 = double(temp.f32);
	// stfs f0,44(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
	// b 0x82cdf894
	goto loc_82CDF894;
loc_82CDF88C:
	// lfs f0,44(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 44);
	f0.f64 = double(temp.f32);
	// stfs f0,48(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
loc_82CDF894:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82cdd6d0
	sub_82CDD6D0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r31,4
	ctx.r4.s64 = r31.s64 + 4;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// lfs f1,88(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82cdf4f8
	sub_82CDF4F8(ctx, base);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// add r3,r11,r31
	ctx.r3.u64 = r11.u64 + r31.u64;
	// bl 0x82cddb80
	sub_82CDDB80(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDF788) {
	__imp__sub_82CDF788(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDF8E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82cdf918
	if (cr6.eq) goto loc_82CDF918;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r3,r31,52
	ctx.r3.s64 = r31.s64 + 52;
	// addi r10,r11,19936
	ctx.r10.s64 = r11.s64 + 19936;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82cde918
	sub_82CDE918(ctx, base);
loc_82CDF918:
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lis r3,-32256
	ctx.r3.s64 = -2113929216;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f13,3084(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3084);
	ctx.f13.f64 = double(temp.f32);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// lfs f12,3080(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 3080);
	ctx.f12.f64 = double(temp.f32);
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// lfs f0,2720(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 2720);
	f0.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addi r8,r10,15840
	ctx.r8.s64 = ctx.r10.s64 + 15840;
	// lfs f11,3056(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 3056);
	ctx.f11.f64 = double(temp.f32);
	// li r11,-10000
	r11.s64 = -10000;
	// stwx r8,r5,r31
	PPC_STORE_U32(ctx.r5.u32 + r31.u32, ctx.r8.u32);
	// lfs f10,2936(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 2936);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,-20492(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + -20492);
	ctx.f9.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f8,-20496(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -20496);
	ctx.f8.f64 = double(temp.f32);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stfs f13,12(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stfs f12,16(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 16, temp.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stfs f11,20(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 20, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stfs f10,28(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 28, temp.u32);
	// stfs f9,36(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 36, temp.u32);
	// stfs f0,40(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 40, temp.u32);
	// stfs f0,44(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
	// stfs f8,48(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDF8E0) {
	__imp__sub_82CDF8E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDF9B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// addi r10,r11,19952
	ctx.r10.s64 = r11.s64 + 19952;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82ce6990
	sub_82CE6990(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDF9B8) {
	__imp__sub_82CDF9B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDF9F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// addi r9,r10,19952
	ctx.r9.s64 = ctx.r10.s64 + 19952;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// b 0x82ce6898
	sub_82CE6898(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CDF9F8) {
	__imp__sub_82CDF9F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDFA10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// li r5,56
	ctx.r5.s64 = 56;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r11,1
	r11.s64 = 1;
	// li r9,32
	ctx.r9.s64 = 32;
	// li r8,0
	ctx.r8.s64 = 0;
	// sth r11,92(r31)
	PPC_STORE_U16(r31.u32 + 92, r11.u16);
	// stb r9,156(r31)
	PPC_STORE_U8(r31.u32 + 156, ctx.r9.u8);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lfs f0,3080(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3080);
	f0.f64 = double(temp.f32);
	// sth r11,94(r31)
	PPC_STORE_U16(r31.u32 + 94, r11.u16);
	// stfs f0,96(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 96, temp.u32);
	// stw r8,152(r31)
	PPC_STORE_U32(r31.u32 + 152, ctx.r8.u32);
	// stfs f0,100(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 100, temp.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// bl 0x82ce68d0
	sub_82CE68D0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CDFA10) {
	__imp__sub_82CDFA10(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDFA78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lbz r11,156(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 156);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cdfab8
	if (cr6.eq) goto loc_82CDFAB8;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,65535
	ctx.r3.u64 = ctx.r3.u64 | 65535;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82CDFAB8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r5,-32768
	ctx.r5.s64 = -2147483648;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,16388
	ctx.r5.u64 = ctx.r5.u64 | 16388;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82cdfab8
	if (!cr6.eq) goto loc_82CDFAB8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r9,152(r31)
	PPC_STORE_U32(r31.u32 + 152, ctx.r9.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDFA78) {
	__imp__sub_82CDFA78(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDFB20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// addi r10,r11,19952
	ctx.r10.s64 = r11.s64 + 19952;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82ce6898
	sub_82CE6898(ctx, base);
	// clrlwi r9,r30,31
	ctx.r9.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82cdfb70
	if (cr6.eq) goto loc_82CDFB70;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lis r5,24962
	ctx.r5.s64 = 1635909632;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
loc_82CDFB70:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDFB20) {
	__imp__sub_82CDFB20(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDFB90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// rlwinm r10,r5,0,27,27
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0x10;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// clrlwi r28,r5,24
	r28.u64 = ctx.r5.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// bne cr6,0x82cdfd4c
	if (!cr6.eq) goto loc_82CDFD4C;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82cdfbd0
	if (!cr6.eq) goto loc_82CDFBD0;
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// stw r11,32(r30)
	PPC_STORE_U32(r30.u32 + 32, r11.u32);
loc_82CDFBD0:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82cdfc94
	if (cr6.eq) goto loc_82CDFC94;
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// lwz r10,12(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// blt cr6,0x82cdfc3c
	if (cr6.lt) goto loc_82CDFC3C;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdfc3c
	if (cr6.eq) goto loc_82CDFC3C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfc3c
	if (!cr6.eq) goto loc_82CDFC3C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfc3c
	if (!cr0.eq) goto loc_82CDFC3C;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDFC3C:
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// lwz r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x82cdfc98
	if (!cr6.gt) goto loc_82CDFC98;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdfc98
	if (cr6.eq) goto loc_82CDFC98;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfc98
	if (!cr6.eq) goto loc_82CDFC98;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfc98
	if (!cr0.eq) goto loc_82CDFC98;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CDFC94:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDFC98:
	// lwz r10,28(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// lwz r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82cdfcf4
	if (cr6.lt) goto loc_82CDFCF4;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdfcf4
	if (cr6.eq) goto loc_82CDFCF4;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfcf4
	if (!cr6.eq) goto loc_82CDFCF4;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfcf4
	if (!cr0.eq) goto loc_82CDFCF4;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDFCF4:
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// lwz r9,12(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ble cr6,0x82cdfd50
	if (!cr6.gt) goto loc_82CDFD50;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdfd50
	if (cr6.eq) goto loc_82CDFD50;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfd50
	if (!cr6.eq) goto loc_82CDFD50;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfd50
	if (!cr0.eq) goto loc_82CDFD50;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CDFD4C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDFD50:
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cdfe20
	if (cr6.eq) goto loc_82CDFE20;
	// rlwinm r10,r28,0,26,26
	ctx.r10.u64 = rotl64(r28.u32 | (r28.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82cdfe20
	if (!cr6.eq) goto loc_82CDFE20;
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// lwz r9,24(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bgt cr6,0x82cdfdc4
	if (cr6.gt) goto loc_82CDFDC4;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdfdc4
	if (cr6.eq) goto loc_82CDFDC4;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfdc4
	if (!cr6.eq) goto loc_82CDFDC4;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfdc4
	if (!cr0.eq) goto loc_82CDFDC4;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDFDC4:
	// lwz r10,28(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// lwz r9,24(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82cdfe20
	if (cr6.lt) goto loc_82CDFE20;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82cdfe20
	if (cr6.eq) goto loc_82CDFE20;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfe20
	if (!cr6.eq) goto loc_82CDFE20;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfe20
	if (!cr0.eq) goto loc_82CDFE20;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CDFE20:
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cdfe94
	if (cr6.eq) goto loc_82CDFE94;
	// lwz r9,16(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82cdfe40
	if (cr6.eq) goto loc_82CDFE40;
	// lwz r9,20(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// b 0x82cdfe44
	goto loc_82CDFE44;
loc_82CDFE40:
	// lwz r9,28(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
loc_82CDFE44:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bgt cr6,0x82cdfe94
	if (cr6.gt) goto loc_82CDFE94;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// beq cr6,0x82cdfe94
	if (cr6.eq) goto loc_82CDFE94;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82cdfe94
	if (!cr6.eq) goto loc_82CDFE94;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82cdfe94
	if (!cr0.eq) goto loc_82CDFE94;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CDFE94:
	// clrlwi r11,r28,31
	r11.u64 = r28.u32 & 0x1;
	// li r4,24
	ctx.r4.s64 = 24;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// bne cr6,0x82cdfeb8
	if (!cr6.eq) goto loc_82CDFEB8;
	// li r5,0
	ctx.r5.s64 = 0;
loc_82CDFEB8:
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,16(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// addi r11,r29,16
	r11.s64 = r29.s64 + 16;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82cdfee0
	if (cr6.eq) goto loc_82CDFEE0;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82cdfefc
	if (!cr6.eq) goto loc_82CDFEFC;
loc_82CDFEE0:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CDFEFC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CDFB90) {
	__imp__sub_82CDFB90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDFF08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lbz r11,156(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 156);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82cdff84
	if (!cr6.eq) goto loc_82CDFF84;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82cdff54
	if (cr6.eq) goto loc_82CDFF54;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82cdff84
	if (!cr6.eq) goto loc_82CDFF84;
loc_82CDFF54:
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// li r5,56
	ctx.r5.s64 = 56;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stb r30,157(r31)
	PPC_STORE_U8(r31.u32 + 157, r30.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,80(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(80) );
	// lfs f1,3080(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3080);
	ctx.f1.f64 = double(temp.f32);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82cdff8c
	goto loc_82CDFF8C;
loc_82CDFF84:
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,65535
	ctx.r3.u64 = ctx.r3.u64 | 65535;
loc_82CDFF8C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CDFF08) {
	__imp__sub_82CDFF08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CDFFA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r30,r11,28784
	r30.s64 = r11.s64 + 28784;
	// mr r31,r13
	r31.u64 = ctx.r13.u64;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82cdffec
	if (cr6.eq) goto loc_82CDFFEC;
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r31,r7
	cr6.compare<uint32_t>(r31.u32, ctx.r7.u32, xer);
	// beq cr6,0x82ce0008
	if (cr6.eq) goto loc_82CE0008;
loc_82CDFFEC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// stw r7,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r7.u32);
	// stb r29,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r29.u8);
	// b 0x82ce000c
	goto loc_82CE000C;
loc_82CE0008:
	// lbz r29,12(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 12);
loc_82CE000C:
	// addi r11,r28,4
	r11.s64 = r28.s64 + 4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r11,12
	ctx.r9.s64 = r11.s64 + 12;
	// stw r10,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r10.u32);
	// lwz r8,16(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(16) );
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce0104
	if (cr6.eq) goto loc_82CE0104;
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// subf r31,r9,r8
	r31.s64 = ctx.r8.s64 - ctx.r9.s64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ce0104
	if (cr6.eq) goto loc_82CE0104;
	// add r10,r9,r31
	ctx.r10.u64 = ctx.r9.u64 + r31.u64;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce0064
	if (cr6.eq) goto loc_82CE0064;
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
loc_82CE0064:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + r31.u64;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// lwz r7,28(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// addic. r10,r7,1
	xer.ca = ctx.r7.u32 > 4294967294;
	ctx.r10.s64 = ctx.r7.s64 + 1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// stw r10,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r10.u32);
	// bne 0x82ce00a8
	if (!cr0.eq) goto loc_82CE00A8;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// stw r10,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r10.u32);
loc_82CE00A8:
	// li r27,0
	r27.s64 = 0;
	// rlwinm r10,r25,0,26,26
	ctx.r10.u64 = rotl64(r25.u32 | (r25.u64 << 32), 0) & 0x20;
	// stb r27,117(r31)
	PPC_STORE_U8(r31.u32 + 117, r27.u8);
	// stb r27,118(r31)
	PPC_STORE_U8(r31.u32 + 118, r27.u8);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stb r27,119(r31)
	PPC_STORE_U8(r31.u32 + 119, r27.u8);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r9,4(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + int32_t(4) );
	// lhz r8,92(r28)
	ctx.r8.u64 = PPC_LOAD_U16(r28.u32 + 92);
	// twllei r8,0
	// divwu r7,r9,r8
	ctx.r7.u32 = ctx.r9.u32 / ctx.r8.u32;
	// stw r7,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r7.u32);
	// lwz r6,8(r26)
	ctx.r6.u64 = PPC_LOAD_U32(r26.u32 + int32_t(8) );
	// stw r6,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r6.u32);
	// beq cr6,0x82ce01a4
	if (cr6.eq) goto loc_82CE01A4;
	// stw r27,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r27.u32);
	// addi r3,r31,40
	ctx.r3.s64 = r31.s64 + 40;
	// stw r27,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r27.u32);
	// addi r4,r26,12
	ctx.r4.s64 = r26.s64 + 12;
	// li r5,72
	ctx.r5.s64 = 72;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// b 0x82ce01e4
	goto loc_82CE01E4;
loc_82CE0104:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce0154
	if (cr6.eq) goto loc_82CE0154;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bne cr6,0x82ce0154
	if (!cr6.eq) goto loc_82CE0154;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r10.u32);
	// bne 0x82ce0154
	if (!cr0.eq) goto loc_82CE0154;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stb r11,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r11.u8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lbz r29,12(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 12);
loc_82CE0154:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce0194
	if (cr6.eq) goto loc_82CE0194;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bne cr6,0x82ce0194
	if (!cr6.eq) goto loc_82CE0194;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// bne 0x82ce0194
	if (!cr0.eq) goto loc_82CE0194;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stb r11,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r11.u8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0194:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_82CE01A4:
	// lhz r11,92(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 92);
	// lwz r10,12(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(12) );
	// twllei r11,0
	// divwu r9,r10,r11
	ctx.r9.u32 = ctx.r10.u32 / r11.u32;
	// stw r9,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r9.u32);
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce01dc
	if (cr6.eq) goto loc_82CE01DC;
	// lhz r9,92(r28)
	ctx.r9.u64 = PPC_LOAD_U16(r28.u32 + 92);
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// divwu r11,r11,r9
	r11.u32 = r11.u32 / ctx.r9.u32;
	// twllei r9,0
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82ce01e0
	goto loc_82CE01E0;
loc_82CE01DC:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
loc_82CE01E0:
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
loc_82CE01E4:
	// stw r27,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r27.u32);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r11,84(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(84) );
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// bl 0x82cdfb90
	sub_82CDFB90(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0254
	if (cr6.eq) goto loc_82CE0254;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce0254
	if (!cr6.eq) goto loc_82CE0254;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// bne 0x82ce0254
	if (!cr0.eq) goto loc_82CE0254;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// lbz r29,12(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 12);
	// mr r11,r27
	r11.u64 = r27.u64;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stb r11,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0254:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82CDFFA8) {
	__imp__sub_82CDFFA8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0260) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce02a4
	if (cr6.eq) goto loc_82CE02A4;
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r7
	cr6.compare<uint32_t>(r29.u32, ctx.r7.u32, xer);
	// beq cr6,0x82ce02c0
	if (cr6.eq) goto loc_82CE02C0;
loc_82CE02A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// b 0x82ce02c4
	goto loc_82CE02C4;
loc_82CE02C0:
	// lbz r28,12(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 12);
loc_82CE02C4:
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r9,r11,12
	ctx.r9.s64 = r11.s64 + 12;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// lwz r8,16(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + int32_t(16) );
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce0424
	if (cr6.eq) goto loc_82CE0424;
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// subf r4,r9,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r9.s64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82ce0424
	if (cr6.eq) goto loc_82CE0424;
	// add r10,r9,r4
	ctx.r10.u64 = ctx.r9.u64 + ctx.r4.u64;
	// lwzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r4.u32);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce031c
	if (cr6.eq) goto loc_82CE031C;
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// stw r7,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r7.u32);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
loc_82CE031C:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r8,4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// lwz r7,28(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// addic. r10,r7,1
	xer.ca = ctx.r7.u32 > 4294967294;
	ctx.r10.s64 = ctx.r7.s64 + 1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// stw r10,112(r4)
	PPC_STORE_U32(ctx.r4.u32 + 112, ctx.r10.u32);
	// bne 0x82ce0360
	if (!cr0.eq) goto loc_82CE0360;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,28(r11)
	PPC_STORE_U32(r11.u32 + 28, ctx.r10.u32);
	// stw r10,112(r4)
	PPC_STORE_U32(ctx.r4.u32 + 112, ctx.r10.u32);
loc_82CE0360:
	// li r29,0
	r29.s64 = 0;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// stb r29,117(r4)
	PPC_STORE_U8(ctx.r4.u32 + 117, r29.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,118(r4)
	PPC_STORE_U8(ctx.r4.u32 + 118, r29.u8);
	// stb r29,119(r4)
	PPC_STORE_U8(ctx.r4.u32 + 119, r29.u8);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// stw r11,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, r11.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lhz r10,92(r27)
	ctx.r10.u64 = PPC_LOAD_U16(r27.u32 + 92);
	// twllei r10,0
	// divwu r8,r9,r10
	ctx.r8.u32 = ctx.r9.u32 / ctx.r10.u32;
	// stw r8,12(r4)
	PPC_STORE_U32(ctx.r4.u32 + 12, ctx.r8.u32);
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// stw r7,16(r4)
	PPC_STORE_U32(ctx.r4.u32 + 16, ctx.r7.u32);
	// lwz r6,12(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// stw r6,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, ctx.r6.u32);
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// stw r11,24(r4)
	PPC_STORE_U32(ctx.r4.u32 + 24, r11.u32);
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// stw r10,28(r4)
	PPC_STORE_U32(ctx.r4.u32 + 28, ctx.r10.u32);
	// lwz r9,24(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// stw r9,32(r4)
	PPC_STORE_U32(ctx.r4.u32 + 32, ctx.r9.u32);
	// lwz r8,28(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// stw r8,36(r4)
	PPC_STORE_U32(ctx.r4.u32 + 36, ctx.r8.u32);
	// bl 0x82cdfb90
	sub_82CDFB90(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0418
	if (cr6.eq) goto loc_82CE0418;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce0418
	if (!cr6.eq) goto loc_82CE0418;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce0418
	if (!cr0.eq) goto loc_82CE0418;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lbz r28,12(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r11,r29
	r11.u64 = r29.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0418:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_82CE0424:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce0474
	if (cr6.eq) goto loc_82CE0474;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bne cr6,0x82ce0474
	if (!cr6.eq) goto loc_82CE0474;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// bne 0x82ce0474
	if (!cr0.eq) goto loc_82CE0474;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lbz r28,12(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 12);
loc_82CE0474:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce04b4
	if (cr6.eq) goto loc_82CE04B4;
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bne cr6,0x82ce04b4
	if (!cr6.eq) goto loc_82CE04B4;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce04b4
	if (!cr0.eq) goto loc_82CE04B4;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE04B4:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE0260) {
	__imp__sub_82CE0260(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE04C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r28,r13
	r28.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0508
	if (cr6.eq) goto loc_82CE0508;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r28,r8
	cr6.compare<uint32_t>(r28.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce0524
	if (cr6.eq) goto loc_82CE0524;
loc_82CE0508:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
	// stb r27,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r27.u8);
	// b 0x82ce0528
	goto loc_82CE0528;
loc_82CE0524:
	// lbz r27,12(r31)
	r27.u64 = PPC_LOAD_U8(r31.u32 + 12);
loc_82CE0528:
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// bne cr6,0x82ce05ac
	if (!cr6.eq) goto loc_82CE05AC;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce0554
	if (cr6.eq) goto loc_82CE0554;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// subf r29,r11,r10
	r29.s64 = ctx.r10.s64 - r11.s64;
	// b 0x82ce0558
	goto loc_82CE0558;
loc_82CE0554:
	// li r29,0
	r29.s64 = 0;
loc_82CE0558:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82ce05ac
	if (!cr6.eq) goto loc_82CE05AC;
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce05a0
	if (cr6.eq) goto loc_82CE05A0;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82ce05a0
	if (!cr6.eq) goto loc_82CE05A0;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce05a0
	if (!cr0.eq) goto loc_82CE05A0;
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE05A0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82CE05AC:
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// addi r10,r30,4
	ctx.r10.s64 = r30.s64 + 4;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x82ce05e0
	if (cr6.eq) goto loc_82CE05E0;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r6,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
loc_82CE05E0:
	// lwz r11,20(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(20) );
	// addi r9,r10,12
	ctx.r9.s64 = ctx.r10.s64 + 12;
	// li r5,48
	ctx.r5.s64 = 48;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r30,104
	ctx.r3.s64 = r30.s64 + 104;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(16) );
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// stw r11,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r5,32
	ctx.r5.s64 = 32;
	// stb r7,116(r29)
	PPC_STORE_U8(r29.u32 + 116, ctx.r7.u8);
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r11,84(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(84) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0684
	if (cr6.eq) goto loc_82CE0684;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce0684
	if (!cr6.eq) goto loc_82CE0684;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce0684
	if (!cr0.eq) goto loc_82CE0684;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0684:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE04C8) {
	__imp__sub_82CE04C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0690) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// rlwinm r11,r5,11,0,20
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 11) & 0xFFFFF800;
	// li r31,0
	r31.s64 = 0;
	// add r4,r3,r4
	ctx.r4.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r10,r11,r3
	ctx.r10.u64 = r11.u64 + ctx.r3.u64;
	// rlwinm r5,r5,14,0,17
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 14) & 0xFFFFC000;
	// rlwinm r6,r6,23,9,31
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 23) & 0x7FFFFF;
	// mr r8,r31
	ctx.r8.u64 = r31.u64;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82CE06B4:
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bge cr6,0x82ce0700
	if (!cr6.lt) goto loc_82CE0700;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ce06c8
	if (cr6.eq) goto loc_82CE06C8;
	// stw r3,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r3.u32);
loc_82CE06C8:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// rlwinm r11,r9,6,26,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0x3F;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// cmplw cr6,r6,r11
	cr6.compare<uint32_t>(ctx.r6.u32, r11.u32, xer);
	// blt cr6,0x82ce070c
	if (cr6.lt) goto loc_82CE070C;
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// rlwinm r11,r9,11,0,20
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 11) & 0xFFFFF800;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// add r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 + ctx.r5.u64;
	// ble cr6,0x82ce06b4
	if (!cr6.gt) goto loc_82CE06B4;
loc_82CE0700:
	// li r3,0
	ctx.r3.s64 = 0;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CE070C:
	// rlwinm r11,r9,21,17,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 21) & 0x7FFF;
	// cmplw cr6,r6,r8
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r8.u32, xer);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// beq cr6,0x82ce074c
	if (cr6.eq) goto loc_82CE074C;
	// subf r9,r8,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r8.s64;
loc_82CE0720:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ce072c
	if (cr6.eq) goto loc_82CE072C;
	// stw r31,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r31.u32);
loc_82CE072C:
	// rlwinm r8,r11,29,3,31
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// clrlwi r6,r11,29
	ctx.r6.u64 = r11.u32 & 0x7;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwzx r4,r8,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// slw r3,r4,r6
	ctx.r3.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r4.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r8,r3,15,17,31
	ctx.r8.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 15) & 0x7FFF;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// bne 0x82ce0720
	if (!cr0.eq) goto loc_82CE0720;
loc_82CE074C:
	// add r3,r11,r5
	ctx.r3.u64 = r11.u64 + ctx.r5.u64;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0690) {
	__imp__sub_82CE0690(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0758) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r11,r5,11,0,20
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 11) & 0xFFFFF800;
	// add r6,r3,r4
	ctx.r6.u64 = ctx.r3.u64 + ctx.r4.u64;
	// add r10,r11,r3
	ctx.r10.u64 = r11.u64 + ctx.r3.u64;
	// rlwinm r8,r5,14,0,17
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 14) & 0xFFFFC000;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// clrlwi r11,r7,24
	r11.u64 = ctx.r7.u32 & 0xFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r11,11,0,20
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 11) & 0xFFFFF800;
	// add r11,r9,r10
	r11.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// bge cr6,0x82ce07bc
	if (!cr6.lt) goto loc_82CE07BC;
loc_82CE078C:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// rlwinm r5,r9,3,0,28
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// clrlwi r9,r7,24
	ctx.r9.u64 = ctx.r7.u32 & 0xFF;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// rlwinm r9,r9,11,0,20
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 11) & 0xFFFFF800;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// blt cr6,0x82ce078c
	if (cr6.lt) goto loc_82CE078C;
loc_82CE07BC:
	// rlwinm r11,r7,0,0,5
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFC000000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce07d0
	if (!cr6.eq) goto loc_82CE07D0;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
loc_82CE07D0:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// rlwinm r9,r11,6,26,31
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 6) & 0x3F;
	// rlwinm r11,r11,21,17,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 21) & 0x7FFF;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// ble cr6,0x82ce080c
	if (!cr6.gt) goto loc_82CE080C;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
loc_82CE07EC:
	// rlwinm r7,r11,29,3,31
	ctx.r7.u64 = rotl64(r11.u32 | (r11.u64 << 32), 29) & 0x1FFFFFFF;
	// clrlwi r6,r11,29
	ctx.r6.u64 = r11.u32 & 0x7;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lwzx r5,r7,r10
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// slw r4,r5,r6
	ctx.r4.u64 = ctx.r6.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r6.u8 & 0x3F));
	// rlwinm r7,r4,15,17,31
	ctx.r7.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 15) & 0x7FFF;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// bne 0x82ce07ec
	if (!cr0.eq) goto loc_82CE07EC;
loc_82CE080C:
	// add r3,r11,r8
	ctx.r3.u64 = r11.u64 + ctx.r8.u64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0758) {
	__imp__sub_82CE0758(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0818) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addi r8,r11,20184
	ctx.r8.s64 = r11.s64 + 20184;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// addi r7,r10,20144
	ctx.r7.s64 = ctx.r10.s64 + 20144;
	// addi r6,r9,20048
	ctx.r6.s64 = ctx.r9.s64 + 20048;
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// addi r30,r31,4
	r30.s64 = r31.s64 + 4;
	// stw r7,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r6,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r6.u32);
	// beq cr6,0x82ce0870
	if (cr6.eq) goto loc_82CE0870;
	// bl 0x82cd3580
	sub_82CD3580(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,184(r31)
	PPC_STORE_U32(r31.u32 + 184, r11.u32);
loc_82CE0870:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cd9958
	sub_82CD9958(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0818) {
	__imp__sub_82CE0818(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0890) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lbz r11,292(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce08c4
	if (!cr6.eq) goto loc_82CE08C4;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16385
	ctx.r3.u64 = ctx.r3.u64 | 16385;
	// b 0x82ce0908
	goto loc_82CE0908;
loc_82CE08C4:
	// li r11,1
	r11.s64 = 1;
	// stb r11,292(r31)
	PPC_STORE_U8(r31.u32 + 292, r11.u8);
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x82ce08f8
	if (!cr6.gt) goto loc_82CE08F8;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x82ce08f8
	if (cr6.eq) goto loc_82CE08F8;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r5,88
	ctx.r5.s64 = 88;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_82CE08F8:
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// ori r5,r11,48
	ctx.r5.u64 = r11.u64 | 48;
	// bl 0x82cdffa8
	sub_82CDFFA8(ctx, base);
loc_82CE0908:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0890) {
	__imp__sub_82CE0890(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0920) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,292(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 292);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x82ce0950
	if (!cr6.eq) goto loc_82CE0950;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16385
	ctx.r3.u64 = ctx.r3.u64 | 16385;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
loc_82CE0950:
	// li r11,2
	r11.s64 = 2;
	// li r8,-1
	ctx.r8.s64 = -1;
	// stb r11,292(r3)
	PPC_STORE_U8(ctx.r3.u32 + 292, r11.u8);
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// cmplwi cr6,r11,255
	cr6.compare<uint32_t>(r11.u32, 255, xer);
	// ble cr6,0x82ce099c
	if (!cr6.gt) goto loc_82CE099C;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x82ce099c
	if (cr6.eq) goto loc_82CE099C;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82CE0980:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ce0980
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82CE0980;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
loc_82CE099C:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce09c0
	if (cr6.eq) goto loc_82CE09C0;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x82ce09b4
	if (!cr6.eq) goto loc_82CE09B4;
	// mr r11,r8
	r11.u64 = ctx.r8.u64;
loc_82CE09B4:
	// stw r11,272(r3)
	PPC_STORE_U32(ctx.r3.u32 + 272, r11.u32);
	// lwz r11,16(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(16) );
	// stw r11,268(r3)
	PPC_STORE_U32(ctx.r3.u32 + 268, r11.u32);
loc_82CE09C0:
	// clrlwi r11,r5,24
	r11.u64 = ctx.r5.u32 & 0xFF;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// ori r5,r11,16
	ctx.r5.u64 = r11.u64 | 16;
	// bl 0x82ce0260
	sub_82CE0260(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0920) {
	__imp__sub_82CE0920(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE09E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lbz r11,40(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 40);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce0a74
	if (cr6.eq) goto loc_82CE0A74;
	// li r30,0
	r30.s64 = 0;
loc_82CE0A08:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,168(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(168) );
	// bl 0x82cd3d40
	sub_82CD3D40(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce0a24
	if (cr6.eq) goto loc_82CE0A24;
	// lis r29,-32764
	r29.s64 = -2147221504;
	// ori r29,r29,2
	r29.u64 = r29.u64 | 2;
loc_82CE0A24:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,168(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(168) );
	// bl 0x82cd3d00
	sub_82CD3D00(ctx, base);
	// lwz r11,168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(168) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// rlwinm r9,r10,0,12,12
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce0a5c
	if (cr6.eq) goto loc_82CE0A5C;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// lis r29,-32764
	r29.s64 = -2147221504;
	// ori r29,r29,3
	r29.u64 = r29.u64 | 3;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// rlwinm r9,r10,0,13,11
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFF7FFFF;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
loc_82CE0A5C:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lbz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 40);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce0a08
	if (cr6.lt) goto loc_82CE0A08;
loc_82CE0A74:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cd9f68
	sub_82CD9F68(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE09E0) {
	__imp__sub_82CE09E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0A90) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0890
	sub_82CE0890(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE0A90) {
	__imp__sub_82CE0A90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0A98) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce0bf0
	sub_82CE0BF0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE0A98) {
	__imp__sub_82CE0A98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0AA0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0bf0
	sub_82CE0BF0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE0AA0) {
	__imp__sub_82CE0AA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0AA8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0920
	sub_82CE0920(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE0AA8) {
	__imp__sub_82CE0AA8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0AB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce0af0
	if (cr6.lt) goto loc_82CE0AF0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_82CE0AF0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0AB0) {
	__imp__sub_82CE0AB0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0B08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,198(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 198);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x82ce0b44
	if (cr6.eq) goto loc_82CE0B44;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r31,255
	cr6.compare<uint32_t>(r31.u32, 255, xer);
	// stw r11,288(r3)
	PPC_STORE_U32(ctx.r3.u32 + 288, r11.u32);
	// li r11,255
	r11.s64 = 255;
	// bgt cr6,0x82ce0b40
	if (cr6.gt) goto loc_82CE0B40;
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
loc_82CE0B40:
	// stb r11,198(r3)
	PPC_STORE_U8(ctx.r3.u32 + 198, r11.u8);
loc_82CE0B44:
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce0b6c
	if (cr6.lt) goto loc_82CE0B6C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// stw r31,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r31.u32);
loc_82CE0B6C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0B08) {
	__imp__sub_82CE0B08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0B80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce0bc0
	if (cr6.lt) goto loc_82CE0BC0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_82CE0BC0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0B80) {
	__imp__sub_82CE0B80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0BD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,168(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(168) );
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0BD8) {
	__imp__sub_82CE0BD8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0BE8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// b 0x82cd9428
	sub_82CD9428(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE0BE8) {
	__imp__sub_82CE0BE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0BF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ce0818
	sub_82CE0818(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0BF0) {
	__imp__sub_82CE0BF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0C20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// bl 0x82cdff08
	sub_82CDFF08(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce0c50
	if (cr6.lt) goto loc_82CE0C50;
	// li r11,1
	r11.s64 = 1;
	// sth r11,108(r31)
	PPC_STORE_U16(r31.u32 + 108, r11.u16);
	// sth r11,110(r31)
	PPC_STORE_U16(r31.u32 + 110, r11.u16);
loc_82CE0C50:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE0C20) {
	__imp__sub_82CE0C20(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0C68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0cac
	if (cr6.eq) goto loc_82CE0CAC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce0cc0
	if (cr6.eq) goto loc_82CE0CC0;
loc_82CE0CAC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE0CC0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stfs f31,112(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r30.u32 + 112, temp.u32);
	// lbz r11,56(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce0d04
	if (cr6.eq) goto loc_82CE0D04;
	// li r11,0
	r11.s64 = 0;
loc_82CE0CDC:
	// lwz r9,200(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(200) );
	// mulli r10,r11,88
	ctx.r10.s64 = r11.s64 * 88;
	// lfs f0,112(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 112);
	f0.f64 = double(temp.f32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stfs f0,40(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 40, temp.u32);
	// lbz r9,56(r30)
	ctx.r9.u64 = PPC_LOAD_U8(r30.u32 + 56);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ce0cdc
	if (cr6.lt) goto loc_82CE0CDC;
loc_82CE0D04:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce0d50
	if (cr6.eq) goto loc_82CE0D50;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce0d50
	if (!cr6.eq) goto loc_82CE0D50;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce0d50
	if (!cr0.eq) goto loc_82CE0D50;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0D50:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE0C68) {
	__imp__sub_82CE0C68(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0D60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r30,r11,28784
	r30.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0da4
	if (cr6.eq) goto loc_82CE0DA4;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce0db8
	if (cr6.eq) goto loc_82CE0DB8;
loc_82CE0DA4:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// stw r29,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r29.u32);
	// stb r28,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r28.u8);
loc_82CE0DB8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// fmr f2,f31
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f31.f64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// lfd f1,3552(r10)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3552);
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// lbz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,116(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 116, temp.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce0e5c
	if (cr6.eq) goto loc_82CE0E5C;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r7,16
	ctx.r7.s64 = 16;
	// li r8,1
	ctx.r8.s64 = 1;
loc_82CE0DF0:
	// rlwinm r11,r10,3,0,28
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// addi r6,r10,8
	ctx.r6.s64 = ctx.r10.s64 + 8;
	// lfs f0,116(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 116);
	f0.f64 = double(temp.f32);
	// add r5,r11,r31
	ctx.r5.u64 = r11.u64 + r31.u64;
	// mulli r11,r10,88
	r11.s64 = ctx.r10.s64 * 88;
	// lwz r3,60(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(60) );
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r3.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// rlwinm r9,r6,3,0,28
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// lbzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r31.u32);
	// lwz r4,80(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// stb r8,12(r11)
	PPC_STORE_U8(r11.u32 + 12, ctx.r8.u8);
	// ori r3,r4,1
	ctx.r3.u64 = ctx.r4.u64 | 1;
	// clrlwi r10,r6,24
	ctx.r10.u64 = ctx.r6.u32 & 0xFF;
	// stb r5,13(r11)
	PPC_STORE_U8(r11.u32 + 13, ctx.r5.u8);
	// stw r3,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r3.u32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfiwx f9,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.f9.u32);
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ce0df0
	if (cr6.lt) goto loc_82CE0DF0;
loc_82CE0E5C:
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce0ea8
	if (cr6.eq) goto loc_82CE0EA8;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce0ea8
	if (!cr6.eq) goto loc_82CE0EA8;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// bne 0x82ce0ea8
	if (!cr0.eq) goto loc_82CE0EA8;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r31,12(r30)
	r31.u64 = PPC_LOAD_U8(r30.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stb r11,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0EA8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE0D60) {
	__imp__sub_82CE0D60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE0EB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r30,r11,28784
	r30.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce0efc
	if (cr6.eq) goto loc_82CE0EFC;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce0f10
	if (cr6.eq) goto loc_82CE0F10;
loc_82CE0EFC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// stw r29,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r29.u32);
	// stb r28,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r28.u8);
loc_82CE0F10:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// stfs f31,116(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 116, temp.u32);
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce0fa0
	if (cr6.eq) goto loc_82CE0FA0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r7,16
	ctx.r7.s64 = 16;
	// li r8,1
	ctx.r8.s64 = 1;
loc_82CE0F34:
	// rlwinm r11,r10,3,0,28
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// addi r6,r10,8
	ctx.r6.s64 = ctx.r10.s64 + 8;
	// lfs f0,116(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 116);
	f0.f64 = double(temp.f32);
	// add r5,r11,r31
	ctx.r5.u64 = r11.u64 + r31.u64;
	// mulli r11,r10,88
	r11.s64 = ctx.r10.s64 * 88;
	// lwz r3,60(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(60) );
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// std r3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r3.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// rlwinm r9,r6,3,0,28
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// lbzx r5,r9,r31
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + r31.u32);
	// lwz r4,80(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// stb r8,12(r11)
	PPC_STORE_U8(r11.u32 + 12, ctx.r8.u8);
	// ori r3,r4,1
	ctx.r3.u64 = ctx.r4.u64 | 1;
	// clrlwi r10,r6,24
	ctx.r10.u64 = ctx.r6.u32 & 0xFF;
	// stb r5,13(r11)
	PPC_STORE_U8(r11.u32 + 13, ctx.r5.u8);
	// stw r3,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r3.u32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfiwx f9,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.f9.u32);
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ce0f34
	if (cr6.lt) goto loc_82CE0F34;
loc_82CE0FA0:
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce0fec
	if (cr6.eq) goto loc_82CE0FEC;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce0fec
	if (!cr6.eq) goto loc_82CE0FEC;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// bne 0x82ce0fec
	if (!cr0.eq) goto loc_82CE0FEC;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r31,12(r30)
	r31.u64 = PPC_LOAD_U8(r30.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stb r11,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE0FEC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE0EB8) {
	__imp__sub_82CE0EB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1000) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// addi r31,r1,-160
	r31.s64 = ctx.r1.s64 + -160;
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lbz r3,56(r9)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r9.u32 + 56);
	// rotlwi r11,r3,1
	r11.u64 = rotl32(ctx.r3.u32, 1);
	// add r11,r3,r11
	r11.u64 = ctx.r3.u64 + r11.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// rlwinm r12,r8,0,0,27
	r12.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFF0;
	// bl 0x82ca9e84
	sub_82CA9E84(ctx, base);
	// lwz r7,0(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(0) );
	// li r24,1
	r24.s64 = 1;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stwux r7,r1,r12
	ea = ctx.r1.u32 + r12.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r1.u32 = ea;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// beq cr6,0x82ce11b0
	if (cr6.eq) goto loc_82CE11B0;
	// lis r11,0
	r11.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r10,r9,64
	ctx.r10.s64 = ctx.r9.s64 + 64;
	// addi r8,r4,4
	ctx.r8.s64 = ctx.r4.s64 + 4;
	// addi r28,r9,276
	r28.s64 = ctx.r9.s64 + 276;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r25,16
	r25.s64 = 16;
	// ori r30,r11,48000
	r30.u64 = r11.u64 | 48000;
loc_82CE1068:
	// lwz r6,-4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-4) );
	// li r26,3968
	r26.s64 = 3968;
	// lwz r11,204(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(204) );
	// mullw r11,r11,r6
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r6.s32);
	// rlwinm r6,r11,8,0,23
	ctx.r6.u64 = rotl64(r11.u32 | (r11.u64 << 32), 8) & 0xFFFFFF00;
	// divwu r11,r6,r30
	r11.u32 = ctx.r6.u32 / r30.u32;
	// addi r6,r11,16
	ctx.r6.s64 = r11.s64 + 16;
	// sth r6,0(r28)
	PPC_STORE_U16(r28.u32 + 0, ctx.r6.u16);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmplwi cr6,r6,1
	cr6.compare<uint32_t>(ctx.r6.u32, 1, xer);
	// beq cr6,0x82ce1098
	if (cr6.eq) goto loc_82CE1098;
	// li r26,1920
	r26.s64 = 1920;
loc_82CE1098:
	// lbz r29,1(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 1);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82ce10ac
	if (!cr6.eq) goto loc_82CE10AC;
	// li r11,8
	r11.s64 = 8;
	// b 0x82ce1110
	goto loc_82CE1110;
loc_82CE10AC:
	// addi r11,r11,127
	r11.s64 = r11.s64 + 127;
	// rlwinm r11,r11,25,7,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 25) & 0x1FFFFFF;
	// addi r6,r11,-1
	ctx.r6.s64 = r11.s64 + -1;
	// rlwinm r27,r11,7,0,24
	r27.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// and r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 & r11.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x82ce10d4
	if (cr6.eq) goto loc_82CE10D4;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// subfic r6,r11,32
	xer.ca = r11.u32 <= 32;
	ctx.r6.s64 = 32 - r11.s64;
	// slw r11,r24,r6
	r11.u64 = ctx.r6.u8 & 0x20 ? 0 : (r24.u32 << (ctx.r6.u8 & 0x3F));
loc_82CE10D4:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bge cr6,0x82ce10e4
	if (!cr6.lt) goto loc_82CE10E4;
	// li r11,2
	r11.s64 = 2;
	// b 0x82ce10f0
	goto loc_82CE10F0;
loc_82CE10E4:
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// ble cr6,0x82ce10f0
	if (!cr6.gt) goto loc_82CE10F0;
	// li r11,8
	r11.s64 = 8;
loc_82CE10F0:
	// rlwinm r6,r11,7,0,24
	ctx.r6.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// cmplwi cr6,r29,255
	cr6.compare<uint32_t>(r29.u32, 255, xer);
	// add r6,r6,r27
	ctx.r6.u64 = ctx.r6.u64 + r27.u64;
	// beq cr6,0x82ce1108
	if (cr6.eq) goto loc_82CE1108;
	// rlwinm r29,r29,7,0,24
	r29.u64 = rotl64(r29.u32 | (r29.u64 << 32), 7) & 0xFFFFFF80;
	// add r6,r29,r6
	ctx.r6.u64 = r29.u64 + ctx.r6.u64;
loc_82CE1108:
	// cmplw cr6,r6,r26
	cr6.compare<uint32_t>(ctx.r6.u32, r26.u32, xer);
	// ble cr6,0x82ce1114
	if (!cr6.gt) goto loc_82CE1114;
loc_82CE1110:
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
loc_82CE1114:
	// lwz r29,-4(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-4) );
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r28,r28,2
	r28.s64 = r28.s64 + 2;
	// stw r29,-4(r8)
	PPC_STORE_U32(ctx.r8.u32 + -4, r29.u32);
	// lbz r29,0(r10)
	r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// stw r6,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r6.u32);
	// stb r11,5(r8)
	PPC_STORE_U8(ctx.r8.u32 + 5, r11.u8);
	// stb r29,4(r8)
	PPC_STORE_U8(ctx.r8.u32 + 4, r29.u8);
	// addi r8,r8,12
	ctx.r8.s64 = ctx.r8.s64 + 12;
	// lwz r11,200(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(200) );
	// add r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 + r11.u64;
	// lbz r11,264(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 264);
	// stw r11,84(r6)
	PPC_STORE_U32(ctx.r6.u32 + 84, r11.u32);
	// lfs f0,116(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 116);
	f0.f64 = double(temp.f32);
	// lwz r29,-4(r10)
	r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-4) );
	// std r29,80(r31)
	PPC_STORE_U64(r31.u32 + 80, r29.u64);
	// lfd f13,80(r31)
	ctx.f13.u64 = PPC_LOAD_U64(r31.u32 + 80);
	// lwz r11,200(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(200) );
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// stb r6,13(r11)
	PPC_STORE_U8(r11.u32 + 13, ctx.r6.u8);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// lwz r29,80(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// ori r29,r29,1
	r29.u64 = r29.u64 | 1;
	// stb r24,12(r11)
	PPC_STORE_U8(r11.u32 + 12, r24.u8);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// stw r29,80(r11)
	PPC_STORE_U32(r11.u32 + 80, r29.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfiwx f9,r11,r25
	PPC_STORE_U32(r11.u32 + r25.u32, ctx.f9.u32);
	// lwz r11,200(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(200) );
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// lwz r6,80(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// ori r6,r6,2
	ctx.r6.u64 = ctx.r6.u64 | 2;
	// addi r7,r7,88
	ctx.r7.s64 = ctx.r7.s64 + 88;
	// stw r30,32(r11)
	PPC_STORE_U32(r11.u32 + 32, r30.u32);
	// stw r6,80(r11)
	PPC_STORE_U32(r11.u32 + 80, ctx.r6.u32);
	// bne 0x82ce1068
	if (!cr0.eq) goto loc_82CE1068;
loc_82CE11B0:
	// lbz r11,264(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 264);
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce11c8
	if (cr6.eq) goto loc_82CE11C8;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
loc_82CE11C8:
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r9,184
	ctx.r6.s64 = ctx.r9.s64 + 184;
	// bl 0x82cd3380
	sub_82CD3380(ctx, base);
	// addi r1,r31,160
	ctx.r1.s64 = r31.s64 + 160;
	// b 0x82ca2c28
	return;
}

PPC_WEAK_FUNC(sub_82CE1000) {
	__imp__sub_82CE1000(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE11E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1218
	if (cr6.eq) goto loc_82CE1218;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce1228
	if (cr6.eq) goto loc_82CE1228;
loc_82CE1218:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE1228:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd36a8
	sub_82CD36A8(ctx, base);
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82ce1288
	if (cr6.eq) goto loc_82CE1288;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd4008
	sub_82CD4008(ctx, base);
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd40a0
	sub_82CD40A0(ctx, base);
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1288
	if (cr6.eq) goto loc_82CE1288;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_82CE1268:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3c50
	sub_82CD3C50(ctx, base);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1268
	if (cr6.lt) goto loc_82CE1268;
loc_82CE1288:
	// stb r28,292(r29)
	PPC_STORE_U8(r29.u32 + 292, r28.u8);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce12d0
	if (cr6.eq) goto loc_82CE12D0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce12d0
	if (!cr6.eq) goto loc_82CE12D0;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce12d0
	if (!cr0.eq) goto loc_82CE12D0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE12D0:
	// addi r3,r29,16
	ctx.r3.s64 = r29.s64 + 16;
	// bl 0x82cdfa78
	sub_82CDFA78(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE11E0) {
	__imp__sub_82CE11E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE12E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// addi r30,r29,296
	r30.s64 = r29.s64 + 296;
	// mr r26,r25
	r26.u64 = r25.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r31,r13
	r31.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1324
	if (cr6.eq) goto loc_82CE1324;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x82ce1334
	if (cr6.eq) goto loc_82CE1334;
loc_82CE1324:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r28,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r28.u8);
	// stw r31,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r31.u32);
loc_82CE1334:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd4168
	sub_82CD4168(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x82ce1370
	if (!cr6.eq) goto loc_82CE1370;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd4008
	sub_82CD4008(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x82ce13e0
	if (cr6.lt) goto loc_82CE13E0;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd40a0
	sub_82CD40A0(ctx, base);
loc_82CE1370:
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce13cc
	if (cr6.eq) goto loc_82CE13CC;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_82CE1380:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd3e60
	sub_82CD3E60(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x82ce13cc
	if (cr6.lt) goto loc_82CE13CC;
	// rlwinm r11,r31,3,0,28
	r11.u64 = rotl64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r9,r31,1
	ctx.r9.s64 = r31.s64 + 1;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// rlwinm r7,r10,7,16,24
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFF80;
	// clrlwi r31,r9,24
	r31.u64 = ctx.r9.u32 & 0xFF;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// sth r7,4(r11)
	PPC_STORE_U16(r11.u32 + 4, ctx.r7.u16);
	// lbz r6,56(r29)
	ctx.r6.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// cmplw cr6,r31,r6
	cr6.compare<uint32_t>(r31.u32, ctx.r6.u32, xer);
	// blt cr6,0x82ce1380
	if (cr6.lt) goto loc_82CE1380;
loc_82CE13CC:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x82ce13e0
	if (!cr6.eq) goto loc_82CE13E0;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd42b8
	sub_82CD42B8(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
loc_82CE13E0:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1424
	if (cr6.eq) goto loc_82CE1424;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce1424
	if (!cr6.eq) goto loc_82CE1424;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// bne 0x82ce1424
	if (!cr0.eq) goto loc_82CE1424;
	// lbz r31,12(r30)
	r31.u64 = PPC_LOAD_U8(r30.u32 + 12);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r25,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r25.u32);
	// stb r25,12(r30)
	PPC_STORE_U8(r30.u32 + 12, r25.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE1424:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82CE12E0) {
	__imp__sub_82CE12E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1430) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce1460
	if (!cr6.eq) goto loc_82CE1460;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16385
	ctx.r3.u64 = ctx.r3.u64 | 16385;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82CE1460:
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1488
	if (cr6.eq) goto loc_82CE1488;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce1498
	if (cr6.eq) goto loc_82CE1498;
loc_82CE1488:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
loc_82CE1498:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r3,r29,120
	ctx.r3.s64 = r29.s64 + 120;
	// li r5,48
	ctx.r5.s64 = 48;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce14fc
	if (cr6.eq) goto loc_82CE14FC;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce14fc
	if (!cr6.eq) goto loc_82CE14FC;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce14fc
	if (!cr0.eq) goto loc_82CE14FC;
	// li r11,0
	r11.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE14FC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE1430) {
	__imp__sub_82CE1430(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1508) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce32a0
	sub_82CE32A0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1508) {
	__imp__sub_82CE1508(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1510) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0c68
	sub_82CE0C68(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1510) {
	__imp__sub_82CE1510(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1518) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce33a0
	sub_82CE33A0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1518) {
	__imp__sub_82CE1518(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1520) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce11e0
	sub_82CE11E0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1520) {
	__imp__sub_82CE1520(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1528) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0b08
	sub_82CE0B08(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1528) {
	__imp__sub_82CE1528(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1530) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0d60
	sub_82CE0D60(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1530) {
	__imp__sub_82CE1530(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1538) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0eb8
	sub_82CE0EB8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1538) {
	__imp__sub_82CE1538(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1540) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce31a0
	sub_82CE31A0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1540) {
	__imp__sub_82CE1540(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1548) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0c20
	sub_82CE0C20(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1548) {
	__imp__sub_82CE1548(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1550) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0b80
	sub_82CE0B80(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1550) {
	__imp__sub_82CE1550(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1558) {
	PPC_FUNC_PROLOGUE();
	// b 0x82ce11e0
	sub_82CE11E0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE1558) {
	__imp__sub_82CE1558(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1560) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 68);
	// lbz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 12);
	// mulli r10,r10,120
	ctx.r10.s64 = ctx.r10.s64 * 120;
	// mulli r11,r9,88
	r11.s64 = ctx.r9.s64 * 88;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r8,r11,312
	ctx.r8.s64 = r11.s64 + 312;
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE1560) {
	__imp__sub_82CE1560(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1588) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r20,0
	r20.s64 = 0;
	// li r25,1
	r25.s64 = 1;
	// mr r24,r20
	r24.u64 = r20.u64;
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce15f8
	if (cr6.eq) goto loc_82CE15F8;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82CE15B4:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3810
	sub_82CD3810(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82ce165c
	if (cr6.eq) goto loc_82CE165C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3878
	sub_82CD3878(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82ce15e0
	if (!cr6.eq) goto loc_82CE15E0;
	// mr r25,r20
	r25.u64 = r20.u64;
loc_82CE15E0:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r31,r11
	r31.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce15b4
	if (cr6.lt) goto loc_82CE15B4;
loc_82CE15F8:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r21,r11,28784
	r21.s64 = r11.s64 + 28784;
	// mr r31,r13
	r31.u64 = ctx.r13.u64;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1624
	if (cr6.eq) goto loc_82CE1624;
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(8) );
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce1638
	if (cr6.eq) goto loc_82CE1638;
loc_82CE1624:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// stw r31,8(r21)
	PPC_STORE_U32(r21.u32 + 8, r31.u32);
	// stb r30,12(r21)
	PPC_STORE_U8(r21.u32 + 12, r30.u8);
loc_82CE1638:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r19,r29,20
	r19.s64 = r29.s64 + 20;
	// stw r11,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r11.u32);
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(20) );
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// beq cr6,0x82ce1668
	if (cr6.eq) goto loc_82CE1668;
	// lwz r10,8(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + int32_t(8) );
	// subf r30,r10,r11
	r30.s64 = r11.s64 - ctx.r10.s64;
	// b 0x82ce166c
	goto loc_82CE166C;
loc_82CE165C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c10
	return;
loc_82CE1668:
	// mr r30,r20
	r30.u64 = r20.u64;
loc_82CE166C:
	// mr r27,r30
	r27.u64 = r30.u64;
	// li r18,4
	r18.s64 = 4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce19e4
	if (cr6.eq) goto loc_82CE19E4;
	// lwz r11,272(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(272) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce177c
	if (cr6.eq) goto loc_82CE177C;
	// lwz r10,168(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(168) );
	// lwz r9,268(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(268) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ce177c
	if (cr6.lt) goto loc_82CE177C;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x82ce16a8
	if (!cr6.eq) goto loc_82CE16A8;
	// li r11,-1
	r11.s64 = -1;
	// b 0x82ce16b0
	goto loc_82CE16B0;
loc_82CE16A8:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,272(r29)
	PPC_STORE_U32(r29.u32 + 272, r11.u32);
loc_82CE16B0:
	// stw r11,272(r29)
	PPC_STORE_U32(r29.u32 + 272, r11.u32);
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// lwz r8,20(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// stw r7,268(r29)
	PPC_STORE_U32(r29.u32 + 268, ctx.r7.u32);
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1710
	if (cr6.eq) goto loc_82CE1710;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce1710
	if (!cr6.eq) goto loc_82CE1710;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce1710
	if (!cr0.eq) goto loc_82CE1710;
	// lbz r28,12(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r20,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r20.u8);
	// stw r20,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r20.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE1710:
	// lwz r11,180(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(180) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce173c
	if (cr6.eq) goto loc_82CE173C;
	// lwz r10,12(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(12) );
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// lwz r9,36(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(36) );
	// stw r20,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r20.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE173C:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1760
	if (cr6.eq) goto loc_82CE1760;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce1770
	if (cr6.eq) goto loc_82CE1770;
loc_82CE1760:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE1770:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
loc_82CE177C:
	// lwz r11,288(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(288) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce17cc
	if (cr6.eq) goto loc_82CE17CC;
	// lbz r11,198(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 198);
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// stw r20,288(r29)
	PPC_STORE_U32(r29.u32 + 288, r20.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,272(r29)
	PPC_STORE_U32(r29.u32 + 272, r11.u32);
	// beq cr6,0x82ce17cc
	if (cr6.eq) goto loc_82CE17CC;
	// addi r30,r29,188
	r30.s64 = r29.s64 + 188;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82CE17A8:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd3d80
	sub_82CD3D80(ctx, base);
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce17a8
	if (cr6.lt) goto loc_82CE17A8;
loc_82CE17CC:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// beq cr6,0x82ce19e4
	if (cr6.eq) goto loc_82CE19E4;
	// lwz r11,8(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + int32_t(8) );
	// lwzx r10,r11,r27
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmplw cr6,r19,r10
	cr6.compare<uint32_t>(r19.u32, ctx.r10.u32, xer);
	// subf r26,r11,r10
	r26.s64 = ctx.r10.s64 - r11.s64;
	// bne cr6,0x82ce17f0
	if (!cr6.eq) goto loc_82CE17F0;
	// mr r26,r20
	r26.u64 = r20.u64;
loc_82CE17F0:
	// lbz r11,116(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 116);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1870
	if (cr6.eq) goto loc_82CE1870;
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// mr r30,r20
	r30.u64 = r20.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce185c
	if (cr6.eq) goto loc_82CE185C;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82CE1810:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r5,8(r27)
	ctx.r5.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3790
	sub_82CD3790(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82ce1840
	if (cr6.eq) goto loc_82CE1840;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3d00
	sub_82CD3D00(ctx, base);
	// rlwinm r11,r3,0,28,28
	r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1858
	if (cr6.eq) goto loc_82CE1858;
loc_82CE1840:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1810
	if (cr6.lt) goto loc_82CE1810;
	// b 0x82ce185c
	goto loc_82CE185C;
loc_82CE1858:
	// li r30,1
	r30.s64 = 1;
loc_82CE185C:
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce1874
	if (!cr6.eq) goto loc_82CE1874;
	// stb r18,116(r27)
	PPC_STORE_U8(r27.u32 + 116, r18.u8);
	// b 0x82ce1874
	goto loc_82CE1874;
loc_82CE1870:
	// li r24,1
	r24.s64 = 1;
loc_82CE1874:
	// lbz r11,116(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 116);
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x82ce19d8
	if (!cr6.eq) goto loc_82CE19D8;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x82ce18a4
	if (!cr6.eq) goto loc_82CE18A4;
	// lbz r11,172(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 172);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce18a4
	if (cr6.eq) goto loc_82CE18A4;
	// rlwinm r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce19d8
	if (cr6.eq) goto loc_82CE19D8;
loc_82CE18A4:
	// lwz r10,4(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce18f4
	if (cr6.eq) goto loc_82CE18F4;
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(8) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce18f4
	if (!cr6.eq) goto loc_82CE18F4;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r11.u32);
	// bne 0x82ce18f4
	if (!cr0.eq) goto loc_82CE18F4;
	// mr r11,r20
	r11.u64 = r20.u64;
	// lbz r31,12(r21)
	r31.u64 = PPC_LOAD_U8(r21.u32 + 12);
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// stb r11,12(r21)
	PPC_STORE_U8(r21.u32 + 12, r11.u8);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// stw r10,8(r21)
	PPC_STORE_U32(r21.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE18F4:
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce193c
	if (cr6.eq) goto loc_82CE193C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce193c
	if (!cr6.eq) goto loc_82CE193C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce193c
	if (!cr0.eq) goto loc_82CE193C;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r20,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r20.u8);
	// stw r20,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r20.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE193C:
	// lwz r11,16(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// addi r3,r29,16
	ctx.r3.s64 = r29.s64 + 16;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce197c
	if (cr6.eq) goto loc_82CE197C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce198c
	if (cr6.eq) goto loc_82CE198C;
loc_82CE197C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE198C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r13
	r31.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce19bc
	if (cr6.eq) goto loc_82CE19BC;
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(8) );
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce19d0
	if (cr6.eq) goto loc_82CE19D0;
loc_82CE19BC:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// stw r31,8(r21)
	PPC_STORE_U32(r21.u32 + 8, r31.u32);
	// stb r30,12(r21)
	PPC_STORE_U8(r21.u32 + 12, r30.u8);
loc_82CE19D0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r11.u32);
loc_82CE19D8:
	// mr r27,r26
	r27.u64 = r26.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// bne cr6,0x82ce17cc
	if (!cr6.eq) goto loc_82CE17CC;
loc_82CE19E4:
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce1a00
	if (!cr6.eq) goto loc_82CE1A00;
	// clrlwi r11,r25,24
	r11.u64 = r25.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce1a00
	if (!cr6.eq) goto loc_82CE1A00;
	// mr r24,r20
	r24.u64 = r20.u64;
loc_82CE1A00:
	// lwz r10,4(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce1a50
	if (cr6.eq) goto loc_82CE1A50;
	// lwz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(8) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce1a50
	if (!cr6.eq) goto loc_82CE1A50;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r11.u32);
	// bne 0x82ce1a50
	if (!cr0.eq) goto loc_82CE1A50;
	// mr r11,r20
	r11.u64 = r20.u64;
	// lbz r31,12(r21)
	r31.u64 = PPC_LOAD_U8(r21.u32 + 12);
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// stb r11,12(r21)
	PPC_STORE_U8(r21.u32 + 12, r11.u8);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// stw r10,8(r21)
	PPC_STORE_U32(r21.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE1A50:
	// clrlwi r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1db8
	if (cr6.eq) goto loc_82CE1DB8;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r13
	r31.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce1a80
	if (cr6.eq) goto loc_82CE1A80;
	// lwz r9,8(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + int32_t(8) );
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// beq cr6,0x82ce1a98
	if (cr6.eq) goto loc_82CE1A98;
loc_82CE1A80:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// lwz r11,4(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
	// stb r30,12(r21)
	PPC_STORE_U8(r21.u32 + 12, r30.u8);
	// stw r9,8(r21)
	PPC_STORE_U32(r21.u32 + 8, ctx.r9.u32);
loc_82CE1A98:
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// stw r10,4(r21)
	PPC_STORE_U32(r21.u32 + 4, ctx.r10.u32);
	// lwz r11,0(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// cmplw cr6,r19,r11
	cr6.compare<uint32_t>(r19.u32, r11.u32, xer);
	// beq cr6,0x82ce1ab8
	if (cr6.eq) goto loc_82CE1AB8;
	// lwz r8,8(r19)
	ctx.r8.u64 = PPC_LOAD_U32(r19.u32 + int32_t(8) );
	// subf r31,r8,r11
	r31.s64 = r11.s64 - ctx.r8.s64;
	// b 0x82ce1abc
	goto loc_82CE1ABC;
loc_82CE1AB8:
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82CE1ABC:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ce1d74
	if (cr6.eq) goto loc_82CE1D74;
	// li r22,3
	r22.s64 = 3;
loc_82CE1AC8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x82ce1d6c
	if (cr6.eq) goto loc_82CE1D6C;
	// lwz r11,8(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + int32_t(8) );
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// cmplw cr6,r19,r10
	cr6.compare<uint32_t>(r19.u32, ctx.r10.u32, xer);
	// subf r23,r11,r10
	r23.s64 = ctx.r10.s64 - r11.s64;
	// bne cr6,0x82ce1aec
	if (!cr6.eq) goto loc_82CE1AEC;
	// mr r23,r20
	r23.u64 = r20.u64;
loc_82CE1AEC:
	// lbz r11,116(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 116);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce1d60
	if (!cr6.eq) goto loc_82CE1D60;
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// li r24,1
	r24.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1d50
	if (cr6.eq) goto loc_82CE1D50;
	// mr r30,r20
	r30.u64 = r20.u64;
loc_82CE1B0C:
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// mr r28,r20
	r28.u64 = r20.u64;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce1b24
	if (!cr6.eq) goto loc_82CE1B24;
	// li r28,1
	r28.s64 = 1;
	// b 0x82ce1b3c
	goto loc_82CE1B3C;
loc_82CE1B24:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce1b3c
	if (cr6.eq) goto loc_82CE1B3C;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3878
	sub_82CD3878(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
loc_82CE1B3C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r6,12(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd36b8
	sub_82CD36B8(ctx, base);
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// beq cr6,0x82ce1ba4
	if (cr6.eq) goto loc_82CE1BA4;
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce1b8c
	if (!cr6.eq) goto loc_82CE1B8C;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// rlwinm r6,r11,25,30,31
	ctx.r6.u64 = rotl64(r11.u32 | (r11.u64 << 32), 25) & 0x3;
	// b 0x82ce1b98
	goto loc_82CE1B98;
loc_82CE1B8C:
	// rlwinm r11,r30,14,0,17
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 14) & 0xFFFFC000;
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// addi r5,r11,32
	ctx.r5.s64 = r11.s64 + 32;
loc_82CE1B98:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3ee8
	sub_82CD3EE8(ctx, base);
loc_82CE1BA4:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1cf0
	if (cr6.eq) goto loc_82CE1CF0;
	// stw r20,188(r29)
	PPC_STORE_U32(r29.u32 + 188, r20.u32);
	// addi r27,r29,188
	r27.s64 = r29.s64 + 188;
	// stw r20,192(r29)
	PPC_STORE_U32(r29.u32 + 192, r20.u32);
	// stw r20,196(r29)
	PPC_STORE_U32(r29.u32 + 196, r20.u32);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// stb r10,198(r29)
	PPC_STORE_U8(r29.u32 + 198, ctx.r10.u8);
	// bne cr6,0x82ce1ca8
	if (!cr6.eq) goto loc_82CE1CA8;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// addi r26,r11,-128
	r26.s64 = r11.s64 + -128;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// stw r20,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r20.u32);
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
	// lwz r6,20(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// rlwinm r28,r6,25,30,31
	r28.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x3;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82ce1c78
	if (!cr6.eq) goto loc_82CE1C78;
	// cmplwi cr6,r6,512
	cr6.compare<uint32_t>(ctx.r6.u32, 512, xer);
	// blt cr6,0x82ce1c58
	if (cr6.lt) goto loc_82CE1C58;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82ce1c8c
	if (!cr6.eq) goto loc_82CE1C8C;
	// addi r6,r6,-512
	ctx.r6.s64 = ctx.r6.s64 + -512;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
	// mr r28,r18
	r28.u64 = r18.u64;
	// b 0x82ce1c8c
	goto loc_82CE1C8C;
loc_82CE1C58:
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
	// mr r28,r20
	r28.u64 = r20.u64;
	// b 0x82ce1c8c
	goto loc_82CE1C8C;
loc_82CE1C78:
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
loc_82CE1C8C:
	// rlwinm r11,r26,25,30,31
	r11.u64 = rotl64(r26.u32 | (r26.u64 << 32), 25) & 0x3;
	// stw r3,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r3.u32);
	// stw r25,192(r29)
	PPC_STORE_U32(r29.u32 + 192, r25.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stb r11,196(r29)
	PPC_STORE_U8(r29.u32 + 196, r11.u8);
	// stb r28,197(r29)
	PPC_STORE_U8(r29.u32 + 197, r28.u8);
	// b 0x82ce1d0c
	goto loc_82CE1D0C;
loc_82CE1CA8:
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = rotl64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// add r9,r30,r10
	ctx.r9.u64 = r30.u64 + ctx.r10.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r9,r31
	r11.u64 = ctx.r9.u64 + r31.u64;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,40(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// stw r6,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r6.u32);
	// lwz r5,44(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(44) );
	// stw r5,192(r29)
	PPC_STORE_U32(r29.u32 + 192, ctx.r5.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lbzx r4,r7,r31
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + r31.u32);
	// stb r4,196(r29)
	PPC_STORE_U8(r29.u32 + 196, ctx.r4.u8);
	// lbz r3,49(r11)
	ctx.r3.u64 = PPC_LOAD_U8(r11.u32 + 49);
	// stb r3,197(r29)
	PPC_STORE_U8(r29.u32 + 197, ctx.r3.u8);
	// b 0x82ce1d0c
	goto loc_82CE1D0C;
loc_82CE1CF0:
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce1d18
	if (!cr6.eq) goto loc_82CE1D18;
	// stw r20,188(r29)
	PPC_STORE_U32(r29.u32 + 188, r20.u32);
	// addi r5,r29,188
	ctx.r5.s64 = r29.s64 + 188;
	// stw r20,192(r29)
	PPC_STORE_U32(r29.u32 + 192, r20.u32);
	// stw r20,196(r29)
	PPC_STORE_U32(r29.u32 + 196, r20.u32);
loc_82CE1D0C:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3d80
	sub_82CD3D80(ctx, base);
loc_82CE1D18:
	// lbz r11,292(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// beq cr6,0x82ce1d38
	if (cr6.eq) goto loc_82CE1D38;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3810
	sub_82CD3810(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82ce1d3c
	if (!cr6.eq) goto loc_82CE1D3C;
loc_82CE1D38:
	// mr r24,r20
	r24.u64 = r20.u64;
loc_82CE1D3C:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1b0c
	if (cr6.lt) goto loc_82CE1B0C;
loc_82CE1D50:
	// clrlwi r11,r24,24
	r11.u64 = r24.u32 & 0xFF;
	// stb r22,116(r31)
	PPC_STORE_U8(r31.u32 + 116, r22.u8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1d6c
	if (cr6.eq) goto loc_82CE1D6C;
loc_82CE1D60:
	// mr r31,r23
	r31.u64 = r23.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// bne cr6,0x82ce1ac8
	if (!cr6.eq) goto loc_82CE1AC8;
loc_82CE1D6C:
	// lwz r9,8(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + int32_t(8) );
	// lwz r10,4(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(4) );
loc_82CE1D74:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce1db8
	if (cr6.eq) goto loc_82CE1DB8;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce1db8
	if (!cr6.eq) goto loc_82CE1DB8;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r21)
	PPC_STORE_U32(r21.u32 + 4, r11.u32);
	// bne 0x82ce1db8
	if (!cr0.eq) goto loc_82CE1DB8;
	// mr r11,r20
	r11.u64 = r20.u64;
	// lbz r31,12(r21)
	r31.u64 = PPC_LOAD_U8(r21.u32 + 12);
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// stb r11,12(r21)
	PPC_STORE_U8(r21.u32 + 12, r11.u8);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// stw r10,8(r21)
	PPC_STORE_U32(r21.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE1DB8:
	// lwz r11,120(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(120) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1e14
	if (cr6.eq) goto loc_82CE1E14;
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1e10
	if (cr6.eq) goto loc_82CE1E10;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82CE1DD4:
	// rlwinm r11,r31,3,0,28
	r11.u64 = rotl64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// addi r10,r31,15
	ctx.r10.s64 = r31.s64 + 15;
	// add r9,r11,r29
	ctx.r9.u64 = r11.u64 + r29.u64;
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lhz r7,124(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 124);
	// lwzx r5,r8,r29
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + r29.u32);
	// rlwinm r6,r7,25,7,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// bl 0x82cd3ee8
	sub_82CD3EE8(ctx, base);
	// addi r6,r31,1
	ctx.r6.s64 = r31.s64 + 1;
	// lbz r5,56(r29)
	ctx.r5.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r31,r6,24
	r31.u64 = ctx.r6.u32 & 0xFF;
	// cmplw cr6,r31,r5
	cr6.compare<uint32_t>(r31.u32, ctx.r5.u32, xer);
	// blt cr6,0x82ce1dd4
	if (cr6.lt) goto loc_82CE1DD4;
loc_82CE1E10:
	// stw r20,120(r29)
	PPC_STORE_U32(r29.u32 + 120, r20.u32);
loc_82CE1E14:
	// lbz r11,56(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1e64
	if (cr6.eq) goto loc_82CE1E64;
	// mr r31,r20
	r31.u64 = r20.u64;
loc_82CE1E24:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3878
	sub_82CD3878(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82ce1e50
	if (!cr6.eq) goto loc_82CE1E50;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3d00
	sub_82CD3D00(ctx, base);
	// rlwinm r11,r3,0,28,28
	r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce165c
	if (cr6.eq) goto loc_82CE165C;
loc_82CE1E50:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 56);
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1e24
	if (cr6.lt) goto loc_82CE1E24;
loc_82CE1E64:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CE1588) {
	__imp__sub_82CE1588(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE1E70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r24,1
	r24.s64 = 1;
	// li r25,0
	r25.s64 = 0;
	// mr r29,r24
	r29.u64 = r24.u64;
	// lbz r11,56(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1ed0
	if (cr6.eq) goto loc_82CE1ED0;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_82CE1E9C:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(184) );
	// bl 0x82cd3878
	sub_82CD3878(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82ce1ecc
	if (cr6.eq) goto loc_82CE1ECC;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r31,r11
	r31.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1e9c
	if (cr6.lt) goto loc_82CE1E9C;
	// b 0x82ce1ed0
	goto loc_82CE1ED0;
loc_82CE1ECC:
	// mr r29,r25
	r29.u64 = r25.u64;
loc_82CE1ED0:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r27,r11,28784
	r27.s64 = r11.s64 + 28784;
	// mr r31,r13
	r31.u64 = ctx.r13.u64;
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce1efc
	if (cr6.eq) goto loc_82CE1EFC;
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x82ce1f10
	if (cr6.eq) goto loc_82CE1F10;
loc_82CE1EFC:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// stw r31,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r31.u32);
	// stb r30,12(r27)
	PPC_STORE_U8(r27.u32 + 12, r30.u8);
loc_82CE1F10:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r28,20
	r11.s64 = r28.s64 + 20;
	// stw r10,4(r27)
	PPC_STORE_U32(r27.u32 + 4, ctx.r10.u32);
	// lwz r9,20(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(20) );
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82ce20ac
	if (cr6.eq) goto loc_82CE20AC;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// subf r30,r11,r9
	r30.s64 = ctx.r9.s64 - r11.s64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce20ac
	if (cr6.eq) goto loc_82CE20AC;
	// lbz r11,116(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 116);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce20ac
	if (cr6.eq) goto loc_82CE20AC;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce20ac
	if (cr6.eq) goto loc_82CE20AC;
	// lbz r11,118(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 118);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1fb0
	if (cr6.eq) goto loc_82CE1FB0;
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce20ac
	if (cr6.eq) goto loc_82CE20AC;
	// lbz r11,119(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 119);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce20ac
	if (!cr6.eq) goto loc_82CE20AC;
	// lbz r11,56(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce1fa8
	if (cr6.eq) goto loc_82CE1FA8;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_82CE1F84:
	// li r5,128
	ctx.r5.s64 = 128;
	// lwz r3,184(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(184) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd3bc8
	sub_82CD3BC8(ctx, base);
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1f84
	if (cr6.lt) goto loc_82CE1F84;
loc_82CE1FA8:
	// stb r24,119(r30)
	PPC_STORE_U8(r30.u32 + 119, r24.u8);
	// b 0x82ce20a8
	goto loc_82CE20A8;
loc_82CE1FB0:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// mr r29,r24
	r29.u64 = r24.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2008
	if (cr6.eq) goto loc_82CE2008;
	// lbz r11,56(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2008
	if (cr6.eq) goto loc_82CE2008;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_82CE1FD0:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(184) );
	// bl 0x82cd3df8
	sub_82CD3DF8(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82ce2000
	if (!cr6.eq) goto loc_82CE2000;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,56(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r31,r11
	r31.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce1fd0
	if (cr6.lt) goto loc_82CE1FD0;
	// b 0x82ce2004
	goto loc_82CE2004;
loc_82CE2000:
	// mr r29,r25
	r29.u64 = r25.u64;
loc_82CE2004:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
loc_82CE2008:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce20ac
	if (cr6.eq) goto loc_82CE20AC;
	// lbz r11,56(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce20a4
	if (cr6.eq) goto loc_82CE20A4;
	// mr r31,r25
	r31.u64 = r25.u64;
	// li r26,3
	r26.s64 = 3;
loc_82CE2028:
	// addi r10,r1,84
	ctx.r10.s64 = ctx.r1.s64 + 84;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r4,12(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// addi r29,r11,-128
	r29.s64 = r11.s64 + -128;
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// stw r25,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r25.u32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// stw r25,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r25.u32);
	// stb r24,90(r1)
	PPC_STORE_U8(ctx.r1.u32 + 90, r24.u8);
	// bl 0x82ce0690
	sub_82CE0690(ctx, base);
	// rlwinm r9,r29,25,30,31
	ctx.r9.u64 = rotl64(r29.u32 | (r29.u64 << 32), 25) & 0x3;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// stb r9,88(r1)
	PPC_STORE_U8(ctx.r1.u32 + 88, ctx.r9.u8);
	// lwz r4,12(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// bl 0x82ce0758
	sub_82CE0758(ctx, base);
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// stb r26,89(r1)
	PPC_STORE_U8(ctx.r1.u32 + 89, r26.u8);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r3,184(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(184) );
	// bl 0x82cd3d80
	sub_82CD3D80(ctx, base);
	// addi r8,r31,1
	ctx.r8.s64 = r31.s64 + 1;
	// lbz r7,56(r28)
	ctx.r7.u64 = PPC_LOAD_U8(r28.u32 + 56);
	// clrlwi r31,r8,24
	r31.u64 = ctx.r8.u32 & 0xFF;
	// cmplw cr6,r31,r7
	cr6.compare<uint32_t>(r31.u32, ctx.r7.u32, xer);
	// blt cr6,0x82ce2028
	if (cr6.lt) goto loc_82CE2028;
loc_82CE20A4:
	// stb r24,118(r30)
	PPC_STORE_U8(r30.u32 + 118, r24.u8);
loc_82CE20A8:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
loc_82CE20AC:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// beq cr6,0x82ce20f4
	if (cr6.eq) goto loc_82CE20F4;
	// lwz r9,8(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce20f4
	if (!cr6.eq) goto loc_82CE20F4;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r11.u32);
	// bne 0x82ce20f4
	if (!cr0.eq) goto loc_82CE20F4;
	// mr r11,r25
	r11.u64 = r25.u64;
	// lbz r31,12(r27)
	r31.u64 = PPC_LOAD_U8(r27.u32 + 12);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// stb r11,12(r27)
	PPC_STORE_U8(r27.u32 + 12, r11.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r10,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE20F4:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c28
	return;
}

PPC_WEAK_FUNC(sub_82CE1E70) {
	__imp__sub_82CE1E70(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE2100) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// lbz r11,264(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 264);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce21c0
	if (cr6.eq) goto loc_82CE21C0;
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce214c
	if (cr6.eq) goto loc_82CE214C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce215c
	if (cr6.eq) goto loc_82CE215C;
loc_82CE214C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
loc_82CE215C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3f40
	sub_82CD3F40(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce21b8
	if (cr6.eq) goto loc_82CE21B8;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce21b8
	if (!cr6.eq) goto loc_82CE21B8;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce21b8
	if (!cr0.eq) goto loc_82CE21B8;
	// lbz r28,12(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r27,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r27.u8);
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE21B8:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82ce22a8
	if (cr6.lt) goto loc_82CE22A8;
loc_82CE21C0:
	// addi r30,r29,16
	r30.s64 = r29.s64 + 16;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce21f0
	if (cr6.eq) goto loc_82CE21F0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce2204
	if (cr6.eq) goto loc_82CE2204;
loc_82CE21F0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE2204:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 156);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ce2228
	if (!cr6.eq) goto loc_82CE2228;
	// li r5,3
	ctx.r5.s64 = 3;
	// li r4,3
	ctx.r4.s64 = 3;
	// b 0x82ce223c
	goto loc_82CE223C;
loc_82CE2228:
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2254
	if (cr6.eq) goto loc_82CE2254;
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,6
	ctx.r4.s64 = 6;
loc_82CE223C:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE2254:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce229c
	if (cr6.eq) goto loc_82CE229C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce229c
	if (!cr6.eq) goto loc_82CE229C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce229c
	if (!cr0.eq) goto loc_82CE229C;
	// mr r11,r27
	r11.u64 = r27.u64;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE229C:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82CE22A8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE2100) {
	__imp__sub_82CE2100(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE22B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r27,r4,31
	r27.u64 = ctx.r4.u32 & 0x1;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ce2378
	if (cr6.eq) goto loc_82CE2378;
	// lbz r11,264(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 264);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2378
	if (cr6.eq) goto loc_82CE2378;
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,300(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(300) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce2310
	if (cr6.eq) goto loc_82CE2310;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce2320
	if (cr6.eq) goto loc_82CE2320;
loc_82CE2310:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
loc_82CE2320:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r3,184(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(184) );
	// bl 0x82cd3630
	sub_82CD3630(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce2378
	if (cr6.eq) goto loc_82CE2378;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce2378
	if (!cr6.eq) goto loc_82CE2378;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce2378
	if (!cr0.eq) goto loc_82CE2378;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r26,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r26.u8);
	// stw r26,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r26.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE2378:
	// addi r30,r29,16
	r30.s64 = r29.s64 + 16;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce23a8
	if (cr6.eq) goto loc_82CE23A8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce23bc
	if (cr6.eq) goto loc_82CE23BC;
loc_82CE23A8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE23BC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 156);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce2444
	if (cr6.eq) goto loc_82CE2444;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82ce2424
	if (!cr6.eq) goto loc_82CE2424;
	// rlwinm r9,r10,0,27,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1E;
	// rlwinm r9,r9,0,30,27
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ce2424
	if (!cr6.eq) goto loc_82CE2424;
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce2444
	if (!cr6.eq) goto loc_82CE2444;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lhz r11,19944(r9)
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 19944);
	// sth r11,158(r30)
	PPC_STORE_U16(r30.u32 + 158, r11.u16);
	// b 0x82ce2440
	goto loc_82CE2440;
loc_82CE2424:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,87
	ctx.r4.s64 = 87;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE2440:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE2444:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce248c
	if (cr6.eq) goto loc_82CE248C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce248c
	if (!cr6.eq) goto loc_82CE248C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce248c
	if (!cr0.eq) goto loc_82CE248C;
	// mr r11,r26
	r11.u64 = r26.u64;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE248C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE22B8) {
	__imp__sub_82CE22B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE2498) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce22b8
	sub_82CE22B8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE2498) {
	__imp__sub_82CE2498(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE24A0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce2100
	sub_82CE2100(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE24A0) {
	__imp__sub_82CE24A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE24A8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce1558
	sub_82CE1558(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE24A8) {
	__imp__sub_82CE24A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE24B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// addi r28,r31,4
	r28.s64 = r31.s64 + 4;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82cd9668
	sub_82CD9668(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// li r30,0
	r30.s64 = 0;
	// addi r8,r11,20184
	ctx.r8.s64 = r11.s64 + 20184;
	// addi r7,r10,20144
	ctx.r7.s64 = ctx.r10.s64 + 20144;
	// stw r30,184(r31)
	PPC_STORE_U32(r31.u32 + 184, r30.u32);
	// addi r6,r9,20048
	ctx.r6.s64 = ctx.r9.s64 + 20048;
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// stw r7,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r7.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r6,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r6.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lbz r11,69(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 69);
	// stb r11,264(r31)
	PPC_STORE_U8(r31.u32 + 264, r11.u8);
	// stw r30,268(r31)
	PPC_STORE_U32(r31.u32 + 268, r30.u32);
	// stw r30,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r30.u32);
	// stw r30,288(r31)
	PPC_STORE_U32(r31.u32 + 288, r30.u32);
	// stb r30,292(r31)
	PPC_STORE_U8(r31.u32 + 292, r30.u8);
	// stw r30,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r30.u32);
	// stw r30,304(r31)
	PPC_STORE_U32(r31.u32 + 304, r30.u32);
	// stb r30,308(r31)
	PPC_STORE_U8(r31.u32 + 308, r30.u8);
	// stw r30,296(r31)
	PPC_STORE_U32(r31.u32 + 296, r30.u32);
	// bl 0x82cd93f8
	sub_82CD93F8(ctx, base);
	// lis r10,744
	ctx.r10.s64 = 48758784;
	// ori r9,r10,47662
	ctx.r9.u64 = ctx.r10.u64 | 47662;
	// lbz r28,56(r31)
	r28.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplw cr6,r28,r9
	cr6.compare<uint32_t>(r28.u32, ctx.r9.u32, xer);
	// mulli r4,r28,88
	ctx.r4.s64 = r28.s64 * 88;
	// ble cr6,0x82ce2554
	if (!cr6.gt) goto loc_82CE2554;
	// li r4,-1
	ctx.r4.s64 = -1;
loc_82CE2554:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x82ce25a8
	if (cr6.eq) goto loc_82CE25A8;
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// mr r29,r26
	r29.u64 = r26.u64;
	// blt 0x82ce25a0
	if (cr0.lt) goto loc_82CE25A0;
loc_82CE2580:
	// li r5,76
	ctx.r5.s64 = 76;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// stw r30,76(r29)
	PPC_STORE_U32(r29.u32 + 76, r30.u32);
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r29,r29,88
	r29.s64 = r29.s64 + 88;
	// bge 0x82ce2580
	if (!cr0.lt) goto loc_82CE2580;
loc_82CE25A0:
	// mr r11,r26
	r11.u64 = r26.u64;
	// b 0x82ce25ac
	goto loc_82CE25AC;
loc_82CE25A8:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82CE25AC:
	// li r10,64
	ctx.r10.s64 = 64;
	// stw r11,200(r31)
	PPC_STORE_U32(r31.u32 + 200, r11.u32);
	// vspltisw v0,15
	simd::store_i32(ctx.v0.u32, simd::set1_i32(int32_t(0xF)));
	// li r9,204
	ctx.r9.s64 = 204;
	// vspltisw128 v63,4
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x4)));
	// addi r3,r31,208
	ctx.r3.s64 = r31.s64 + 208;
	// vspltisw v13,1
	simd::store_i32(ctx.v13.u32, simd::set1_i32(int32_t(0x1)));
	// addi r4,r31,52
	ctx.r4.s64 = r31.s64 + 52;
	// li r5,56
	ctx.r5.s64 = 56;
	// lvlx128 v62,r27,r10
	temp.u32 = r27.u32 + ctx.r10.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vexptefp128 v61,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32(v61.f32, simd::log2_f32(simd::to_vec128f(v62)));
	// vcfpsxws128 v12,v61,4
	simd::store_i32(ctx.v12.s32, simd::vctsxs(simd::mul_f32(simd::load_f32_aligned(v61.f32), simd::set1_f32(16))));
	// vadduws v11,v12,v0
	simd::store_u32(ctx.v11.u32, simd::add_saturate_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v0.u32)));
	// vsraw128 v10,v11,v63
simd::store_shuffled(ctx.v10, simd::shift_right_arithmetic_i32(simd::to_vec128i(ctx.v11), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vmaxsw v9,v10,v13
	simd::store_i32(ctx.v9.u32, simd::max_i32(simd::load_i32(ctx.v10.u32), simd::load_i32(ctx.v13.u32)));
	// vspltw128 v60,v9,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v9.u32), 3));
	// stvewx128 v60,r31,r9
	PPC_STORE_U32((r31.u32 + ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v60.u32), 3 - ((r31.u32 + ctx.r9.u32) & 0xF) >> 2));
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// stw r30,188(r31)
	PPC_STORE_U32(r31.u32 + 188, r30.u32);
	// stw r30,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r30.u32);
	// li r11,1
	r11.s64 = 1;
	// stw r30,196(r31)
	PPC_STORE_U32(r31.u32 + 196, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// sth r11,108(r31)
	PPC_STORE_U16(r31.u32 + 108, r11.u16);
	// sth r11,110(r31)
	PPC_STORE_U16(r31.u32 + 110, r11.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE24B0) {
	__imp__sub_82CE24B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE2618) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd0
	// stfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, f31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r23,0
	r23.s64 = 0;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// addi r24,r31,296
	r24.s64 = r31.s64 + 296;
	// mr r26,r23
	r26.u64 = r23.u64;
	// mr r22,r23
	r22.u64 = r23.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(300) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce2664
	if (cr6.eq) goto loc_82CE2664;
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce2674
	if (cr6.eq) goto loc_82CE2674;
loc_82CE2664:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stb r29,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r29.u8);
	// stw r30,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r30.u32);
loc_82CE2674:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd36a8
	sub_82CD36A8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82ce3088
	if (cr6.eq) goto loc_82CE3088;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd4008
	sub_82CD4008(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x82ce26f8
	if (!cr6.lt) goto loc_82CE26F8;
loc_82CE26A4:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce26e8
	if (cr6.eq) goto loc_82CE26E8;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce26e8
	if (!cr6.eq) goto loc_82CE26E8;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// bne 0x82ce26e8
	if (!cr0.eq) goto loc_82CE26E8;
	// lbz r31,12(r24)
	r31.u64 = PPC_LOAD_U8(r24.u32 + 12);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r23,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r23.u32);
	// stb r23,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r23.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE26E8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x82ca2c20
	return;
loc_82CE26F8:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r25,r11,28784
	r25.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce2724
	if (cr6.eq) goto loc_82CE2724;
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce2738
	if (cr6.eq) goto loc_82CE2738;
loc_82CE2724:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// stw r30,8(r25)
	PPC_STORE_U32(r25.u32 + 8, r30.u32);
	// stb r29,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r29.u8);
loc_82CE2738:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82cd7d68
	sub_82CD7D68(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82ce27b0
	if (cr6.lt) goto loc_82CE27B0;
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2790
	if (cr6.eq) goto loc_82CE2790;
	// clrlwi r8,r11,24
	ctx.r8.u64 = r11.u32 & 0xFF;
	// mr r11,r23
	r11.u64 = r23.u64;
loc_82CE2770:
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// rlwinm r6,r9,3,0,28
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// clrlwi r11,r7,24
	r11.u64 = ctx.r7.u32 & 0xFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// lbzx r9,r6,r31
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r6.u32 + r31.u32);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// blt cr6,0x82ce2770
	if (cr6.lt) goto loc_82CE2770;
loc_82CE2790:
	// stb r10,89(r1)
	PPC_STORE_U8(ctx.r1.u32 + 89, ctx.r10.u8);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82cd7fe0
	sub_82CD7FE0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x82ce2a64
	if (!cr6.lt) goto loc_82CE2A64;
loc_82CE27B0:
	// lwz r9,4(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce27fc
	if (cr6.eq) goto loc_82CE27FC;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce27fc
	if (!cr6.eq) goto loc_82CE27FC;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// bne 0x82ce27fc
	if (!cr0.eq) goto loc_82CE27FC;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r29,12(r25)
	r29.u64 = PPC_LOAD_U8(r25.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r25)
	PPC_STORE_U32(r25.u32 + 8, ctx.r10.u32);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stb r11,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE27FC:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82ce26a4
	if (!cr6.eq) goto loc_82CE26A4;
loc_82CE2804:
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd40a0
	sub_82CD40A0(ctx, base);
	// lbz r11,292(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 292);
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ce2820
	if (!cr6.eq) goto loc_82CE2820;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce1e70
	sub_82CE1E70(ctx, base);
loc_82CE2820:
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce29a0
	if (cr6.eq) goto loc_82CE29A0;
	// mr r27,r23
	r27.u64 = r23.u64;
loc_82CE2830:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd38d8
	sub_82CD38D8(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r29,r23
	r29.u64 = r23.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ce28e0
	if (cr6.eq) goto loc_82CE28E0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82cd3b20
	sub_82CD3B20(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce28e0
	if (cr6.eq) goto loc_82CE28E0;
	// mulli r30,r27,88
	r30.s64 = r27.s64 * 88;
loc_82CE2868:
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r3,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r3.u32);
	// beq cr6,0x82ce2888
	if (cr6.eq) goto loc_82CE2888;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
loc_82CE2888:
	// stw r3,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r3.u32);
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// add r3,r30,r11
	ctx.r3.u64 = r30.u64 + r11.u64;
	// bl 0x82ce7730
	sub_82CE7730(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r29,r5,r29
	r29.u64 = ctx.r5.u64 + r29.u64;
	// bl 0x82cd3968
	sub_82CD3968(ctx, base);
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// lwz r9,28(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// subf. r8,r9,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x82ce28e0
	if (cr0.eq) goto loc_82CE28E0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82cd3b20
	sub_82CD3B20(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82ce2868
	if (!cr6.eq) goto loc_82CE2868;
loc_82CE28E0:
	// addi r11,r27,138
	r11.s64 = r27.s64 + 138;
	// subf r10,r29,r28
	ctx.r10.s64 = r28.s64 - r29.s64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r8,r9,r31
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r9.u32 + r31.u32);
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bgt cr6,0x82ce28fc
	if (cr6.gt) goto loc_82CE28FC;
	// li r22,1
	r22.s64 = 1;
loc_82CE28FC:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x82cdd5d8
	sub_82CDD5D8(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,5
	ctx.r3.s64 = 5;
	// bl 0x82cdd5d8
	sub_82CDD5D8(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd3e40
	sub_82CD3E40(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// mulli r11,r27,88
	r11.s64 = r27.s64 * 88;
	// stw r3,168(r31)
	PPC_STORE_U32(r31.u32 + 168, ctx.r3.u32);
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,28(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbz r29,13(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 13);
	// beq 0x82ce2978
	if (cr0.eq) goto loc_82CE2978;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ce2974
	if (cr6.eq) goto loc_82CE2974;
	// rlwinm r26,r10,2,0,29
	r26.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r11,2,0,29
	r28.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82CE2954:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r28,r11
	ctx.r3.u64 = r28.u64 + r11.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r28,r28,1024
	r28.s64 = r28.s64 + 1024;
	// bne 0x82ce2954
	if (!cr0.eq) goto loc_82CE2954;
loc_82CE2974:
	// li r26,1
	r26.s64 = 1;
loc_82CE2978:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82ce298c
	if (!cr6.eq) goto loc_82CE298C;
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x82cdd5d8
	sub_82CDD5D8(ctx, base);
loc_82CE298C:
	// addi r11,r27,1
	r11.s64 = r27.s64 + 1;
	// lbz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// clrlwi r27,r11,24
	r27.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce2830
	if (cr6.lt) goto loc_82CE2830;
loc_82CE29A0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce1588
	sub_82CE1588(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce29d0
	if (cr6.eq) goto loc_82CE29D0;
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce29e4
	if (cr6.eq) goto loc_82CE29E4;
loc_82CE29D0:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// stw r30,8(r25)
	PPC_STORE_U32(r25.u32 + 8, r30.u32);
	// stb r29,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r29.u8);
loc_82CE29E4:
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// stw r7,4(r25)
	PPC_STORE_U32(r25.u32 + 4, ctx.r7.u32);
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2a28
	if (cr6.eq) goto loc_82CE2A28;
	// mr r11,r23
	r11.u64 = r23.u64;
loc_82CE29FC:
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// mulli r10,r11,88
	ctx.r10.s64 = r11.s64 * 88;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// lfs f0,40(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	f0.f64 = double(temp.f32);
	// stfs f0,36(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 36, temp.u32);
	// lbz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce29fc
	if (cr6.lt) goto loc_82CE29FC;
	// lwz r7,4(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
loc_82CE2A28:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2dbc
	if (cr6.eq) goto loc_82CE2DBC;
	// lbz r11,172(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 172);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce2d8c
	if (!cr6.eq) goto loc_82CE2D8C;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// addi r9,r31,20
	ctx.r9.s64 = r31.s64 + 20;
loc_82CE2A50:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2d18
	if (cr6.eq) goto loc_82CE2D18;
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82ce2d1c
	goto loc_82CE2D1C;
loc_82CE2A64:
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2ac4
	if (cr6.eq) goto loc_82CE2AC4;
	// lwz r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// li r7,256
	ctx.r7.s64 = 256;
loc_82CE2A7C:
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// mulli r8,r10,88
	ctx.r8.s64 = ctx.r10.s64 * 88;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// addi r8,r10,8
	ctx.r8.s64 = ctx.r10.s64 + 8;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// rlwinm r5,r8,3,0,28
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// clrlwi r10,r6,24
	ctx.r10.u64 = ctx.r6.u32 & 0xFF;
	// stw r9,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r9.u32);
	// stw r7,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r7.u32);
	// stw r23,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r23.u32);
	// lwz r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// lbzx r11,r5,r31
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + r31.u32);
	// rotlwi r11,r11,10
	r11.u64 = rotl32(r11.u32, 10);
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lbz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplw cr6,r10,r3
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r3.u32, xer);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// blt cr6,0x82ce2a7c
	if (cr6.lt) goto loc_82CE2A7C;
loc_82CE2AC4:
	// lbz r11,172(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 172);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2c50
	if (cr6.eq) goto loc_82CE2C50;
	// rlwinm r10,r11,0,27,27
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce2c50
	if (!cr6.eq) goto loc_82CE2C50;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// rlwinm r9,r11,0,30,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lfs f31,3084(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3084);
	f31.f64 = double(temp.f32);
	// beq cr6,0x82ce2b80
	if (cr6.eq) goto loc_82CE2B80;
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2b64
	if (cr6.eq) goto loc_82CE2B64;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
loc_82CE2B04:
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// mulli r11,r8,88
	r11.s64 = ctx.r8.s64 * 88;
	// lfs f0,112(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 112);
	f0.f64 = double(temp.f32);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// li r9,6
	ctx.r9.s64 = 6;
	// stfs f0,40(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 40, temp.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lfs f13,40(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,36(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 36, temp.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,52
	ctx.r10.s64 = r11.s64 + 52;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82CE2B40:
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ce2b40
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82CE2B40;
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// stfs f31,48(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 48, temp.u32);
	// lbz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// clrlwi r8,r10,24
	ctx.r8.u64 = ctx.r10.u32 & 0xFF;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ce2b04
	if (cr6.lt) goto loc_82CE2B04;
loc_82CE2B64:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE2B80:
	// lbz r11,172(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 172);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2c00
	if (cr6.eq) goto loc_82CE2C00;
	// lhz r10,174(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 174);
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// addis r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 65536;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// clrlwi r7,r8,16
	ctx.r7.u64 = ctx.r8.u32 & 0xFFFF;
	// lfs f13,40(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// sth r7,174(r31)
	PPC_STORE_U16(r31.u32 + 174, ctx.r7.u16);
	// beq cr6,0x82ce2bc8
	if (cr6.eq) goto loc_82CE2BC8;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,19948(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19948);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x82ce2bcc
	goto loc_82CE2BCC;
loc_82CE2BC8:
	// fmr f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64;
loc_82CE2BCC:
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2c00
	if (cr6.eq) goto loc_82CE2C00;
	// mr r11,r23
	r11.u64 = r23.u64;
loc_82CE2BDC:
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// mulli r9,r11,88
	ctx.r9.s64 = r11.s64 * 88;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// stfs f0,40(r10)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 40, temp.u32);
	// lbz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ce2bdc
	if (cr6.lt) goto loc_82CE2BDC;
loc_82CE2C00:
	// lwz r9,4(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce2804
	if (cr6.eq) goto loc_82CE2804;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce2804
	if (!cr6.eq) goto loc_82CE2804;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// bne 0x82ce2804
	if (!cr0.eq) goto loc_82CE2804;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r30,12(r25)
	r30.u64 = PPC_LOAD_U8(r25.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r25)
	PPC_STORE_U32(r25.u32 + 8, ctx.r10.u32);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stb r11,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// b 0x82ce2804
	goto loc_82CE2804;
loc_82CE2C50:
	// lbz r11,56(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2cc4
	if (cr6.eq) goto loc_82CE2CC4;
	// mr r26,r23
	r26.u64 = r23.u64;
loc_82CE2C60:
	// lwz r11,200(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(200) );
	// mulli r10,r26,88
	ctx.r10.s64 = r26.s64 * 88;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,24(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(24) );
	// lbz r30,13(r28)
	r30.u64 = PPC_LOAD_U8(r28.u32 + 13);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2cac
	if (cr6.eq) goto loc_82CE2CAC;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce2cac
	if (cr6.eq) goto loc_82CE2CAC;
	// rlwinm r27,r11,2,0,29
	r27.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r23
	r29.u64 = r23.u64;
loc_82CE2C8C:
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(20) );
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r29
	ctx.r3.u64 = r11.u64 + r29.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r29,r29,1024
	r29.s64 = r29.s64 + 1024;
	// bne 0x82ce2c8c
	if (!cr0.eq) goto loc_82CE2C8C;
loc_82CE2CAC:
	// addi r11,r26,1
	r11.s64 = r26.s64 + 1;
	// lbz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r26,r11
	r26.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce2c60
	if (cr6.lt) goto loc_82CE2C60;
loc_82CE2CC4:
	// lwz r9,4(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce2d10
	if (cr6.eq) goto loc_82CE2D10;
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce2d10
	if (!cr6.eq) goto loc_82CE2D10;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// bne 0x82ce2d10
	if (!cr0.eq) goto loc_82CE2D10;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r31,12(r25)
	r31.u64 = PPC_LOAD_U8(r25.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r25)
	PPC_STORE_U32(r25.u32 + 8, ctx.r10.u32);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stb r11,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE2D10:
	// li r30,1
	r30.s64 = 1;
	// b 0x82ce26a4
	goto loc_82CE26A4;
loc_82CE2D18:
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE2D1C:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x82ce2d34
	if (cr6.eq) goto loc_82CE2D34;
	// lwz r10,8(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// b 0x82ce2d38
	goto loc_82CE2D38;
loc_82CE2D34:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_82CE2D38:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2d60
	if (cr6.eq) goto loc_82CE2D60;
	// lbz r11,116(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 116);
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// cntlzw r6,r11
	ctx.r6.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r5,r6,27,31,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// xori r4,r5,1
	ctx.r4.u64 = ctx.r5.u64 ^ 1;
	// or r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 | ctx.r8.u64;
	// b 0x82ce2a50
	goto loc_82CE2A50;
loc_82CE2D60:
	// clrlwi r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce2dbc
	if (!cr6.eq) goto loc_82CE2DBC;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ce2db8
	goto loc_82CE2DB8;
loc_82CE2D8C:
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// bne cr6,0x82ce2dbc
	if (!cr6.eq) goto loc_82CE2DBC;
	// rlwinm r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce2dbc
	if (!cr6.eq) goto loc_82CE2DBC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE2DB8:
	// lwz r7,4(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
loc_82CE2DBC:
	// lbz r11,172(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 172);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2e00
	if (cr6.eq) goto loc_82CE2E00;
	// lhz r10,174(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 174);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2de4
	if (cr6.eq) goto loc_82CE2DE4;
	// rlwinm r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2e00
	if (cr6.eq) goto loc_82CE2E00;
loc_82CE2DE4:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,4(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
loc_82CE2E00:
	// lbz r11,172(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 172);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce2e1c
	if (cr6.eq) goto loc_82CE2E1C;
	// rlwinm r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2fbc
	if (cr6.eq) goto loc_82CE2FBC;
loc_82CE2E1C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// addi r27,r31,20
	r27.s64 = r31.s64 + 20;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x82ce2e38
	if (cr6.eq) goto loc_82CE2E38;
	// lwz r10,8(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// b 0x82ce2e3c
	goto loc_82CE2E3C;
loc_82CE2E38:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_82CE2E3C:
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce2fbc
	if (cr6.eq) goto loc_82CE2FBC;
loc_82CE2E48:
	// mr r30,r28
	r30.u64 = r28.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ce2e60
	if (cr6.eq) goto loc_82CE2E60;
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// b 0x82ce2e64
	goto loc_82CE2E64;
loc_82CE2E60:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_82CE2E64:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x82ce2e7c
	if (cr6.eq) goto loc_82CE2E7C;
	// lwz r10,8(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// subf r28,r10,r11
	r28.s64 = r11.s64 - ctx.r10.s64;
	// b 0x82ce2e80
	goto loc_82CE2E80;
loc_82CE2E7C:
	// mr r28,r23
	r28.u64 = r23.u64;
loc_82CE2E80:
	// lbz r11,116(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 116);
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x82ce2fb4
	if (!cr6.eq) goto loc_82CE2FB4;
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ce2ed4
	if (cr6.eq) goto loc_82CE2ED4;
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce2ed4
	if (!cr6.eq) goto loc_82CE2ED4;
	// addic. r11,r7,-1
	xer.ca = ctx.r7.u32 > 0;
	r11.s64 = ctx.r7.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// bne 0x82ce2ed4
	if (!cr0.eq) goto loc_82CE2ED4;
	// mr r11,r23
	r11.u64 = r23.u64;
	// lbz r29,12(r25)
	r29.u64 = PPC_LOAD_U8(r25.u32 + 12);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// stb r11,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r11.u8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r10,8(r25)
	PPC_STORE_U32(r25.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE2ED4:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce2f18
	if (cr6.eq) goto loc_82CE2F18;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce2f18
	if (!cr6.eq) goto loc_82CE2F18;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// bne 0x82ce2f18
	if (!cr0.eq) goto loc_82CE2F18;
	// lbz r29,12(r24)
	r29.u64 = PPC_LOAD_U8(r24.u32 + 12);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stb r23,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r23.u8);
	// stw r23,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r23.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE2F18:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r9,4(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce2f58
	if (cr6.eq) goto loc_82CE2F58;
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ce2f68
	if (cr6.eq) goto loc_82CE2F68;
loc_82CE2F58:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// stw r30,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r30.u32);
	// stb r29,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r29.u8);
loc_82CE2F68:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce2f98
	if (cr6.eq) goto loc_82CE2F98;
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce2fac
	if (cr6.eq) goto loc_82CE2FAC;
loc_82CE2F98:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(4) );
	// stw r30,8(r25)
	PPC_STORE_U32(r25.u32 + 8, r30.u32);
	// stb r29,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r29.u8);
loc_82CE2FAC:
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// stw r7,4(r25)
	PPC_STORE_U32(r25.u32 + 4, ctx.r7.u32);
loc_82CE2FB4:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82ce2e48
	if (!cr6.eq) goto loc_82CE2E48;
loc_82CE2FBC:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ce3004
	if (cr6.eq) goto loc_82CE3004;
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce3004
	if (!cr6.eq) goto loc_82CE3004;
	// addic. r11,r7,-1
	xer.ca = ctx.r7.u32 > 0;
	r11.s64 = ctx.r7.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r25)
	PPC_STORE_U32(r25.u32 + 4, r11.u32);
	// bne 0x82ce3004
	if (!cr0.eq) goto loc_82CE3004;
	// mr r11,r23
	r11.u64 = r23.u64;
	// lbz r30,12(r25)
	r30.u64 = PPC_LOAD_U8(r25.u32 + 12);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// stb r11,12(r25)
	PPC_STORE_U8(r25.u32 + 12, r11.u8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// stw r10,8(r25)
	PPC_STORE_U32(r25.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3004:
	// clrlwi r11,r22,24
	r11.u64 = r22.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce3088
	if (cr6.eq) goto loc_82CE3088;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd36a8
	sub_82CD36A8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82ce3088
	if (cr6.eq) goto loc_82CE3088;
	// lwz r3,184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(184) );
	// bl 0x82cd42b8
	sub_82CD42B8(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bge cr6,0x82ce3088
	if (!cr6.lt) goto loc_82CE3088;
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3078
	if (cr6.eq) goto loc_82CE3078;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3078
	if (!cr6.eq) goto loc_82CE3078;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// bne 0x82ce3078
	if (!cr0.eq) goto loc_82CE3078;
	// lbz r30,12(r24)
	r30.u64 = PPC_LOAD_U8(r24.u32 + 12);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r23,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r23.u32);
	// stb r23,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r23.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3078:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x82ca2c20
	return;
loc_82CE3088:
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce30cc
	if (cr6.eq) goto loc_82CE30CC;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce30cc
	if (!cr6.eq) goto loc_82CE30CC;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r11.u32);
	// bne 0x82ce30cc
	if (!cr0.eq) goto loc_82CE30CC;
	// lbz r31,12(r24)
	r31.u64 = PPC_LOAD_U8(r24.u32 + 12);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r23,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r23.u32);
	// stb r23,12(r24)
	PPC_STORE_U8(r24.u32 + 12, r23.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE30CC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x82ca2c20
	return;
}

PPC_WEAK_FUNC(sub_82CE2618) {
	__imp__sub_82CE2618(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE30E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r4,312
	ctx.r4.s64 = 312;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce3130
	if (cr6.eq) goto loc_82CE3130;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82ce24b0
	sub_82CE24B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82ce3140
	if (!cr6.eq) goto loc_82CE3140;
loc_82CE3130:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82CE3140:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,120(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(120) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82ce316c
	if (cr6.lt) goto loc_82CE316C;
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82CE316C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE30E0) {
	__imp__sub_82CE30E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3190) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce33a8
	sub_82CE33A8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3190) {
	__imp__sub_82CE3190(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3198) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce33a8
	sub_82CE33A8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3198) {
	__imp__sub_82CE3198(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE31A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// addi r3,r9,1
	ctx.r3.s64 = ctx.r9.s64 + 1;
	// stw r3,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r3.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE31A0) {
	__imp__sub_82CE31A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE31B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// addic. r3,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r3.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r3,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r3.u32);
	// bne 0x82ce31f0
	if (!cr0.eq) goto loc_82CE31F0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(12) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_82CE31F0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE31B8) {
	__imp__sub_82CE31B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3200) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r10,4
	ctx.r10.s64 = 4;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r10,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r10.u8);
	// lhz r11,19944(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 19944);
	// sth r11,2(r4)
	PPC_STORE_U16(ctx.r4.u32 + 2, r11.u16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3200) {
	__imp__sub_82CE3200(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3220) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// b 0x82cdffa8
	sub_82CDFFA8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3220) {
	__imp__sub_82CE3220(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3228) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// b 0x82ce0260
	sub_82CE0260(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3228) {
	__imp__sub_82CE3228(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3230) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// b 0x82cdfa78
	sub_82CDFA78(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3230) {
	__imp__sub_82CE3230(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3238) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r11,172(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 172);
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3238) {
	__imp__sub_82CE3238(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3248) {
	PPC_FUNC_PROLOGUE();
	PPCRegister f0{};
	PPCRegister temp{};
	// lfs f0,112(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 112);
	f0.f64 = double(temp.f32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stfs f0,0(r4)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3248) {
	__imp__sub_82CE3248(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3258) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f1,116(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f1.f64 = double(temp.f32);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x821f3c80
	sub_821F3C80(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r3,0
	ctx.r3.s64 = 0;
	// lfs f0,16264(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16264);
	f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f12,0(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3258) {
	__imp__sub_82CE3258(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE32A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister f0{};
	PPCRegister temp{};
	// lfs f0,116(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	f0.f64 = double(temp.f32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stfs f0,0(r4)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE32A0) {
	__imp__sub_82CE32A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE32B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lhz r10,108(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 108);
	// lwz r9,120(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(120) );
	// mullw r8,r10,r9
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// stw r8,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r8.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE32B0) {
	__imp__sub_82CE32B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE32D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce330c
	if (cr6.lt) goto loc_82CE330C;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// stw r31,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r31.u32);
loc_82CE330C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE32D0) {
	__imp__sub_82CE32D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3320) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// addi r31,r3,16
	r31.s64 = ctx.r3.s64 + 16;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce3358
	if (cr6.eq) goto loc_82CE3358;
	// addi r4,r31,36
	ctx.r4.s64 = r31.s64 + 36;
	// li r5,56
	ctx.r5.s64 = 56;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_82CE3358:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// beq cr6,0x82ce336c
	if (cr6.eq) goto loc_82CE336C;
	// lbz r11,157(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 157);
	// stb r11,0(r30)
	PPC_STORE_U8(r30.u32 + 0, r11.u8);
loc_82CE336C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3320) {
	__imp__sub_82CE3320(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3388) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addi r11,r3,-4
	r11.s64 = ctx.r3.s64 + -4;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3388) {
	__imp__sub_82CE3388(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3398) {
	PPC_FUNC_PROLOGUE();
	// stw r3,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r3.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3398) {
	__imp__sub_82CE3398(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE33A0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// b 0x82cd9520
	sub_82CD9520(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE33A0) {
	__imp__sub_82CE33A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE33A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addi r8,r11,20448
	ctx.r8.s64 = r11.s64 + 20448;
	// addi r7,r10,20408
	ctx.r7.s64 = ctx.r10.s64 + 20408;
	// addi r6,r9,20312
	ctx.r6.s64 = ctx.r9.s64 + 20312;
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// stw r7,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r7.u32);
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// stw r6,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r6.u32);
	// bl 0x82cd9958
	sub_82CD9958(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE33A8) {
	__imp__sub_82CE33A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3400) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// bl 0x82cdff08
	sub_82CDFF08(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce344c
	if (cr6.lt) goto loc_82CE344C;
	// lbz r11,196(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 196);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lbz r9,197(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 197);
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r7,r10,20568
	ctx.r7.s64 = ctx.r10.s64 + 20568;
	// rotlwi r6,r11,2
	ctx.r6.u64 = rotl32(r11.u32, 2);
	// lwzx r5,r6,r7
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
	// sth r8,110(r31)
	PPC_STORE_U16(r31.u32 + 110, ctx.r8.u16);
	// mullw r4,r5,r9
	ctx.r4.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r9.s32);
	// sth r4,108(r31)
	PPC_STORE_U16(r31.u32 + 108, ctx.r4.u16);
loc_82CE344C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3400) {
	__imp__sub_82CE3400(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3460) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce34a4
	if (cr6.eq) goto loc_82CE34A4;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce34b8
	if (cr6.eq) goto loc_82CE34B8;
loc_82CE34A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE34B8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stfs f31,112(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r29.u32 + 112, temp.u32);
	// stfs f31,224(r29)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r29.u32 + 224, temp.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3514
	if (cr6.eq) goto loc_82CE3514;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3514
	if (!cr6.eq) goto loc_82CE3514;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3514
	if (!cr0.eq) goto loc_82CE3514;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3514:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE3460) {
	__imp__sub_82CE3460(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3528) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce356c
	if (cr6.eq) goto loc_82CE356C;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce3580
	if (cr6.eq) goto loc_82CE3580;
loc_82CE356C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE3580:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// fmr f2,f31
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f31.f64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lfd f1,3552(r10)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3552);
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// lwz r7,60(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(60) );
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// lbz r8,56(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 56);
	// lbz r6,52(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 52);
	// stfs f0,116(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 116, temp.u32);
	// stb r8,197(r30)
	PPC_STORE_U8(r30.u32 + 197, ctx.r8.u8);
	// addi r11,r30,184
	r11.s64 = r30.s64 + 184;
	// stb r6,196(r30)
	PPC_STORE_U8(r30.u32 + 196, ctx.r6.u8);
	// li r5,16
	ctx.r5.s64 = 16;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lwz r4,264(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(264) );
	// ori r3,r4,1
	ctx.r3.u64 = ctx.r4.u64 | 1;
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// stw r3,264(r30)
	PPC_STORE_U32(r30.u32 + 264, ctx.r3.u32);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfiwx f9,r11,r5
	PPC_STORE_U32(r11.u32 + ctx.r5.u32, ctx.f9.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3630
	if (cr6.eq) goto loc_82CE3630;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3630
	if (!cr6.eq) goto loc_82CE3630;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3630
	if (!cr0.eq) goto loc_82CE3630;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3630:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE3528) {
	__imp__sub_82CE3528(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3640) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3684
	if (cr6.eq) goto loc_82CE3684;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce3698
	if (cr6.eq) goto loc_82CE3698;
loc_82CE3684:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE3698:
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// addi r11,r30,184
	r11.s64 = r30.s64 + 184;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stfs f31,116(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r30.u32 + 116, temp.u32);
	// lwz r9,60(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(60) );
	// li r6,16
	ctx.r6.s64 = 16;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lbz r8,56(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 56);
	// lbz r7,52(r30)
	ctx.r7.u64 = PPC_LOAD_U8(r30.u32 + 52);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stb r7,196(r30)
	PPC_STORE_U8(r30.u32 + 196, ctx.r7.u8);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stb r8,197(r30)
	PPC_STORE_U8(r30.u32 + 197, ctx.r8.u8);
	// lwz r5,264(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + int32_t(264) );
	// ori r4,r5,1
	ctx.r4.u64 = ctx.r5.u64 | 1;
	// fmuls f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * f31.f64));
	// stw r4,264(r30)
	PPC_STORE_U32(r30.u32 + 264, ctx.r4.u32);
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfiwx f10,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.f10.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3734
	if (cr6.eq) goto loc_82CE3734;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3734
	if (!cr6.eq) goto loc_82CE3734;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3734
	if (!cr0.eq) goto loc_82CE3734;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3734:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE3640) {
	__imp__sub_82CE3640(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3748) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3230
	sub_82CE3230(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3748) {
	__imp__sub_82CE3748(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3750) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3400
	sub_82CE3400(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3750) {
	__imp__sub_82CE3750(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3758) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce3200
	sub_82CE3200(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3758) {
	__imp__sub_82CE3758(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3760) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3220
	sub_82CE3220(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3760) {
	__imp__sub_82CE3760(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3768) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3238
	sub_82CE3238(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3768) {
	__imp__sub_82CE3768(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3770) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3460
	sub_82CE3460(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3770) {
	__imp__sub_82CE3770(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3778) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3320
	sub_82CE3320(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3778) {
	__imp__sub_82CE3778(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3780) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce32d0
	sub_82CE32D0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3780) {
	__imp__sub_82CE3780(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3788) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3528
	sub_82CE3528(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3788) {
	__imp__sub_82CE3788(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3790) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3248
	sub_82CE3248(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3790) {
	__imp__sub_82CE3790(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3798) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce0be8
	sub_82CE0BE8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3798) {
	__imp__sub_82CE3798(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE37A0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3228
	sub_82CE3228(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE37A0) {
	__imp__sub_82CE37A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE37A8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3640
	sub_82CE3640(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE37A8) {
	__imp__sub_82CE37A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE37B0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce0ab0
	sub_82CE0AB0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE37B0) {
	__imp__sub_82CE37B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE37B8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3258
	sub_82CE3258(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE37B8) {
	__imp__sub_82CE37B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE37C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3800
	if (cr6.eq) goto loc_82CE3800;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce3814
	if (cr6.eq) goto loc_82CE3814;
loc_82CE3800:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE3814:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// lwz r8,0(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// lhz r7,108(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 108);
	// divwu r6,r8,r7
	ctx.r6.u32 = ctx.r8.u32 / ctx.r7.u32;
	// stw r9,272(r30)
	PPC_STORE_U32(r30.u32 + 272, ctx.r9.u32);
	// twllei r7,0
	// stw r6,120(r30)
	PPC_STORE_U32(r30.u32 + 120, ctx.r6.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3884
	if (cr6.eq) goto loc_82CE3884;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3884
	if (!cr6.eq) goto loc_82CE3884;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3884
	if (!cr0.eq) goto loc_82CE3884;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3884:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE37C0) {
	__imp__sub_82CE37C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3890) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r30,r3,16
	r30.s64 = ctx.r3.s64 + 16;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce38cc
	if (cr6.eq) goto loc_82CE38CC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce38e0
	if (cr6.eq) goto loc_82CE38E0;
loc_82CE38CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE38E0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 156);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ce3904
	if (!cr6.eq) goto loc_82CE3904;
	// li r5,3
	ctx.r5.s64 = 3;
	// li r4,3
	ctx.r4.s64 = 3;
	// b 0x82ce3918
	goto loc_82CE3918;
loc_82CE3904:
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce3930
	if (cr6.eq) goto loc_82CE3930;
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,6
	ctx.r4.s64 = 6;
loc_82CE3918:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE3930:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3978
	if (cr6.eq) goto loc_82CE3978;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3978
	if (!cr6.eq) goto loc_82CE3978;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3978
	if (!cr0.eq) goto loc_82CE3978;
	// li r11,0
	r11.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3978:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE3890) {
	__imp__sub_82CE3890(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3988) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// addi r30,r3,16
	r30.s64 = ctx.r3.s64 + 16;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r29,r13
	r29.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce39c8
	if (cr6.eq) goto loc_82CE39C8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce39dc
	if (cr6.eq) goto loc_82CE39DC;
loc_82CE39C8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE39DC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 156);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce3a68
	if (cr6.eq) goto loc_82CE3A68;
	// clrlwi r9,r27,31
	ctx.r9.u64 = r27.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ce3a48
	if (!cr6.eq) goto loc_82CE3A48;
	// rlwinm r9,r10,0,27,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1E;
	// rlwinm r9,r9,0,30,27
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF3;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ce3a48
	if (!cr6.eq) goto loc_82CE3A48;
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce3a68
	if (!cr6.eq) goto loc_82CE3A68;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lhz r11,19944(r9)
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 19944);
	// sth r11,158(r30)
	PPC_STORE_U16(r30.u32 + 158, r11.u16);
	// b 0x82ce3a64
	goto loc_82CE3A64;
loc_82CE3A48:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,87
	ctx.r4.s64 = 87;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE3A64:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE3A68:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3ab0
	if (cr6.eq) goto loc_82CE3AB0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3ab0
	if (!cr6.eq) goto loc_82CE3AB0;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3ab0
	if (!cr0.eq) goto loc_82CE3AB0;
	// li r11,0
	r11.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3AB0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE3988) {
	__imp__sub_82CE3988(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3AC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r29,r3,16
	r29.s64 = ctx.r3.s64 + 16;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3afc
	if (cr6.eq) goto loc_82CE3AFC;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r8
	cr6.compare<uint32_t>(r30.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce3b14
	if (cr6.eq) goto loc_82CE3B14;
loc_82CE3AFC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
loc_82CE3B14:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r29,4
	ctx.r10.s64 = r29.s64 + 4;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x82ce3b60
	if (cr6.eq) goto loc_82CE3B60;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce3b60
	if (cr6.eq) goto loc_82CE3B60;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// li r5,8
	ctx.r5.s64 = 8;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE3B60:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3ba4
	if (cr6.eq) goto loc_82CE3BA4;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bne cr6,0x82ce3ba4
	if (!cr6.eq) goto loc_82CE3BA4;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3ba4
	if (!cr0.eq) goto loc_82CE3BA4;
	// li r11,0
	r11.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3BA4:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE3AC0) {
	__imp__sub_82CE3AC0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3BB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r11,68(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 68);
	// li r3,0
	ctx.r3.s64 = 0;
	// mulli r11,r11,120
	r11.s64 = r11.s64 * 120;
	// addi r10,r11,276
	ctx.r10.s64 = r11.s64 + 276;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE3BB0) {
	__imp__sub_82CE3BB0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3BC8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3ac0
	sub_82CE3AC0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3BC8) {
	__imp__sub_82CE3BC8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3BD0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3988
	sub_82CE3988(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3BD0) {
	__imp__sub_82CE3BD0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3BD8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-16
	ctx.r3.s64 = ctx.r3.s64 + -16;
	// b 0x82ce3890
	sub_82CE3890(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE3BD8) {
	__imp__sub_82CE3BD8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3BE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// addi r29,r31,4
	r29.s64 = r31.s64 + 4;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82cd9668
	sub_82CD9668(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// addi r30,r31,184
	r30.s64 = r31.s64 + 184;
	// addi r8,r11,20448
	ctx.r8.s64 = r11.s64 + 20448;
	// addi r7,r10,20408
	ctx.r7.s64 = ctx.r10.s64 + 20408;
	// addi r6,r9,20312
	ctx.r6.s64 = ctx.r9.s64 + 20312;
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// li r5,76
	ctx.r5.s64 = 76;
	// stw r7,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r7.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r6,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r6.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r11,260(r31)
	PPC_STORE_U32(r31.u32 + 260, r11.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r11,272(r31)
	PPC_STORE_U32(r31.u32 + 272, r11.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82cd93f8
	sub_82CD93F8(ctx, base);
	// lis r4,0
	ctx.r4.s64 = 0;
	// lfs f0,116(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 116);
	f0.f64 = double(temp.f32);
	// lbz r8,69(r28)
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + 69);
	// ori r11,r4,48000
	r11.u64 = ctx.r4.u64 | 48000;
	// lbz r6,56(r31)
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + 56);
	// lis r3,-32255
	ctx.r3.s64 = -2113863680;
	// li r5,16
	ctx.r5.s64 = 16;
	// addi r10,r3,20568
	ctx.r10.s64 = ctx.r3.s64 + 20568;
	// stw r8,268(r31)
	PPC_STORE_U32(r31.u32 + 268, ctx.r8.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r4,60(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(60) );
	// lwz r8,264(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(264) );
	// ori r7,r8,1
	ctx.r7.u64 = ctx.r8.u64 | 1;
	// std r4,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r4.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// lbz r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U8(r31.u32 + 52);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stb r6,197(r31)
	PPC_STORE_U8(r31.u32 + 197, ctx.r6.u8);
	// rotlwi r6,r7,0
	ctx.r6.u64 = rotl32(ctx.r7.u32, 0);
	// stb r3,196(r31)
	PPC_STORE_U8(r31.u32 + 196, ctx.r3.u8);
	// stw r7,264(r31)
	PPC_STORE_U32(r31.u32 + 264, ctx.r7.u32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfiwx f9,r30,r5
	PPC_STORE_U32(r30.u32 + ctx.r5.u32, ctx.f9.u32);
	// ori r5,r6,2
	ctx.r5.u64 = ctx.r6.u64 | 2;
	// stw r11,216(r31)
	PPC_STORE_U32(r31.u32 + 216, r11.u32);
	// stw r5,264(r31)
	PPC_STORE_U32(r31.u32 + 264, ctx.r5.u32);
	// lbz r4,197(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 197);
	// lbz r3,196(r31)
	ctx.r3.u64 = PPC_LOAD_U8(r31.u32 + 196);
	// rotlwi r11,r3,2
	r11.u64 = rotl32(ctx.r3.u32, 2);
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// mullw r8,r10,r4
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r4.s32);
	// sth r9,110(r31)
	PPC_STORE_U16(r31.u32 + 110, ctx.r9.u16);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// sth r8,108(r31)
	PPC_STORE_U16(r31.u32 + 108, ctx.r8.u16);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE3BE0) {
	__imp__sub_82CE3BE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE3CF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -88, f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r23,0
	r23.s64 = 0;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// mr r25,r23
	r25.u64 = r23.u64;
	// addi r28,r5,-8
	r28.s64 = ctx.r5.s64 + -8;
	// bne cr6,0x82ce3d1c
	if (!cr6.eq) goto loc_82CE3D1C;
	// mr r28,r23
	r28.u64 = r23.u64;
loc_82CE3D1C:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3d48
	if (cr6.eq) goto loc_82CE3D48;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce3d5c
	if (cr6.eq) goto loc_82CE3D5C;
loc_82CE3D48:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE3D5C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82cd7d40
	sub_82CD7D40(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82ce3da0
	if (cr6.lt) goto loc_82CE3DA0;
	// lbz r11,56(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 56);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, r11.u8);
	// bl 0x82cd7eb8
	sub_82CD7EB8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x82ce3e78
	if (!cr6.lt) goto loc_82CE3E78;
loc_82CE3DA0:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce3dec
	if (cr6.eq) goto loc_82CE3DEC;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce3dec
	if (!cr6.eq) goto loc_82CE3DEC;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3dec
	if (!cr0.eq) goto loc_82CE3DEC;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r29,12(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE3DEC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bne cr6,0x82ce43f4
	if (!cr6.eq) goto loc_82CE43F4;
loc_82CE3DF8:
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,6
	ctx.r3.s64 = 6;
	// bl 0x82cdd5d8
	sub_82CDD5D8(ctx, base);
	// li r24,1
	r24.s64 = 1;
loc_82CE3E08:
	// mr r30,r25
	r30.u64 = r25.u64;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x82ce4068
	if (cr6.eq) goto loc_82CE4068;
	// lwz r11,188(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(188) );
	// lwz r10,192(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(192) );
	// subf. r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x82ce4068
	if (!cr0.eq) goto loc_82CE4068;
	// lwz r11,16(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce4048
	if (cr6.eq) goto loc_82CE4048;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x82ce3e40
	if (cr6.eq) goto loc_82CE3E40;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,16(r25)
	PPC_STORE_U32(r25.u32 + 16, r11.u32);
loc_82CE3E40:
	// lwz r11,20(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(20) );
	// stw r11,120(r26)
	PPC_STORE_U32(r26.u32 + 120, r11.u32);
	// lwz r11,180(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(180) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce4068
	if (cr6.eq) goto loc_82CE4068;
	// lwz r10,12(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(12) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r9,36(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(36) );
	// stw r23,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r23.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ce4068
	goto loc_82CE4068;
loc_82CE3E78:
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// li r10,256
	ctx.r10.s64 = 256;
	// stw r23,212(r26)
	PPC_STORE_U32(r26.u32 + 212, r23.u32);
	// addi r30,r26,184
	r30.s64 = r26.s64 + 184;
	// stw r10,208(r26)
	PPC_STORE_U32(r26.u32 + 208, ctx.r10.u32);
	// stw r11,204(r26)
	PPC_STORE_U32(r26.u32 + 204, r11.u32);
	// lbz r11,172(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 172);
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce3fa8
	if (cr6.eq) goto loc_82CE3FA8;
	// rlwinm r10,r11,0,27,27
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce3fa8
	if (!cr6.eq) goto loc_82CE3FA8;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// rlwinm r9,r11,0,30,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lfs f31,3084(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3084);
	f31.f64 = double(temp.f32);
	// beq cr6,0x82ce3f0c
	if (cr6.eq) goto loc_82CE3F0C;
	// lfs f0,112(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 112);
	f0.f64 = double(temp.f32);
	// addi r11,r30,52
	r11.s64 = r30.s64 + 52;
	// stfs f0,224(r26)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r26.u32 + 224, temp.u32);
	// mr r9,r23
	ctx.r9.u64 = r23.u64;
	// lfs f13,40(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// li r10,6
	ctx.r10.s64 = 6;
	// stfs f13,36(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 36, temp.u32);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82CE3EE0:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82ce3ee0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82CE3EE0;
	// stfs f31,48(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r30.u32 + 48, temp.u32);
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(16) );
	// addi r3,r26,16
	ctx.r3.s64 = r26.s64 + 16;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE3F0C:
	// lbz r11,172(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 172);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce3f58
	if (cr6.eq) goto loc_82CE3F58;
	// lhz r10,174(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 174);
	// lfs f13,224(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 224);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r26,16
	r11.s64 = r26.s64 + 16;
	// addis r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 65536;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// clrlwi r8,r9,16
	ctx.r8.u64 = ctx.r9.u32 & 0xFFFF;
	// sth r8,174(r26)
	PPC_STORE_U16(r26.u32 + 174, ctx.r8.u16);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82ce3f50
	if (cr6.eq) goto loc_82CE3F50;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f0,19948(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19948);
	f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x82ce3f54
	goto loc_82CE3F54;
loc_82CE3F50:
	// fmr f0,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64;
loc_82CE3F54:
	// stfs f0,224(r26)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r26.u32 + 224, temp.u32);
loc_82CE3F58:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce3df8
	if (cr6.eq) goto loc_82CE3DF8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce3df8
	if (!cr6.eq) goto loc_82CE3DF8;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3df8
	if (!cr0.eq) goto loc_82CE3DF8;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// b 0x82ce3df8
	goto loc_82CE3DF8;
loc_82CE3FA8:
	// lwz r11,28(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// lbz r29,13(r30)
	r29.u64 = PPC_LOAD_U8(r30.u32 + 13);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82ce3fec
	if (cr0.eq) goto loc_82CE3FEC;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ce3fec
	if (cr6.eq) goto loc_82CE3FEC;
	// rlwinm r27,r10,2,0,29
	r27.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r11,2,0,29
	r28.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82CE3FCC:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r28,r11
	ctx.r3.u64 = r28.u64 + r11.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r28,r28,1024
	r28.s64 = r28.s64 + 1024;
	// bne 0x82ce3fcc
	if (!cr0.eq) goto loc_82CE3FCC;
loc_82CE3FEC:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce4038
	if (cr6.eq) goto loc_82CE4038;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce4038
	if (!cr6.eq) goto loc_82CE4038;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce4038
	if (!cr0.eq) goto loc_82CE4038;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE4038:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x82ca2c24
	return;
loc_82CE4048:
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(16) );
	// addi r3,r26,16
	ctx.r3.s64 = r26.s64 + 16;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r30,r23
	r30.u64 = r23.u64;
loc_82CE4068:
	// lwz r11,208(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(208) );
	// addi r28,r26,184
	r28.s64 = r26.s64 + 184;
	// lwz r10,212(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(212) );
	// subf. r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x82ce426c
	if (cr0.eq) goto loc_82CE426C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82ce416c
	if (!cr6.eq) goto loc_82CE416C;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce40a8
	if (cr6.eq) goto loc_82CE40A8;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r8
	cr6.compare<uint32_t>(r30.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce40c0
	if (cr6.eq) goto loc_82CE40C0;
loc_82CE40A8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
loc_82CE40C0:
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r11,r26,20
	r11.s64 = r26.s64 + 20;
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// lwz r10,20(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(20) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce40e4
	if (cr6.eq) goto loc_82CE40E4;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// b 0x82ce40e8
	goto loc_82CE40E8;
loc_82CE40E4:
	// mr r11,r23
	r11.u64 = r23.u64;
loc_82CE40E8:
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce4128
	if (cr6.eq) goto loc_82CE4128;
	// lbz r10,117(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 117);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce4128
	if (!cr6.eq) goto loc_82CE4128;
	// lwz r10,272(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(272) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce4114
	if (cr6.eq) goto loc_82CE4114;
	// stw r23,272(r26)
	PPC_STORE_U32(r26.u32 + 272, r23.u32);
	// b 0x82ce411c
	goto loc_82CE411C;
loc_82CE4114:
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// stw r10,120(r26)
	PPC_STORE_U32(r26.u32 + 120, ctx.r10.u32);
loc_82CE411C:
	// stb r24,117(r11)
	PPC_STORE_U8(r11.u32 + 117, r24.u8);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
loc_82CE4128:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce416c
	if (cr6.eq) goto loc_82CE416C;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82ce416c
	if (!cr6.eq) goto loc_82CE416C;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce416c
	if (!cr0.eq) goto loc_82CE416C;
	// mr r11,r23
	r11.u64 = r23.u64;
	// lbz r29,12(r31)
	r29.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE416C:
	// mr r25,r30
	r25.u64 = r30.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce426c
	if (cr6.eq) goto loc_82CE426C;
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce418c
	if (cr6.eq) goto loc_82CE418C;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// b 0x82ce4190
	goto loc_82CE4190;
loc_82CE418C:
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
loc_82CE4190:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// lwz r27,120(r26)
	r27.u64 = PPC_LOAD_U32(r26.u32 + int32_t(120) );
	// stw r11,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r11.u32);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// bge cr6,0x82ce41ac
	if (!cr6.lt) goto loc_82CE41AC;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_82CE41AC:
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82ce7730
	sub_82CE7730(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce41e0
	if (cr6.eq) goto loc_82CE41E0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce41f4
	if (cr6.eq) goto loc_82CE41F4;
loc_82CE41E0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE41F4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r11,120(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(120) );
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bne cr6,0x82ce4210
	if (!cr6.eq) goto loc_82CE4210;
	// add r11,r28,r27
	r11.u64 = r28.u64 + r27.u64;
	// stw r11,120(r26)
	PPC_STORE_U32(r26.u32 + 120, r11.u32);
loc_82CE4210:
	// lwz r11,168(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(168) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// stw r11,168(r26)
	PPC_STORE_U32(r26.u32 + 168, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce3e08
	if (cr6.eq) goto loc_82CE3E08;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce3e08
	if (!cr6.eq) goto loc_82CE3E08;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce3e08
	if (!cr0.eq) goto loc_82CE3E08;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// b 0x82ce3e08
	goto loc_82CE3E08;
loc_82CE426C:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce4290
	if (cr6.eq) goto loc_82CE4290;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce42a4
	if (cr6.eq) goto loc_82CE42A4;
loc_82CE4290:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE42A4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r30,r26,184
	r30.s64 = r26.s64 + 184;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lfs f0,224(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 224);
	f0.f64 = double(temp.f32);
	// stfs f0,220(r26)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r26.u32 + 220, temp.u32);
	// lbz r29,197(r26)
	r29.u64 = PPC_LOAD_U8(r26.u32 + 197);
	// lwz r11,212(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(212) );
	// lwz r10,208(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(208) );
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82ce4364
	if (cr0.eq) goto loc_82CE4364;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ce42fc
	if (cr6.eq) goto loc_82CE42FC;
	// rlwinm r27,r10,2,0,29
	r27.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r11,2,0,29
	r28.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
loc_82CE42DC:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r28,r11
	ctx.r3.u64 = r28.u64 + r11.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r28,r28,1024
	r28.s64 = r28.s64 + 1024;
	// bne 0x82ce42dc
	if (!cr0.eq) goto loc_82CE42DC;
loc_82CE42FC:
	// lbz r11,172(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 172);
	// rlwinm r10,r11,0,28,28
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce434c
	if (!cr6.eq) goto loc_82CE434C;
	// lwz r10,20(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(20) );
	// addi r11,r26,20
	r11.s64 = r26.s64 + 20;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce432c
	if (cr6.eq) goto loc_82CE432C;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce4364
	if (!cr6.eq) goto loc_82CE4364;
loc_82CE432C:
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(16) );
	// addi r3,r26,16
	ctx.r3.s64 = r26.s64 + 16;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,16
	ctx.r4.s64 = 16;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ce4364
	goto loc_82CE4364;
loc_82CE434C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE4364:
	// lbz r11,172(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 172);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce43a4
	if (cr6.eq) goto loc_82CE43A4;
	// lhz r10,174(r26)
	ctx.r10.u64 = PPC_LOAD_U16(r26.u32 + 174);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce438c
	if (cr6.eq) goto loc_82CE438C;
	// rlwinm r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce43a4
	if (cr6.eq) goto loc_82CE43A4;
loc_82CE438C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE43A4:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce43f0
	if (cr6.eq) goto loc_82CE43F0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce43f0
	if (!cr6.eq) goto loc_82CE43F0;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce43f0
	if (!cr0.eq) goto loc_82CE43F0;
	// mr r10,r23
	ctx.r10.u64 = r23.u64;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// mr r11,r23
	r11.u64 = r23.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE43F0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82CE43F4:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x82ca2c24
	return;
}

PPC_WEAK_FUNC(sub_82CE3CF0) {
	__imp__sub_82CE3CF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4400) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// li r4,276
	ctx.r4.s64 = 276;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce4460
	if (cr6.eq) goto loc_82CE4460;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82ce3be0
	sub_82CE3BE0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce4460
	if (cr6.eq) goto loc_82CE4460;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82CE4460:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE4400) {
	__imp__sub_82CE4400(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4470) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r10,r11,20584
	ctx.r10.s64 = r11.s64 + 20584;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82ce62c0
	sub_82CE62C0(ctx, base);
	// lwz r11,68(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(68) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce44c4
	if (cr6.eq) goto loc_82CE44C4;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r8,68(r31)
	PPC_STORE_U32(r31.u32 + 68, ctx.r8.u32);
loc_82CE44C4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce5ba0
	sub_82CE5BA0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4470) {
	__imp__sub_82CE4470(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE44E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82ce67b8
	sub_82CE67B8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce4524
	if (cr6.lt) goto loc_82CE4524;
	// lwz r3,68(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(68) );
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE4524:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE44E0) {
	__imp__sub_82CE44E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4540) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lwz r3,28900(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28900) );
	// b 0x82cdb3c0
	sub_82CDB3C0(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE4540) {
	__imp__sub_82CE4540(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4550) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x82ce45b4
	if (cr6.lt) goto loc_82CE45B4;
	// beq cr6,0x82ce45a0
	if (cr6.eq) goto loc_82CE45A0;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ce45b8
	goto loc_82CE45B8;
loc_82CE45A0:
	// lbz r11,76(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 76);
	// lwz r10,124(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(124) );
	// mulli r11,r11,44
	r11.s64 = r11.s64 * 44;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82ce45b8
	goto loc_82CE45B8;
loc_82CE45B4:
	// addi r3,r30,80
	ctx.r3.s64 = r30.s64 + 80;
loc_82CE45B8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4550) {
	__imp__sub_82CE4550(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE45D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ce4470
	sub_82CE4470(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE45D0) {
	__imp__sub_82CE45D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4600) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// li r30,0
	r30.s64 = 0;
	// lis r10,0
	ctx.r10.s64 = 0;
	// li r9,6
	ctx.r9.s64 = 6;
	// ori r8,r10,48000
	ctx.r8.u64 = ctx.r10.u64 | 48000;
	// std r30,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r30.u64);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// std r30,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r30.u64);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// std r30,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r30.u64);
	// stb r30,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r30.u8);
	// stb r9,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r9.u8);
	// stw r8,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r8.u32);
	// beq cr6,0x82ce4664
	if (cr6.eq) goto loc_82CE4664;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// lwz r9,8(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
loc_82CE4664:
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce6680
	sub_82CE6680(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce4710
	if (cr6.lt) goto loc_82CE4710;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r10,2
	ctx.r10.s64 = 2;
	// li r4,28
	ctx.r4.s64 = 28;
	// std r30,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r30.u64);
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
	// stb r10,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r10.u8);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(20) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce46c0
	if (cr6.eq) goto loc_82CE46C0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// bl 0x82cd8390
	sub_82CD8390(ctx, base);
	// b 0x82ce46c4
	goto loc_82CE46C4;
loc_82CE46C0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_82CE46C4:
	// stw r3,68(r31)
	PPC_STORE_U32(r31.u32 + 68, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82ce46dc
	if (!cr6.eq) goto loc_82CE46DC;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// b 0x82ce4710
	goto loc_82CE4710;
loc_82CE46DC:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce4710
	if (cr6.lt) goto loc_82CE4710;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,56(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(56) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE4710:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4600) {
	__imp__sub_82CE4600(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4728) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x82ce59d0
	sub_82CE59D0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// blt cr6,0x82ce4760
	if (cr6.lt) goto loc_82CE4760;
	// li r5,3
	ctx.r5.s64 = 3;
	// lwz r3,32(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82cd7fe0
	sub_82CD7FE0(ctx, base);
loc_82CE4760:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4728) {
	__imp__sub_82CE4728(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4780) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lbz r11,61(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 61);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce47bc
	if (cr6.eq) goto loc_82CE47BC;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82CE47BC:
	// li r5,3
	ctx.r5.s64 = 3;
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82cd7fe0
	sub_82CD7FE0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce6098
	sub_82CE6098(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4780) {
	__imp__sub_82CE4780(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE47E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce4824
	if (cr6.eq) goto loc_82CE4824;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r8
	cr6.compare<uint32_t>(r30.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce483c
	if (cr6.eq) goto loc_82CE483C;
loc_82CE4824:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
loc_82CE483C:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r28,24
	r11.s64 = r28.s64 + 24;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// lwz r9,24(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(24) );
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x82ce4878
	if (cr6.eq) goto loc_82CE4878;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE4878:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce48bc
	if (cr6.eq) goto loc_82CE48BC;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82ce48bc
	if (!cr6.eq) goto loc_82CE48BC;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce48bc
	if (!cr0.eq) goto loc_82CE48BC;
	// li r11,0
	r11.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE48BC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE47E8) {
	__imp__sub_82CE47E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE48C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// li r11,0
	r11.s64 = 0;
	// lis r9,0
	ctx.r9.s64 = 0;
	// li r8,6
	ctx.r8.s64 = 6;
	// ori r7,r9,48000
	ctx.r7.u64 = ctx.r9.u64 | 48000;
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// std r11,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, r11.u64);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// std r11,16(r10)
	PPC_STORE_U64(ctx.r10.u32 + 16, r11.u64);
	// stb r11,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r11.u8);
	// stb r8,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r8.u8);
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// beq cr6,0x82ce4928
	if (cr6.eq) goto loc_82CE4928;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
loc_82CE4928:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ce5ab8
	sub_82CE5AB8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce4948
	if (cr6.lt) goto loc_82CE4948;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// addi r10,r11,28
	ctx.r10.s64 = r11.s64 + 28;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
loc_82CE4948:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE48C8) {
	__imp__sub_82CE48C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4960) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r3,28900(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28900) );
	// bl 0x82ce4550
	sub_82CE4550(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ce4a44
	if (cr6.eq) goto loc_82CE4A44;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce49b8
	if (cr6.eq) goto loc_82CE49B8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce49cc
	if (cr6.eq) goto loc_82CE49CC;
loc_82CE49B8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE49CC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r9,40(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(40) );
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// add r11,r11,r28
	r11.u64 = r11.u64 + r28.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce4a44
	if (cr6.eq) goto loc_82CE4A44;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce4a44
	if (!cr6.eq) goto loc_82CE4A44;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce4a44
	if (!cr0.eq) goto loc_82CE4A44;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE4A44:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE4960) {
	__imp__sub_82CE4960(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4A50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82ce48c8
	sub_82CE48C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce4a88
	if (cr6.lt) goto loc_82CE4A88;
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,72
	ctx.r4.s64 = r11.s64 + 72;
	// b 0x82ce4a8c
	goto loc_82CE4A8C;
loc_82CE4A88:
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
loc_82CE4A8C:
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce4b30
	if (cr6.lt) goto loc_82CE4B30;
	// lis r3,24962
	ctx.r3.s64 = 1635909632;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// ori r3,r3,6
	ctx.r3.u64 = ctx.r3.u64 | 6;
	// bl 0x82ce4fe0
	sub_82CE4FE0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82ce4b30
	if (cr6.lt) goto loc_82CE4B30;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// li r4,72
	ctx.r4.s64 = 72;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ce4b10
	if (cr6.eq) goto loc_82CE4B10;
	// li r5,2
	ctx.r5.s64 = 2;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// bl 0x82ce5e08
	sub_82CE5E08(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r10,r11,20584
	ctx.r10.s64 = r11.s64 + 20584;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82ce4600
	sub_82CE4600(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// blt cr6,0x82ce4b1c
	if (cr6.lt) goto loc_82CE4B1C;
	// stw r31,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r31.u32);
	// b 0x82ce4b30
	goto loc_82CE4B30;
loc_82CE4B10:
	// lis r30,-32761
	r30.s64 = -2147024896;
	// ori r30,r30,14
	r30.u64 = r30.u64 | 14;
	// b 0x82ce4b30
	goto loc_82CE4B30;
loc_82CE4B1C:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE4B30:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce4b4c
	if (cr6.eq) goto loc_82CE4B4C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE4B4C:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE4A50) {
	__imp__sub_82CE4A50(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4B58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// addi r11,r4,3
	r11.s64 = ctx.r4.s64 + 3;
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// rlwinm r11,r11,0,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4B58) {
	__imp__sub_82CE4B58(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4B70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82ce4bac
	if (!cr6.eq) goto loc_82CE4BAC;
loc_82CE4B94:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82CE4BAC:
	// stw r5,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r5.u32);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// bl 0x82cd9fc8
	sub_82CD9FC8(ctx, base);
	// stw r3,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82ce4b94
	if (!cr6.eq) goto loc_82CE4B94;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4B70) {
	__imp__sub_82CE4B70(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4BE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r10,r4,3
	ctx.r10.s64 = ctx.r4.s64 + 3;
	// rlwinm r30,r10,0,0,29
	r30.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplw cr6,r30,r3
	cr6.compare<uint32_t>(r30.u32, ctx.r3.u32, xer);
	// ble cr6,0x82ce4c28
	if (!cr6.gt) goto loc_82CE4C28;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ce4c3c
	goto loc_82CE4C3C;
loc_82CE4C28:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// stw r9,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r9.u32);
loc_82CE4C3C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4BE8) {
	__imp__sub_82CE4BE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4C58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addi r9,r11,20732
	ctx.r9.s64 = r11.s64 + 20732;
	// addi r8,r10,20672
	ctx.r8.s64 = ctx.r10.s64 + 20672;
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// addi r30,r31,4
	r30.s64 = r31.s64 + 4;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// stw r8,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r8.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82ce4cb0
	if (cr6.eq) goto loc_82CE4CB0;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lwz r5,12(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
loc_82CE4CB0:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r10,r11,15792
	ctx.r10.s64 = r11.s64 + 15792;
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4C58) {
	__imp__sub_82CE4C58(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4CD8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce4d68
	sub_82CE4D68(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE4CD8) {
	__imp__sub_82CE4CD8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4CE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4CE0) {
	__imp__sub_82CE4CE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4CF0) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce4dc8
	sub_82CE4DC8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE4CF0) {
	__imp__sub_82CE4CF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4CF8) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-4
	ctx.r3.s64 = ctx.r3.s64 + -4;
	// b 0x82ce4e40
	sub_82CE4E40(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82CE4CF8) {
	__imp__sub_82CE4CF8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4D00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r31,r3,-4
	r31.s64 = ctx.r3.s64 + -4;
	// lwz r30,8(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ce4d4c
	if (cr6.eq) goto loc_82CE4D4C;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
loc_82CE4D4C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4D00) {
	__imp__sub_82CE4D00(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4D68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82ce4c58
	sub_82CE4C58(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce4da8
	if (cr6.eq) goto loc_82CE4DA8;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lis r5,24962
	ctx.r5.s64 = 1635909632;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
loc_82CE4DA8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4D68) {
	__imp__sub_82CE4D68(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4DC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addi r9,r11,20732
	ctx.r9.s64 = r11.s64 + 20732;
	// addi r8,r10,20692
	ctx.r8.s64 = ctx.r10.s64 + 20692;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r8,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r8.u32);
	// bl 0x82ce4c58
	sub_82CE4C58(ctx, base);
	// clrlwi r7,r30,31
	ctx.r7.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ce4e20
	if (cr6.eq) goto loc_82CE4E20;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
loc_82CE4E20:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4DC8) {
	__imp__sub_82CE4DC8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4E40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// addi r9,r11,20732
	ctx.r9.s64 = r11.s64 + 20732;
	// addi r8,r10,20712
	ctx.r8.s64 = ctx.r10.s64 + 20712;
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// stw r8,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r8.u32);
	// stw r7,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r7.u32);
	// bl 0x82ce4c58
	sub_82CE4C58(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4E40) {
	__imp__sub_82CE4E40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4E90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// li r4,28
	ctx.r4.s64 = 28;
	// bl 0x82cd9fc8
	sub_82CD9FC8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce4f24
	if (cr6.eq) goto loc_82CE4F24;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r7,r10,15792
	ctx.r7.s64 = ctx.r10.s64 + 15792;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// addi r5,r9,20732
	ctx.r5.s64 = ctx.r9.s64 + 20732;
	// stw r6,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r6.u32);
	// addi r4,r8,20692
	ctx.r4.s64 = ctx.r8.s64 + 20692;
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r10,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r10.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// stw r4,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r4.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82CE4F24:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE4E90) {
	__imp__sub_82CE4E90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4F40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// addi r4,r30,28
	ctx.r4.s64 = r30.s64 + 28;
	// bl 0x82cd9fc8
	sub_82CD9FC8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce4f88
	if (!cr6.eq) goto loc_82CE4F88;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82CE4F88:
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r7,r10,15792
	ctx.r7.s64 = ctx.r10.s64 + 15792;
	// addi r5,r9,20732
	ctx.r5.s64 = ctx.r9.s64 + 20732;
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r8,20712
	ctx.r4.s64 = ctx.r8.s64 + 20712;
	// stw r6,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r6.u32);
	// addi r9,r11,28
	ctx.r9.s64 = r11.s64 + 28;
	// stw r3,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r3.u32);
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r4,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r4.u32);
	// stw r31,12(r11)
	PPC_STORE_U32(r11.u32 + 12, r31.u32);
	// stw r30,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r30.u32);
	// stw r9,24(r11)
	PPC_STORE_U32(r11.u32 + 24, ctx.r9.u32);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE4F40) {
	__imp__sub_82CE4F40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE4FE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r31,0(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82ce5040
	if (!cr6.eq) goto loc_82CE5040;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// beq cr6,0x82ce5020
	if (cr6.eq) goto loc_82CE5020;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// bl 0x82ce4f40
	sub_82CE4F40(ctx, base);
	// b 0x82ce5028
	goto loc_82CE5028;
loc_82CE5020:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ce4e90
	sub_82CE4E90(ctx, base);
loc_82CE5028:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5084
	if (cr6.lt) goto loc_82CE5084;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82CE5040:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,12(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(12) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE5084:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE4FE0) {
	__imp__sub_82CE4FE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5090) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ce5e08
	sub_82CE5E08(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r10,r11,20760
	ctx.r10.s64 = r11.s64 + 20760;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5090) {
	__imp__sub_82CE5090(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE50D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// bl 0x82ce5aa0
	sub_82CE5AA0(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE50D0) {
	__imp__sub_82CE50D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5120) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ce5ab8
	sub_82CE5AB8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce51bc
	if (cr6.lt) goto loc_82CE51BC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lbz r9,25(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 25);
	// li r11,0
	r11.s64 = 0;
	// li r8,1
	ctx.r8.s64 = 1;
	// lis r7,-31949
	ctx.r7.s64 = -2093809664;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// std r11,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, r11.u64);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// stb r11,105(r1)
	PPC_STORE_U8(ctx.r1.u32 + 105, r11.u8);
	// stb r9,104(r1)
	PPC_STORE_U8(ctx.r1.u32 + 104, ctx.r9.u8);
	// stb r8,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r8.u8);
	// lwz r11,28900(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(28900) );
	// lwz r3,60(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(60) );
	// bl 0x82cdba80
	sub_82CDBA80(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce51bc
	if (cr6.lt) goto loc_82CE51BC;
	// lbz r11,24(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 24);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// rotlwi r9,r11,1
	ctx.r9.u64 = rotl32(r11.u32, 1);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r11,r7,2,0,29
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// stw r6,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r6.u32);
loc_82CE51BC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5120) {
	__imp__sub_82CE5120(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE51D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lbz r11,24(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 24);
	// stb r11,68(r31)
	PPC_STORE_U8(r31.u32 + 68, r11.u8);
	// bl 0x82ce6680
	sub_82CE6680(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce52d4
	if (cr6.lt) goto loc_82CE52D4;
	// lbz r11,68(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 68);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5240
	if (cr6.eq) goto loc_82CE5240;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(20) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,72(r31)
	PPC_STORE_U32(r31.u32 + 72, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce52dc
	if (cr6.eq) goto loc_82CE52DC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_82CE5240:
	// lbz r11,68(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 68);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce52b4
	if (cr6.eq) goto loc_82CE52B4;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lbz r9,25(r28)
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + 25);
	// li r8,1
	ctx.r8.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// std r30,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r30.u64);
	// std r30,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, r30.u64);
	// stb r9,88(r1)
	PPC_STORE_U8(ctx.r1.u32 + 88, ctx.r9.u8);
	// stb r8,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r8.u8);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stb r30,89(r1)
	PPC_STORE_U8(ctx.r1.u32 + 89, r30.u8);
	// beq cr6,0x82ce52b4
	if (cr6.eq) goto loc_82CE52B4;
	// mr r29,r30
	r29.u64 = r30.u64;
loc_82CE527C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce52d4
	if (cr6.lt) goto loc_82CE52D4;
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(72) );
	// li r6,255
	ctx.r6.s64 = 255;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,4
	ctx.r4.s64 = r11.s64 + 4;
	// bl 0x82ce5c70
	sub_82CE5C70(ctx, base);
	// lbz r11,68(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 68);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r29,r29,12
	r29.s64 = r29.s64 + 12;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x82ce527c
	if (cr6.lt) goto loc_82CE527C;
loc_82CE52B4:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce52d4
	if (cr6.lt) goto loc_82CE52D4;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,28(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE52D4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
loc_82CE52DC:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE51D8) {
	__imp__sub_82CE51D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE52F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82ce5318
	if (!cr6.eq) goto loc_82CE5318;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r31,r11,-21952
	r31.s64 = r11.s64 + -21952;
loc_82CE5318:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce532c
	if (cr6.eq) goto loc_82CE532C;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// b 0x82ce533c
	goto loc_82CE533C;
loc_82CE532C:
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lwz r11,28900(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28900) );
	// lwz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
loc_82CE533C:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce5390
	if (!cr6.eq) goto loc_82CE5390;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce53a8
	if (cr6.lt) goto loc_82CE53A8;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lbz r4,81(r1)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// lbz r3,53(r29)
	ctx.r3.u64 = PPC_LOAD_U8(r29.u32 + 53);
	// bl 0x82cd2660
	sub_82CD2660(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82CE5390:
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE53A8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE52F0) {
	__imp__sub_82CE52F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE53B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// mr r29,r28
	r29.u64 = r28.u64;
	// lbz r11,69(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 69);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5404
	if (cr6.eq) goto loc_82CE5404;
	// mr r31,r28
	r31.u64 = r28.u64;
loc_82CE53D8:
	// lwz r11,72(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(72) );
	// lwzx r3,r11,r31
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r8,69(r30)
	ctx.r8.u64 = PPC_LOAD_U8(r30.u32 + 69);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// cmplw cr6,r29,r8
	cr6.compare<uint32_t>(r29.u32, ctx.r8.u32, xer);
	// blt cr6,0x82ce53d8
	if (cr6.lt) goto loc_82CE53D8;
loc_82CE5404:
	// stb r28,69(r30)
	PPC_STORE_U8(r30.u32 + 69, r28.u8);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE53B0) {
	__imp__sub_82CE53B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5410) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r31,0
	r31.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r11,61(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// not r9,r11
	ctx.r9.u64 = ~r11.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// clrlwi r27,r9,31
	r27.u64 = ctx.r9.u32 & 0x1;
	// beq cr6,0x82ce54a0
	if (cr6.eq) goto loc_82CE54A0;
	// li r30,0
	r30.s64 = 0;
loc_82CE5448:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce54a0
	if (cr6.lt) goto loc_82CE54A0;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r9,72(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(72) );
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// rotlwi r11,r10,1
	r11.u64 = rotl32(ctx.r10.u32, 1);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// lwz r3,4(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,56(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(56) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplw cr6,r31,r7
	cr6.compare<uint32_t>(r31.u32, ctx.r7.u32, xer);
	// blt cr6,0x82ce5448
	if (cr6.lt) goto loc_82CE5448;
loc_82CE54A0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE5410) {
	__imp__sub_82CE5410(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE54A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r10,r11,20760
	ctx.r10.s64 = r11.s64 + 20760;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82ce62c0
	sub_82CE62C0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce53b0
	sub_82CE53B0(ctx, base);
	// lbz r9,68(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 68);
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mr r29,r28
	r29.u64 = r28.u64;
	// beq cr6,0x82ce5534
	if (cr6.eq) goto loc_82CE5534;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_82CE54EC:
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(72) );
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce5520
	if (cr6.eq) goto loc_82CE5520;
	// rotlwi r3,r10,0
	ctx.r3.u64 = rotl32(ctx.r10.u32, 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(72) );
	// add r9,r11,r30
	ctx.r9.u64 = r11.u64 + r30.u64;
	// stw r28,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r28.u32);
loc_82CE5520:
	// lbz r11,68(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 68);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r30,r30,12
	r30.s64 = r30.s64 + 12;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x82ce54ec
	if (cr6.lt) goto loc_82CE54EC;
loc_82CE5534:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ce5ba0
	sub_82CE5BA0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE54A8) {
	__imp__sub_82CE54A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r26,0
	r26.s64 = 0;
	// bl 0x82ce53b0
	sub_82CE53B0(ctx, base);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ce55dc
	if (cr6.eq) goto loc_82CE55DC;
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce55c8
	if (cr6.eq) goto loc_82CE55C8;
	// li r30,0
	r30.s64 = 0;
	// li r31,0
	r31.s64 = 0;
loc_82CE5584:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x82ce55d0
	if (cr6.lt) goto loc_82CE55D0;
	// lwz r10,72(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(72) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// add r5,r10,r31
	ctx.r5.u64 = ctx.r10.u64 + r31.u64;
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// bl 0x82ce52f0
	sub_82CE52F0(ctx, base);
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x82ce5584
	if (cr6.lt) goto loc_82CE5584;
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x82ce55d0
	if (cr6.lt) goto loc_82CE55D0;
loc_82CE55C8:
	// lbz r11,0(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// stb r11,69(r28)
	PPC_STORE_U8(r28.u32 + 69, r11.u8);
loc_82CE55D0:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
loc_82CE55D4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_82CE55DC:
	// lbz r11,68(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 68);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce55d0
	if (cr6.eq) goto loc_82CE55D0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,72(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + int32_t(72) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82ce52f0
	sub_82CE52F0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce55d4
	if (cr6.lt) goto loc_82CE55D4;
	// li r11,1
	r11.s64 = 1;
	// stb r11,69(r28)
	PPC_STORE_U8(r28.u32 + 69, r11.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE5548) {
	__imp__sub_82CE5548(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5610) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// bl 0x82ce67b8
	sub_82CE67B8(ctx, base);
	// lbz r11,69(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 69);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5698
	if (cr6.eq) goto loc_82CE5698;
	// li r31,0
	r31.s64 = 0;
loc_82CE563C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5698
	if (cr6.lt) goto loc_82CE5698;
	// lwz r11,72(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(72) );
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// lwz r8,72(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(72) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,72(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(72) );
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// add r7,r11,r31
	ctx.r7.u64 = r11.u64 + r31.u64;
	// lwz r3,4(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(4) );
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r11,24(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(24) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r10,69(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 69);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce563c
	if (cr6.lt) goto loc_82CE563C;
loc_82CE5698:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE5610) {
	__imp__sub_82CE5610(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE56A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce56dc
	if (cr6.eq) goto loc_82CE56DC;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r8
	cr6.compare<uint32_t>(r30.u32, ctx.r8.u32, xer);
	// beq cr6,0x82ce56f4
	if (cr6.eq) goto loc_82CE56F4;
loc_82CE56DC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
loc_82CE56F4:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r28,24
	r11.s64 = r28.s64 + 24;
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// lwz r9,24(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(24) );
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// beq cr6,0x82ce5730
	if (cr6.eq) goto loc_82CE5730;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r9,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r9.u32);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE5730:
	// mr r11,r13
	r11.u64 = ctx.r13.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82ce5774
	if (cr6.eq) goto loc_82CE5774;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82ce5774
	if (!cr6.eq) goto loc_82CE5774;
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce5774
	if (!cr0.eq) goto loc_82CE5774;
	// li r11,0
	r11.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE5774:
	// lbz r11,69(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 69);
	// li r3,0
	ctx.r3.s64 = 0;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce57dc
	if (cr6.eq) goto loc_82CE57DC;
	// li r31,0
	r31.s64 = 0;
loc_82CE578C:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce57dc
	if (cr6.lt) goto loc_82CE57DC;
	// lwz r11,72(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(72) );
	// lwzx r3,r11,r31
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// lwz r10,40(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(40) );
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// beq 0x82ce57b4
	if (cr0.eq) goto loc_82CE57B4;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ce57c8
	goto loc_82CE57C8;
loc_82CE57B4:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(60) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE57C8:
	// lbz r11,69(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 69);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x82ce578c
	if (cr6.lt) goto loc_82CE578C;
loc_82CE57DC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE56A0) {
	__imp__sub_82CE56A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE57E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82ce54a8
	sub_82CE54A8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5828
	if (cr6.eq) goto loc_82CE5828;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lis r5,24962
	ctx.r5.s64 = 1635909632;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
loc_82CE5828:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE57E8) {
	__imp__sub_82CE57E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5848) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r31,0
	r31.s64 = 0;
	// lbz r11,69(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 69);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce58b8
	if (cr6.eq) goto loc_82CE58B8;
	// li r30,0
	r30.s64 = 0;
loc_82CE5870:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce59c4
	if (cr6.lt) goto loc_82CE59C4;
	// lwz r11,72(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(72) );
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(40) );
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// stw r10,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, ctx.r10.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,56(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(56) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r7,69(r29)
	ctx.r7.u64 = PPC_LOAD_U8(r29.u32 + 69);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,12
	r30.s64 = r30.s64 + 12;
	// cmplw cr6,r31,r7
	cr6.compare<uint32_t>(r31.u32, ctx.r7.u32, xer);
	// blt cr6,0x82ce5870
	if (cr6.lt) goto loc_82CE5870;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce59c4
	if (cr6.lt) goto loc_82CE59C4;
loc_82CE58B8:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// lis r10,-31949
	ctx.r10.s64 = -2093809664;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r31,28900(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(28900) );
	// lwz r9,52(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x82ce58fc
	if (cr6.lt) goto loc_82CE58FC;
	// bne cr6,0x82ce59c0
	if (!cr6.eq) goto loc_82CE59C0;
	// lbz r10,76(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 76);
	// lwz r11,124(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(124) );
	// mulli r10,r10,44
	ctx.r10.s64 = ctx.r10.s64 * 44;
	// add r27,r10,r11
	r27.u64 = ctx.r10.u64 + r11.u64;
	// b 0x82ce5900
	goto loc_82CE5900;
loc_82CE58FC:
	// addi r27,r31,80
	r27.s64 = r31.s64 + 80;
loc_82CE5900:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ce59c0
	if (cr6.eq) goto loc_82CE59C0;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce5934
	if (cr6.eq) goto loc_82CE5934;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce5948
	if (cr6.eq) goto loc_82CE5948;
loc_82CE5934:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
loc_82CE5948:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lwz r9,40(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(40) );
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce59c0
	if (cr6.eq) goto loc_82CE59C0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce59c0
	if (!cr6.eq) goto loc_82CE59C0;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce59c0
	if (!cr0.eq) goto loc_82CE59C0;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE59C0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82CE59C4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82CE5848) {
	__imp__sub_82CE5848(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE59D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(72) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(44) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5a20
	if (cr6.eq) goto loc_82CE5A20;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE5A20:
	// lbz r11,61(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 61);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce5a48
	if (cr6.eq) goto loc_82CE5A48;
	// lhz r11,64(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 64);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5a84
	if (cr6.eq) goto loc_82CE5A84;
	// addis r11,r11,1
	r11.s64 = r11.s64 + 65536;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// sth r11,64(r31)
	PPC_STORE_U16(r31.u32 + 64, r11.u16);
loc_82CE5A48:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(84) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE5A60:
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82CE5A84:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(60) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ce5a60
	goto loc_82CE5A60;
}

PPC_WEAK_FUNC(sub_82CE59D0) {
	__imp__sub_82CE59D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5AA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// stw r10,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, ctx.r10.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5AA0) {
	__imp__sub_82CE5AA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5AB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r31,0
	r31.s64 = 0;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r29,r31
	r29.u64 = r31.u64;
	// mr r30,r31
	r30.u64 = r31.u64;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(12) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ce5b2c
	if (!cr6.eq) goto loc_82CE5B2C;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// lbz r10,1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 1);
	// lis r9,0
	ctx.r9.s64 = 0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// ori r8,r9,48000
	ctx.r8.u64 = ctx.r9.u64 | 48000;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// std r31,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r31.u64);
	// std r31,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r31.u64);
	// stw r31,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r31.u32);
	// stb r31,100(r1)
	PPC_STORE_U8(ctx.r1.u32 + 100, r31.u8);
	// stb r10,101(r1)
	PPC_STORE_U8(ctx.r1.u32 + 101, ctx.r10.u8);
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// stb r31,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r31.u8);
	// bl 0x82cd7ff8
	sub_82CD7FF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5b94
	if (cr6.lt) goto loc_82CE5B94;
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82CE5B2C:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(8) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5b88
	if (cr6.eq) goto loc_82CE5B88;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce5b88
	if (cr6.eq) goto loc_82CE5B88;
	// lis r27,-31949
	r27.s64 = -2093809664;
loc_82CE5B48:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r11,28900(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(28900) );
	// lwzx r4,r10,r31
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + r31.u32);
	// lwz r3,60(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(60) );
	// bl 0x82cdba80
	sub_82CDBA80(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5b94
	if (cr6.lt) goto loc_82CE5B94;
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(8) );
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// add r29,r10,r29
	r29.u64 = ctx.r10.u64 + r29.u64;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce5b48
	if (cr6.lt) goto loc_82CE5B48;
loc_82CE5B88:
	// rlwinm r11,r30,3,0,28
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
loc_82CE5B94:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE5AB8) {
	__imp__sub_82CE5AB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5BA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r28,0
	r28.s64 = 0;
	// addi r9,r11,20856
	ctx.r9.s64 = r11.s64 + 20856;
	// lbz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 60);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce5c18
	if (cr6.eq) goto loc_82CE5C18;
	// mr r29,r28
	r29.u64 = r28.u64;
loc_82CE5BD0:
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// rlwinm r30,r29,3,0,28
	r30.u64 = rotl64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r30,r10
	r11.u64 = PPC_LOAD_U32(r30.u32 + ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5c00
	if (cr6.eq) goto loc_82CE5C00;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwzx r3,r30,r10
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// stwx r28,r30,r9
	PPC_STORE_U32(r30.u32 + ctx.r9.u32, r28.u32);
loc_82CE5C00:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lbz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 60);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce5bd0
	if (cr6.lt) goto loc_82CE5BD0;
loc_82CE5C18:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce5c2c
	if (cr6.eq) goto loc_82CE5C2C;
	// bl 0x82cd7d20
	sub_82CD7D20(ctx, base);
	// stw r28,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r28.u32);
loc_82CE5C2C:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r10,r11,16268
	ctx.r10.s64 = r11.s64 + 16268;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// beq cr6,0x82ce5c58
	if (cr6.eq) goto loc_82CE5C58;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
loc_82CE5C58:
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r10,r11,15792
	ctx.r10.s64 = r11.s64 + 15792;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE5BA0) {
	__imp__sub_82CE5BA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5C70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r11,28900(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28900) );
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// lwz r3,60(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(60) );
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// bl 0x82cdbaa0
	sub_82CDBAA0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5d00
	if (cr6.lt) goto loc_82CE5D00;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5d00
	if (cr6.lt) goto loc_82CE5D00;
	// lbz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// li r9,1
	ctx.r9.s64 = 1;
	// lhz r11,82(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// stb r29,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r29.u8);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stb r9,5(r31)
	PPC_STORE_U8(r31.u32 + 5, ctx.r9.u8);
	// stb r10,6(r31)
	PPC_STORE_U8(r31.u32 + 6, ctx.r10.u8);
	// lhz r7,62(r30)
	ctx.r7.u64 = PPC_LOAD_U16(r30.u32 + 62);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// ble cr6,0x82ce5d00
	if (!cr6.gt) goto loc_82CE5D00;
	// sth r11,62(r30)
	PPC_STORE_U16(r30.u32 + 62, r11.u16);
loc_82CE5D00:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE5C70) {
	__imp__sub_82CE5C70(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5D08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce5d2c
	if (cr6.lt) goto loc_82CE5D2C;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// blr 
	return;
loc_82CE5D2C:
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(36) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// lwzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

PPC_WEAK_FUNC(sub_82CE5D08) {
	__imp__sub_82CE5D08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5D50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce5d74
	if (cr6.lt) goto loc_82CE5D74;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// blr 
	return;
loc_82CE5D74:
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(36) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// lwzx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

PPC_WEAK_FUNC(sub_82CE5D50) {
	__imp__sub_82CE5D50(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5D98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce5db4
	if (cr6.lt) goto loc_82CE5DB4;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// blr 
	return;
loc_82CE5DB4:
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(36) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lbz r11,5(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// stb r11,0(r5)
	PPC_STORE_U8(ctx.r5.u32 + 0, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5D98) {
	__imp__sub_82CE5D98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5DD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lbz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 60);
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce5dec
	if (cr6.lt) goto loc_82CE5DEC;
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,87
	ctx.r3.u64 = ctx.r3.u64 | 87;
	// blr 
	return;
loc_82CE5DEC:
	// lwz r10,36(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(36) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stb r5,5(r11)
	PPC_STORE_U8(r11.u32 + 5, ctx.r5.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5DD0) {
	__imp__sub_82CE5DD0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5E08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r9,r11,16268
	ctx.r9.s64 = r11.s64 + 16268;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// stw r4,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r4.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82ce5e58
	if (cr6.eq) goto loc_82CE5E58;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE5E58:
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stb r30,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r30.u8);
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// addi r9,r10,20856
	ctx.r9.s64 = ctx.r10.s64 + 20856;
	// addi r10,r31,24
	ctx.r10.s64 = r31.s64 + 24;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// stw r8,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r8.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5E08) {
	__imp__sub_82CE5E08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5EA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82ce5ba0
	sub_82CE5BA0(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5ee8
	if (cr6.eq) goto loc_82CE5EE8;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lis r5,24962
	ctx.r5.s64 = 1635909632;
	// addi r3,r11,28812
	ctx.r3.s64 = r11.s64 + 28812;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82cd9fd8
	sub_82CD9FD8(ctx, base);
loc_82CE5EE8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5EA8) {
	__imp__sub_82CE5EA8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5F08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce5f70
	if (cr6.eq) goto loc_82CE5F70;
loc_82CE5F30:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce5f70
	if (cr6.lt) goto loc_82CE5F70;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// rlwinm r8,r6,2,22,29
	ctx.r8.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x3FC;
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(36) );
	// rlwinm r11,r6,3,21,28
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0x7F8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// clrlwi r29,r6,24
	r29.u64 = ctx.r6.u32 & 0xFF;
	// lwzx r5,r8,r9
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// bl 0x82ce5c70
	sub_82CE5C70(ctx, base);
	// addi r7,r29,1
	ctx.r7.s64 = r29.s64 + 1;
	// lbz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// clrlwi r6,r7,24
	ctx.r6.u64 = ctx.r7.u32 & 0xFF;
	// cmplw cr6,r6,r5
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r5.u32, xer);
	// blt cr6,0x82ce5f30
	if (cr6.lt) goto loc_82CE5F30;
loc_82CE5F70:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE5F08) {
	__imp__sub_82CE5F08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE5F78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lbz r11,5(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ce5fa8
	if (!cr6.eq) goto loc_82CE5FA8;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
loc_82CE5FA8:
	// lbz r11,6(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// rlwinm r10,r11,0,29,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x6;
	// cmplwi cr6,r10,6
	cr6.compare<uint32_t>(ctx.r10.u32, 6, xer);
	// bne cr6,0x82ce6034
	if (!cr6.eq) goto loc_82CE6034;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// lwz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// lbz r9,268(r13)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r13.u32 + 268);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// lwz r8,28900(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28900) );
	// beq cr6,0x82ce5ffc
	if (cr6.eq) goto loc_82CE5FFC;
	// rlwinm r10,r9,3,0,28
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// li r11,0
	r11.s64 = 0;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
loc_82CE5FE0:
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r7,r6
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r6.u32, xer);
	// beq cr6,0x82ce6010
	if (cr6.eq) goto loc_82CE6010;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// blt cr6,0x82ce5fe0
	if (cr6.lt) goto loc_82CE5FE0;
loc_82CE5FFC:
	// rlwinm r11,r9,3,0,28
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// b 0x82ce606c
	goto loc_82CE606C;
loc_82CE6010:
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r10,r11,3
	ctx.r10.s64 = r11.s64 + 3;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r9,r8
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// b 0x82ce606c
	goto loc_82CE606C;
loc_82CE6034:
	// clrlwi r10,r11,30
	ctx.r10.u64 = r11.u32 & 0x3;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce604c
	if (cr6.eq) goto loc_82CE604C;
	// lwz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// li r11,0
	r11.s64 = 0;
	// b 0x82ce606c
	goto loc_82CE606C;
loc_82CE604C:
	// rlwinm r11,r11,0,29,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce6064
	if (cr6.eq) goto loc_82CE6064;
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// li r7,0
	ctx.r7.s64 = 0;
	// b 0x82ce606c
	goto loc_82CE606C;
loc_82CE6064:
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82CE606C:
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE5F78) {
	__imp__sub_82CE5F78(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6098) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce60dc
	if (cr6.eq) goto loc_82CE60DC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce60f0
	if (cr6.eq) goto loc_82CE60F0;
loc_82CE60DC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE60F0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r11,61(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce61a8
	if (cr6.eq) goto loc_82CE61A8;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce612c
	if (cr6.eq) goto loc_82CE612C;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce6140
	if (cr6.eq) goto loc_82CE6140;
loc_82CE612C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE6140:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r11,61(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// rlwinm r9,r9,0,30,28
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFB;
	// stb r9,61(r28)
	PPC_STORE_U8(r28.u32 + 61, ctx.r9.u8);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce6260
	if (cr6.eq) goto loc_82CE6260;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce6260
	if (!cr6.eq) goto loc_82CE6260;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce6260
	if (!cr0.eq) goto loc_82CE6260;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// b 0x82ce625c
	goto loc_82CE625C;
loc_82CE61A8:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce61cc
	if (cr6.eq) goto loc_82CE61CC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce61e0
	if (cr6.eq) goto loc_82CE61E0;
loc_82CE61CC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE61E0:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r11,61(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// rlwinm r9,r11,0,24,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFE;
	// rlwinm r9,r9,0,26,24
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFBF;
	// ori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 | 1;
	// stb r8,61(r28)
	PPC_STORE_U8(r28.u32 + 61, ctx.r8.u8);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce6248
	if (cr6.eq) goto loc_82CE6248;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce6248
	if (!cr6.eq) goto loc_82CE6248;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce6248
	if (!cr0.eq) goto loc_82CE6248;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE6248:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82CE625C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE6260:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// beq cr6,0x82ce62a8
	if (cr6.eq) goto loc_82CE62A8;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce62a8
	if (!cr6.eq) goto loc_82CE62A8;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce62a8
	if (!cr0.eq) goto loc_82CE62A8;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE62A8:
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE6098) {
	__imp__sub_82CE6098(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE62C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// clrlwi r11,r31,31
	r11.u64 = r31.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce64c0
	if (cr6.eq) goto loc_82CE64C0;
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce6314
	if (cr6.eq) goto loc_82CE6314;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce6328
	if (cr6.eq) goto loc_82CE6328;
loc_82CE6314:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE6328:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// lis r10,-31949
	ctx.r10.s64 = -2093809664;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r3,28900(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(28900) );
	// bl 0x82cda478
	sub_82CDA478(ctx, base);
	// lbz r9,61(r28)
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82ce63b0
	if (!cr6.eq) goto loc_82CE63B0;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ce666c
	if (cr6.eq) goto loc_82CE666C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ce666c
	if (!cr6.eq) goto loc_82CE666C;
	// addic. r11,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	r11.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce666c
	if (!cr0.eq) goto loc_82CE666C;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82CE63B0:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce63d4
	if (cr6.eq) goto loc_82CE63D4;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce63e8
	if (cr6.eq) goto loc_82CE63E8;
loc_82CE63D4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE63E8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// li r12,186
	r12.s64 = 186;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// lbz r11,61(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// and r9,r11,r12
	ctx.r9.u64 = r11.u64 & r12.u64;
	// stb r9,61(r28)
	PPC_STORE_U8(r28.u32 + 61, ctx.r9.u8);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce644c
	if (cr6.eq) goto loc_82CE644C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce644c
	if (!cr6.eq) goto loc_82CE644C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce644c
	if (!cr0.eq) goto loc_82CE644C;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE644C:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce666c
	if (cr6.eq) goto loc_82CE666C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce666c
	if (!cr6.eq) goto loc_82CE666C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce666c
	if (!cr0.eq) goto loc_82CE666C;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82CE64C0:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r31,r11,28784
	r31.s64 = r11.s64 + 28784;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce64ec
	if (cr6.eq) goto loc_82CE64EC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce650c
	if (cr6.eq) goto loc_82CE650C;
loc_82CE64EC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stb r9,12(r31)
	PPC_STORE_U8(r31.u32 + 12, ctx.r9.u8);
	// b 0x82ce6510
	goto loc_82CE6510;
loc_82CE650C:
	// lbz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 12);
loc_82CE6510:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r8,61(r28)
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// clrlwi r7,r8,31
	ctx.r7.u64 = ctx.r8.u32 & 0x1;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// bne cr6,0x82ce6580
	if (!cr6.eq) goto loc_82CE6580;
	// mr r8,r13
	ctx.r8.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce666c
	if (cr6.eq) goto loc_82CE666C;
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce666c
	if (!cr6.eq) goto loc_82CE666C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce666c
	if (!cr0.eq) goto loc_82CE666C;
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82CE6580:
	// bl 0x832b2c0c
	__imp__KeRaiseIrqlToDpcLevel(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r13
	r30.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce65a4
	if (cr6.eq) goto loc_82CE65A4;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ce65b8
	if (cr6.eq) goto loc_82CE65B8;
loc_82CE65A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b2bfc
	__imp__KeAcquireSpinLockAtRaisedIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r29,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r29.u8);
loc_82CE65B8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// lbz r11,61(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 61);
	// ori r9,r11,4
	ctx.r9.u64 = r11.u64 | 4;
	// stb r9,61(r28)
	PPC_STORE_U8(r28.u32 + 61, ctx.r9.u8);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce661c
	if (cr6.eq) goto loc_82CE661C;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce661c
	if (!cr6.eq) goto loc_82CE661C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce661c
	if (!cr0.eq) goto loc_82CE661C;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
loc_82CE661C:
	// mr r10,r13
	ctx.r10.u64 = ctx.r13.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ce6664
	if (cr6.eq) goto loc_82CE6664;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ce6664
	if (!cr6.eq) goto loc_82CE6664;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// bne 0x82ce6664
	if (!cr0.eq) goto loc_82CE6664;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r30,12(r31)
	r30.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// li r11,0
	r11.s64 = 0;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r11.u8);
	// bl 0x832b2c3c
	__imp__KeReleaseSpinLockFromRaisedIrql(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b2c1c
	__imp__KfLowerIrql(ctx, base);
loc_82CE6664:
	// lhz r11,62(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 62);
	// sth r11,64(r28)
	PPC_STORE_U16(r28.u32 + 64, r11.u16);
loc_82CE666C:
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x831fd318
	sub_831FD318(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE62C0) {
	__imp__sub_82CE62C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6680) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// mr r28,r29
	r28.u64 = r29.u64;
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// stw r11,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r11.u32);
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// stw r9,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r9.u32);
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// stw r8,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r8.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce66d0
	if (cr6.eq) goto loc_82CE66D0;
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// b 0x82ce66d4
	goto loc_82CE66D4;
loc_82CE66D0:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82CE66D4:
	// stb r11,60(r31)
	PPC_STORE_U8(r31.u32 + 60, r11.u8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce66fc
	if (cr6.eq) goto loc_82CE66FC;
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lwz r3,12(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce6744
	if (cr6.eq) goto loc_82CE6744;
	// bl 0x82cd7d00
	sub_82CD7D00(ctx, base);
	// b 0x82ce6744
	goto loc_82CE6744;
loc_82CE66FC:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// lbz r10,1(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 1);
	// lis r9,0
	ctx.r9.s64 = 0;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r5,r31,32
	ctx.r5.s64 = r31.s64 + 32;
	// ori r8,r9,48000
	ctx.r8.u64 = ctx.r9.u64 | 48000;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// std r29,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r29.u64);
	// std r29,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r29.u64);
	// stw r29,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r29.u32);
	// stb r29,84(r1)
	PPC_STORE_U8(ctx.r1.u32 + 84, r29.u8);
	// stb r10,85(r1)
	PPC_STORE_U8(ctx.r1.u32 + 85, ctx.r10.u8);
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// stb r29,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r29.u8);
	// bl 0x82cd8050
	sub_82CD8050(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// blt cr6,0x82ce67a8
	if (cr6.lt) goto loc_82CE67A8;
loc_82CE6744:
	// lbz r11,60(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 60);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce6778
	if (cr6.eq) goto loc_82CE6778;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ce6798
	if (cr6.eq) goto loc_82CE6798;
	// mr r28,r29
	r28.u64 = r29.u64;
loc_82CE6778:
	// lbz r11,60(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 60);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce67a8
	if (cr6.eq) goto loc_82CE67A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,8(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// bl 0x82ce5f08
	sub_82CE5F08(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
loc_82CE6798:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
loc_82CE67A8:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE6680) {
	__imp__sub_82CE6680(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE67B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// li r11,0
	r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 60);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ce6818
	if (cr6.eq) goto loc_82CE6818;
loc_82CE67E0:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x82ce6818
	if (cr6.lt) goto loc_82CE6818;
	// lwz r9,36(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// rlwinm r10,r11,3,21,28
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0x7F8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// add r4,r10,r9
	ctx.r4.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// bl 0x82ce5f78
	sub_82CE5F78(ctx, base);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lbz r10,60(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 60);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce67e0
	if (cr6.lt) goto loc_82CE67E0;
loc_82CE6818:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE67B8) {
	__imp__sub_82CE67B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6820) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// bgt cr6,0x82ce6830
	if (cr6.gt) goto loc_82CE6830;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_82CE6830:
	// cmplwi cr6,r3,9973
	cr6.compare<uint32_t>(ctx.r3.u32, 9973, xer);
	// blt cr6,0x82ce6840
	if (cr6.lt) goto loc_82CE6840;
	// li r3,9973
	ctx.r3.s64 = 9973;
	// blr 
	return;
loc_82CE6840:
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,1229
	ctx.r8.s64 = 1229;
	// li r11,1
	r11.s64 = 1;
	// addi r6,r10,20944
	ctx.r6.s64 = ctx.r10.s64 + 20944;
loc_82CE6854:
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// rlwinm r11,r10,31,1,31
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r5,r6
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r6.u32);
	// cmplw cr6,r10,r3
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r3.u32, xer);
	// beq cr6,0x82ce6888
	if (cr6.eq) goto loc_82CE6888;
	// ble cr6,0x82ce687c
	if (!cr6.gt) goto loc_82CE687C;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// b 0x82ce6880
	goto loc_82CE6880;
loc_82CE687C:
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
loc_82CE6880:
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// bne cr6,0x82ce6854
	if (!cr6.eq) goto loc_82CE6854;
loc_82CE6888:
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r11,r6
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE6820) {
	__imp__sub_82CE6820(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6898) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r31,12
	ctx.r3.s64 = r31.s64 + 12;
	// bl 0x82cdad08
	sub_82CDAD08(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cdad08
	sub_82CDAD08(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE6898) {
	__imp__sub_82CE6898(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE68D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r30,r4,24
	r30.u64 = ctx.r4.u32 & 0xFF;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce6974
	if (cr6.eq) goto loc_82CE6974;
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// mulli r4,r30,120
	ctx.r4.s64 = r30.s64 * 120;
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r3.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ce6974
	if (cr6.eq) goto loc_82CE6974;
	// addi r9,r31,12
	ctx.r9.s64 = r31.s64 + 12;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
loc_82CE6924:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// addi r10,r10,120
	ctx.r10.s64 = ctx.r10.s64 + 120;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// lwz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, r11.u32);
	// bne 0x82ce6924
	if (!cr0.eq) goto loc_82CE6924;
loc_82CE6974:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE68D0) {
	__imp__sub_82CE68D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6990) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r3,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r3.u32);
	// addi r11,r3,12
	r11.s64 = ctx.r3.s64 + 12;
	// stw r3,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r3.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// stw r10,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r10.u32);
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE6990) {
	__imp__sub_82CE6990(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE69B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lfs f0,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 36, temp.u32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// addi r4,r8,26704
	ctx.r4.s64 = ctx.r8.s64 + 26704;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// addi r7,r9,26736
	ctx.r7.s64 = ctx.r9.s64 + 26736;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// lvx128 v8,r0,r4
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// lvlx128 v62,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r30,r1,104
	r30.s64 = ctx.r1.s64 + 104;
	// vspltw128 v11,v62,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// lis r29,-32252
	r29.s64 = -2113667072;
	// lvx128 v12,r0,r7
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r28,r1,88
	r28.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v61,1
	simd::store_i32(v61.u32, simd::set1_i32(int32_t(0x1)));
	// li r4,0
	ctx.r4.s64 = 0;
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfs f13,-16944(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// lvlx128 v60,r10,r9
	temp.u32 = ctx.r10.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vspltw128 v0,v60,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lvlx128 v59,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r30,16
	r30.s64 = 16;
	// vspltw128 v9,v59,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v58,v63,v0,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vadduwm v7,v0,v13
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v57,v58,v13,4
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v56,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v10,v56,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vsldoi128 v13,v57,v7,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(ctx.v7.u8), 12));
	// vmaddfp v12,v10,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmaddfp v11,v10,v8,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// fmuls f2,f0,f13
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vadduwm v8,v0,v0
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// lvlx128 v55,r0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v60,v55,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vadduwm v10,v13,v0
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// li r7,2
	ctx.r7.s64 = 2;
	// vadduwm v13,v13,v9
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v9.u32)));
	// vadduwm v0,v10,v9
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v9.u32)));
loc_82CE6AE4:
	// li r10,8
	ctx.r10.s64 = 8;
loc_82CE6AE8:
	// rldicl r9,r11,32,32
	ctx.r9.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r9,r7
	temp.u32 = ctx.r9.u32 + ctx.r7.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v54,2
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v54.u8), 14));
	// vsldoi128 v62,v62,v53,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v53.u8), 14));
	// bne 0x82ce6ae8
	if (!cr0.eq) goto loc_82CE6AE8;
	// vupkhsb128 v52,v63,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v63.s16)));
	// vsrw128 v51,v13,v61
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v51, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v13), simd::and_u32(simd::to_vec128i(v61), simd::set1_i32(0x1F))));
	// vupklsb128 v50,v63,v96
	simd::store_i32(v50.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v63.s16)));
	// vsrw128 v49,v0,v61
simd::store_shuffled(v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(v61), simd::set1_i32(0x1F))));
	// vupkhsb128 v48,v62,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v62.s16)));
	// rldicl r10,r11,32,32
	ctx.r10.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// vupklsb128 v47,v62,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v62.s16)));
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// vcsxwfp128 v46,v52,15
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v45,v50,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcuxwfp128 v44,v51,31
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v51.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vcsxwfp128 v43,v48,15
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcuxwfp128 v42,v49,31
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r9,r4,r3
	ctx.r9.u64 = ctx.r4.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v41,v47,15
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r4,r9,0,0,24
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vmulfp128 v10,v46,v11
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v9,v45,v12
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v7,v44,v11
	simd::store_f32_aligned(ctx.v7.f32, simd::mul_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vsubfp128 v6,v43,v46
	simd::store_f32_aligned(ctx.v6.f32, simd::sub_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v46.f32)));
	// vmulfp128 v5,v42,v12
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v4,v41,v45
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v45.f32)));
	// vmaddfp v3,v6,v7,v10
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vmaddfp v2,v4,v5,v9
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v3,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v3), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v2,r5,r30
	ea = (ctx.r5.u32 + r30.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v2), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce6b90
	if (cr6.eq) goto loc_82CE6B90;
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r3
loc_82CE6B90:
	// vadduwm v0,v0,v8
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v8.u32)));
	// addic. r6,r6,-8
	xer.ca = ctx.r6.u32 > 7;
	ctx.r6.s64 = ctx.r6.s64 + -8;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v13,v13,v8
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v8.u32)));
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// vaddfp128 v12,v12,v60
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v60.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v11,v11,v60
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(v60.f32)));
	// bgt 0x82ce6ae4
	if (cr0.gt) goto loc_82CE6AE4;
	// lbz r10,13(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// rotlwi r8,r10,1
	ctx.r8.u64 = rotl32(ctx.r10.u32, 1);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce6bd8
	if (cr6.lt) goto loc_82CE6BD8;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_82CE6BD8:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ce6bf8
	if (!cr6.lt) goto loc_82CE6BF8;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82CE6BF8:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE69B8) {
	__imp__sub_82CE69B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6C20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// lfs f0,36(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r7,r9,26736
	ctx.r7.s64 = ctx.r9.s64 + 26736;
	// stfs f13,36(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 36, temp.u32);
	// addi r4,r8,26704
	ctx.r4.s64 = ctx.r8.s64 + 26704;
	// lfs f10,48(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// li r8,4
	ctx.r8.s64 = 4;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lvx128 v12,r0,r7
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lvx128 v8,r0,r4
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// vspltisw128 v59,1
	simd::store_i32(v59.u32, simd::set1_i32(int32_t(0x1)));
	// lis r31,-32252
	r31.s64 = -2113667072;
	// lvlx128 v62,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r28,r1,88
	r28.s64 = ctx.r1.s64 + 88;
	// vspltw128 v11,v62,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// li r29,0
	r29.s64 = 0;
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r7,88(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfs f13,-16944(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// li r31,2
	r31.s64 = 2;
	// lvlx128 v60,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// li r26,1024
	r26.s64 = 1024;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r27,1040
	r27.s64 = 1040;
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vspltw128 v0,v60,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lvlx128 v58,r8,r9
	temp.u32 = ctx.r8.u32 + ctx.r9.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// vspltw128 v9,v58,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v57,v63,v0,4
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vadduwm v7,v0,v13
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v56,v57,v13,4
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v55,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v10,v55,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vsldoi128 v13,v56,v7,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(ctx.v7.u8), 12));
	// vmaddfp v12,v10,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmaddfp v11,v10,v8,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// fmuls f2,f0,f13
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vadduwm v7,v0,v0
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// lvlx128 v54,r0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v58,v54,0
	simd::store_i32(v58.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// vadduwm v8,v13,v9
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v9.u32)));
	// li r4,6
	ctx.r4.s64 = 6;
	// vadduwm v13,v13,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// li r28,16
	r28.s64 = 16;
	// vadduwm v10,v13,v9
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v9.u32)));
loc_82CE6D60:
	// li r9,8
	ctx.r9.s64 = 8;
loc_82CE6D64:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v53,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v52,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v53,2
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 14));
	// lvlx128 v51,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v52,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v52.u8), 14));
	// lvlx128 v50,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v51,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v51.u8), 14));
	// vsldoi128 v60,v60,v50,2
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v50.u8), 14));
	// bne 0x82ce6d64
	if (!cr0.eq) goto loc_82CE6D64;
	// vsrw128 v49,v10,v59
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v10), simd::and_u32(simd::to_vec128i(v59), simd::set1_i32(0x1F))));
	// vupkhsb128 v48,v63,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v63.s16)));
	// vsrw128 v47,v8,v59
simd::store_shuffled(v47, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v59), simd::set1_i32(0x1F))));
	// vupklsb128 v46,v63,v96
	simd::store_i32(v46.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v63.s16)));
	// vupkhsb128 v45,v62,v96
	simd::store_i32(v45.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v62.s16)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vupklsb128 v44,v62,v96
	simd::store_i32(v44.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v62.s16)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vupkhsb128 v43,v61,v96
	simd::store_i32(v43.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v61.s16)));
	// vcsxwfp128 v42,v48,15
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v41,v61,v96
	simd::store_i32(v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v61.s16)));
	// vcsxwfp128 v40,v46,15
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v39,v60,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v60.s16)));
	// vcuxwfp128 v38,v49,31
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v37,v60,v96
	simd::store_i32(v37.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v60.s16)));
	// vcuxwfp128 v36,v47,31
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v47.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v35,v45,15
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vcsxwfp128 v34,v44,15
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v33,v43,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vcsxwfp128 v32,v41,15
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v57,v39,15
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// xor r9,r29,r3
	ctx.r9.u64 = r29.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v56,v37,15
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r11,r9,0,0,24
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// vmulfp128 v9,v42,v11
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vmulfp128 v6,v40,v12
	simd::store_f32_aligned(ctx.v6.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v0,v38,v12
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v13,v36,v11
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vsubfp128 v5,v35,v42
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v42.f32)));
	// vsubfp128 v4,v34,v40
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v40.f32)));
	// vmulfp128 v3,v33,v11
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v2,v32,v12
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v1,v57,v33
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(v57.f32), simd::load_f32_aligned(v33.f32)));
	// vsubfp128 v31,v56,v32
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v32.f32)));
	// vmaddfp v30,v5,v13,v9
	simd::store_f32_aligned(v30.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v29,v4,v0,v6
	simd::store_f32_aligned(v29.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// vmaddfp v28,v1,v13,v3
	simd::store_f32_aligned(v28.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v27,v31,v0,v2
	simd::store_f32_aligned(v27.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// stvx128 v30,r5,r26
	ea = (ctx.r5.u32 + r26.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v30), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v29,r5,r27
	ea = (ctx.r5.u32 + r27.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v29), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v28,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v28), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v27,r5,r28
	ea = (ctx.r5.u32 + r28.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v27), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce6e5c
	if (cr6.eq) goto loc_82CE6E5C;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE6E5C:
	// vadduwm v10,v10,v7
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v7.u32)));
	// addic. r6,r6,-8
	xer.ca = ctx.r6.u32 > 7;
	ctx.r6.s64 = ctx.r6.s64 + -8;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v8,v8,v7
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v7.u32)));
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// vaddfp128 v12,v12,v58
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v58.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v11,v11,v58
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(v58.f32)));
	// bgt 0x82ce6d60
	if (cr0.gt) goto loc_82CE6D60;
	// lbz r11,13(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 13);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce6ea4
	if (cr6.lt) goto loc_82CE6EA4;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE6EA4:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce6ec4
	if (!cr6.lt) goto loc_82CE6EC4;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE6EC4:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r30)
	PPC_STORE_U32(r30.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 48, temp.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE6C20) {
	__imp__sub_82CE6C20(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE6EF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// lfs f0,36(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// extsw r8,r6
	ctx.r8.s64 = ctx.r6.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// li r7,4
	ctx.r7.s64 = 4;
	// lfs f10,48(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// stfs f13,36(r24)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r24.u32 + 36, temp.u32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// lis r29,-32252
	r29.s64 = -2113667072;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// addi r30,r1,88
	r30.s64 = ctx.r1.s64 + 88;
	// addi r8,r11,26736
	ctx.r8.s64 = r11.s64 + 26736;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r28,r1,88
	r28.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v55,1
	simd::store_i32(v55.u32, simd::set1_i32(int32_t(0x1)));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lfs f13,-16944(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r11,r10,26704
	r11.s64 = ctx.r10.s64 + 26704;
	// vor128 v57,v63,v63
	simd::store_i8(v57.u8, simd::load_i8(v63.u8));
	// lvx128 v9,r0,r8
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// vor128 v59,v63,v63
	simd::store_i8(v59.u8, simd::load_i8(v63.u8));
	// li r25,0
	r25.s64 = 0;
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// addi r8,r5,2064
	ctx.r8.s64 = ctx.r5.s64 + 2064;
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r26,88(r1)
	r26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// std r26,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r26.u64);
	// stfd f6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f6.u64);
	// lfd f5,104(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// ld r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// lvlx128 v62,r7,r9
	temp.u32 = ctx.r7.u32 + ctx.r9.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lvlx128 v54,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vspltw128 v0,v62,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// lvlx128 v58,r7,r4
	temp.u32 = ctx.r7.u32 + ctx.r4.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v11,v58,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v52,r0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v60,v63,v0,4
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vor128 v56,v63,v63
	simd::store_i8(v56.u8, simd::load_i8(v63.u8));
	// vspltw128 v12,v54,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// vor128 v58,v63,v63
	simd::store_i8(v58.u8, simd::load_i8(v63.u8));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// li r4,8
	ctx.r4.s64 = 8;
	// vadduwm v10,v0,v13
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// li r27,6
	r27.s64 = 6;
	// vsldoi128 v53,v60,v13,4
	simd::store_i8(v53.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// li r29,12
	r29.s64 = 12;
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v51,r0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v13,v53,v10,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v53.u8), simd::load_i8(ctx.v10.u8), 12));
	// vadduwm v7,v0,v0
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// li r28,14
	r28.s64 = 14;
	// vadduwm v10,v13,v0
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// vadduwm v8,v13,v11
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// vspltw128 v13,v52,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// vadduwm v10,v10,v11
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v11.u32)));
	// lvx128 v11,r0,r11
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// vmaddfp v0,v13,v9,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v13,v13,v11,v12
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vspltw128 v54,v51,0
	simd::store_i32(v54.u32, simd::broadcast_lane_i32(simd::load_i32(v51.u32), 3));
	// li r30,2
	r30.s64 = 2;
	// li r31,10
	r31.s64 = 10;
	// li r18,1008
	r18.s64 = 1008;
	// li r19,1024
	r19.s64 = 1024;
	// li r20,-16
	r20.s64 = -16;
	// li r21,-1040
	r21.s64 = -1040;
	// li r22,-1024
	r22.s64 = -1024;
	// li r23,-2048
	r23.s64 = -2048;
loc_82CE7060:
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
loc_82CE7064:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r26,r10
	ctx.r10.u64 = r26.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v50,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v49,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v50,2
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v50.u8), 14));
	// lvlx128 v48,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v49,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v49.u8), 14));
	// lvlx128 v47,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v48,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v48.u8), 14));
	// lvlx128 v46,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v60,v60,v47,2
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v47.u8), 14));
	// lvlx128 v45,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v59,v59,v46,2
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(v46.u8), 14));
	// lvlx128 v44,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v58,v45,2
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(v45.u8), 14));
	// lvlx128 v43,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v57,v57,v44,2
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(v44.u8), 14));
	// vsldoi128 v56,v56,v43,2
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(v43.u8), 14));
	// bne 0x82ce7064
	if (!cr0.eq) goto loc_82CE7064;
	// vsrw128 v42,v10,v55
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v42, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v10), simd::and_u32(simd::to_vec128i(v55), simd::set1_i32(0x1F))));
	// vupkhsb128 v38,v62,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v62.s16)));
	// vsrw128 v40,v8,v55
simd::store_shuffled(v40, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v55), simd::set1_i32(0x1F))));
	// vupkhsb128 v36,v61,v96
	simd::store_i32(v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v61.s16)));
	// vupkhsb128 v41,v63,v96
	simd::store_i32(v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v63.s16)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vupklsb128 v39,v63,v96
	simd::store_i32(v39.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v63.s16)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vupklsb128 v37,v62,v96
	simd::store_i32(v37.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v62.s16)));
	// vcuxwfp128 v53,v42,31
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v42.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v34,v61,v96
	simd::store_i32(v34.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v61.s16)));
	// vcsxwfp128 v45,v36,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v32,v60,v96
	simd::store_i32(v32.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v60.s16)));
	// vcuxwfp128 v51,v40,31
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v40.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v52,v60,v96
	simd::store_i32(v52.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v60.s16)));
	// vcsxwfp128 v49,v38,15
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v50,v59,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v35,v41,15
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v48,v59,v96
	simd::store_i32(v48.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v33,v39,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v46,v58,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v58.s16)));
	// vcsxwfp128 v47,v37,15
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v44,v58,v96
	simd::store_i32(v44.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v58.s16)));
	// vcsxwfp128 v43,v34,15
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v42,v57,v96
	simd::store_i32(v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v41,v32,15
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v40,v56,v96
	simd::store_i32(v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v56.s16)));
	// vcsxwfp128 v39,v52,15
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v38,v57,v96
	simd::store_i32(v38.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v37,v50,15
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v36,v56,v96
	simd::store_i32(v36.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v56.s16)));
	// vcsxwfp128 v34,v48,15
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v32,v46,15
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// vcsxwfp128 v52,v44,15
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v50,v42,15
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v42.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vcsxwfp128 v48,v38,15
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v46,v40,15
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// xor r9,r25,r3
	ctx.r9.u64 = r25.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v44,v36,15
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vmulfp128 v12,v51,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// rlwinm r11,r9,0,0,24
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// vmulfp128 v11,v53,v0
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(v53.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v9,v35,v13
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vsubfp128 v6,v49,v35
	simd::store_f32_aligned(ctx.v6.f32, simd::sub_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v35.f32)));
	// vmulfp128 v5,v33,v0
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v4,v47,v33
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v33.f32)));
	// vmulfp128 v3,v45,v13
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v2,v41,v45
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v45.f32)));
	// vmulfp128 v1,v43,v0
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v31,v39,v43
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v43.f32)));
	// vmulfp128 v30,v37,v13
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::load_f32_aligned(v37.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v29,v32,v37
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v37.f32)));
	// vmulfp128 v28,v34,v0
	simd::store_f32_aligned(v28.f32, simd::mul_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v27,v52,v34
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v34.f32)));
	// vmulfp128 v26,v50,v13
	simd::store_f32_aligned(v26.f32, simd::mul_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v25,v46,v50
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v50.f32)));
	// vmulfp128 v24,v48,v0
	simd::store_f32_aligned(v24.f32, simd::mul_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v23,v44,v48
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(v48.f32)));
	// vmaddfp v22,v6,v12,v9
	simd::store_f32_aligned(v22.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v21,v4,v11,v5
	simd::store_f32_aligned(v21.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v20,v2,v12,v3
	simd::store_f32_aligned(v20.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v19,v31,v11,v1
	simd::store_f32_aligned(v19.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v18,v29,v12,v30
	simd::store_f32_aligned(v18.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v30.f32)));
	// vmaddfp v17,v27,v11,v28
	simd::store_f32_aligned(v17.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v28.f32)));
	// vmaddfp v16,v25,v12,v26
	simd::store_f32_aligned(v16.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v26.f32)));
	// vmaddfp v15,v23,v11,v24
	simd::store_f32_aligned(v15.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v24.f32)));
	// stvx128 v22,r8,r18
	ea = (ctx.r8.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v22), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v21,r8,r19
	ea = (ctx.r8.u32 + r19.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v21), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v20,r8,r20
	ea = (ctx.r8.u32 + r20.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v20), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v19,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v19), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v18,r8,r21
	ea = (ctx.r8.u32 + r21.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v18), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v17,r8,r22
	ea = (ctx.r8.u32 + r22.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v17), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v16,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v16), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v15,r8,r23
	ea = (ctx.r8.u32 + r23.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v15), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce71fc
	if (cr6.eq) goto loc_82CE71FC;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE71FC:
	// vadduwm v10,v10,v7
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v7.u32)));
	// addic. r6,r6,-8
	xer.ca = ctx.r6.u32 > 7;
	ctx.r6.s64 = ctx.r6.s64 + -8;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v8,v8,v7
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v7.u32)));
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// vaddfp128 v0,v0,v54
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v54.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v13,v13,v54
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v54.f32)));
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// bgt 0x82ce7060
	if (cr0.gt) goto loc_82CE7060;
	// lbz r11,13(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 13);
	// lwz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce7248
	if (cr6.lt) goto loc_82CE7248;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE7248:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(20) );
	// stw r9,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce7268
	if (!cr6.lt) goto loc_82CE7268;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE7268:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r24)
	PPC_STORE_U32(r24.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r24)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r24.u32 + 48, temp.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CE6EF0) {
	__imp__sub_82CE6EF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE7290) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f0,36(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	f0.f64 = double(temp.f32);
	// stw r6,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, ctx.r6.u32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stw r7,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r7.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// li r22,4
	r22.s64 = 4;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// lfs f10,48(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfs f13,36(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 36, temp.u32);
	// addi r6,r1,104
	ctx.r6.s64 = ctx.r1.s64 + 104;
	// addi r31,r9,26736
	r31.s64 = ctx.r9.s64 + 26736;
	// vor128 v53,v63,v63
	simd::store_i8(v53.u8, simd::load_i8(v63.u8));
	// lvlx128 v62,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// lis r10,-32252
	ctx.r10.s64 = -2113667072;
	// vspltw128 v12,v62,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// vor128 v55,v63,v63
	simd::store_i8(v55.u8, simd::load_i8(v63.u8));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// vor128 v57,v63,v63
	simd::store_i8(v57.u8, simd::load_i8(v63.u8));
	// addi r7,r7,26704
	ctx.r7.s64 = ctx.r7.s64 + 26704;
	// vor128 v59,v63,v63
	simd::store_i8(v59.u8, simd::load_i8(v63.u8));
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// li r20,0
	r20.s64 = 0;
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r21,88(r1)
	r21.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// std r21,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r21.u64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lvlx128 v60,r22,r8
	temp.u32 = r22.u32 + ctx.r8.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r9,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r9.u64);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// vspltw128 v0,v60,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lvlx128 v58,r22,r6
	temp.u32 = r22.u32 + ctx.r6.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisw128 v51,1
	simd::store_i32(v51.u32, simd::set1_i32(int32_t(0x1)));
	// vspltw128 v11,v58,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// lfs f13,-16944(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r5,4112
	ctx.r10.s64 = ctx.r5.s64 + 4112;
	// vor128 v52,v63,v63
	simd::store_i8(v52.u8, simd::load_i8(v63.u8));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v50,v63,v0,4
	simd::store_i8(v50.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vor128 v54,v63,v63
	simd::store_i8(v54.u8, simd::load_i8(v63.u8));
	// vor128 v56,v63,v63
	simd::store_i8(v56.u8, simd::load_i8(v63.u8));
	// vor128 v58,v63,v63
	simd::store_i8(v58.u8, simd::load_i8(v63.u8));
	// vadduwm v10,v0,v13
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v49,v50,v13,4
	simd::store_i8(v49.u8, simd::shift_left_insert_bytes(simd::load_i8(v50.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// vsldoi128 v13,v49,v10,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v49.u8), simd::load_i8(ctx.v10.u8), 12));
	// vadduwm v8,v0,v0
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vadduwm v10,v13,v0
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// lvx128 v0,r0,r31
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r31.u32) & ~0xF), VectorMaskL));
	// vadduwm v9,v13,v11
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v48,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v48,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v47,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vadduwm v10,v10,v11
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v11.u32)));
	// lvx128 v11,r0,r7
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// vmaddfp v0,v13,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vspltw128 v50,v47,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v47.u32), 3));
	// vmaddfp v13,v13,v11,v12
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// li r4,8
	ctx.r4.s64 = 8;
	// li r23,10
	r23.s64 = 10;
	// li r24,22
	r24.s64 = 22;
	// li r25,20
	r25.s64 = 20;
	// li r26,6
	r26.s64 = 6;
	// li r27,18
	r27.s64 = 18;
	// li r28,16
	r28.s64 = 16;
	// li r29,2
	r29.s64 = 2;
	// li r30,14
	r30.s64 = 14;
	// li r31,12
	r31.s64 = 12;
	// li r14,-1040
	r14.s64 = -1040;
	// li r6,-1024
	ctx.r6.s64 = -1024;
	// li r15,-2064
	r15.s64 = -2064;
	// li r16,-2048
	r16.s64 = -2048;
	// li r17,-3088
	r17.s64 = -3088;
	// li r18,-3072
	r18.s64 = -3072;
	// li r19,-4096
	r19.s64 = -4096;
loc_82CE7428:
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
loc_82CE742C:
	// rldicl r11,r9,32,32
	r11.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r21,r9
	ctx.r9.u64 = r21.u64 + ctx.r9.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v46,r11,r23
	temp.u32 = r11.u32 + r23.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v45,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v46,2
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v46.u8), 14));
	// lvlx128 v44,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v45,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v45.u8), 14));
	// lvlx128 v43,r11,r25
	temp.u32 = r11.u32 + r25.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v44,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v44.u8), 14));
	// lvlx128 v42,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v60,v60,v43,2
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v43.u8), 14));
	// lvlx128 v41,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v59,v59,v42,2
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(v42.u8), 14));
	// lvlx128 v40,r11,r22
	temp.u32 = r11.u32 + r22.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v58,v41,2
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(v41.u8), 14));
	// lvlx128 v39,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v57,v57,v40,2
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(v40.u8), 14));
	// lvlx128 v38,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v56,v56,v39,2
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(v39.u8), 14));
	// lvlx128 v37,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v55,v55,v38,2
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(v38.u8), 14));
	// lvlx128 v36,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v54,v54,v37,2
	simd::store_i8(v54.u8, simd::shift_left_insert_bytes(simd::load_i8(v54.u8), simd::load_i8(v37.u8), 14));
	// lvlx128 v35,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v53,v53,v36,2
	simd::store_i8(v53.u8, simd::shift_left_insert_bytes(simd::load_i8(v53.u8), simd::load_i8(v36.u8), 14));
	// vsldoi128 v52,v52,v35,2
	simd::store_i8(v52.u8, simd::shift_left_insert_bytes(simd::load_i8(v52.u8), simd::load_i8(v35.u8), 14));
	// bne 0x82ce742c
	if (!cr0.eq) goto loc_82CE742C;
	// vsrw128 v34,v10,v51
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v34, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v10), simd::and_u32(simd::to_vec128i(v51), simd::set1_i32(0x1F))));
	// vupkhsb128 v33,v63,v96
	simd::store_i32(v33.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v63.s16)));
	// vsrw128 v32,v9,v51
simd::store_shuffled(v32, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v9), simd::and_u32(simd::to_vec128i(v51), simd::set1_i32(0x1F))));
	// vupkhsb128 v48,v62,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v62.s16)));
	// vupklsb128 v47,v62,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v62.s16)));
	// rldicl r11,r9,32,32
	r11.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vupkhsb128 v46,v61,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v61.s16)));
	// std r4,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r4.u64);
	// vupkhsb128 v42,v60,v96
	simd::store_i32(v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v60.s16)));
	// vcuxwfp128 v41,v34,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v34.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v38,v59,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vcuxwfp128 v39,v32,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v32.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v49,v63,v96
	simd::store_i32(v49.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v63.s16)));
	// vcsxwfp128 v45,v33,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v36,v59,v96
	simd::store_i32(v36.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v37,v48,15
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v44,v61,v96
	simd::store_i32(v44.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v61.s16)));
	// vcsxwfp128 v35,v47,15
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v34,v58,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v58.s16)));
	// vcsxwfp128 v33,v46,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v40,v60,v96
	simd::store_i32(v40.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v60.s16)));
	// vcsxwfp128 v47,v42,15
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v42.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v32,v58,v96
	simd::store_i32(v32.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v58.s16)));
	// vcsxwfp128 v42,v38,15
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v48,v57,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v38,v36,15
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v46,v56,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v56.s16)));
	// vcsxwfp128 v43,v49,15
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v49,v44,15
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v36,v34,15
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// li r7,1008
	ctx.r7.s64 = 1008;
	// vcsxwfp128 v44,v40,15
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v40,v57,v96
	simd::store_i32(v40.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v34,v32,15
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// li r4,1024
	ctx.r4.s64 = 1024;
	// vcsxwfp128 v32,v48,15
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// std r31,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r31.u64);
	// vcsxwfp128 v48,v46,15
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// li r31,-16
	r31.s64 = -16;
	// vmulfp128 v12,v39,v13
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// vmulfp128 v11,v41,v0
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vupklsb128 v46,v56,v96
	simd::store_i32(v46.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v56.s16)));
	// vsubfp128 v6,v37,v45
	simd::store_f32_aligned(ctx.v6.f32, simd::sub_f32(simd::load_f32_aligned(v37.f32), simd::load_f32_aligned(v45.f32)));
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vmulfp128 v7,v45,v13
	simd::store_f32_aligned(ctx.v7.f32, simd::mul_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// vmulfp128 v3,v33,v13
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vmulfp128 v5,v43,v0
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vupkhsb128 v45,v55,v96
	simd::store_i32(v45.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v55.s16)));
	// vsubfp128 v4,v35,v43
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v43.f32)));
	// vupklsb128 v41,v55,v96
	simd::store_i32(v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v55.s16)));
	// vsubfp128 v2,v47,v33
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v33.f32)));
	// vupkhsb128 v39,v54,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v54.s16)));
	// vmulfp128 v1,v49,v0
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v31,v44,v49
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(v49.f32)));
	// vmulfp128 v28,v38,v0
	simd::store_f32_aligned(v28.f32, simd::mul_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v27,v34,v38
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v38.f32)));
	// vmulfp128 v30,v42,v13
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v29,v36,v42
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(v42.f32)));
	// vmulfp128 v26,v32,v13
	simd::store_f32_aligned(v26.f32, simd::mul_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v25,v48,v32
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v32.f32)));
	// vmaddfp v24,v6,v12,v7
	simd::store_f32_aligned(v24.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vcsxwfp128 v38,v40,15
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vmaddfp v23,v4,v11,v5
	simd::store_f32_aligned(v23.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v22,v2,v12,v3
	simd::store_f32_aligned(v22.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vcsxwfp128 v37,v46,15
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vmaddfp v21,v31,v11,v1
	simd::store_f32_aligned(v21.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v19,v27,v11,v28
	simd::store_f32_aligned(v19.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v28.f32)));
	// vmaddfp v20,v29,v12,v30
	simd::store_f32_aligned(v20.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v30.f32)));
	// vmaddfp v18,v25,v12,v26
	simd::store_f32_aligned(v18.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v26.f32)));
	// stvx128 v24,r10,r7
	ea = (ctx.r10.u32 + ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v24), &VectorMaskL[(ea & 0xF) * 16]);
	// vmulfp128 v17,v38,v0
	simd::store_f32_aligned(v17.f32, simd::mul_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// stvx128 v23,r10,r4
	ea = (ctx.r10.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v23), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v22,r10,r31
	ea = (ctx.r10.u32 + r31.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v22), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v21,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v21), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v19,r10,r6
	ea = (ctx.r10.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v19), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v20,r10,r14
	ea = (ctx.r10.u32 + r14.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v20), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v18,r10,r15
	ea = (ctx.r10.u32 + r15.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v18), &VectorMaskL[(ea & 0xF) * 16]);
	// vupklsb128 v36,v54,v96
	simd::store_i32(v36.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v54.s16)));
	// vcsxwfp128 v35,v45,15
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v34,v53,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vcsxwfp128 v33,v41,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupklsb128 v32,v53,v96
	simd::store_i32(v32.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v53.s16)));
	// vcsxwfp128 v49,v39,15
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v48,v52,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// vsubfp128 v16,v37,v38
	simd::store_f32_aligned(v16.f32, simd::sub_f32(simd::load_f32_aligned(v37.f32), simd::load_f32_aligned(v38.f32)));
	// vupklsb128 v47,v52,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v52.s16)));
	// vcsxwfp128 v46,v36,15
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v45,v34,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// xor r8,r20,r3
	ctx.r8.u64 = r20.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v44,v32,15
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// ld r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// vcsxwfp128 v43,v48,15
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r7,r8,0,0,24
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// vcsxwfp128 v42,v47,15
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// ld r31,96(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vmulfp128 v15,v35,v13
	simd::store_f32_aligned(v15.f32, simd::mul_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v14,v33,v0
	simd::store_f32_aligned(v14.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v7,v49,v35
	simd::store_f32_aligned(ctx.v7.f32, simd::sub_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v35.f32)));
	// vmaddfp v6,v16,v11,v17
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v16.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v17.f32)));
	// vsubfp128 v5,v46,v33
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v33.f32)));
	// vmulfp128 v4,v45,v13
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v3,v44,v0
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v2,v43,v45
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v45.f32)));
	// vsubfp128 v1,v42,v44
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v44.f32)));
	// vmaddfp v31,v7,v12,v15
	simd::store_f32_aligned(v31.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v15.f32)));
	// stvx128 v6,r10,r16
	ea = (ctx.r10.u32 + r16.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v6), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v30,v5,v11,v14
	simd::store_f32_aligned(v30.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v14.f32)));
	// vmaddfp v29,v2,v12,v4
	simd::store_f32_aligned(v29.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v28,v1,v11,v3
	simd::store_f32_aligned(v28.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// stvx128 v31,r10,r17
	ea = (ctx.r10.u32 + r17.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v31), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v30,r10,r18
	ea = (ctx.r10.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v30), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v29,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v29), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v28,r10,r19
	ea = (ctx.r10.u32 + r19.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v28), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce7690
	if (cr6.eq) goto loc_82CE7690;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE7690:
	// lwz r11,316(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(316) );
	// vadduwm v10,v10,v8
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v8.u32)));
	// vadduwm v9,v9,v8
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v9.u32), simd::load_u32(ctx.v8.u32)));
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// addic. r11,r11,-8
	xer.ca = r11.u32 > 7;
	r11.s64 = r11.s64 + -8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// vaddfp128 v0,v0,v50
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v50.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v13,v13,v50
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v50.f32)));
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// bgt 0x82ce7428
	if (cr0.gt) goto loc_82CE7428;
	// lwz r31,324(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(324) );
	// lbz r11,13(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// subf r7,r10,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r10.s64;
	// twllei r8,0
	// divwu r10,r7,r8
	ctx.r10.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ce76e8
	if (cr6.lt) goto loc_82CE76E8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CE76E8:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82ce7708
	if (!cr6.lt) goto loc_82CE7708;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CE7708:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82CE7290) {
	__imp__sub_82CE7290(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE7730) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r22{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x82ce7760
	if (cr6.lt) goto loc_82CE7760;
loc_82CE7758:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ce78d4
	goto loc_82CE78D4;
loc_82CE7760:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ce7758
	if (!cr6.lt) goto loc_82CE7758;
	// lwz r11,80(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(80) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce78bc
	if (cr6.eq) goto loc_82CE78BC;
	// clrlwi r11,r11,30
	r11.u64 = r11.u32 & 0x3;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ce78b4
	if (cr6.eq) goto loc_82CE78B4;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r7,16(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// lwz r11,84(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(84) );
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f0
	ctx.f11.f64 = double(f0.s64);
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// frsp f10,f12
	ctx.f10.f64 = double(float(ctx.f12.f64));
	// frsp f9,f11
	ctx.f9.f64 = double(float(ctx.f11.f64));
	// fdivs f8,f10,f9
	ctx.f8.f64 = double(float(ctx.f10.f64 / ctx.f9.f64));
	// stfs f8,44(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
	// beq cr6,0x82ce7820
	if (cr6.eq) goto loc_82CE7820;
	// rotlwi r8,r7,0
	ctx.r8.u64 = rotl32(ctx.r7.u32, 0);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce77f0
	if (!cr6.eq) goto loc_82CE77F0;
	// rlwinm r11,r11,0,24,24
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x80;
	// subfic r10,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r10.s64 = 0 - r11.s64;
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r9,0,31,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// b 0x82ce7834
	goto loc_82CE7834;
loc_82CE77F0:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ce7820
	if (cr6.eq) goto loc_82CE7820;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ce7820
	if (!cr6.eq) goto loc_82CE7820;
	// rlwinm r11,r11,0,24,24
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x80;
	// subfic r10,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r10.s64 = 0 - r11.s64;
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r9,0,31,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// addi r10,r11,5
	ctx.r10.s64 = r11.s64 + 5;
	// b 0x82ce7834
	goto loc_82CE7834;
loc_82CE7820:
	// rlwinm r11,r11,0,24,24
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x80;
	// subfic r10,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r10.s64 = 0 - r11.s64;
	// subfe r9,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r9.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r9,0,31,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// addi r10,r11,3
	ctx.r10.s64 = r11.s64 + 3;
loc_82CE7834:
	// lbz r11,13(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// lbz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cmplwi cr6,r9,5
	cr6.compare<uint32_t>(ctx.r9.u32, 5, xer);
	// bgt cr6,0x82ce7884
	if (cr6.gt) goto loc_82CE7884;
	// lis r12,-32050
	r12.s64 = -2100428800;
	// addi r12,r12,30816
	r12.s64 = r12.s64 + 30816;
	// rlwinm r0,r9,2,0,29
	r0.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (ctx.r9.u64) {
	case 0:
		goto loc_82CE7878;
	case 1:
		goto loc_82CE7878;
	case 2:
		goto loc_82CE7884;
	case 3:
		goto loc_82CE7878;
	case 4:
		goto loc_82CE7884;
	case 5:
		goto loc_82CE7878;
	default:
		__builtin_unreachable();
	}
	// lwz r22,30840(r14)
	r22.u64 = PPC_LOAD_U32(r14.u32 + int32_t(30840) );
	// lwz r22,30840(r14)
	r22.u64 = PPC_LOAD_U32(r14.u32 + int32_t(30840) );
	// lwz r22,30852(r14)
	r22.u64 = PPC_LOAD_U32(r14.u32 + int32_t(30852) );
	// lwz r22,30840(r14)
	r22.u64 = PPC_LOAD_U32(r14.u32 + int32_t(30840) );
	// lwz r22,30852(r14)
	r22.u64 = PPC_LOAD_U32(r14.u32 + int32_t(30852) );
	// lwz r22,30840(r14)
	r22.u64 = PPC_LOAD_U32(r14.u32 + int32_t(30840) );
loc_82CE7878:
	// rlwinm r11,r11,31,1,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// b 0x82ce7888
	goto loc_82CE7888;
loc_82CE7884:
	// li r9,0
	ctx.r9.s64 = 0;
loc_82CE7888:
	// rlwinm r11,r10,2,0,29
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r6,r7,25864
	ctx.r6.s64 = ctx.r7.s64 + 25864;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r4,r6
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r6.u32);
	// stw r3,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r3.u32);
loc_82CE78B4:
	// li r11,0
	r11.s64 = 0;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
loc_82CE78BC:
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(76) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// subf r3,r30,r10
	ctx.r3.s64 = ctx.r10.s64 - r30.s64;
loc_82CE78D4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CE7730) {
	__imp__sub_82CE7730(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE78F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lfs f0,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,44(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	ctx.f10.f64 = double(temp.f32);
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// addi r7,r10,26704
	ctx.r7.s64 = ctx.r10.s64 + 26704;
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 36, temp.u32);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f0,26696(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + 26696);
	// fcfid f4,f11
	ctx.f4.f64 = double(ctx.f11.s64);
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// lvx128 v13,r0,r7
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// li r8,4
	ctx.r8.s64 = 4;
	// lfs f9,48(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 48);
	ctx.f9.f64 = double(temp.f32);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// fmul f7,f9,f0
	ctx.f7.f64 = ctx.f9.f64 * f0.f64;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r29,r1,88
	r29.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v62,1
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x1)));
	// lfs f13,2636(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2636);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r30,r1,80
	r30.s64 = ctx.r1.s64 + 80;
	// vor128 v12,v63,v63
	simd::store_i8(ctx.v12.u8, simd::load_i8(v63.u8));
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f6.u64);
	// ld r7,88(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lvlx128 v61,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v60,r8,r10
	temp.u32 = ctx.r8.u32 + ctx.r10.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// lvlx128 v57,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v11,v61,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vspltw128 v0,v60,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lvlx128 v56,r0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v63,v0,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// vspltw128 v9,v57,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vmaddfp v11,v9,v13,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// lvlx128 v59,r8,r30
	temp.u32 = ctx.r8.u32 + r30.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v10,v59,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// vadduwm v8,v0,v13
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v55,v58,v13,4
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(ctx.v13.u8), 12));
	// li r4,0
	ctx.r4.s64 = 0;
	// vspltw128 v61,v56,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vadduwm v13,v13,v13
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v0,v55,v8,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(ctx.v8.u8), 12));
	// vadduwm v0,v0,v10
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v10.u32)));
loc_82CE79F8:
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
loc_82CE79FC:
	// rldicl r9,r11,32,32
	ctx.r9.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r9,r8
	temp.u32 = ctx.r9.u32 + ctx.r8.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v12,v12,v54,4
	simd::store_i8(ctx.v12.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(v54.u8), 12));
	// vsldoi128 v63,v63,v53,4
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// bne 0x82ce79fc
	if (!cr0.eq) goto loc_82CE79FC;
	// vsrw128 v52,v0,v62
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v52, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vsubfp128 v10,v63,v12
	simd::store_f32_aligned(ctx.v10.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// rldicl r10,r11,32,32
	ctx.r10.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// vcuxwfp128 v9,v52,31
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v52.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// xor r9,r4,r3
	ctx.r9.u64 = ctx.r4.u64 ^ ctx.r3.u64;
	// rlwinm r4,r9,0,0,24
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vmaddfp v8,v10,v9,v12
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v51,v8,v11
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// stvx128 v51,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v51), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce7a64
	if (cr6.eq) goto loc_82CE7A64;
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r3
loc_82CE7A64:
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addic. r6,r6,-4
	xer.ca = ctx.r6.u32 > 3;
	ctx.r6.s64 = ctx.r6.s64 + -4;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// vaddfp128 v11,v11,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(v61.f32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// bgt 0x82ce79f8
	if (cr0.gt) goto loc_82CE79F8;
	// lbz r10,13(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// rotlwi r8,r10,2
	ctx.r8.u64 = rotl32(ctx.r10.u32, 2);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce7aa4
	if (cr6.lt) goto loc_82CE7AA4;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_82CE7AA4:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ce7ac4
	if (!cr6.lt) goto loc_82CE7AC4;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82CE7AC4:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82CE78F0) {
	__imp__sub_82CE78F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE7AF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// lfs f0,36(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f13,40(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,44(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 44);
	ctx.f10.f64 = double(temp.f32);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// stfs f13,36(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 36, temp.u32);
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// lfs f9,48(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 48);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r10,26704
	ctx.r7.s64 = ctx.r10.s64 + 26704;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lfd f0,26696(r9)
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + 26696);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f4,f11
	ctx.f4.f64 = double(ctx.f11.s64);
	// li r8,4
	ctx.r8.s64 = 4;
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// lvx128 v13,r0,r7
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// stfd f6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f6.u64);
	// ld r7,88(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// fmul f7,f9,f0
	ctx.f7.f64 = ctx.f9.f64 * f0.f64;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// vspltisw128 v61,1
	simd::store_i32(v61.u32, simd::set1_i32(int32_t(0x1)));
	// addi r31,r1,88
	r31.s64 = ctx.r1.s64 + 88;
	// vor128 v11,v63,v63
	simd::store_i8(ctx.v11.u8, simd::load_i8(v63.u8));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// vor128 v12,v63,v63
	simd::store_i8(ctx.v12.u8, simd::load_i8(v63.u8));
	// lvlx128 v60,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v0,v60,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fctidz f5,f7
	ctx.f5.s64 = (ctx.f7.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f7.f64);
	// lvlx128 v57,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v63,v0,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// lfs f13,2636(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2636);
	ctx.f13.f64 = double(temp.f32);
	// li r29,0
	r29.s64 = 0;
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v56,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r31,12
	r31.s64 = 12;
	// lvlx128 v62,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stfd f5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f5.u64);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// vspltw128 v10,v62,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// vspltw128 v8,v57,0
	simd::store_i32(ctx.v8.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// lvlx128 v59,r8,r4
	temp.u32 = ctx.r8.u32 + ctx.r4.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v9,v59,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// vmaddfp v10,v8,v13,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vspltw128 v60,v56,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// li r4,8
	ctx.r4.s64 = 8;
	// li r28,1024
	r28.s64 = 1024;
	// vadduwm v7,v0,v13
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v55,v58,v13,4
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v8,v13,v13
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v0,v55,v7,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(ctx.v7.u8), 12));
	// vadduwm v13,v0,v9
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v9.u32)));
loc_82CE7C0C:
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
loc_82CE7C10:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v54,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v12,v12,v54,4
	simd::store_i8(ctx.v12.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(v54.u8), 12));
	// lvlx128 v52,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v53,4
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// lvlx128 v51,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v11,v11,v52,4
	simd::store_i8(ctx.v11.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v11.u8), simd::load_i8(v52.u8), 12));
	// vsldoi128 v62,v62,v51,4
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v51.u8), 12));
	// bne 0x82ce7c10
	if (!cr0.eq) goto loc_82CE7C10;
	// vsrw128 v50,v13,v61
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v50, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v13), simd::and_u32(simd::to_vec128i(v61), simd::set1_i32(0x1F))));
	// vsubfp128 v9,v63,v12
	simd::store_f32_aligned(ctx.v9.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v7,v62,v11
	simd::store_f32_aligned(ctx.v7.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// vcuxwfp128 v0,v50,31
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v50.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// xor r9,r29,r3
	ctx.r9.u64 = r29.u64 ^ ctx.r3.u64;
	// rlwinm r11,r9,0,0,24
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vmaddfp v6,v9,v0,v12
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v5,v7,v0,v11
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v49,v6,v10
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// vmulfp128 v48,v5,v10
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// stvx128 v49,r5,r28
	ea = (ctx.r5.u32 + r28.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v49), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v48,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v48), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce7c98
	if (cr6.eq) goto loc_82CE7C98;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE7C98:
	// vadduwm v13,v13,v8
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v8.u32)));
	// addic. r6,r6,-4
	xer.ca = ctx.r6.u32 > 3;
	ctx.r6.s64 = ctx.r6.s64 + -4;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// vaddfp128 v10,v10,v60
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(v60.f32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// bgt 0x82ce7c0c
	if (cr0.gt) goto loc_82CE7C0C;
	// lbz r11,13(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 13);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// rotlwi r8,r11,2
	ctx.r8.u64 = rotl32(r11.u32, 2);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce7cd8
	if (cr6.lt) goto loc_82CE7CD8;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE7CD8:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce7cf8
	if (!cr6.lt) goto loc_82CE7CF8;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE7CF8:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r30)
	PPC_STORE_U32(r30.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 48, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE7AF0) {
	__imp__sub_82CE7AF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE7D20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// lfs f0,36(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// li r8,4
	ctx.r8.s64 = 4;
	// lfs f10,48(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stfs f13,36(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r26.u32 + 36, temp.u32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r7,r9,26704
	ctx.r7.s64 = ctx.r9.s64 + 26704;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v59,1
	simd::store_i32(v59.u32, simd::set1_i32(int32_t(0x1)));
	// addi r31,r1,88
	r31.s64 = ctx.r1.s64 + 88;
	// lvlx128 v62,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r25,0
	r25.s64 = 0;
	// lvx128 v7,r0,r7
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// vspltw128 v5,v62,0
	simd::store_i32(ctx.v5.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// vor128 v11,v63,v63
	simd::store_i8(ctx.v11.u8, simd::load_i8(v63.u8));
	// vor128 v12,v63,v63
	simd::store_i8(ctx.v12.u8, simd::load_i8(v63.u8));
	// li r28,12
	r28.s64 = 12;
	// vor128 v13,v63,v63
	simd::store_i8(ctx.v13.u8, simd::load_i8(v63.u8));
	// li r29,28
	r29.s64 = 28;
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r27,88(r1)
	r27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// std r27,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r27.u64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lvlx128 v58,r8,r4
	temp.u32 = ctx.r8.u32 + ctx.r4.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// vspltw128 v10,v58,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// lvlx128 v57,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v0,v63,v63
	simd::store_i8(ctx.v0.u8, simd::load_i8(v63.u8));
	// lfs f13,2636(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2636);
	ctx.f13.f64 = double(temp.f32);
	// vspltw128 v8,v57,0
	simd::store_i32(ctx.v8.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// li r30,8
	r30.s64 = 8;
	// vadduwm v9,v10,v10
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v10.u32)));
	// vsldoi128 v56,v63,v10,4
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v10.u8), 12));
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// li r4,20
	ctx.r4.s64 = 20;
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// li r7,16
	ctx.r7.s64 = 16;
	// li r22,3072
	r22.s64 = 3072;
	// vadduwm v4,v10,v9
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v9.u32)));
	// vsldoi128 v55,v56,v9,4
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(ctx.v9.u8), 12));
	// vadduwm v6,v9,v9
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v9.u32), simd::load_u32(ctx.v9.u32)));
	// li r23,2048
	r23.s64 = 2048;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// li r24,1024
	r24.s64 = 1024;
	// vsldoi128 v10,v55,v4,4
	simd::store_i8(ctx.v10.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(ctx.v4.u8), 12));
	// vadduwm v8,v10,v8
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v8.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v3,v54,0
	simd::store_i32(ctx.v3.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v53,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmaddfp v9,v3,v7,v5
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vspltw128 v58,v53,0
	simd::store_i32(v58.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// li r31,24
	r31.s64 = 24;
loc_82CE7E64:
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
loc_82CE7E68:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r11,r11,4,0,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r27,r10
	ctx.r10.u64 = r27.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v52,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v0,v0,v52,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v0.u8), simd::load_i8(v52.u8), 12));
	// lvlx128 v50,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v51,4
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// lvlx128 v49,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v13,v13,v50,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v13.u8), simd::load_i8(v50.u8), 12));
	// lvlx128 v48,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v49,4
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v49.u8), 12));
	// lvlx128 v47,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v12,v12,v48,4
	simd::store_i8(ctx.v12.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(v48.u8), 12));
	// lvlx128 v46,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v47,4
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v47.u8), 12));
	// lvlx128 v45,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v11,v11,v46,4
	simd::store_i8(ctx.v11.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v11.u8), simd::load_i8(v46.u8), 12));
	// vsldoi128 v60,v60,v45,4
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v45.u8), 12));
	// bne 0x82ce7e68
	if (!cr0.eq) goto loc_82CE7E68;
	// vsrw128 v44,v8,v59
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v44, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v59), simd::set1_i32(0x1F))));
	// vsubfp128 v7,v63,v0
	simd::store_f32_aligned(ctx.v7.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v5,v62,v13
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vsubfp128 v4,v61,v12
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vsubfp128 v3,v60,v11
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// rlwinm r11,r11,4,0,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// vcuxwfp128 v10,v44,31
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v44.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// xor r9,r25,r3
	ctx.r9.u64 = r25.u64 ^ ctx.r3.u64;
	// rlwinm r11,r9,0,0,24
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vmaddfp v2,v7,v10,v0
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v1,v5,v10,v13
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v31,v4,v10,v12
	simd::store_f32_aligned(v31.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v30,v3,v10,v11
	simd::store_f32_aligned(v30.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v43,v2,v9
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v42,v1,v9
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v41,v31,v9
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v40,v30,v9
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v43,r5,r22
	ea = (ctx.r5.u32 + r22.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v43), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v42,r5,r23
	ea = (ctx.r5.u32 + r23.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v42), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v41,r5,r24
	ea = (ctx.r5.u32 + r24.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v41), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v40,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v40), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce7f30
	if (cr6.eq) goto loc_82CE7F30;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE7F30:
	// vadduwm v8,v8,v6
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v6.u32)));
	// addic. r6,r6,-4
	xer.ca = ctx.r6.u32 > 3;
	ctx.r6.s64 = ctx.r6.s64 + -4;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// vaddfp128 v9,v9,v58
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(v58.f32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// bgt 0x82ce7e64
	if (cr0.gt) goto loc_82CE7E64;
	// lbz r11,13(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 13);
	// lwz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// rotlwi r8,r11,2
	ctx.r8.u64 = rotl32(r11.u32, 2);
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce7f70
	if (cr6.lt) goto loc_82CE7F70;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE7F70:
	// lwz r11,20(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(20) );
	// stw r9,8(r26)
	PPC_STORE_U32(r26.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce7f90
	if (!cr6.lt) goto loc_82CE7F90;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE7F90:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r26)
	PPC_STORE_U32(r26.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r26.u32 + 48, temp.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c20
	return;
}

PPC_WEAK_FUNC(sub_82CE7D20) {
	__imp__sub_82CE7D20(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE7FB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb4
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r20,r7
	r20.u64 = ctx.r7.u64;
	// lfs f0,36(r20)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r20.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// li r7,4
	ctx.r7.s64 = 4;
	// lfs f10,48(r20)
	temp.u32 = PPC_LOAD_U32(r20.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stfs f13,36(r20)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r20.u32 + 36, temp.u32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r8,r9,26704
	ctx.r8.s64 = ctx.r9.s64 + 26704;
	// vspltisw128 v57,1
	simd::store_i32(v57.u32, simd::set1_i32(int32_t(0x1)));
	// addi r11,r1,104
	r11.s64 = ctx.r1.s64 + 104;
	// vor128 v7,v63,v63
	simd::store_i8(ctx.v7.u8, simd::load_i8(v63.u8));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// vor128 v8,v63,v63
	simd::store_i8(ctx.v8.u8, simd::load_i8(v63.u8));
	// addi r31,r1,88
	r31.s64 = ctx.r1.s64 + 88;
	// vor128 v9,v63,v63
	simd::store_i8(ctx.v9.u8, simd::load_i8(v63.u8));
	// lvlx128 v62,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r21,0
	r21.s64 = 0;
	// vspltw128 v3,v62,0
	simd::store_i32(ctx.v3.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// vor128 v10,v63,v63
	simd::store_i8(ctx.v10.u8, simd::load_i8(v63.u8));
	// vor128 v11,v63,v63
	simd::store_i8(ctx.v11.u8, simd::load_i8(v63.u8));
	// li r23,20
	r23.s64 = 20;
	// vor128 v12,v63,v63
	simd::store_i8(ctx.v12.u8, simd::load_i8(v63.u8));
	// li r24,44
	r24.s64 = 44;
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r22,88(r1)
	r22.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// std r22,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r22.u64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lvlx128 v61,r7,r4
	temp.u32 = ctx.r7.u32 + ctx.r4.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// vspltw128 v0,v61,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// lvx128 v5,r0,r8
	simd::store_shuffled(ctx.v5, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lvlx128 v56,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v58,v63,v63
	simd::store_i8(v58.u8, simd::load_i8(v63.u8));
	// vspltw128 v6,v56,0
	simd::store_i32(ctx.v6.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vor128 v59,v63,v63
	simd::store_i8(v59.u8, simd::load_i8(v63.u8));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v55,v63,v0,4
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// li r25,16
	r25.s64 = 16;
	// lfs f13,2636(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2636);
	ctx.f13.f64 = double(temp.f32);
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// li r26,40
	r26.s64 = 40;
	// vadduwm v2,v0,v13
	simd::store_u32(ctx.v2.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v53,v55,v13,4
	simd::store_i8(v53.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v4,v13,v13
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// li r27,12
	r27.s64 = 12;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// li r28,36
	r28.s64 = 36;
	// li r29,8
	r29.s64 = 8;
	// vsldoi128 v0,v53,v2,4
	simd::store_i8(ctx.v0.u8, simd::shift_left_insert_bytes(simd::load_i8(v53.u8), simd::load_i8(ctx.v2.u8), 12));
	// li r30,32
	r30.s64 = 32;
	// vadduwm v6,v0,v6
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v6.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v1,v54,0
	simd::store_i32(ctx.v1.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v52,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v56,v52,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// vmaddfp v13,v1,v5,v3
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// li r31,28
	r31.s64 = 28;
	// li r4,24
	ctx.r4.s64 = 24;
	// li r15,5120
	r15.s64 = 5120;
	// li r16,4096
	r16.s64 = 4096;
	// li r17,3072
	r17.s64 = 3072;
	// li r18,2048
	r18.s64 = 2048;
	// li r19,1024
	r19.s64 = 1024;
loc_82CE8124:
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
loc_82CE8128:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r22,r10
	ctx.r10.u64 = r22.u64 + ctx.r10.u64;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v51,r11,r23
	temp.u32 = r11.u32 + r23.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v50,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v12,v12,v51,4
	simd::store_i8(ctx.v12.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v12.u8), simd::load_i8(v51.u8), 12));
	// lvlx128 v49,r11,r25
	temp.u32 = r11.u32 + r25.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v63,v63,v50,4
	simd::store_i8(v63.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v50.u8), 12));
	// lvlx128 v48,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v11,v11,v49,4
	simd::store_i8(ctx.v11.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v11.u8), simd::load_i8(v49.u8), 12));
	// lvlx128 v47,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v48,4
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v48.u8), 12));
	// lvlx128 v46,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v10,v10,v47,4
	simd::store_i8(ctx.v10.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v10.u8), simd::load_i8(v47.u8), 12));
	// lvlx128 v45,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v46,4
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v46.u8), 12));
	// lvlx128 v44,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v9,v9,v45,4
	simd::store_i8(ctx.v9.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v9.u8), simd::load_i8(v45.u8), 12));
	// lvlx128 v43,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v60,v60,v44,4
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v44.u8), 12));
	// lvlx128 v42,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v8,v8,v43,4
	simd::store_i8(ctx.v8.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v8.u8), simd::load_i8(v43.u8), 12));
	// lvlx128 v41,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v59,v59,v42,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(v42.u8), 12));
	// lvlx128 v40,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v7,v7,v41,4
	simd::store_i8(ctx.v7.u8, simd::shift_left_insert_bytes(simd::load_i8(ctx.v7.u8), simd::load_i8(v41.u8), 12));
	// vsldoi128 v58,v58,v40,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(v40.u8), 12));
	// bne 0x82ce8128
	if (!cr0.eq) goto loc_82CE8128;
	// vsrw128 v39,v6,v57
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v39, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v6), simd::and_u32(simd::to_vec128i(v57), simd::set1_i32(0x1F))));
	// vsubfp128 v5,v63,v12
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v3,v62,v11
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vsubfp128 v2,v61,v10
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vsubfp128 v1,v60,v9
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vcuxwfp128 v0,v39,31
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v39.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsubfp128 v31,v59,v8
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// vsubfp128 v30,v58,v7
	simd::store_f32_aligned(v30.f32, simd::sub_f32(simd::load_f32_aligned(v58.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// xor r9,r21,r3
	ctx.r9.u64 = r21.u64 ^ ctx.r3.u64;
	// rlwinm r8,r9,0,0,24
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vmaddfp v29,v5,v0,v12
	simd::store_f32_aligned(v29.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v28,v3,v0,v11
	simd::store_f32_aligned(v28.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmaddfp v27,v2,v0,v10
	simd::store_f32_aligned(v27.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vmaddfp v26,v1,v0,v9
	simd::store_f32_aligned(v26.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v25,v31,v0,v8
	simd::store_f32_aligned(v25.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v8.f32)));
	// vmaddfp v24,v30,v0,v7
	simd::store_f32_aligned(v24.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v38,v29,v13
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v37,v28,v13
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v36,v27,v13
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v35,v26,v13
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v34,v25,v13
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v33,v24,v13
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// stvx128 v38,r5,r15
	ea = (ctx.r5.u32 + r15.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v38), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v37,r5,r16
	ea = (ctx.r5.u32 + r16.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v37), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v36,r5,r17
	ea = (ctx.r5.u32 + r17.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v36), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v35,r5,r18
	ea = (ctx.r5.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v35), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v34,r5,r19
	ea = (ctx.r5.u32 + r19.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v34), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v33,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v33), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce8240
	if (cr6.eq) goto loc_82CE8240;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE8240:
	// vadduwm v6,v6,v4
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v4.u32)));
	// addic. r6,r6,-4
	xer.ca = ctx.r6.u32 > 3;
	ctx.r6.s64 = ctx.r6.s64 + -4;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
	// vaddfp128 v13,v13,v56
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v56.f32)));
	// addi r5,r5,16
	ctx.r5.s64 = ctx.r5.s64 + 16;
	// bgt 0x82ce8124
	if (cr0.gt) goto loc_82CE8124;
	// lbz r11,13(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 13);
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// rotlwi r8,r11,2
	ctx.r8.u64 = rotl32(r11.u32, 2);
	// lwz r11,4(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce8280
	if (cr6.lt) goto loc_82CE8280;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE8280:
	// lwz r11,20(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(20) );
	// stw r9,8(r20)
	PPC_STORE_U32(r20.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce82a0
	if (!cr6.lt) goto loc_82CE82A0;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE82A0:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r20)
	PPC_STORE_U32(r20.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r20)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r20.u32 + 48, temp.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c04
	return;
}

PPC_WEAK_FUNC(sub_82CE7FB8) {
	__imp__sub_82CE7FB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE82C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lfs f0,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 36, temp.u32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lfs f11,44(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// addi r11,r9,26752
	r11.s64 = ctx.r9.s64 + 26752;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// addi r30,r1,104
	r30.s64 = ctx.r1.s64 + 104;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// lvx128 v10,r0,r11
	simd::store_shuffled(ctx.v10, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// addi r9,r8,26720
	ctx.r9.s64 = ctx.r8.s64 + 26720;
	// addi r8,r7,26736
	ctx.r8.s64 = ctx.r7.s64 + 26736;
	// vspltisw128 v63,1
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x1)));
	// addi r7,r4,26704
	ctx.r7.s64 = ctx.r4.s64 + 26704;
	// vslb v1,v0,v0
	simd::store_shifted_i8(ctx.v1, ctx.v0, ctx.v0);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// vor128 v31,v62,v62
	simd::store_i8(v31.u8, simd::load_i8(v62.u8));
	// lis r29,-32256
	r29.s64 = -2113929216;
	// vor128 v30,v62,v62
	simd::store_i8(v30.u8, simd::load_i8(v62.u8));
	// lvx128 v9,r0,r9
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f6,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.f6.u64);
	// ld r11,104(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// lvx128 v7,r0,r8
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lvx128 v5,r0,r7
	simd::store_shuffled(ctx.v5, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// lvlx128 v58,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lvlx128 v60,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lvlx128 v61,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v0,v61,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// vspltw128 v13,v60,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lfs f13,2908(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 2908);
	ctx.f13.f64 = double(temp.f32);
	// vspltw128 v11,v58,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// li r4,0
	ctx.r4.s64 = 0;
	// vadduwm v12,v0,v0
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v59,v62,v0,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(ctx.v0.u8), 12));
	// vadduwm v8,v0,v12
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// vsldoi128 v57,v59,v12,4
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(ctx.v12.u8), 12));
	// vadduwm v0,v12,v12
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v12.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v56,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vsldoi128 v8,v57,v8,4
	simd::store_i8(ctx.v8.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(ctx.v8.u8), 12));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vmaddfp v10,v12,v10,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// lvlx128 v55,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmaddfp v9,v12,v9,v13
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v7,v12,v7,v13
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vspltw128 v62,v55,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vadduwm v6,v8,v0
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v0.u32)));
	// vmaddfp v5,v12,v5,v13
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vadduwm v3,v8,v11
	simd::store_u32(ctx.v3.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v11.u32)));
	// vadduwm v12,v6,v0
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v0.u32)));
	// vadduwm v0,v12,v0
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v0.u32)));
	// li r30,16
	r30.s64 = 16;
	// vadduwm v4,v6,v11
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v11.u32)));
	// li r7,1
	ctx.r7.s64 = 1;
	// vadduwm v2,v13,v13
	simd::store_u32(ctx.v2.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// li r28,32
	r28.s64 = 32;
	// vadduwm v6,v12,v11
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v11.u32)));
	// li r29,48
	r29.s64 = 48;
	// vadduwm v8,v0,v11
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v11.u32)));
loc_82CE843C:
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_82CE8440:
	// rldicl r9,r11,32,32
	ctx.r9.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r9,r7
	temp.u32 = ctx.r9.u32 + ctx.r7.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v31,v31,v54,1
	simd::store_i8(v31.u8, simd::shift_left_insert_bytes(simd::load_i8(v31.u8), simd::load_i8(v54.u8), 15));
	// vsldoi128 v30,v30,v53,1
	simd::store_i8(v30.u8, simd::shift_left_insert_bytes(simd::load_i8(v30.u8), simd::load_i8(v53.u8), 15));
	// bne 0x82ce8440
	if (!cr0.eq) goto loc_82CE8440;
	// vaddubm v0,v31,v1
	simd::store_u8(ctx.v0.u8, simd::add_u8(simd::load_u8(v31.u8), simd::load_u8(ctx.v1.u8)));
	// rldicl r10,r11,32,32
	ctx.r10.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// vaddubm v13,v30,v1
	simd::store_u8(ctx.v13.u8, simd::add_u8(simd::load_u8(v30.u8), simd::load_u8(ctx.v1.u8)));
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// vsrw128 v52,v3,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v52, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v3), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vsrw128 v51,v4,v63
simd::store_shuffled(v51, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v4), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupkhsb128 v48,v0,v0
	simd::store_i16(v48.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v0.s8)));
	// vsrw128 v50,v6,v63
simd::store_shuffled(v50, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v6), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupklsb128 v47,v0,v0
	simd::store_i16(v47.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v0.s8)));
	// vsrw128 v49,v8,v63
simd::store_shuffled(v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupkhsb128 v46,v13,v0
	simd::store_i16(v46.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v13.s8)));
	// vcuxwfp128 v29,v52,31
	simd::store_f32_aligned(v29.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v52.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v45,v13,v0
	simd::store_i16(v45.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v13.s8)));
	// vcuxwfp128 v28,v51,31
	simd::store_f32_aligned(v28.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v51.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v44,v48,v96
	simd::store_i32(v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v48.s16)));
	// vcuxwfp128 v27,v50,31
	simd::store_f32_aligned(v27.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v50.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v43,v48,v96
	simd::store_i32(v43.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v48.s16)));
	// vcuxwfp128 v26,v49,31
	simd::store_f32_aligned(v26.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v42,v47,v96
	simd::store_i32(v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v47.s16)));
	// xor r10,r4,r3
	ctx.r10.u64 = ctx.r4.u64 ^ ctx.r3.u64;
	// vupklsb128 v41,v47,v96
	simd::store_i32(v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v47.s16)));
	// vupkhsb128 v40,v46,v96
	simd::store_i32(v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v46.s16)));
	// vcsxwfp128 v0,v44,7
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v39,v46,v96
	simd::store_i32(v39.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v46.s16)));
	// vcsxwfp128 v13,v43,7
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v38,v45,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v45.s16)));
	// vcsxwfp128 v12,v42,7
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v42.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v37,v45,v96
	simd::store_i32(v37.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v45.s16)));
	// vcsxwfp128 v11,v41,7
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v36,v40,7
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// rlwinm r9,r10,0,0,24
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// vcsxwfp128 v35,v39,7
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v34,v38,7
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vcsxwfp128 v33,v37,7
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v25,v36,v0
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v24,v35,v13
	simd::store_f32_aligned(v24.f32, simd::sub_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v23,v34,v12
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v22,v33,v11
	simd::store_f32_aligned(v22.f32, simd::sub_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vmaddfp v21,v25,v29,v0
	simd::store_f32_aligned(v21.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(v29.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v20,v24,v28,v13
	simd::store_f32_aligned(v20.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(v28.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v19,v23,v27,v12
	simd::store_f32_aligned(v19.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(v27.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v18,v22,v26,v11
	simd::store_f32_aligned(v18.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v22.f32), simd::load_f32_aligned(v26.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v32,v21,v5
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::load_f32_aligned(v21.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vmulfp128 v61,v20,v7
	simd::store_f32_aligned(v61.f32, simd::mul_f32(simd::load_f32_aligned(v20.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v60,v19,v9
	simd::store_f32_aligned(v60.f32, simd::mul_f32(simd::load_f32_aligned(v19.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v59,v18,v10
	simd::store_f32_aligned(v59.f32, simd::mul_f32(simd::load_f32_aligned(v18.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// stvx128 v32,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v32), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v61,r5,r30
	ea = (ctx.r5.u32 + r30.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v61), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v60,r5,r28
	ea = (ctx.r5.u32 + r28.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v60), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v59,r5,r29
	ea = (ctx.r5.u32 + r29.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v59), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce8540
	if (cr6.eq) goto loc_82CE8540;
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r3
loc_82CE8540:
	// vadduwm v8,v8,v2
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v2.u32)));
	// addic. r6,r6,-16
	xer.ca = ctx.r6.u32 > 15;
	ctx.r6.s64 = ctx.r6.s64 + -16;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v6,v6,v2
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v2.u32)));
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// vadduwm v4,v4,v2
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v4.u32), simd::load_u32(ctx.v2.u32)));
	// vaddfp128 v10,v10,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(v62.f32)));
	// vadduwm v3,v3,v2
	simd::store_u32(ctx.v3.u32, simd::add_u32(simd::load_u32(ctx.v3.u32), simd::load_u32(ctx.v2.u32)));
	// vaddfp128 v9,v9,v62
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(v62.f32)));
	// vaddfp128 v7,v7,v62
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(v62.f32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vaddfp128 v5,v5,v62
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(v62.f32)));
	// bgt 0x82ce843c
	if (cr0.gt) goto loc_82CE843C;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lbz r9,13(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// subf r8,r10,r3
	ctx.r8.s64 = ctx.r3.s64 - ctx.r10.s64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// twllei r9,0
	// divwu r9,r8,r9
	ctx.r9.u32 = ctx.r8.u32 / ctx.r9.u32;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce8594
	if (cr6.lt) goto loc_82CE8594;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_82CE8594:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ce85b4
	if (!cr6.lt) goto loc_82CE85B4;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82CE85B4:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE82C8) {
	__imp__sub_82CE82C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE85E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// lfs f0,36(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r28.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r28)
	temp.u32 = PPC_LOAD_U32(r28.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,48(r28)
	temp.u32 = PPC_LOAD_U32(r28.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfs f13,36(r28)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r28.u32 + 36, temp.u32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// addi r10,r9,26752
	ctx.r10.s64 = ctx.r9.s64 + 26752;
	// lfs f11,44(r28)
	temp.u32 = PPC_LOAD_U32(r28.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// addi r9,r8,26720
	ctx.r9.s64 = ctx.r8.s64 + 26720;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// lvx128 v9,r0,r10
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// addi r8,r7,26736
	ctx.r8.s64 = ctx.r7.s64 + 26736;
	// addi r7,r4,26704
	ctx.r7.s64 = ctx.r4.s64 + 26704;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// li r11,4
	r11.s64 = 4;
	// vspltisw128 v63,1
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x1)));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// vor128 v22,v62,v62
	simd::store_i8(v22.u8, simd::load_i8(v62.u8));
	// lvx128 v8,r0,r9
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v7,r0,r8
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// lvx128 v3,r0,r7
	simd::store_shuffled(ctx.v3, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,104
	ctx.r7.s64 = ctx.r1.s64 + 104;
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f6,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.f6.u64);
	// ld r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// lvlx128 v59,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r29,88(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r29,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r29.u64);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// lvlx128 v61,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v61,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vadduwm v11,v13,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v58,v62,v13,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(ctx.v13.u8), 12));
	// lvlx128 v60,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v60,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// vspltw128 v10,v59,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// lfs f13,2908(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2908);
	ctx.f13.f64 = double(temp.f32);
	// vadduwm v6,v13,v11
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// li r30,0
	r30.s64 = 0;
	// vsldoi128 v57,v58,v11,4
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(ctx.v11.u8), 12));
	// vadduwm v13,v11,v11
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v11.u32)));
	// vor128 v24,v62,v62
	simd::store_i8(v24.u8, simd::load_i8(v62.u8));
	// vor128 v21,v62,v62
	simd::store_i8(v21.u8, simd::load_i8(v62.u8));
	// vor128 v23,v62,v62
	simd::store_i8(v23.u8, simd::load_i8(v62.u8));
	// vsldoi128 v6,v57,v6,4
	simd::store_i8(ctx.v6.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(ctx.v6.u8), 12));
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v56,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v11,v56,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vmaddfp v9,v11,v9,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// lvlx128 v55,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmaddfp v8,v11,v8,v12
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v4,v13,v13
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vmaddfp v7,v11,v7,v12
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v5,v6,v13
	simd::store_u32(ctx.v5.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v13.u32)));
	// vmaddfp v12,v11,v3,v12
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v3.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v25,v6,v10
	simd::store_u32(v25.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v10.u32)));
	// vadduwm v11,v5,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v5.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v62,v55,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// addi r9,r5,1056
	ctx.r9.s64 = ctx.r5.s64 + 1056;
	// vadduwm v26,v5,v10
	simd::store_u32(v26.u32, simd::add_u32(simd::load_u32(ctx.v5.u32), simd::load_u32(ctx.v10.u32)));
	// vadduwm v20,v4,v4
	simd::store_u32(v20.u32, simd::add_u32(simd::load_u32(ctx.v4.u32), simd::load_u32(ctx.v4.u32)));
	// li r27,16
	r27.s64 = 16;
	// li r31,1
	r31.s64 = 1;
	// vadduwm v13,v11,v13
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v13.u32)));
	// li r4,3
	ctx.r4.s64 = 3;
	// vadduwm v27,v11,v10
	simd::store_u32(v27.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v10.u32)));
	// li r7,2
	ctx.r7.s64 = 2;
	// li r22,-32
	r22.s64 = -32;
	// li r23,-16
	r23.s64 = -16;
	// vadduwm v28,v13,v10
	simd::store_u32(v28.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v10.u32)));
	// li r24,-1040
	r24.s64 = -1040;
	// li r25,-1024
	r25.s64 = -1024;
	// li r26,-1008
	r26.s64 = -1008;
loc_82CE8774:
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
loc_82CE8778:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r29,r10
	ctx.r10.u64 = r29.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v54,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v24,v24,v54,1
	simd::store_i8(v24.u8, simd::shift_left_insert_bytes(simd::load_i8(v24.u8), simd::load_i8(v54.u8), 15));
	// lvlx128 v52,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v23,v23,v53,1
	simd::store_i8(v23.u8, simd::shift_left_insert_bytes(simd::load_i8(v23.u8), simd::load_i8(v53.u8), 15));
	// lvlx128 v51,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v22,v22,v52,1
	simd::store_i8(v22.u8, simd::shift_left_insert_bytes(simd::load_i8(v22.u8), simd::load_i8(v52.u8), 15));
	// vsldoi128 v21,v21,v51,1
	simd::store_i8(v21.u8, simd::shift_left_insert_bytes(simd::load_i8(v21.u8), simd::load_i8(v51.u8), 15));
	// bne 0x82ce8778
	if (!cr0.eq) goto loc_82CE8778;
	// vaddubm v10,v22,v0
	simd::store_u8(ctx.v10.u8, simd::add_u8(simd::load_u8(v22.u8), simd::load_u8(ctx.v0.u8)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vaddubm v6,v21,v0
	simd::store_u8(ctx.v6.u8, simd::add_u8(simd::load_u8(v21.u8), simd::load_u8(ctx.v0.u8)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vaddubm v13,v24,v0
	simd::store_u8(ctx.v13.u8, simd::add_u8(simd::load_u8(v24.u8), simd::load_u8(ctx.v0.u8)));
	// rlwinm r11,r11,1,0,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vaddubm v11,v23,v0
	simd::store_u8(ctx.v11.u8, simd::add_u8(simd::load_u8(v23.u8), simd::load_u8(ctx.v0.u8)));
	// vor128 v50,v10,v10
	simd::store_i8(v50.u8, simd::load_i8(ctx.v10.u8));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vor128 v49,v6,v6
	simd::store_i8(v49.u8, simd::load_i8(ctx.v6.u8));
	// vupkhsb128 v48,v13,v0
	simd::store_i16(v48.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v13.s8)));
	// vsrw128 v47,v28,v63
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v47, simd::shift_right_logical_i32(simd::to_vec128i(v28), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupklsb128 v46,v13,v0
	simd::store_i16(v46.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v13.s8)));
	// vsrw128 v45,v27,v63
simd::store_shuffled(v45, simd::shift_right_logical_i32(simd::to_vec128i(v27), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupkhsb128 v44,v11,v0
	simd::store_i16(v44.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v11.s8)));
	// vsrw128 v43,v26,v63
simd::store_shuffled(v43, simd::shift_right_logical_i32(simd::to_vec128i(v26), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupklsb128 v42,v11,v0
	simd::store_i16(v42.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v11.s8)));
	// vsrw128 v41,v25,v63
simd::store_shuffled(v41, simd::shift_right_logical_i32(simd::to_vec128i(v25), simd::and_u32(simd::to_vec128i(v63), simd::set1_i32(0x1F))));
	// vupkhsb128 v40,v50,v0
	simd::store_i16(v40.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v50.s8)));
	// vcuxwfp128 v13,v47,31
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v47.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v39,v49,v0
	simd::store_i16(v39.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v49.s8)));
	// vcuxwfp128 v11,v45,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v45.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v38,v48,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v48.s16)));
	// vcuxwfp128 v10,v43,31
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v43.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v37,v48,v96
	simd::store_i32(v37.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v48.s16)));
	// vcuxwfp128 v6,v41,31
	simd::store_f32_aligned(ctx.v6.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v41.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v36,v46,v96
	simd::store_i32(v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v46.s16)));
	// xor r8,r30,r3
	ctx.r8.u64 = r30.u64 ^ ctx.r3.u64;
	// vupklsb128 v35,v46,v96
	simd::store_i32(v35.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v46.s16)));
	// vupkhsb128 v34,v44,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v44.s16)));
	// vcsxwfp128 v5,v38,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v33,v44,v96
	simd::store_i32(v33.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v44.s16)));
	// vcsxwfp128 v4,v37,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v32,v42,v96
	simd::store_i32(v32.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v42.s16)));
	// vcsxwfp128 v3,v36,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v61,v42,v96
	simd::store_i32(v61.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v42.s16)));
	// vcsxwfp128 v2,v35,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v60,v40,v96
	simd::store_i32(v60.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v40.s16)));
	// vcsxwfp128 v59,v34,7
	simd::store_f32_aligned(v59.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v58,v39,v96
	simd::store_i32(v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v39.s16)));
	// vcsxwfp128 v57,v33,7
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v56,v32,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v55,v40,v96
	simd::store_i32(v55.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v40.s16)));
	// vcsxwfp128 v54,v61,7
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v61.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v53,v39,v96
	simd::store_i32(v53.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v39.s16)));
	// vcsxwfp128 v1,v60,7
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v52,v50,v0
	simd::store_i16(v52.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v50.s8)));
	// vcsxwfp128 v51,v58,7
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v50,v49,v0
	simd::store_i16(v50.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v49.s8)));
	// vcsxwfp128 v31,v55,7
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v49,v53,7
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v53.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v48,v52,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// vupklsb128 v47,v52,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v52.s16)));
	// vupkhsb128 v46,v50,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v50.s16)));
	// vupklsb128 v45,v50,v96
	simd::store_i32(v45.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v50.s16)));
	// vsubfp128 v19,v59,v5
	simd::store_f32_aligned(v19.f32, simd::sub_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vsubfp128 v18,v57,v4
	simd::store_f32_aligned(v18.f32, simd::sub_f32(simd::load_f32_aligned(v57.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// vsubfp128 v17,v56,v3
	simd::store_f32_aligned(v17.f32, simd::sub_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// vsubfp128 v16,v54,v2
	simd::store_f32_aligned(v16.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vcsxwfp128 v30,v48,7
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v15,v51,v1
	simd::store_f32_aligned(v15.f32, simd::sub_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(ctx.v1.f32)));
	// vcsxwfp128 v29,v47,7
	simd::store_f32_aligned(v29.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v14,v49,v31
	simd::store_f32_aligned(v14.f32, simd::sub_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v31.f32)));
	// vcsxwfp128 v39,v46,7
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v38,v45,7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmaddfp v5,v19,v6,v5
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v19.f32), simd::load_f32_aligned(ctx.v6.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v4,v18,v10,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v18.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v3,v17,v11,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v17.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v2,v16,v13,v2
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v16.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vmaddfp v1,v15,v6,v1
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v15.f32), simd::load_f32_aligned(ctx.v6.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v31,v14,v10,v31
	simd::store_f32_aligned(v31.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v14.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v31.f32)));
	// vmulfp128 v44,v5,v12
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v43,v4,v7
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v42,v3,v8
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v41,v2,v9
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v40,v1,v12
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// stvx128 v44,r9,r22
	ea = (ctx.r9.u32 + r22.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v44), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v43,r9,r23
	ea = (ctx.r9.u32 + r23.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v43), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v42,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v42), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v41,r9,r27
	ea = (ctx.r9.u32 + r27.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v41), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v40,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v40), &VectorMaskL[(ea & 0xF) * 16]);
	// vsubfp128 v19,v39,v30
	simd::store_f32_aligned(v19.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v30.f32)));
	// rlwinm r11,r8,0,0,24
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// vsubfp128 v18,v38,v29
	simd::store_f32_aligned(v18.f32, simd::sub_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v29.f32)));
	// vmulfp128 v37,v31,v7
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vmaddfp v17,v19,v11,v30
	simd::store_f32_aligned(v17.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v19.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v30.f32)));
	// vmaddfp v16,v18,v13,v29
	simd::store_f32_aligned(v16.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v18.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(v29.f32)));
	// stvx128 v37,r9,r24
	ea = (ctx.r9.u32 + r24.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v37), &VectorMaskL[(ea & 0xF) * 16]);
	// vmulfp128 v36,v17,v8
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::load_f32_aligned(v17.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v35,v16,v9
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::load_f32_aligned(v16.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v36,r9,r25
	ea = (ctx.r9.u32 + r25.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v36), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v35,r9,r26
	ea = (ctx.r9.u32 + r26.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v35), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce8930
	if (cr6.eq) goto loc_82CE8930;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE8930:
	// vadduwm v28,v28,v20
	simd::store_u32(v28.u32, simd::add_u32(simd::load_u32(v28.u32), simd::load_u32(v20.u32)));
	// addic. r6,r6,-16
	xer.ca = ctx.r6.u32 > 15;
	ctx.r6.s64 = ctx.r6.s64 + -16;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v27,v27,v20
	simd::store_u32(v27.u32, simd::add_u32(simd::load_u32(v27.u32), simd::load_u32(v20.u32)));
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// vadduwm v26,v26,v20
	simd::store_u32(v26.u32, simd::add_u32(simd::load_u32(v26.u32), simd::load_u32(v20.u32)));
	// vaddfp128 v9,v9,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(v62.f32)));
	// vadduwm v25,v25,v20
	simd::store_u32(v25.u32, simd::add_u32(simd::load_u32(v25.u32), simd::load_u32(v20.u32)));
	// vaddfp128 v8,v8,v62
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(v62.f32)));
	// vaddfp128 v7,v7,v62
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(v62.f32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vaddfp128 v12,v12,v62
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v62.f32)));
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// bgt 0x82ce8774
	if (cr0.gt) goto loc_82CE8774;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lbz r9,13(r28)
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + 13);
	// subf r8,r11,r3
	ctx.r8.s64 = ctx.r3.s64 - r11.s64;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(4) );
	// twllei r9,0
	// divwu r9,r8,r9
	ctx.r9.u32 = ctx.r8.u32 / ctx.r9.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce8988
	if (cr6.lt) goto loc_82CE8988;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE8988:
	// lwz r11,20(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(20) );
	// stw r9,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce89a8
	if (!cr6.lt) goto loc_82CE89A8;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE89A8:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r28)
	PPC_STORE_U32(r28.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r28)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r28.u32 + 48, temp.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c20
	return;
}

PPC_WEAK_FUNC(sub_82CE85E0) {
	__imp__sub_82CE85E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE89D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f0,36(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	f0.f64 = double(temp.f32);
	// stw r6,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r6.u32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stw r7,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, ctx.r7.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,48(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfs f13,36(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 36, temp.u32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lfs f11,44(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// addi r4,r9,26752
	ctx.r4.s64 = ctx.r9.s64 + 26752;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// addi r9,r6,26704
	ctx.r9.s64 = ctx.r6.s64 + 26704;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// li r27,4
	r27.s64 = 4;
	// addi r10,r7,26736
	ctx.r10.s64 = ctx.r7.s64 + 26736;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lvx128 v9,r0,r4
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// lvx128 v3,r0,r9
	simd::store_shuffled(ctx.v3, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// addi r11,r8,26720
	r11.s64 = ctx.r8.s64 + 26720;
	// vor128 v20,v63,v63
	simd::store_i8(v20.u8, simd::load_i8(v63.u8));
	// lvx128 v7,r0,r10
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// vor128 v22,v63,v63
	simd::store_i8(v22.u8, simd::load_i8(v63.u8));
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f6,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.f6.u64);
	// ld r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// lvlx128 v60,r0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// std r9,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r9.u64);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// lvlx128 v61,r27,r7
	temp.u32 = r27.u32 + ctx.r7.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r25,88(r1)
	r25.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r25,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r25.u64);
	// lvx128 v8,r0,r11
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// lvlx128 v62,r27,r8
	temp.u32 = r27.u32 + ctx.r8.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v62,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vadduwm v11,v13,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v59,v63,v13,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v13.u8), 12));
	// lfs f13,2908(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2908);
	ctx.f13.f64 = double(temp.f32);
	// vspltw128 v12,v60,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// li r26,0
	r26.s64 = 0;
	// vspltw128 v10,v61,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// vor128 v24,v63,v63
	simd::store_i8(v24.u8, simd::load_i8(v63.u8));
	// vadduwm v6,v13,v11
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// vsldoi128 v58,v59,v11,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(ctx.v11.u8), 12));
	// vadduwm v13,v11,v11
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v11.u32)));
	// vor128 v26,v63,v63
	simd::store_i8(v26.u8, simd::load_i8(v63.u8));
	// vor128 v19,v63,v63
	simd::store_i8(v19.u8, simd::load_i8(v63.u8));
	// vsldoi128 v6,v58,v6,4
	simd::store_i8(ctx.v6.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(ctx.v6.u8), 12));
	// vadduwm v4,v13,v13
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v57,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v11,v57,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v56,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmaddfp v9,v11,v9,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v8,v11,v8,v12
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v5,v6,v13
	simd::store_u32(ctx.v5.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v13.u32)));
	// vmaddfp v7,v11,v7,v12
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v15,v6,v10
	simd::store_u32(v15.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v10.u32)));
	// vmaddfp v6,v11,v3,v12
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v3.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v12,v5,v13
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v5.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v61,v56,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vor128 v21,v63,v63
	simd::store_i8(v21.u8, simd::load_i8(v63.u8));
	// vspltisw128 v62,1
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x1)));
	// addi r11,r5,3104
	r11.s64 = ctx.r5.s64 + 3104;
	// vor128 v23,v63,v63
	simd::store_i8(v23.u8, simd::load_i8(v63.u8));
	// li r24,16
	r24.s64 = 16;
	// vadduwm v13,v12,v13
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v13.u32)));
	// li r28,3
	r28.s64 = 3;
	// vadduwm v16,v5,v10
	simd::store_u32(v16.u32, simd::add_u32(simd::load_u32(ctx.v5.u32), simd::load_u32(ctx.v10.u32)));
	// li r29,7
	r29.s64 = 7;
	// vadduwm v14,v4,v4
	simd::store_u32(v14.u32, simd::add_u32(simd::load_u32(ctx.v4.u32), simd::load_u32(ctx.v4.u32)));
	// li r30,2
	r30.s64 = 2;
	// vadduwm v17,v12,v10
	simd::store_u32(v17.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v10.u32)));
	// li r31,6
	r31.s64 = 6;
	// vor128 v25,v63,v63
	simd::store_i8(v25.u8, simd::load_i8(v63.u8));
	// li r4,1
	ctx.r4.s64 = 1;
	// vadduwm v18,v13,v10
	simd::store_u32(v18.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v10.u32)));
	// li r7,5
	ctx.r7.s64 = 5;
	// li r14,-1056
	r14.s64 = -1056;
	// li r6,-1040
	ctx.r6.s64 = -1040;
	// li r15,-1024
	r15.s64 = -1024;
	// li r16,-1008
	r16.s64 = -1008;
	// li r17,-2080
	r17.s64 = -2080;
	// li r18,-2064
	r18.s64 = -2064;
	// li r19,-2048
	r19.s64 = -2048;
	// li r20,-2032
	r20.s64 = -2032;
	// li r21,-3088
	r21.s64 = -3088;
	// li r22,-3072
	r22.s64 = -3072;
	// li r23,-3056
	r23.s64 = -3056;
loc_82CE8B9C:
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
loc_82CE8BA0:
	// rldicl r10,r9,32,32
	ctx.r10.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r25,r9
	ctx.r9.u64 = r25.u64 + ctx.r9.u64;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lvlx128 v55,r10,r28
	temp.u32 = ctx.r10.u32 + r28.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v54,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v26,v26,v55,1
	simd::store_i8(v26.u8, simd::shift_left_insert_bytes(simd::load_i8(v26.u8), simd::load_i8(v55.u8), 15));
	// lvlx128 v53,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v25,v25,v54,1
	simd::store_i8(v25.u8, simd::shift_left_insert_bytes(simd::load_i8(v25.u8), simd::load_i8(v54.u8), 15));
	// lvlx128 v52,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v24,v24,v53,1
	simd::store_i8(v24.u8, simd::shift_left_insert_bytes(simd::load_i8(v24.u8), simd::load_i8(v53.u8), 15));
	// lvlx128 v51,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v23,v23,v52,1
	simd::store_i8(v23.u8, simd::shift_left_insert_bytes(simd::load_i8(v23.u8), simd::load_i8(v52.u8), 15));
	// lvlx128 v50,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v22,v22,v51,1
	simd::store_i8(v22.u8, simd::shift_left_insert_bytes(simd::load_i8(v22.u8), simd::load_i8(v51.u8), 15));
	// lvlx128 v49,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v21,v21,v50,1
	simd::store_i8(v21.u8, simd::shift_left_insert_bytes(simd::load_i8(v21.u8), simd::load_i8(v50.u8), 15));
	// lvlx128 v48,r10,r27
	temp.u32 = ctx.r10.u32 + r27.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v20,v20,v49,1
	simd::store_i8(v20.u8, simd::shift_left_insert_bytes(simd::load_i8(v20.u8), simd::load_i8(v49.u8), 15));
	// vsldoi128 v19,v19,v48,1
	simd::store_i8(v19.u8, simd::shift_left_insert_bytes(simd::load_i8(v19.u8), simd::load_i8(v48.u8), 15));
	// bne 0x82ce8ba0
	if (!cr0.eq) goto loc_82CE8BA0;
	// vaddubm v13,v26,v0
	simd::store_u8(ctx.v13.u8, simd::add_u8(simd::load_u8(v26.u8), simd::load_u8(ctx.v0.u8)));
	// li r10,-16
	ctx.r10.s64 = -16;
	// vaddubm v12,v25,v0
	simd::store_u8(ctx.v12.u8, simd::add_u8(simd::load_u8(v25.u8), simd::load_u8(ctx.v0.u8)));
	// li r8,-32
	ctx.r8.s64 = -32;
	// vaddubm v11,v24,v0
	simd::store_u8(ctx.v11.u8, simd::add_u8(simd::load_u8(v24.u8), simd::load_u8(ctx.v0.u8)));
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// vaddubm v10,v23,v0
	simd::store_u8(ctx.v10.u8, simd::add_u8(simd::load_u8(v23.u8), simd::load_u8(ctx.v0.u8)));
	// std r7,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r7.u64);
	// vupkhsb128 v45,v13,v0
	simd::store_i16(v45.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v13.s8)));
	// vsrw128 v38,v15,v62
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v38, simd::shift_right_logical_i32(simd::to_vec128i(v15), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vupkhsb128 v41,v12,v0
	simd::store_i16(v41.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v12.s8)));
	// vsrw128 v40,v16,v62
simd::store_shuffled(v40, simd::shift_right_logical_i32(simd::to_vec128i(v16), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vor128 v47,v11,v11
	simd::store_i8(v47.u8, simd::load_i8(ctx.v11.u8));
	// vupklsb128 v43,v13,v0
	simd::store_i16(v43.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v13.s8)));
	// vor128 v46,v10,v10
	simd::store_i8(v46.u8, simd::load_i8(ctx.v10.u8));
	// vupklsb128 v39,v12,v0
	simd::store_i16(v39.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v12.s8)));
	// vupkhsb128 v35,v45,v96
	simd::store_i32(v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v45.s16)));
	// vcuxwfp128 v10,v38,31
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v38.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v63,v41,v96
	simd::store_i32(v63.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcuxwfp128 v11,v40,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v40.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupklsb128 v34,v45,v96
	simd::store_i32(v34.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v45.s16)));
	// vsrw128 v42,v17,v62
simd::store_shuffled(v42, simd::shift_right_logical_i32(simd::to_vec128i(v17), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vupklsb128 v60,v41,v96
	simd::store_i32(v60.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v41.s16)));
	// vsrw128 v44,v18,v62
simd::store_shuffled(v44, simd::shift_right_logical_i32(simd::to_vec128i(v18), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vcsxwfp128 v5,v35,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v37,v47,v0
	simd::store_i16(v37.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v47.s8)));
	// vcsxwfp128 v56,v63,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v63.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v36,v46,v0
	simd::store_i16(v36.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v46.s8)));
	// vcsxwfp128 v4,v34,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v33,v43,v96
	simd::store_i32(v33.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v43.s16)));
	// vcsxwfp128 v54,v60,7
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v32,v43,v96
	simd::store_i32(v32.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v43.s16)));
	// vupkhsb128 v59,v39,v96
	simd::store_i32(v59.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v39.s16)));
	// vor128 v43,v26,v26
	simd::store_i8(v43.u8, simd::load_i8(v26.u8));
	// vupklsb128 v58,v39,v96
	simd::store_i32(v58.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v39.s16)));
	// vcuxwfp128 v12,v42,31
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v42.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v57,v37,v96
	simd::store_i32(v57.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v37.s16)));
	// vcsxwfp128 v3,v33,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v55,v36,v96
	simd::store_i32(v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v36.s16)));
	// vcsxwfp128 v2,v32,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v53,v59,7
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vor128 v42,v25,v25
	simd::store_i8(v42.u8, simd::load_i8(v25.u8));
	// vcsxwfp128 v51,v58,7
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vor128 v41,v24,v24
	simd::store_i8(v41.u8, simd::load_i8(v24.u8));
	// vcsxwfp128 v1,v57,7
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v57.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vor128 v40,v23,v23
	simd::store_i8(v40.u8, simd::load_i8(v23.u8));
	// vcsxwfp128 v48,v55,7
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v52,v47,v0
	simd::store_i16(v52.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v47.s8)));
	// vcuxwfp128 v13,v44,31
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v44.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rldicl r7,r9,32,32
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vsubfp128 v27,v56,v5
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vupklsb128 v49,v46,v0
	simd::store_i16(v49.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v46.s8)));
	// vupklsb128 v50,v37,v96
	simd::store_i32(v50.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v37.s16)));
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vsubfp128 v26,v54,v4
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// vupkhsb128 v46,v52,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// vupklsb128 v45,v52,v96
	simd::store_i32(v45.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v52.s16)));
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// vupklsb128 v47,v36,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v36.s16)));
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vaddubm v28,v22,v0
	simd::store_u8(v28.u8, simd::add_u8(simd::load_u8(v22.u8), simd::load_u8(ctx.v0.u8)));
	// vcsxwfp128 v31,v50,7
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v44,v49,v96
	simd::store_i32(v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vcsxwfp128 v30,v46,7
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v25,v53,v3
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v53.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// vsubfp128 v24,v51,v2
	simd::store_f32_aligned(v24.f32, simd::sub_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vcsxwfp128 v29,v45,7
	simd::store_f32_aligned(v29.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v23,v48,v1
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v5,v27,v10,v5
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v4,v26,v11,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v3,v25,v12,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v2,v24,v13,v2
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vmaddfp v1,v23,v10,v1
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmulfp128 v39,v5,v6
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vmulfp128 v38,v4,v7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v37,v3,v8
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v36,v2,v9
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v35,v1,v6
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// stvx128 v39,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v39), &VectorMaskL[(ea & 0xF) * 16]);
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// stvx128 v38,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v38), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v32,v44,7
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v34,v49,v96
	simd::store_i32(v34.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v49.s16)));
	// vupkhsb128 v60,v28,v0
	simd::store_i16(v60.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v28.s8)));
	// vcsxwfp128 v33,v47,7
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v59,v28,v0
	simd::store_i16(v59.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v28.s8)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vaddubm v27,v21,v0
	simd::store_u8(v27.u8, simd::add_u8(simd::load_u8(v21.u8), simd::load_u8(ctx.v0.u8)));
	// xor r10,r26,r3
	ctx.r10.u64 = r26.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v63,v34,7
	simd::store_f32_aligned(v63.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// addi r26,r1,144
	r26.s64 = ctx.r1.s64 + 144;
	// vaddubm v5,v20,v0
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(v20.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v54,v60,v96
	simd::store_i32(v54.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v60.s16)));
	// vaddubm v4,v19,v0
	simd::store_u8(ctx.v4.u8, simd::add_u8(simd::load_u8(v19.u8), simd::load_u8(ctx.v0.u8)));
	// vupklsb128 v53,v60,v96
	simd::store_i32(v53.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v60.s16)));
	// vupkhsb128 v56,v27,v0
	simd::store_i16(v56.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v27.s8)));
	// stvx128 v36,r11,r24
	ea = (r11.u32 + r24.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v36), &VectorMaskL[(ea & 0xF) * 16]);
	// vupklsb128 v55,v27,v0
	simd::store_i16(v55.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v27.s8)));
	// rlwinm r8,r10,0,0,24
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// vupkhsb128 v52,v59,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vor128 v58,v5,v5
	simd::store_i8(v58.u8, simd::load_i8(ctx.v5.u8));
	// vupklsb128 v51,v59,v96
	simd::store_i32(v51.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v59.s16)));
	// vor128 v57,v4,v4
	simd::store_i8(v57.u8, simd::load_i8(ctx.v4.u8));
	// vupkhsb128 v50,v56,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v56.s16)));
	// vcsxwfp128 v5,v54,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v54.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v28,v32,v30
	simd::store_f32_aligned(v28.f32, simd::sub_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v30.f32)));
	// vupklsb128 v49,v56,v96
	simd::store_i32(v49.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v56.s16)));
	// vsubfp128 v1,v33,v31
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(v31.f32)));
	// vupkhsb128 v48,v55,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v55.s16)));
	// vupklsb128 v47,v55,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v55.s16)));
	// vcsxwfp128 v4,v53,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v53.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v3,v52,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// vsubfp128 v27,v63,v29
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v29.f32)));
	// stvx128 v27,r0,r26
	ea = (r26.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v27), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v2,v51,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vor128 v56,v0,v0
	simd::store_i8(v56.u8, simd::load_i8(ctx.v0.u8));
	// vcsxwfp128 v46,v50,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// stvx128 v37,r0,r11
	ea = (r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v37), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v44,v49,7
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// stvx128 v35,r11,r14
	ea = (r11.u32 + r14.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v35), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v38,v48,7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v45,v58,v0
	simd::store_i16(v45.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v58.s8)));
	// vcsxwfp128 v36,v47,7
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v39,v58,v0
	simd::store_i16(v39.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v58.s8)));
	// vupkhsb128 v37,v57,v0
	simd::store_i16(v37.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v57.s8)));
	// vupklsb128 v35,v57,v0
	simd::store_i16(v35.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v57.s8)));
	// stvx128 v28,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v28), &VectorMaskL[(ea & 0xF) * 16]);
	// vupkhsb128 v34,v45,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v45.s16)));
	// vmaddfp v1,v1,v11,v31
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v31.f32)));
	// stvx128 v1,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v1), &VectorMaskL[(ea & 0xF) * 16]);
	// vupklsb128 v33,v45,v96
	simd::store_i32(v33.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v45.s16)));
	// vupkhsb128 v32,v39,v96
	simd::store_i32(v32.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v39.s16)));
	// vupklsb128 v63,v39,v96
	simd::store_i32(v63.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v39.s16)));
	// vcsxwfp128 v1,v34,7
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v60,v37,v96
	simd::store_i32(v60.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v37.s16)));
	// vsubfp128 v26,v46,v5
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vupklsb128 v59,v37,v96
	simd::store_i32(v59.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v37.s16)));
	// vsubfp128 v25,v44,v4
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// vupkhsb128 v58,v35,v96
	simd::store_i32(v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v35.s16)));
	// vsubfp128 v24,v38,v3
	simd::store_f32_aligned(v24.f32, simd::sub_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// vupklsb128 v57,v35,v96
	simd::store_i32(v57.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v35.s16)));
	// vsubfp128 v23,v36,v2
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vcsxwfp128 v31,v33,7
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v28,v32,7
	simd::store_f32_aligned(v28.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v27,v63,7
	simd::store_f32_aligned(v27.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v63.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v55,v60,7
	simd::store_f32_aligned(v55.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v52,v59,7
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v51,v58,7
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v50,v57,7
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v57.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmaddfp v5,v26,v10,v5
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v4,v25,v11,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// lvx128 v0,r0,r7
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// vmaddfp v3,v24,v12,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v30,v0,v12,v30
	simd::store_f32_aligned(v30.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v30.f32)));
	// lvx128 v0,r0,r26
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r26.u32) & ~0xF), VectorMaskL));
	// vmaddfp v29,v0,v13,v29
	simd::store_f32_aligned(v29.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(v29.f32)));
	// lvx128 v54,r0,r10
	simd::store_shuffled(v54, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// vmulfp128 v53,v54,v7
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v2,v23,v13,v2
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vmulfp128 v49,v30,v8
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v48,v29,v9
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmulfp128 v47,v5,v6
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// stvx128 v53,r11,r6
	ea = (r11.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v53), &VectorMaskL[(ea & 0xF) * 16]);
	// vmulfp128 v46,v4,v7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// stvx128 v49,r11,r15
	ea = (r11.u32 + r15.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v49), &VectorMaskL[(ea & 0xF) * 16]);
	// vsubfp128 v30,v55,v1
	simd::store_f32_aligned(v30.f32, simd::sub_f32(simd::load_f32_aligned(v55.f32), simd::load_f32_aligned(ctx.v1.f32)));
	// stvx128 v48,r11,r16
	ea = (r11.u32 + r16.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v48), &VectorMaskL[(ea & 0xF) * 16]);
	// vsubfp128 v29,v52,v31
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v31.f32)));
	// ld r7,112(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// vsubfp128 v5,v51,v28
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v28.f32)));
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// vsubfp128 v4,v50,v27
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v27.f32)));
	// vor128 v26,v43,v43
	simd::store_i8(v26.u8, simd::load_i8(v43.u8));
	// vmulfp128 v45,v3,v8
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vor128 v25,v42,v42
	simd::store_i8(v25.u8, simd::load_i8(v42.u8));
	// vmulfp128 v44,v2,v9
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vor128 v24,v41,v41
	simd::store_i8(v24.u8, simd::load_i8(v41.u8));
	// vor128 v23,v40,v40
	simd::store_i8(v23.u8, simd::load_i8(v40.u8));
	// vor128 v0,v56,v56
	simd::store_i8(ctx.v0.u8, simd::load_i8(v56.u8));
	// stvx128 v47,r11,r17
	ea = (r11.u32 + r17.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v47), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v46,r11,r18
	ea = (r11.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v46), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v3,v30,v10,v1
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v2,v29,v11,v31
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v31.f32)));
	// vmaddfp v1,v5,v12,v28
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v28.f32)));
	// vmaddfp v31,v4,v13,v27
	simd::store_f32_aligned(v31.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(v27.f32)));
	// stvx128 v45,r11,r19
	ea = (r11.u32 + r19.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v45), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v44,r11,r20
	ea = (r11.u32 + r20.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v44), &VectorMaskL[(ea & 0xF) * 16]);
	// vmulfp128 v39,v3,v6
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vmulfp128 v38,v2,v7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v37,v1,v8
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v36,v31,v9
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v39,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v39), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v38,r11,r21
	ea = (r11.u32 + r21.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v38), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v37,r11,r22
	ea = (r11.u32 + r22.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v37), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v36,r11,r23
	ea = (r11.u32 + r23.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v36), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce8f14
	if (cr6.eq) goto loc_82CE8F14;
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r3
loc_82CE8F14:
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(364) );
	// vadduwm v18,v18,v14
	simd::store_u32(v18.u32, simd::add_u32(simd::load_u32(v18.u32), simd::load_u32(v14.u32)));
	// vadduwm v17,v17,v14
	simd::store_u32(v17.u32, simd::add_u32(simd::load_u32(v17.u32), simd::load_u32(v14.u32)));
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// addic. r10,r10,-16
	xer.ca = ctx.r10.u32 > 15;
	ctx.r10.s64 = ctx.r10.s64 + -16;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// vadduwm v16,v16,v14
	simd::store_u32(v16.u32, simd::add_u32(simd::load_u32(v16.u32), simd::load_u32(v14.u32)));
	// vadduwm v15,v15,v14
	simd::store_u32(v15.u32, simd::add_u32(simd::load_u32(v15.u32), simd::load_u32(v14.u32)));
	// vaddfp128 v9,v9,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(v61.f32)));
	// stw r10,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, ctx.r10.u32);
	// vaddfp128 v8,v8,v61
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(v61.f32)));
	// vaddfp128 v7,v7,v61
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(v61.f32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// vaddfp128 v6,v6,v61
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(v61.f32)));
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// bgt 0x82ce8b9c
	if (cr0.gt) goto loc_82CE8B9C;
	// lwz r31,372(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(372) );
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lbz r10,13(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// subf r8,r11,r3
	ctx.r8.s64 = ctx.r3.s64 - r11.s64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// twllei r10,0
	// divwu r10,r8,r10
	ctx.r10.u32 = ctx.r8.u32 / ctx.r10.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ce8f78
	if (cr6.lt) goto loc_82CE8F78;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CE8F78:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82ce8f98
	if (!cr6.lt) goto loc_82CE8F98;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CE8F98:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82CE89D0) {
	__imp__sub_82CE89D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE8FC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f0,36(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	f0.f64 = double(temp.f32);
	// stw r6,460(r1)
	PPC_STORE_U32(ctx.r1.u32 + 460, ctx.r6.u32);
	// stfs f0,112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stw r7,468(r1)
	PPC_STORE_U32(ctx.r1.u32 + 468, ctx.r7.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,48(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f11,44(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r9,26752
	ctx.r6.s64 = ctx.r9.s64 + 26752;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// li r22,4
	r22.s64 = 4;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// addi r4,r8,26720
	ctx.r4.s64 = ctx.r8.s64 + 26720;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// stfs f13,36(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 36, temp.u32);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// lvx128 v9,r0,r6
	simd::store_shuffled(ctx.v9, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r11,r7,26736
	r11.s64 = ctx.r7.s64 + 26736;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// lvx128 v8,r0,r4
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// vor128 v16,v63,v63
	simd::store_i8(v16.u8, simd::load_i8(v63.u8));
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// li r19,0
	r19.s64 = 0;
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f6,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f6.u64);
	// ld r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// stfd f7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f7.u64);
	// lvlx128 v61,r22,r8
	temp.u32 = r22.u32 + ctx.r8.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r21,80(r1)
	r21.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r21,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r21.u64);
	// lvx128 v7,r0,r11
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// lvlx128 v62,r22,r10
	temp.u32 = r22.u32 + ctx.r10.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v62,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// lvlx128 v60,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vadduwm v11,v13,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v59,v63,v13,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v13.u8), 12));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r11,26704
	ctx.r7.s64 = r11.s64 + 26704;
	// vspltw128 v12,v60,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vspltw128 v10,v61,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// lfs f13,2908(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2908);
	ctx.f13.f64 = double(temp.f32);
	// vadduwm v6,v13,v11
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// vsldoi128 v58,v59,v11,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(ctx.v11.u8), 12));
	// vadduwm v13,v11,v11
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v11.u32)));
	// vor128 v18,v63,v63
	simd::store_i8(v18.u8, simd::load_i8(v63.u8));
	// lvx128 v3,r0,r7
	simd::store_shuffled(ctx.v3, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// vor128 v20,v63,v63
	simd::store_i8(v20.u8, simd::load_i8(v63.u8));
	// vor128 v22,v63,v63
	simd::store_i8(v22.u8, simd::load_i8(v63.u8));
	// vsldoi128 v6,v58,v6,4
	simd::store_i8(ctx.v6.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(ctx.v6.u8), 12));
	// vor128 v24,v63,v63
	simd::store_i8(v24.u8, simd::load_i8(v63.u8));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx128 v57,r0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v11,v57,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx128 v56,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vmaddfp v9,v11,v9,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v8,v11,v8,v12
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v4,v13,v13
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vadduwm v2,v6,v10
	simd::store_u32(ctx.v2.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v10.u32)));
	// vmaddfp v7,v11,v7,v12
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vadduwm v5,v6,v13
	simd::store_u32(ctx.v5.u32, simd::add_u32(simd::load_u32(ctx.v6.u32), simd::load_u32(ctx.v13.u32)));
	// vmaddfp v6,v11,v3,v12
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v3.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// stvx128 v2,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v2), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// vadduwm v31,v4,v4
	simd::store_u32(v31.u32, simd::add_u32(simd::load_u32(ctx.v4.u32), simd::load_u32(ctx.v4.u32)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vor128 v26,v63,v63
	simd::store_i8(v26.u8, simd::load_i8(v63.u8));
	// vadduwm v12,v5,v13
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v5.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v61,v56,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vadduwm v1,v5,v10
	simd::store_u32(ctx.v1.u32, simd::add_u32(simd::load_u32(ctx.v5.u32), simd::load_u32(ctx.v10.u32)));
	// vspltisw128 v62,1
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x1)));
	// vor128 v15,v63,v63
	simd::store_i8(v15.u8, simd::load_i8(v63.u8));
	// addi r11,r5,5152
	r11.s64 = ctx.r5.s64 + 5152;
	// stvx128 v31,r0,r4
	ea = (ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v31), &VectorMaskL[(ea & 0xF) * 16]);
	// vor128 v17,v63,v63
	simd::store_i8(v17.u8, simd::load_i8(v63.u8));
	// vadduwm v13,v12,v13
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v13.u32)));
	// li r20,16
	r20.s64 = 16;
	// vadduwm v30,v12,v10
	simd::store_u32(v30.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v10.u32)));
	// stvx128 v1,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v1), &VectorMaskL[(ea & 0xF) * 16]);
	// vor128 v19,v63,v63
	simd::store_i8(v19.u8, simd::load_i8(v63.u8));
	// li r23,5
	r23.s64 = 5;
	// vor128 v21,v63,v63
	simd::store_i8(v21.u8, simd::load_i8(v63.u8));
	// li r24,11
	r24.s64 = 11;
	// vor128 v23,v63,v63
	simd::store_i8(v23.u8, simd::load_i8(v63.u8));
	// li r25,10
	r25.s64 = 10;
	// stvx128 v30,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v30), &VectorMaskL[(ea & 0xF) * 16]);
	// vadduwm v14,v13,v10
	simd::store_u32(v14.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v10.u32)));
	// vor128 v25,v63,v63
	simd::store_i8(v25.u8, simd::load_i8(v63.u8));
	// li r26,3
	r26.s64 = 3;
	// li r27,9
	r27.s64 = 9;
	// li r28,2
	r28.s64 = 2;
	// li r29,8
	r29.s64 = 8;
	// li r30,1
	r30.s64 = 1;
	// li r31,7
	r31.s64 = 7;
	// li r4,6
	ctx.r4.s64 = 6;
	// li r14,-4112
	r14.s64 = -4112;
	// li r6,-4096
	ctx.r6.s64 = -4096;
	// li r15,-4080
	r15.s64 = -4080;
	// li r16,-5136
	r16.s64 = -5136;
	// li r17,-5120
	r17.s64 = -5120;
	// li r18,-5104
	r18.s64 = -5104;
loc_82CE91B8:
	// mr r8,r20
	ctx.r8.u64 = r20.u64;
loc_82CE91BC:
	// rldicl r10,r9,32,32
	ctx.r10.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r21,r9
	ctx.r9.u64 = r21.u64 + ctx.r9.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// lvlx128 v55,r10,r23
	temp.u32 = ctx.r10.u32 + r23.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v54,r10,r24
	temp.u32 = ctx.r10.u32 + r24.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v26,v26,v55,1
	simd::store_i8(v26.u8, simd::shift_left_insert_bytes(simd::load_i8(v26.u8), simd::load_i8(v55.u8), 15));
	// lvlx128 v53,r10,r22
	temp.u32 = ctx.r10.u32 + r22.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v25,v25,v54,1
	simd::store_i8(v25.u8, simd::shift_left_insert_bytes(simd::load_i8(v25.u8), simd::load_i8(v54.u8), 15));
	// lvlx128 v52,r10,r25
	temp.u32 = ctx.r10.u32 + r25.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v24,v24,v53,1
	simd::store_i8(v24.u8, simd::shift_left_insert_bytes(simd::load_i8(v24.u8), simd::load_i8(v53.u8), 15));
	// lvlx128 v51,r10,r26
	temp.u32 = ctx.r10.u32 + r26.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v23,v23,v52,1
	simd::store_i8(v23.u8, simd::shift_left_insert_bytes(simd::load_i8(v23.u8), simd::load_i8(v52.u8), 15));
	// lvlx128 v50,r10,r27
	temp.u32 = ctx.r10.u32 + r27.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v22,v22,v51,1
	simd::store_i8(v22.u8, simd::shift_left_insert_bytes(simd::load_i8(v22.u8), simd::load_i8(v51.u8), 15));
	// lvlx128 v49,r10,r28
	temp.u32 = ctx.r10.u32 + r28.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v21,v21,v50,1
	simd::store_i8(v21.u8, simd::shift_left_insert_bytes(simd::load_i8(v21.u8), simd::load_i8(v50.u8), 15));
	// lvlx128 v48,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v20,v20,v49,1
	simd::store_i8(v20.u8, simd::shift_left_insert_bytes(simd::load_i8(v20.u8), simd::load_i8(v49.u8), 15));
	// lvlx128 v47,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v19,v19,v48,1
	simd::store_i8(v19.u8, simd::shift_left_insert_bytes(simd::load_i8(v19.u8), simd::load_i8(v48.u8), 15));
	// lvlx128 v46,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v18,v18,v47,1
	simd::store_i8(v18.u8, simd::shift_left_insert_bytes(simd::load_i8(v18.u8), simd::load_i8(v47.u8), 15));
	// lvlx128 v45,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v17,v17,v46,1
	simd::store_i8(v17.u8, simd::shift_left_insert_bytes(simd::load_i8(v17.u8), simd::load_i8(v46.u8), 15));
	// lvlx128 v44,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v16,v16,v45,1
	simd::store_i8(v16.u8, simd::shift_left_insert_bytes(simd::load_i8(v16.u8), simd::load_i8(v45.u8), 15));
	// vsldoi128 v15,v15,v44,1
	simd::store_i8(v15.u8, simd::shift_left_insert_bytes(simd::load_i8(v15.u8), simd::load_i8(v44.u8), 15));
	// bne 0x82ce91bc
	if (!cr0.eq) goto loc_82CE91BC;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// vaddubm v13,v26,v0
	simd::store_u8(ctx.v13.u8, simd::add_u8(simd::load_u8(v26.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v12,v25,v0
	simd::store_u8(ctx.v12.u8, simd::add_u8(simd::load_u8(v25.u8), simd::load_u8(ctx.v0.u8)));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// vaddubm v11,v24,v0
	simd::store_u8(ctx.v11.u8, simd::add_u8(simd::load_u8(v24.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v10,v23,v0
	simd::store_u8(ctx.v10.u8, simd::add_u8(simd::load_u8(v23.u8), simd::load_u8(ctx.v0.u8)));
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// vupkhsb128 v38,v13,v0
	simd::store_i16(v38.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v13.s8)));
	// vsrw128 v37,v14,v62
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v37, simd::shift_right_logical_i32(simd::to_vec128i(v14), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vupklsb128 v36,v13,v0
	simd::store_i16(v36.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v13.s8)));
	// rldicl r10,r9,32,32
	ctx.r10.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vupkhsb128 v34,v12,v0
	simd::store_i16(v34.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v12.s8)));
	// vor128 v42,v11,v11
	simd::store_i8(v42.u8, simd::load_i8(ctx.v11.u8));
	// vupklsb128 v32,v12,v0
	simd::store_i16(v32.s16, simd::extend_i8_lo_to_i16(simd::load_i8(ctx.v12.s8)));
	// vor128 v40,v10,v10
	simd::store_i8(v40.u8, simd::load_i8(ctx.v10.u8));
	// vupkhsb128 v58,v38,v96
	simd::store_i32(v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// lvx128 v43,r0,r8
	simd::store_shuffled(v43, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// vupklsb128 v57,v38,v96
	simd::store_i32(v57.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v38.s16)));
	// lvx128 v41,r0,r7
	simd::store_shuffled(v41, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// vupkhsb128 v56,v36,v96
	simd::store_i32(v56.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v36.s16)));
	// vsrw128 v35,v43,v62
simd::store_shuffled(v35, simd::shift_right_logical_i32(simd::to_vec128i(v43), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vupkhsb128 v54,v34,v96
	simd::store_i32(v54.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v34.s16)));
	// vsrw128 v33,v41,v62
simd::store_shuffled(v33, simd::shift_right_logical_i32(simd::to_vec128i(v41), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vupklsb128 v53,v34,v96
	simd::store_i32(v53.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v34.s16)));
	// vcsxwfp128 v5,v58,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v52,v32,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v32.s16)));
	// vcsxwfp128 v4,v57,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v57.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v3,v56,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v56.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v60,v42,v0
	simd::store_i16(v60.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v42.s8)));
	// vcsxwfp128 v49,v54,7
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v54.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v59,v40,v0
	simd::store_i16(v59.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v40.s8)));
	// vcsxwfp128 v47,v53,7
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v53.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v55,v36,v96
	simd::store_i32(v55.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v36.s16)));
	// vcsxwfp128 v46,v52,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v51,v32,v96
	simd::store_i32(v51.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v32.s16)));
	// vupkhsb128 v50,v60,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v60.s16)));
	// vcuxwfp128 v13,v37,31
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v37.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v48,v59,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vor128 v37,v26,v26
	simd::store_i8(v37.u8, simd::load_i8(v26.u8));
	// vupklsb128 v45,v42,v0
	simd::store_i16(v45.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v42.s8)));
	// vor128 v36,v25,v25
	simd::store_i8(v36.u8, simd::load_i8(v25.u8));
	// vcuxwfp128 v12,v35,31
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v35.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vor128 v35,v24,v24
	simd::store_i8(v35.u8, simd::load_i8(v24.u8));
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// vcuxwfp128 v11,v33,31
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v33.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v2,v55,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vor128 v34,v23,v23
	simd::store_i8(v34.u8, simd::load_i8(v23.u8));
	// vcsxwfp128 v1,v50,7
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v43,v60,v96
	simd::store_i32(v43.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v60.s16)));
	// vcsxwfp128 v44,v51,7
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v41,v45,v96
	simd::store_i32(v41.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v45.s16)));
	// vcsxwfp128 v42,v48,7
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// lvx128 v39,r0,r8
	simd::store_shuffled(v39, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// vsubfp128 v27,v49,v5
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vsrw128 v63,v39,v62
simd::store_shuffled(v63, simd::shift_right_logical_i32(simd::to_vec128i(v39), simd::and_u32(simd::to_vec128i(v62), simd::set1_i32(0x1F))));
	// vsubfp128 v26,v47,v4
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// vsubfp128 v25,v46,v3
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vupklsb128 v39,v45,v96
	simd::store_i32(v39.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v45.s16)));
	// vaddubm v28,v22,v0
	simd::store_u8(v28.u8, simd::add_u8(simd::load_u8(v22.u8), simd::load_u8(ctx.v0.u8)));
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// vupklsb128 v38,v40,v0
	simd::store_i16(v38.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v40.s8)));
	// vcuxwfp128 v10,v63,31
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v63.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// li r8,-32
	ctx.r8.s64 = -32;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v31,v43,7
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v30,v41,7
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vcsxwfp128 v29,v39,7
	simd::store_f32_aligned(v29.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v24,v44,v2
	simd::store_f32_aligned(v24.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vsubfp128 v23,v42,v1
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v4,v26,v11,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v3,v25,v12,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v5,v27,v10,v5
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v2,v24,v13,v2
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vmaddfp v1,v23,v10,v1
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmulfp128 v32,v4,v7
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v63,v3,v8
	simd::store_f32_aligned(v63.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v33,v5,v6
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vupklsb128 v55,v38,v96
	simd::store_i32(v55.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v38.s16)));
	// stvx128 v33,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v33), &VectorMaskL[(ea & 0xF) * 16]);
	// vupklsb128 v57,v59,v96
	simd::store_i32(v57.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v59.s16)));
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// vupkhsb128 v56,v38,v96
	simd::store_i32(v56.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// vaddubm v27,v21,v0
	simd::store_u8(v27.u8, simd::add_u8(simd::load_u8(v21.u8), simd::load_u8(ctx.v0.u8)));
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// vupkhsb128 v53,v28,v0
	simd::store_i16(v53.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v28.s8)));
	// vcsxwfp128 v51,v55,7
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vaddubm v5,v20,v0
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(v20.u8), simd::load_u8(ctx.v0.u8)));
	// vcsxwfp128 v54,v57,7
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v57.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v50,v28,v0
	simd::store_i16(v50.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v28.s8)));
	// vcsxwfp128 v52,v56,7
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v56.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v48,v27,v0
	simd::store_i16(v48.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v27.s8)));
	// vmulfp128 v60,v2,v9
	simd::store_f32_aligned(v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vupkhsb128 v47,v53,v96
	simd::store_i32(v47.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vmulfp128 v58,v1,v6
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// li r7,-16
	ctx.r7.s64 = -16;
	// li r10,-1056
	ctx.r10.s64 = -1056;
	// vor128 v49,v5,v5
	simd::store_i8(v49.u8, simd::load_i8(ctx.v5.u8));
	// vupkhsb128 v44,v48,v96
	simd::store_i32(v44.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v48.s16)));
	// stvx128 v63,r0,r11
	ea = (r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v63), &VectorMaskL[(ea & 0xF) * 16]);
	// vupklsb128 v46,v27,v0
	simd::store_i16(v46.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v27.s8)));
	// vcsxwfp128 v5,v47,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vaddubm v4,v19,v0
	simd::store_u8(ctx.v4.u8, simd::add_u8(simd::load_u8(v19.u8), simd::load_u8(ctx.v0.u8)));
	// vupklsb128 v45,v53,v96
	simd::store_i32(v45.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v53.s16)));
	// stvx128 v32,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v32), &VectorMaskL[(ea & 0xF) * 16]);
	// vupkhsb128 v42,v50,v96
	simd::store_i32(v42.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v50.s16)));
	// vcsxwfp128 v39,v44,7
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v41,v50,v96
	simd::store_i32(v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v50.s16)));
	// vupklsb128 v40,v48,v96
	simd::store_i32(v40.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v48.s16)));
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// vsubfp128 v3,v51,v29
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v29.f32)));
	// stvx128 v3,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v3), &VectorMaskL[(ea & 0xF) * 16]);
	// vsubfp128 v28,v54,v31
	simd::store_f32_aligned(v28.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v31.f32)));
	// vupkhsb128 v38,v46,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v46.s16)));
	// vsubfp128 v27,v52,v30
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v30.f32)));
	// vupklsb128 v33,v46,v96
	simd::store_i32(v33.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v46.s16)));
	// stvx128 v60,r11,r20
	ea = (r11.u32 + r20.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v60), &VectorMaskL[(ea & 0xF) * 16]);
	// vor128 v43,v4,v4
	simd::store_i8(v43.u8, simd::load_i8(ctx.v4.u8));
	// stvx128 v58,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v58), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v4,v45,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v3,v42,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v42.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v59,v49,v0
	simd::store_i16(v59.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v49.s8)));
	// vcsxwfp128 v2,v41,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v57,v49,v0
	simd::store_i16(v57.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v49.s8)));
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// vcsxwfp128 v56,v40,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v54,v38,7
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vaddubm v1,v18,v0
	simd::store_u8(ctx.v1.u8, simd::add_u8(simd::load_u8(v18.u8), simd::load_u8(ctx.v0.u8)));
	// vcsxwfp128 v52,v33,7
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v55,v59,v96
	simd::store_i32(v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vsubfp128 v26,v39,v5
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vupklsb128 v53,v59,v96
	simd::store_i32(v53.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v59.s16)));
	// vupkhsb128 v51,v57,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v57.s16)));
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// std r4,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r4.u64);
	// stvx128 v1,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v1), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v31,v28,v11,v31
	simd::store_f32_aligned(v31.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v31.f32)));
	// std r31,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r31.u64);
	// vmaddfp v30,v27,v12,v30
	simd::store_f32_aligned(v30.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v30.f32)));
	// std r30,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r30.u64);
	// li r9,-1040
	ctx.r9.s64 = -1040;
	// vupkhsb128 v50,v43,v0
	simd::store_i16(v50.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v43.s8)));
	// li r4,-1024
	ctx.r4.s64 = -1024;
	// vupklsb128 v49,v43,v0
	simd::store_i16(v49.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v43.s8)));
	// li r31,-1008
	r31.s64 = -1008;
	// vcsxwfp128 v1,v55,7
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// li r30,-2080
	r30.s64 = -2080;
	// vcsxwfp128 v27,v51,7
	simd::store_f32_aligned(v27.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// li r8,-2064
	ctx.r8.s64 = -2064;
	// vsubfp128 v25,v56,v4
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// lvx128 v28,r0,r10
	simd::store_shuffled(v28, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// vsubfp128 v24,v54,v3
	simd::store_f32_aligned(v24.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v29,v28,v13,v29
	simd::store_f32_aligned(v29.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(v29.f32)));
	// vcsxwfp128 v28,v53,7
	simd::store_f32_aligned(v28.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v53.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsubfp128 v23,v52,v2
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vmaddfp v5,v26,v10,v5
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmulfp128 v48,v31,v7
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v47,v30,v8
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v46,v29,v9
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v4,v25,v11,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vupklsb128 v38,v50,v96
	simd::store_i32(v38.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v50.s16)));
	// vupkhsb128 v39,v50,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v50.s16)));
	// vaddubm v31,v17,v0
	simd::store_u8(v31.u8, simd::add_u8(simd::load_u8(v17.u8), simd::load_u8(ctx.v0.u8)));
	// vmaddfp v3,v24,v12,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// lvx128 v40,r0,r7
	simd::store_shuffled(v40, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// vmulfp128 v45,v5,v6
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vupklsb128 v41,v57,v96
	simd::store_i32(v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v57.s16)));
	// vmaddfp v2,v23,v13,v2
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vupkhsb128 v63,v40,v0
	simd::store_i16(v63.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v40.s8)));
	// vcsxwfp128 v58,v38,7
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v57,v31,v0
	simd::store_i16(v57.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v31.s8)));
	// vcsxwfp128 v60,v39,7
	simd::store_f32_aligned(v60.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v33,v49,v96
	simd::store_i32(v33.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vupklsb128 v32,v49,v96
	simd::store_i32(v32.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v49.s16)));
	// stvx128 v48,r11,r9
	ea = (r11.u32 + ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v48), &VectorMaskL[(ea & 0xF) * 16]);
	// vupkhsb128 v55,v63,v96
	simd::store_i32(v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v63.s16)));
	// stvx128 v47,r11,r4
	ea = (r11.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v47), &VectorMaskL[(ea & 0xF) * 16]);
	// vupklsb128 v53,v63,v96
	simd::store_i32(v53.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v63.s16)));
	// stvx128 v46,r11,r31
	ea = (r11.u32 + r31.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v46), &VectorMaskL[(ea & 0xF) * 16]);
	// vupkhsb128 v51,v57,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v5,v41,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v50,v57,v96
	simd::store_i32(v50.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v56,v33,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v44,v4,v7
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vupklsb128 v52,v31,v0
	simd::store_i16(v52.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v31.s8)));
	// vcsxwfp128 v54,v32,7
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// std r28,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, r28.u64);
	// vmulfp128 v43,v3,v8
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// addi r28,r1,208
	r28.s64 = ctx.r1.s64 + 208;
	// stvx128 v45,r11,r30
	ea = (r11.u32 + r30.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v45), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v4,v55,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v42,v2,v9
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vupklsb128 v59,v40,v0
	simd::store_i16(v59.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v40.s8)));
	// vsubfp128 v31,v58,v28
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v58.f32), simd::load_f32_aligned(v28.f32)));
	// vaddubm v30,v16,v0
	simd::store_u8(v30.u8, simd::add_u8(simd::load_u8(v16.u8), simd::load_u8(ctx.v0.u8)));
	// vsubfp128 v2,v60,v1
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(ctx.v1.f32)));
	// stvx128 v2,r0,r28
	ea = (r28.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v2), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v3,v53,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v53.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vaddubm v29,v15,v0
	simd::store_u8(v29.u8, simd::add_u8(simd::load_u8(v15.u8), simd::load_u8(ctx.v0.u8)));
	// vcsxwfp128 v47,v51,7
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v49,v59,v96
	simd::store_i32(v49.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v45,v50,7
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v48,v59,v96
	simd::store_i32(v48.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v59.s16)));
	// vupkhsb128 v46,v52,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// li r7,-2048
	ctx.r7.s64 = -2048;
	// vupklsb128 v41,v52,v96
	simd::store_i32(v41.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v52.s16)));
	// vsubfp128 v26,v56,v27
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v27.f32)));
	// stvx128 v44,r11,r8
	ea = (r11.u32 + ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v44), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r8,r1,240
	ctx.r8.s64 = ctx.r1.s64 + 240;
	// vsubfp128 v25,v54,v5
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vupkhsb128 v38,v30,v0
	simd::store_i16(v38.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v30.s8)));
	// vupklsb128 v33,v30,v0
	simd::store_i16(v33.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v30.s8)));
	// vcsxwfp128 v2,v49,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v32,v29,v0
	simd::store_i16(v32.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v29.s8)));
	// vcsxwfp128 v40,v46,7
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v63,v29,v0
	simd::store_i16(v63.s16, simd::extend_i8_lo_to_i16(simd::load_i8(v29.s8)));
	// vcsxwfp128 v39,v41,7
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// stvx128 v31,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v31), &VectorMaskL[(ea & 0xF) * 16]);
	// vcsxwfp128 v31,v48,7
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// li r10,-2032
	ctx.r10.s64 = -2032;
	// std r29,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r29.u64);
	// li r9,-3104
	ctx.r9.s64 = -3104;
	// stvx128 v43,r11,r7
	ea = (r11.u32 + ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v43), &VectorMaskL[(ea & 0xF) * 16]);
	// vsubfp128 v30,v47,v4
	simd::store_f32_aligned(v30.f32, simd::sub_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// li r4,-3088
	ctx.r4.s64 = -3088;
	// vsubfp128 v29,v45,v3
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// li r31,-3072
	r31.s64 = -3072;
	// li r30,-3056
	r30.s64 = -3056;
	// vmaddfp v27,v26,v12,v27
	simd::store_f32_aligned(v27.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v27.f32)));
	// li r29,-4128
	r29.s64 = -4128;
	// stvx128 v42,r11,r10
	ea = (r11.u32 + ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v42), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v5,v25,v13,v5
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// lvx128 v24,r0,r28
	simd::store_shuffled(v24, simd::load_and_shuffle(base + ((r28.u32) & ~0xF), VectorMaskL));
	// vmaddfp v1,v24,v10,v1
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// lvx128 v24,r0,r8
	simd::store_shuffled(v24, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// vmaddfp v28,v24,v11,v28
	simd::store_f32_aligned(v28.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v28.f32)));
	// vsubfp128 v26,v40,v2
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vsubfp128 v25,v39,v31
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v31.f32)));
	// vmaddfp v4,v30,v10,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v3,v29,v11,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v2,v26,v12,v2
	simd::store_f32_aligned(ctx.v2.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vupkhsb128 v52,v38,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// vupkhsb128 v48,v32,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v32.s16)));
	// vmulfp128 v57,v5,v9
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// vupklsb128 v51,v38,v96
	simd::store_i32(v51.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v38.s16)));
	// vmulfp128 v56,v4,v6
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vupkhsb128 v50,v33,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v33.s16)));
	// vmulfp128 v55,v3,v7
	simd::store_f32_aligned(v55.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vupklsb128 v49,v33,v96
	simd::store_i32(v49.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v33.s16)));
	// vcsxwfp128 v5,v52,7
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v47,v32,v96
	simd::store_i32(v47.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v32.s16)));
	// vcsxwfp128 v44,v48,7
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v46,v63,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v63.s16)));
	// vcsxwfp128 v4,v51,7
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupklsb128 v45,v63,v96
	simd::store_i32(v45.s32, simd::extend_i16_lo_to_i32(simd::load_i16(v63.s16)));
	// vcsxwfp128 v3,v50,7
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v60,v1,v6
	simd::store_f32_aligned(v60.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// xor r8,r19,r3
	ctx.r8.u64 = r19.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v43,v47,7
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// ld r28,120(r1)
	r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// vcsxwfp128 v42,v46,7
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// rlwinm r7,r8,0,0,24
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// vcsxwfp128 v41,v45,7
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vor128 v26,v37,v37
	simd::store_i8(v26.u8, simd::load_i8(v37.u8));
	// vmulfp128 v54,v2,v8
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vcsxwfp128 v2,v49,7
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// stvx128 v57,r11,r30
	ea = (r11.u32 + r30.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v57), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v1,v25,v13,v31
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(v31.f32)));
	// ld r30,104(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// vmulfp128 v59,v28,v7
	simd::store_f32_aligned(v59.f32, simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vor128 v25,v36,v36
	simd::store_i8(v25.u8, simd::load_i8(v36.u8));
	// vmulfp128 v58,v27,v8
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vor128 v24,v35,v35
	simd::store_i8(v24.u8, simd::load_i8(v35.u8));
	// vsubfp128 v31,v44,v5
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vor128 v23,v34,v34
	simd::store_i8(v23.u8, simd::load_i8(v34.u8));
	// stvx128 v60,r11,r9
	ea = (r11.u32 + ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v60), &VectorMaskL[(ea & 0xF) * 16]);
	// ld r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// vsubfp128 v30,v43,v4
	simd::store_f32_aligned(v30.f32, simd::sub_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(ctx.v4.f32)));
	// vsubfp128 v29,v42,v3
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// vsubfp128 v28,v41,v2
	simd::store_f32_aligned(v28.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(ctx.v2.f32)));
	// vmulfp128 v53,v1,v9
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v59,r11,r4
	ea = (r11.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v59), &VectorMaskL[(ea & 0xF) * 16]);
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// stvx128 v58,r11,r31
	ea = (r11.u32 + r31.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v58), &VectorMaskL[(ea & 0xF) * 16]);
	// ld r31,112(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// vmaddfp v27,v31,v10,v5
	simd::store_f32_aligned(v27.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// stvx128 v56,r11,r29
	ea = (r11.u32 + r29.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v56), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v55,r11,r14
	ea = (r11.u32 + r14.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v55), &VectorMaskL[(ea & 0xF) * 16]);
	// ld r29,80(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stvx128 v54,r11,r6
	ea = (r11.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v54), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v11,v30,v11,v4
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v10,v29,v12,v3
	simd::store_f32_aligned(ctx.v10.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v5,v28,v13,v2
	simd::store_f32_aligned(ctx.v5.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// stvx128 v53,r11,r15
	ea = (r11.u32 + r15.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v53), &VectorMaskL[(ea & 0xF) * 16]);
	// vmulfp128 v40,v27,v6
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// vmulfp128 v39,v11,v7
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v7.f32)));
	// vmulfp128 v38,v10,v8
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v8.f32)));
	// vmulfp128 v33,v5,v9
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v40,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v40), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v39,r11,r16
	ea = (r11.u32 + r16.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v39), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v38,r11,r17
	ea = (r11.u32 + r17.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v38), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v33,r11,r18
	ea = (r11.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v33), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce9710
	if (cr6.eq) goto loc_82CE9710;
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r3
loc_82CE9710:
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// std r9,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r9.u64);
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// lwz r8,460(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(460) );
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// std r6,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r6.u64);
	// addi r19,r1,144
	r19.s64 = ctx.r1.s64 + 144;
	// vaddfp128 v9,v9,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(v61.f32)));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// vaddfp128 v8,v8,v61
	simd::store_f32_aligned(ctx.v8.f32, simd::add_f32(simd::load_f32_aligned(ctx.v8.f32), simd::load_f32_aligned(v61.f32)));
	// lvx128 v12,r0,r10
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// lvx128 v13,r0,r9
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// ld r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// lvx128 v11,r0,r7
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// lvx128 v10,r0,r19
	simd::store_shuffled(ctx.v10, simd::load_and_shuffle(base + ((r19.u32) & ~0xF), VectorMaskL));
	// vadduwm v5,v12,v13
	simd::store_u32(ctx.v5.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v13.u32)));
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// vadduwm v4,v11,v13
	simd::store_u32(ctx.v4.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v13.u32)));
	// vadduwm v3,v10,v13
	simd::store_u32(ctx.v3.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v13.u32)));
	// addic. r10,r8,-16
	xer.ca = ctx.r8.u32 > 15;
	ctx.r10.s64 = ctx.r8.s64 + -16;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// vaddfp128 v7,v7,v61
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(v61.f32)));
	// stw r10,460(r1)
	PPC_STORE_U32(ctx.r1.u32 + 460, ctx.r10.u32);
	// stvx128 v5,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v5), &VectorMaskL[(ea & 0xF) * 16]);
	// ld r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// vaddfp128 v6,v6,v61
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(v61.f32)));
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// vadduwm v14,v14,v13
	simd::store_u32(v14.u32, simd::add_u32(simd::load_u32(v14.u32), simd::load_u32(ctx.v13.u32)));
	// addi r5,r5,64
	ctx.r5.s64 = ctx.r5.s64 + 64;
	// stvx128 v4,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v4), &VectorMaskL[(ea & 0xF) * 16]);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stvx128 v3,r0,r7
	ea = (ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v3), &VectorMaskL[(ea & 0xF) * 16]);
	// bgt 0x82ce91b8
	if (cr0.gt) goto loc_82CE91B8;
	// lwz r31,468(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(468) );
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lbz r10,13(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// subf r8,r11,r3
	ctx.r8.s64 = ctx.r3.s64 - r11.s64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// twllei r10,0
	// divwu r10,r8,r10
	ctx.r10.u32 = ctx.r8.u32 / ctx.r10.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ce97c4
	if (cr6.lt) goto loc_82CE97C4;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CE97C4:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82ce97e4
	if (!cr6.lt) goto loc_82CE97E4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CE97E4:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82CE8FC0) {
	__imp__sub_82CE8FC0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE9810) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v31{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// lfs f0,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// stfs f13,36(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 36, temp.u32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfs f10,48(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// addi r4,r8,26704
	ctx.r4.s64 = ctx.r8.s64 + 26704;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// addi r7,r9,26736
	ctx.r7.s64 = ctx.r9.s64 + 26736;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// lvx128 v8,r0,r4
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// lvlx128 v59,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r30,r1,104
	r30.s64 = ctx.r1.s64 + 104;
	// vspltw128 v11,v59,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// lis r29,-32252
	r29.s64 = -2113667072;
	// lvx128 v12,r0,r7
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// addi r28,r1,88
	r28.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v60,1
	simd::store_i32(v60.u32, simd::set1_i32(int32_t(0x1)));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// li r4,0
	ctx.r4.s64 = 0;
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfs f13,-16944(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// lvlx128 v58,r10,r9
	temp.u32 = ctx.r10.u32 + ctx.r9.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r9,r9,26464
	ctx.r9.s64 = ctx.r9.s64 + 26464;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vspltw128 v0,v58,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// lvlx128 v57,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// vspltw128 v9,v57,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// li r30,16
	r30.s64 = 16;
	// addi r10,r10,26496
	ctx.r10.s64 = ctx.r10.s64 + 26496;
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v56,v63,v0,4
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vadduwm v7,v0,v13
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v55,v56,v13,4
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// lvx128 v6,r0,r10
	simd::store_shuffled(ctx.v6, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v54,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v10,v54,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// vsldoi128 v13,v55,v7,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(ctx.v7.u8), 12));
	// vmaddfp v12,v10,v12,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmaddfp v11,v10,v8,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// fmuls f2,f0,f13
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vadduwm v8,v0,v0
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// lvlx128 v53,r0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v59,v53,0
	simd::store_i32(v59.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vadduwm v10,v13,v0
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// vadduwm v13,v13,v9
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v9.u32)));
	// li r7,2
	ctx.r7.s64 = 2;
	// vadduwm v0,v10,v9
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v9.u32)));
loc_82CE9958:
	// li r10,8
	ctx.r10.s64 = 8;
loc_82CE995C:
	// rldicl r9,r11,32,32
	ctx.r9.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r9,r9,r3
	ctx.r9.u64 = ctx.r9.u64 + ctx.r3.u64;
	// lvlx128 v52,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r9,r7
	temp.u32 = ctx.r9.u32 + ctx.r7.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v52,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v52.u8), 14));
	// vsldoi128 v61,v61,v51,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v51.u8), 14));
	// bne 0x82ce995c
	if (!cr0.eq) goto loc_82CE995C;
	// vperm128 v50,v62,v63,v7
	simd::store_i8(v50.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vsrw128 v49,v13,v60
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v49, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v13), simd::and_u32(simd::to_vec128i(v60), simd::set1_i32(0x1F))));
	// vperm128 v48,v62,v63,v6
	simd::store_i8(v48.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v6.u8)));
	// vsrw128 v47,v0,v60
simd::store_shuffled(v47, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v0), simd::and_u32(simd::to_vec128i(v60), simd::set1_i32(0x1F))));
	// vperm128 v46,v61,v63,v7
	simd::store_i8(v46.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// rldicl r10,r11,32,32
	ctx.r10.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF;
	// vperm128 v45,v61,v63,v6
	simd::store_i8(v45.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v6.u8)));
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// vcsxwfp128 v44,v50,31
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v43,v48,31
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcuxwfp128 v42,v49,31
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v49.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r10,r3
	ctx.r3.u64 = ctx.r10.u64 + ctx.r3.u64;
	// vcsxwfp128 v41,v46,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcuxwfp128 v40,v47,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v47.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r9,r4,r3
	ctx.r9.u64 = ctx.r4.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v39,v45,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r4,r9,0,0,24
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vmulfp128 v10,v44,v11
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v9,v43,v12
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v5,v42,v11
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v11.f32)));
	// vsubfp128 v4,v41,v44
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v44.f32)));
	// vmulfp128 v3,v40,v12
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v2,v39,v43
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v43.f32)));
	// vmaddfp v1,v4,v5,v10
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v10.f32)));
	// vmaddfp v31,v2,v3,v9
	simd::store_f32_aligned(v31.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v3.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// stvx128 v1,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v1), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v31,r5,r30
	ea = (ctx.r5.u32 + r30.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v31), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce9a04
	if (cr6.eq) goto loc_82CE9A04;
	// li r10,128
	ctx.r10.s64 = 128;
	// dcbt r10,r3
loc_82CE9A04:
	// vadduwm v0,v0,v8
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v8.u32)));
	// addic. r6,r6,-8
	xer.ca = ctx.r6.u32 > 7;
	ctx.r6.s64 = ctx.r6.s64 + -8;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v13,v13,v8
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v8.u32)));
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// vaddfp128 v12,v12,v59
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v59.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v11,v11,v59
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(v59.f32)));
	// bgt 0x82ce9958
	if (cr0.gt) goto loc_82CE9958;
	// lbz r10,13(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// rotlwi r8,r10,1
	ctx.r8.u64 = rotl32(ctx.r10.u32, 1);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ce9a4c
	if (cr6.lt) goto loc_82CE9A4C;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_82CE9A4C:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// subf r9,r10,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r10.s64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ce9a6c
	if (!cr6.lt) goto loc_82CE9A6C;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82CE9A6C:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stw r10,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r10.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82CE9810) {
	__imp__sub_82CE9810(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE9A98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// lfs f0,36(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r7,r9,26736
	ctx.r7.s64 = ctx.r9.s64 + 26736;
	// stfs f13,36(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 36, temp.u32);
	// addi r4,r8,26704
	ctx.r4.s64 = ctx.r8.s64 + 26704;
	// lfs f10,48(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// li r8,4
	ctx.r8.s64 = 4;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lvx128 v0,r0,r7
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lvx128 v8,r0,r4
	simd::store_shuffled(ctx.v8, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r9,r1,104
	ctx.r9.s64 = ctx.r1.s64 + 104;
	// vspltisw128 v58,1
	simd::store_i32(v58.u32, simd::set1_i32(int32_t(0x1)));
	// lis r31,-32252
	r31.s64 = -2113667072;
	// lvlx128 v57,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r28,r1,88
	r28.s64 = ctx.r1.s64 + 88;
	// vspltw128 v13,v57,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// li r29,0
	r29.s64 = 0;
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r7,88(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfs f13,-16944(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// vor128 v59,v63,v63
	simd::store_i8(v59.u8, simd::load_i8(v63.u8));
	// lvlx128 v56,r8,r11
	temp.u32 = ctx.r8.u32 + r11.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// li r31,2
	r31.s64 = 2;
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// lvlx128 v55,r8,r9
	temp.u32 = ctx.r8.u32 + ctx.r9.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// vspltw128 v9,v55,0
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// li r26,1024
	r26.s64 = 1024;
	// li r27,1040
	r27.s64 = 1040;
	// vadduwm v11,v12,v12
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v12.u32)));
	// vsldoi128 v54,v63,v12,4
	simd::store_i8(v54.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v12.u8), 12));
	// addi r11,r11,26496
	r11.s64 = r11.s64 + 26496;
	// addi r9,r9,26464
	ctx.r9.s64 = ctx.r9.s64 + 26464;
	// vadduwm v7,v12,v11
	simd::store_u32(ctx.v7.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v11.u32)));
	// vsldoi128 v53,v54,v11,4
	simd::store_i8(v53.u8, simd::shift_left_insert_bytes(simd::load_i8(v54.u8), simd::load_i8(ctx.v11.u8), 12));
	// vadduwm v12,v11,v11
	simd::store_u32(ctx.v12.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v11.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v52,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v10,v52,0
	simd::store_i32(ctx.v10.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// vsldoi128 v11,v53,v7,4
	simd::store_i8(ctx.v11.u8, simd::shift_left_insert_bytes(simd::load_i8(v53.u8), simd::load_i8(ctx.v7.u8), 12));
	// vmaddfp v0,v10,v0,v13
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v13,v10,v8,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v13.f32)));
	// fmuls f2,f0,f13
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vadduwm v5,v12,v12
	simd::store_u32(ctx.v5.u32, simd::add_u32(simd::load_u32(ctx.v12.u32), simd::load_u32(ctx.v12.u32)));
	// lvlx128 v51,r0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v57,v51,0
	simd::store_i32(v57.u32, simd::broadcast_lane_i32(simd::load_i32(v51.u32), 3));
	// vadduwm v8,v11,v9
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v9.u32)));
	// li r4,6
	ctx.r4.s64 = 6;
	// vadduwm v11,v11,v12
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v12.u32)));
	// vadduwm v10,v11,v9
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v11.u32), simd::load_u32(ctx.v9.u32)));
	// lvx128 v6,r0,r11
	simd::store_shuffled(ctx.v6, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// li r28,16
	r28.s64 = 16;
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
loc_82CE9BF4:
	// li r9,8
	ctx.r9.s64 = 8;
loc_82CE9BF8:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v50,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v49,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v50,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v50.u8), 14));
	// lvlx128 v48,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v49,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v49.u8), 14));
	// lvlx128 v47,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v60,v60,v48,2
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v48.u8), 14));
	// vsldoi128 v59,v59,v47,2
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(v47.u8), 14));
	// bne 0x82ce9bf8
	if (!cr0.eq) goto loc_82CE9BF8;
	// vsrw128 v46,v10,v58
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v46, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v10), simd::and_u32(simd::to_vec128i(v58), simd::set1_i32(0x1F))));
	// vperm128 v45,v62,v63,v7
	simd::store_i8(v45.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vsrw128 v44,v8,v58
simd::store_shuffled(v44, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v58), simd::set1_i32(0x1F))));
	// vperm128 v43,v62,v63,v6
	simd::store_i8(v43.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v6.u8)));
	// vperm128 v42,v61,v63,v7
	simd::store_i8(v42.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vperm128 v41,v61,v63,v6
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v6.u8)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vperm128 v40,v60,v63,v7
	simd::store_i8(v40.u8, simd::permute_bytes(simd::load_i8(v60.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v39,v45,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v38,v60,v63,v6
	simd::store_i8(v38.u8, simd::permute_bytes(simd::load_i8(v60.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v6.u8)));
	// vcsxwfp128 v37,v43,31
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v36,v59,v63,v7
	simd::store_i8(v36.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcuxwfp128 v35,v46,31
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v46.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v34,v59,v63,v6
	simd::store_i8(v34.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v6.u8)));
	// vcuxwfp128 v33,v44,31
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v44.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v32,v42,31
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v42.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vcsxwfp128 v56,v41,31
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v55,v40,31
	simd::store_f32_aligned(v55.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vcsxwfp128 v54,v38,31
	simd::store_f32_aligned(v54.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v53,v36,31
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r9,r29,r3
	ctx.r9.u64 = r29.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v52,v34,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r11,r9,0,0,24
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// vmulfp128 v9,v39,v13
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vmulfp128 v4,v37,v0
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::load_f32_aligned(v37.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v12,v35,v0
	simd::store_f32_aligned(ctx.v12.f32, simd::mul_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v11,v33,v13
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v3,v32,v39
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v39.f32)));
	// vsubfp128 v2,v56,v37
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v37.f32)));
	// vmulfp128 v1,v55,v13
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::load_f32_aligned(v55.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v31,v54,v0
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vsubfp128 v30,v53,v55
	simd::store_f32_aligned(v30.f32, simd::sub_f32(simd::load_f32_aligned(v53.f32), simd::load_f32_aligned(v55.f32)));
	// vsubfp128 v29,v52,v54
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v54.f32)));
	// vmaddfp v28,v3,v11,v9
	simd::store_f32_aligned(v28.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v27,v2,v12,v4
	simd::store_f32_aligned(v27.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v26,v30,v11,v1
	simd::store_f32_aligned(v26.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v25,v29,v12,v31
	simd::store_f32_aligned(v25.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v12.f32)), simd::load_f32_aligned(v31.f32)));
	// stvx128 v28,r5,r26
	ea = (ctx.r5.u32 + r26.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v28), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v27,r5,r27
	ea = (ctx.r5.u32 + r27.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v27), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v26,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v26), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v25,r5,r28
	ea = (ctx.r5.u32 + r28.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v25), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82ce9cf0
	if (cr6.eq) goto loc_82CE9CF0;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CE9CF0:
	// vadduwm v10,v10,v5
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v10.u32), simd::load_u32(ctx.v5.u32)));
	// addic. r6,r6,-8
	xer.ca = ctx.r6.u32 > 7;
	ctx.r6.s64 = ctx.r6.s64 + -8;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v8,v8,v5
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v5.u32)));
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// vaddfp128 v0,v0,v57
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v57.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v13,v13,v57
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v57.f32)));
	// bgt 0x82ce9bf4
	if (cr0.gt) goto loc_82CE9BF4;
	// lbz r11,13(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 13);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ce9d38
	if (cr6.lt) goto loc_82CE9D38;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CE9D38:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82ce9d58
	if (!cr6.lt) goto loc_82CE9D58;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CE9D58:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r30)
	PPC_STORE_U32(r30.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 48, temp.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82CE9A98) {
	__imp__sub_82CE9A98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CE9D80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// lfs f0,36(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 36);
	f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// std r4,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r4.u64);
	// li r7,4
	ctx.r7.s64 = 4;
	// lfs f10,48(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// stfs f13,36(r24)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r24.u32 + 36, temp.u32);
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// lis r29,-32252
	r29.s64 = -2113667072;
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// addi r4,r11,26736
	ctx.r4.s64 = r11.s64 + 26736;
	// addi r30,r1,88
	r30.s64 = ctx.r1.s64 + 88;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// vspltisw128 v54,1
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x1)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lfs f13,-16944(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r10,26704
	r11.s64 = ctx.r10.s64 + 26704;
	// lvx128 v10,r0,r4
	simd::store_shuffled(ctx.v10, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// vor128 v56,v63,v63
	simd::store_i8(v56.u8, simd::load_i8(v63.u8));
	// vor128 v58,v63,v63
	simd::store_i8(v58.u8, simd::load_i8(v63.u8));
	// li r25,0
	r25.s64 = 0;
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r26,88(r1)
	r26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// std r26,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r26.u64);
	// stfd f6,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f6.u64);
	// lfd f5,104(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// ld r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// lvlx128 v61,r7,r9
	temp.u32 = ctx.r7.u32 + ctx.r9.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lvlx128 v52,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vspltw128 v0,v61,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// lvlx128 v53,r7,r8
	temp.u32 = ctx.r7.u32 + ctx.r8.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v11,v53,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v50,r0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v59,v63,v0,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// vspltw128 v12,v52,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// vor128 v55,v63,v63
	simd::store_i8(v55.u8, simd::load_i8(v63.u8));
	// vor128 v57,v63,v63
	simd::store_i8(v57.u8, simd::load_i8(v63.u8));
	// addi r8,r5,2064
	ctx.r8.s64 = ctx.r5.s64 + 2064;
	// vadduwm v9,v0,v13
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// vsldoi128 v51,v59,v13,4
	simd::store_i8(v51.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// vor128 v59,v63,v63
	simd::store_i8(v59.u8, simd::load_i8(v63.u8));
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v49,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v13,v51,v9,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v51.u8), simd::load_i8(ctx.v9.u8), 12));
	// vadduwm v6,v0,v0
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// li r4,8
	ctx.r4.s64 = 8;
	// vadduwm v8,v13,v11
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// vadduwm v13,v13,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// vspltw128 v0,v50,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// vadduwm v9,v13,v11
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// lvx128 v11,r0,r11
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// vmaddfp v13,v0,v10,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// vmaddfp v12,v0,v11,v12
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vspltw128 v53,v49,0
	simd::store_i32(v53.u32, simd::broadcast_lane_i32(simd::load_i32(v49.u32), 3));
	// addi r11,r11,26496
	r11.s64 = r11.s64 + 26496;
	// addi r9,r9,26464
	ctx.r9.s64 = ctx.r9.s64 + 26464;
	// li r27,6
	r27.s64 = 6;
	// li r28,14
	r28.s64 = 14;
	// li r29,12
	r29.s64 = 12;
	// lvx128 v7,r0,r11
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// li r30,2
	r30.s64 = 2;
	// lvx128 v0,r0,r9
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// li r31,10
	r31.s64 = 10;
	// li r18,1008
	r18.s64 = 1008;
	// li r19,1024
	r19.s64 = 1024;
	// li r20,-16
	r20.s64 = -16;
	// li r21,-1040
	r21.s64 = -1040;
	// li r22,-1024
	r22.s64 = -1024;
	// li r23,-2048
	r23.s64 = -2048;
loc_82CE9F0C:
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
loc_82CE9F10:
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r26,r10
	ctx.r10.u64 = r26.u64 + ctx.r10.u64;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v48,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v47,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v48,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v48.u8), 14));
	// lvlx128 v46,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v47,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v47.u8), 14));
	// lvlx128 v45,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v60,v60,v46,2
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v46.u8), 14));
	// lvlx128 v44,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v59,v59,v45,2
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(v45.u8), 14));
	// lvlx128 v43,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v58,v44,2
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(v44.u8), 14));
	// lvlx128 v42,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v57,v57,v43,2
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(v43.u8), 14));
	// lvlx128 v41,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v56,v56,v42,2
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(v42.u8), 14));
	// vsldoi128 v55,v55,v41,2
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(v41.u8), 14));
	// bne 0x82ce9f10
	if (!cr0.eq) goto loc_82CE9F10;
	// vsrw128 v40,v9,v54
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v40, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v9), simd::and_u32(simd::to_vec128i(v54), simd::set1_i32(0x1F))));
	// vperm128 v39,v62,v63,v0
	simd::store_i8(v39.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vperm128 v37,v62,v63,v7
	simd::store_i8(v37.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vsrw128 v38,v8,v54
simd::store_shuffled(v38, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v54), simd::set1_i32(0x1F))));
	// vperm128 v35,v61,v63,v7
	simd::store_i8(v35.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// rldicl r11,r10,32,32
	r11.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF;
	// vperm128 v32,v60,v63,v7
	simd::store_i8(v32.u8, simd::permute_bytes(simd::load_i8(v60.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// vperm128 v34,v60,v63,v0
	simd::store_i8(v34.u8, simd::permute_bytes(simd::load_i8(v60.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v33,v39,31
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v36,v61,v63,v0
	simd::store_i8(v36.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v52,v37,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v51,v59,v63,v0
	simd::store_i8(v51.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcuxwfp128 v50,v40,31
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v40.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v47,v58,v63,v0
	simd::store_i8(v47.u8, simd::permute_bytes(simd::load_i8(v58.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v40,v32,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v49,v59,v63,v7
	simd::store_i8(v49.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v44,v35,31
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v45,v58,v63,v7
	simd::store_i8(v45.u8, simd::permute_bytes(simd::load_i8(v58.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v42,v34,31
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v41,v57,v63,v7
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcuxwfp128 v48,v38,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v38.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v43,v57,v63,v0
	simd::store_i8(v43.u8, simd::permute_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v34,v47,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v39,v56,v63,v0
	simd::store_i8(v39.u8, simd::permute_bytes(simd::load_i8(v56.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v46,v36,31
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v37,v55,v63,v0
	simd::store_i8(v37.u8, simd::permute_bytes(simd::load_i8(v55.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v38,v51,31
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v35,v56,v63,v7
	simd::store_i8(v35.u8, simd::permute_bytes(simd::load_i8(v56.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v51,v45,31
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v32,v55,v63,v7
	simd::store_i8(v32.u8, simd::permute_bytes(simd::load_i8(v55.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v36,v49,31
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v47,v41,31
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// vcsxwfp128 v45,v39,31
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v49,v43,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vcsxwfp128 v41,v37,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v43,v35,31
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r9,r25,r3
	ctx.r9.u64 = r25.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v39,v32,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vmulfp128 v11,v48,v12
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// rlwinm r11,r9,0,0,24
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFF80;
	// vmulfp128 v10,v50,v13
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v5,v33,v12
	simd::store_f32_aligned(ctx.v5.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// vsubfp128 v4,v46,v33
	simd::store_f32_aligned(ctx.v4.f32, simd::sub_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v33.f32)));
	// vmulfp128 v3,v52,v13
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v2,v44,v52
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(v52.f32)));
	// vmulfp128 v1,v42,v12
	simd::store_f32_aligned(ctx.v1.f32, simd::mul_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v31,v38,v42
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v42.f32)));
	// vmulfp128 v30,v40,v13
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v29,v36,v40
	simd::store_f32_aligned(v29.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(v40.f32)));
	// vmulfp128 v28,v34,v12
	simd::store_f32_aligned(v28.f32, simd::mul_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v27,v49,v34
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v34.f32)));
	// vmulfp128 v26,v51,v13
	simd::store_f32_aligned(v26.f32, simd::mul_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v25,v47,v51
	simd::store_f32_aligned(v25.f32, simd::sub_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v51.f32)));
	// vmulfp128 v24,v45,v12
	simd::store_f32_aligned(v24.f32, simd::mul_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v23,v41,v45
	simd::store_f32_aligned(v23.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v45.f32)));
	// vmulfp128 v22,v43,v13
	simd::store_f32_aligned(v22.f32, simd::mul_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v21,v39,v43
	simd::store_f32_aligned(v21.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v43.f32)));
	// vmaddfp v20,v4,v11,v5
	simd::store_f32_aligned(v20.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v4.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v5.f32)));
	// vmaddfp v19,v2,v10,v3
	simd::store_f32_aligned(v19.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v18,v31,v11,v1
	simd::store_f32_aligned(v18.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v1.f32)));
	// vmaddfp v17,v29,v10,v30
	simd::store_f32_aligned(v17.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v29.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v30.f32)));
	// vmaddfp v16,v27,v11,v28
	simd::store_f32_aligned(v16.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v28.f32)));
	// vmaddfp v15,v25,v10,v26
	simd::store_f32_aligned(v15.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v26.f32)));
	// vmaddfp v14,v23,v11,v24
	simd::store_f32_aligned(v14.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v23.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v24.f32)));
	// vmaddfp v11,v21,v10,v22
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v21.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v22.f32)));
	// stvx128 v20,r8,r18
	ea = (ctx.r8.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v20), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v19,r8,r19
	ea = (ctx.r8.u32 + r19.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v19), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v18,r8,r20
	ea = (ctx.r8.u32 + r20.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v18), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v17,r0,r8
	ea = (ctx.r8.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v17), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v16,r8,r21
	ea = (ctx.r8.u32 + r21.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v16), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v15,r8,r22
	ea = (ctx.r8.u32 + r22.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v15), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v14,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v14), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v11,r8,r23
	ea = (ctx.r8.u32 + r23.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v11), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82cea0a8
	if (cr6.eq) goto loc_82CEA0A8;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CEA0A8:
	// vadduwm v9,v9,v6
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v9.u32), simd::load_u32(ctx.v6.u32)));
	// addic. r6,r6,-8
	xer.ca = ctx.r6.u32 > 7;
	ctx.r6.s64 = ctx.r6.s64 + -8;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// vadduwm v8,v8,v6
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v6.u32)));
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// vaddfp128 v13,v13,v53
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v53.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v12,v12,v53
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v53.f32)));
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// bgt 0x82ce9f0c
	if (cr0.gt) goto loc_82CE9F0C;
	// lbz r11,13(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 13);
	// lwz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(4) );
	// subf r7,r9,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82cea0f4
	if (cr6.lt) goto loc_82CEA0F4;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CEA0F4:
	// lwz r11,20(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(20) );
	// stw r9,8(r24)
	PPC_STORE_U32(r24.u32 + 8, ctx.r9.u32);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(24) );
	// rlwinm r9,r9,30,2,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82cea114
	if (!cr6.lt) goto loc_82CEA114;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEA114:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// stw r11,28(r24)
	PPC_STORE_U32(r24.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r24)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r24.u32 + 48, temp.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CE9D80) {
	__imp__sub_82CE9D80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEA140) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v14{};
	PPCVRegister v15{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lfs f0,36(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 36);
	f0.f64 = double(temp.f32);
	// stw r6,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, ctx.r6.u32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stw r7,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r7.u32);
	// dcbt r0,r3
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfs f13,40(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// li r22,4
	r22.s64 = 4;
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f11,44(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 44);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// lfs f10,48(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 48);
	ctx.f10.f64 = double(temp.f32);
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// lfd f0,26696(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26696);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fmul f9,f11,f0
	ctx.f9.f64 = ctx.f11.f64 * f0.f64;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// fmul f8,f10,f0
	ctx.f8.f64 = ctx.f10.f64 * f0.f64;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// stfs f13,36(r7)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r7.u32 + 36, temp.u32);
	// addi r6,r1,104
	ctx.r6.s64 = ctx.r1.s64 + 104;
	// addi r31,r9,26736
	r31.s64 = ctx.r9.s64 + 26736;
	// vspltisw128 v50,1
	simd::store_i32(v50.u32, simd::set1_i32(int32_t(0x1)));
	// lvlx128 v62,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// lis r10,-32252
	ctx.r10.s64 = -2113667072;
	// vspltw128 v12,v62,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v62.u32), 3));
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// vor128 v52,v63,v63
	simd::store_i8(v52.u8, simd::load_i8(v63.u8));
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// vor128 v54,v63,v63
	simd::store_i8(v54.u8, simd::load_i8(v63.u8));
	// vor128 v56,v63,v63
	simd::store_i8(v56.u8, simd::load_i8(v63.u8));
	// li r20,0
	r20.s64 = 0;
	// addi r7,r7,26704
	ctx.r7.s64 = ctx.r7.s64 + 26704;
	// vor128 v58,v63,v63
	simd::store_i8(v58.u8, simd::load_i8(v63.u8));
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f7.u64);
	// ld r21,88(r1)
	r21.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fctidz f6,f8
	ctx.f6.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// std r21,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r21.u64);
	// stfd f6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f6.u64);
	// lvlx128 v61,r22,r8
	temp.u32 = r22.u32 + ctx.r8.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f5,96(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r9,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r9.u64);
	// fcfid f4,f5
	ctx.f4.f64 = double(ctx.f5.s64);
	// vspltw128 v0,v61,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// lvlx128 v60,r22,r6
	temp.u32 = r22.u32 + ctx.r6.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor128 v62,v63,v63
	simd::store_i8(v62.u8, simd::load_i8(v63.u8));
	// vspltw128 v11,v60,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lfs f13,-16944(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16944);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r5,4112
	ctx.r10.s64 = ctx.r5.s64 + 4112;
	// vor128 v60,v63,v63
	simd::store_i8(v60.u8, simd::load_i8(v63.u8));
	// vadduwm v13,v0,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vsldoi128 v59,v63,v0,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8), 12));
	// vor128 v51,v63,v63
	simd::store_i8(v51.u8, simd::load_i8(v63.u8));
	// vor128 v53,v63,v63
	simd::store_i8(v53.u8, simd::load_i8(v63.u8));
	// vor128 v55,v63,v63
	simd::store_i8(v55.u8, simd::load_i8(v63.u8));
	// vadduwm v10,v0,v13
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsldoi128 v49,v59,v13,4
	simd::store_i8(v49.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(ctx.v13.u8), 12));
	// vadduwm v0,v13,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v13.u32)));
	// frsp f3,f4
	ctx.f3.f64 = double(float(ctx.f4.f64));
	// vor128 v57,v63,v63
	simd::store_i8(v57.u8, simd::load_i8(v63.u8));
	// vor128 v59,v63,v63
	simd::store_i8(v59.u8, simd::load_i8(v63.u8));
	// vsldoi128 v13,v49,v10,4
	simd::store_i8(ctx.v13.u8, simd::shift_left_insert_bytes(simd::load_i8(v49.u8), simd::load_i8(ctx.v10.u8), 12));
	// vor128 v61,v63,v63
	simd::store_i8(v61.u8, simd::load_i8(v63.u8));
	// vadduwm v6,v0,v0
	simd::store_u32(ctx.v6.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v0.u32)));
	// vadduwm v8,v13,v11
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// vadduwm v13,v13,v0
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v0.u32)));
	// fdivs f0,f12,f3
	f0.f64 = double(float(ctx.f12.f64 / ctx.f3.f64));
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx128 v48,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v0,v48,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vadduwm v9,v13,v11
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v11.u32)));
	// fmuls f2,f0,f13
	ctx.f2.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lvx128 v13,r0,r31
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((r31.u32) & ~0xF), VectorMaskL));
	// stfs f2,88(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vmaddfp v13,v0,v13,v12
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// lvlx128 v47,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v49,v47,0
	simd::store_i32(v49.u32, simd::broadcast_lane_i32(simd::load_i32(v47.u32), 3));
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// lvx128 v11,r0,r7
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// lis r11,-32255
	r11.s64 = -2113863680;
	// vmaddfp v12,v0,v11,v12
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// addi r6,r8,26464
	ctx.r6.s64 = ctx.r8.s64 + 26464;
	// addi r7,r11,26496
	ctx.r7.s64 = r11.s64 + 26496;
	// li r4,8
	ctx.r4.s64 = 8;
	// li r23,10
	r23.s64 = 10;
	// li r24,22
	r24.s64 = 22;
	// lvx128 v0,r0,r6
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// li r25,20
	r25.s64 = 20;
	// lvx128 v7,r0,r7
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// li r26,6
	r26.s64 = 6;
	// li r27,18
	r27.s64 = 18;
	// li r28,16
	r28.s64 = 16;
	// li r29,2
	r29.s64 = 2;
	// li r30,14
	r30.s64 = 14;
	// li r31,12
	r31.s64 = 12;
	// li r14,-1040
	r14.s64 = -1040;
	// li r6,-1024
	ctx.r6.s64 = -1024;
	// li r15,-2064
	r15.s64 = -2064;
	// li r16,-2048
	r16.s64 = -2048;
	// li r17,-3088
	r17.s64 = -3088;
	// li r18,-3072
	r18.s64 = -3072;
	// li r19,-4096
	r19.s64 = -4096;
loc_82CEA2F4:
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
loc_82CEA2F8:
	// rldicl r11,r9,32,32
	r11.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r21,r9
	ctx.r9.u64 = r21.u64 + ctx.r9.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// lvlx128 v46,r11,r23
	temp.u32 = r11.u32 + r23.u32;
	simd::store_shuffled(v46,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v45,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	simd::store_shuffled(v45,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v62,v62,v46,2
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v46.u8), 14));
	// lvlx128 v44,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v61,v61,v45,2
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v61.u8), simd::load_i8(v45.u8), 14));
	// lvlx128 v43,r11,r25
	temp.u32 = r11.u32 + r25.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v60,v60,v44,2
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v60.u8), simd::load_i8(v44.u8), 14));
	// lvlx128 v42,r11,r26
	temp.u32 = r11.u32 + r26.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v59,v59,v43,2
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v59.u8), simd::load_i8(v43.u8), 14));
	// lvlx128 v41,r11,r27
	temp.u32 = r11.u32 + r27.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v58,v58,v42,2
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v58.u8), simd::load_i8(v42.u8), 14));
	// lvlx128 v40,r11,r22
	temp.u32 = r11.u32 + r22.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v57,v57,v41,2
	simd::store_i8(v57.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(v41.u8), 14));
	// lvlx128 v39,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v56,v56,v40,2
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(v40.u8), 14));
	// lvlx128 v38,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v55,v55,v39,2
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(v39.u8), 14));
	// lvlx128 v37,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v54,v54,v38,2
	simd::store_i8(v54.u8, simd::shift_left_insert_bytes(simd::load_i8(v54.u8), simd::load_i8(v38.u8), 14));
	// lvlx128 v36,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v53,v53,v37,2
	simd::store_i8(v53.u8, simd::shift_left_insert_bytes(simd::load_i8(v53.u8), simd::load_i8(v37.u8), 14));
	// lvlx128 v35,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsldoi128 v52,v52,v36,2
	simd::store_i8(v52.u8, simd::shift_left_insert_bytes(simd::load_i8(v52.u8), simd::load_i8(v36.u8), 14));
	// vsldoi128 v51,v51,v35,2
	simd::store_i8(v51.u8, simd::shift_left_insert_bytes(simd::load_i8(v51.u8), simd::load_i8(v35.u8), 14));
	// bne 0x82cea2f8
	if (!cr0.eq) goto loc_82CEA2F8;
	// vsrw128 v34,v9,v50
	ctx.fpscr.enableFlushMode();
simd::store_shuffled(v34, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v9), simd::and_u32(simd::to_vec128i(v50), simd::set1_i32(0x1F))));
	// vperm128 v33,v62,v63,v0
	simd::store_i8(v33.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vperm128 v48,v62,v63,v7
	simd::store_i8(v48.u8, simd::permute_bytes(simd::load_i8(v62.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vsrw128 v32,v8,v50
simd::store_shuffled(v32, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v8), simd::and_u32(simd::to_vec128i(v50), simd::set1_i32(0x1F))));
	// vperm128 v47,v61,v63,v0
	simd::store_i8(v47.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// rldicl r11,r9,32,32
	r11.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF;
	// vperm128 v46,v61,v63,v7
	simd::store_i8(v46.u8, simd::permute_bytes(simd::load_i8(v61.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// std r4,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r4.u64);
	// vperm128 v43,v60,v63,v7
	simd::store_i8(v43.u8, simd::permute_bytes(simd::load_i8(v60.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v44,v33,31
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v45,v60,v63,v0
	simd::store_i8(v45.u8, simd::permute_bytes(simd::load_i8(v60.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v42,v48,31
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v41,v59,v63,v0
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcuxwfp128 v40,v34,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v34.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v36,v47,31
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v39,v59,v63,v7
	simd::store_i8(v39.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v34,v46,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v37,v58,v63,v0
	simd::store_i8(v37.u8, simd::permute_bytes(simd::load_i8(v58.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v47,v43,31
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v35,v58,v63,v7
	simd::store_i8(v35.u8, simd::permute_bytes(simd::load_i8(v58.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v33,v57,v63,v0
	simd::store_i8(v33.u8, simd::permute_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcuxwfp128 v38,v32,31
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(v32.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v48,v57,v63,v7
	simd::store_i8(v48.u8, simd::permute_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v32,v45,31
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v46,v56,v63,v0
	simd::store_i8(v46.u8, simd::permute_bytes(simd::load_i8(v56.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v45,v41,31
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v43,v55,v63,v0
	simd::store_i8(v43.u8, simd::permute_bytes(simd::load_i8(v55.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v41,v39,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v39,v37,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v37,v56,v63,v7
	simd::store_i8(v37.u8, simd::permute_bytes(simd::load_i8(v56.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v35,v35,31
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v33,v33,31
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// li r7,1008
	ctx.r7.s64 = 1008;
	// vcsxwfp128 v48,v48,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// li r4,1024
	ctx.r4.s64 = 1024;
	// vcsxwfp128 v46,v46,31
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// std r31,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r31.u64);
	// vcsxwfp128 v43,v43,31
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// li r31,-16
	r31.s64 = -16;
	// vmulfp128 v10,v40,v13
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// vsubfp128 v3,v36,v44
	simd::store_f32_aligned(ctx.v3.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(v44.f32)));
	// vperm128 v5,v55,v63,v7
	simd::store_i8(ctx.v5.u8, simd::permute_bytes(simd::load_i8(v55.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vmulfp128 v11,v38,v12
	simd::store_f32_aligned(ctx.v11.f32, simd::mul_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// vsubfp128 v1,v34,v42
	simd::store_f32_aligned(ctx.v1.f32, simd::sub_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v42.f32)));
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// vmulfp128 v4,v44,v12
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// add r3,r11,r3
	ctx.r3.u64 = r11.u64 + ctx.r3.u64;
	// vmulfp128 v2,v42,v13
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vperm128 v40,v54,v63,v0
	simd::store_i8(v40.u8, simd::permute_bytes(simd::load_i8(v54.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vmulfp128 v31,v32,v12
	simd::store_f32_aligned(v31.f32, simd::mul_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vperm128 v38,v54,v63,v7
	simd::store_i8(v38.u8, simd::permute_bytes(simd::load_i8(v54.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vsubfp128 v30,v45,v32
	simd::store_f32_aligned(v30.f32, simd::sub_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(v32.f32)));
	// vperm128 v36,v53,v63,v0
	simd::store_i8(v36.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vmulfp128 v29,v47,v13
	simd::store_f32_aligned(v29.f32, simd::mul_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v28,v41,v47
	simd::store_f32_aligned(v28.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v47.f32)));
	// vmulfp128 v25,v35,v13
	simd::store_f32_aligned(v25.f32, simd::mul_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v24,v48,v35
	simd::store_f32_aligned(v24.f32, simd::sub_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v35.f32)));
	// vmulfp128 v27,v39,v12
	simd::store_f32_aligned(v27.f32, simd::mul_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v26,v33,v39
	simd::store_f32_aligned(v26.f32, simd::sub_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(v39.f32)));
	// vmulfp128 v23,v46,v12
	simd::store_f32_aligned(v23.f32, simd::mul_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vsubfp128 v22,v43,v46
	simd::store_f32_aligned(v22.f32, simd::sub_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v46.f32)));
	// vmaddfp v21,v3,v11,v4
	simd::store_f32_aligned(v21.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v3.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v20,v1,v10,v2
	simd::store_f32_aligned(v20.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v1.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v2.f32)));
	// vcsxwfp128 v35,v37,31
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vmaddfp v19,v30,v11,v31
	simd::store_f32_aligned(v19.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v30.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v31.f32)));
	// vcsxwfp128 v34,v5,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v5.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vmaddfp v18,v28,v10,v29
	simd::store_f32_aligned(v18.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v29.f32)));
	// vmaddfp v16,v24,v10,v25
	simd::store_f32_aligned(v16.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v24.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v25.f32)));
	// vmaddfp v17,v26,v11,v27
	simd::store_f32_aligned(v17.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v26.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v27.f32)));
	// vmaddfp v15,v22,v11,v23
	simd::store_f32_aligned(v15.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v22.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v23.f32)));
	// stvx128 v21,r10,r7
	ea = (ctx.r10.u32 + ctx.r7.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v21), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v20,r10,r4
	ea = (ctx.r10.u32 + ctx.r4.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v20), &VectorMaskL[(ea & 0xF) * 16]);
	// vmulfp128 v14,v35,v13
	simd::store_f32_aligned(v14.f32, simd::mul_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// stvx128 v19,r10,r31
	ea = (ctx.r10.u32 + r31.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v19), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v18,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v18), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v16,r10,r6
	ea = (ctx.r10.u32 + ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v16), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v17,r10,r14
	ea = (ctx.r10.u32 + r14.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v17), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v15,r10,r15
	ea = (ctx.r10.u32 + r15.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v15), &VectorMaskL[(ea & 0xF) * 16]);
	// vperm128 v33,v53,v63,v7
	simd::store_i8(v33.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v32,v40,31
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v48,v52,v63,v0
	simd::store_i8(v48.u8, simd::permute_bytes(simd::load_i8(v52.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vcsxwfp128 v47,v38,31
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v46,v52,v63,v7
	simd::store_i8(v46.u8, simd::permute_bytes(simd::load_i8(v52.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v45,v36,31
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v44,v51,v63,v0
	simd::store_i8(v44.u8, simd::permute_bytes(simd::load_i8(v51.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v0.u8)));
	// vsubfp128 v5,v34,v35
	simd::store_f32_aligned(ctx.v5.f32, simd::sub_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v35.f32)));
	// vperm128 v43,v51,v63,v7
	simd::store_i8(v43.u8, simd::permute_bytes(simd::load_i8(v51.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v42,v33,31
	simd::store_f32_aligned(v42.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v41,v48,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r8,r20,r3
	ctx.r8.u64 = r20.u64 ^ ctx.r3.u64;
	// vcsxwfp128 v40,v46,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// ld r4,104(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// vcsxwfp128 v39,v44,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v44.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r7,r8,0,0,24
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// vcsxwfp128 v38,v43,31
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v43.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// ld r31,96(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// vmulfp128 v4,v32,v12
	simd::store_f32_aligned(ctx.v4.f32, simd::mul_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v3,v47,v13
	simd::store_f32_aligned(ctx.v3.f32, simd::mul_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v2,v45,v32
	simd::store_f32_aligned(ctx.v2.f32, simd::sub_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(v32.f32)));
	// vmaddfp v1,v5,v10,v14
	simd::store_f32_aligned(ctx.v1.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v5.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v14.f32)));
	// vsubfp128 v31,v42,v47
	simd::store_f32_aligned(v31.f32, simd::sub_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v47.f32)));
	// vmulfp128 v30,v41,v12
	simd::store_f32_aligned(v30.f32, simd::mul_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(ctx.v12.f32)));
	// vmulfp128 v29,v40,v13
	simd::store_f32_aligned(v29.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vsubfp128 v28,v39,v41
	simd::store_f32_aligned(v28.f32, simd::sub_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v41.f32)));
	// vsubfp128 v27,v38,v40
	simd::store_f32_aligned(v27.f32, simd::sub_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v40.f32)));
	// vmaddfp v26,v2,v11,v4
	simd::store_f32_aligned(v26.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// stvx128 v1,r10,r16
	ea = (ctx.r10.u32 + r16.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v1), &VectorMaskL[(ea & 0xF) * 16]);
	// vmaddfp v25,v31,v10,v3
	simd::store_f32_aligned(v25.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v31.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v24,v28,v11,v30
	simd::store_f32_aligned(v24.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v28.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(v30.f32)));
	// vmaddfp v23,v27,v10,v29
	simd::store_f32_aligned(v23.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(v27.f32), simd::load_f32_aligned(ctx.v10.f32)), simd::load_f32_aligned(v29.f32)));
	// stvx128 v26,r10,r17
	ea = (ctx.r10.u32 + r17.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v26), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v25,r10,r18
	ea = (ctx.r10.u32 + r18.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v25), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v24,r0,r5
	ea = (ctx.r5.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v24), &VectorMaskL[(ea & 0xF) * 16]);
	// stvx128 v23,r10,r19
	ea = (ctx.r10.u32 + r19.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v23), &VectorMaskL[(ea & 0xF) * 16]);
	// beq cr6,0x82cea55c
	if (cr6.eq) goto loc_82CEA55C;
	// li r11,128
	r11.s64 = 128;
	// dcbt r11,r3
loc_82CEA55C:
	// lwz r11,316(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(316) );
	// vadduwm v9,v9,v6
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v9.u32), simd::load_u32(ctx.v6.u32)));
	// vadduwm v8,v8,v6
	simd::store_u32(ctx.v8.u32, simd::add_u32(simd::load_u32(ctx.v8.u32), simd::load_u32(ctx.v6.u32)));
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// addic. r11,r11,-8
	xer.ca = r11.u32 > 7;
	r11.s64 = r11.s64 + -8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// vaddfp128 v13,v13,v49
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v49.f32)));
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// vaddfp128 v12,v12,v49
	simd::store_f32_aligned(ctx.v12.f32, simd::add_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v49.f32)));
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// bgt 0x82cea2f4
	if (cr0.gt) goto loc_82CEA2F4;
	// lwz r31,324(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(324) );
	// lbz r11,13(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 13);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// subf r7,r10,r3
	ctx.r7.s64 = ctx.r3.s64 - ctx.r10.s64;
	// twllei r8,0
	// divwu r10,r7,r8
	ctx.r10.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cea5b4
	if (cr6.lt) goto loc_82CEA5B4;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CEA5B4:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cea5d4
	if (!cr6.lt) goto loc_82CEA5D4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEA5D4:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// lfd f0,26688(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 26688);
	// fmul f0,f1,f0
	f0.f64 = ctx.f1.f64 * f0.f64;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// stfs f13,48(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82CEA140) {
	__imp__sub_82CEA140(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEA600) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bbc
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r24,r3,13
	r24.s64 = ctx.r3.s64 + 13;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r20,r3,8
	r20.s64 = ctx.r3.s64 + 8;
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r17,r3,28
	r17.s64 = ctx.r3.s64 + 28;
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r9,r5,r10
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r30,4(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r31,r4,r6
	r31.s64 = ctx.r6.s64 - ctx.r4.s64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r4,2,0,29
	ctx.r7.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r31.u32);
	// addi r18,r3,24
	r18.s64 = ctx.r3.s64 + 24;
	// addi r23,r3,4
	r23.s64 = ctx.r3.s64 + 4;
	// addi r19,r3,20
	r19.s64 = ctx.r3.s64 + 20;
	// addi r27,r3,52
	r27.s64 = ctx.r3.s64 + 52;
	// add r6,r9,r11
	ctx.r6.u64 = ctx.r9.u64 + r11.u64;
	// subf r25,r5,r30
	r25.s64 = r30.s64 - ctx.r5.s64;
	// add r4,r7,r8
	ctx.r4.u64 = ctx.r7.u64 + ctx.r8.u64;
	// dcbt r0,r6
	// li r8,1
	ctx.r8.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r9,40
	ctx.r9.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r5,r8,32,63
	ctx.r5.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r11,4
	r11.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v60,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// lvlx128 v63,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// addi r21,r3,36
	r21.s64 = ctx.r3.s64 + 36;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r7,r1,-136
	ctx.r7.s64 = ctx.r1.s64 + -136;
	// addi r22,r3,48
	r22.s64 = ctx.r3.s64 + 48;
	// li r30,0
	r30.s64 = 0;
	// lvlx128 v61,r0,r21
	temp.u32 = r0.u32 + r21.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r8)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + 3248);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r5,-136(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// lvlx128 v55,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r29,-136(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r29,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r29.u64);
	// fdiv f0,f0,f11
	f0.f64 = f0.f64 / ctx.f11.f64;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r5,0
	cr6.compare<int64_t>(ctx.r5.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cea794
	if (!cr6.lt) goto loc_82CEA794;
loc_82CEA710:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82cea794
	if (!cr6.gt) goto loc_82CEA794;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cea778
	if (cr6.eq) goto loc_82CEA778;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// mr r11,r27
	r11.u64 = r27.u64;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// subf r7,r27,r6
	ctx.r7.s64 = ctx.r6.s64 - r27.s64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v11,v0,v12
	simd::store_i8(ctx.v11.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// vcuxwfp128 v52,v11,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v11.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v51,v63,v53,4
	simd::store_i8(v51.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// vmulfp128 v50,v52,v51
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v51.f32)));
	// vsldoi128 v62,v50,v63,8
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v50.u8), simd::load_i8(v63.u8), 8));
loc_82CEA750:
	// lvlx128 v49,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vspltw128 v48,v49,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v49.u32), 3));
	// lvlx128 v47,r7,r11
	temp.u32 = ctx.r7.u32 + r11.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// vrlimi128 v48,v47,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v47.f32))));
	// vmsum3fp128 v46,v48,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v46.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v62.f32), 0xEF));
	// stvewx128 v46,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v46.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,1024
	ctx.r8.s64 = ctx.r8.s64 + 1024;
	// bne 0x82cea750
	if (!cr0.eq) goto loc_82CEA750;
loc_82CEA778:
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// cmpdi cr6,r5,0
	cr6.compare<int64_t>(ctx.r5.s64, 0, xer);
	// blt cr6,0x82cea710
	if (cr6.lt) goto loc_82CEA710;
loc_82CEA794:
	// sradi r11,r5,63
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	r11.s64 = ctx.r5.s64 >> 63;
	// sradi r9,r5,32
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r5.s64 >> 32;
	// addi r26,r25,-1
	r26.s64 = r25.s64 + -1;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// extsw r28,r26
	r28.s64 = r26.s32;
	// cmpd cr6,r11,r28
	cr6.compare<int64_t>(r11.s64, r28.s64, xer);
	// bge cr6,0x82cea880
	if (!cr6.lt) goto loc_82CEA880;
loc_82CEA7B0:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82cea8e4
	if (cr6.eq) goto loc_82CEA8E4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cea834
	if (cr6.eq) goto loc_82CEA834;
	// vspltisw128 v45,0
	simd::store_i32(v45.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// vsubfp128 v44,v45,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v44.f32, simd::sub_f32(simd::load_f32_aligned(v45.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v11,v0,v12
	simd::store_i8(ctx.v11.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// vcuxwfp128 v43,v11,31
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v11.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// vsldoi128 v42,v63,v44,4
	simd::store_i8(v42.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v44.u8), 12));
	// vmulfp128 v41,v43,v42
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v42.f32)));
	// rlwinm r11,r8,2,0,29
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r6
	ctx.r8.u64 = r11.u64 + ctx.r6.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// vsldoi128 v62,v41,v63,8
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v41.u8), simd::load_i8(v63.u8), 8));
loc_82CEA808:
	// lvlx128 v40,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// vspltw128 v39,v40,0
	simd::store_i32(v39.u32, simd::broadcast_lane_i32(simd::load_i32(v40.u32), 3));
	// lvlx128 v38,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// vrlimi128 v39,v38,8,0
	simd::store_f32(v39.f32, simd::blend_f32<8>(simd::load_f32(v39.f32), simd::permute_f32<228>(simd::load_f32(v38.f32))));
	// vmsum3fp128 v37,v39,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v62.f32), 0xEF));
	// stvewx128 v37,r0,r7
	PPC_STORE_U32((ctx.r7.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r7.u32) & 0xF) >> 2));
	// addi r7,r7,1024
	ctx.r7.s64 = ctx.r7.s64 + 1024;
	// bne 0x82cea808
	if (!cr0.eq) goto loc_82CEA808;
loc_82CEA834:
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// sradi r11,r5,32
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0xFFFFFFFF) != 0);
	r11.s64 = ctx.r5.s64 >> 32;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// extsw r9,r11
	ctx.r9.s64 = r11.s32;
	// mullw r8,r9,r10
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// xor r7,r9,r30
	ctx.r7.u64 = ctx.r9.u64 ^ r30.u64;
	// rlwinm r8,r7,0,0,24
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82cea874
	if (cr6.eq) goto loc_82CEA874;
	// li r8,128
	ctx.r8.s64 = 128;
	// dcbt r8,r9
loc_82CEA874:
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// cmpd cr6,r11,r28
	cr6.compare<int64_t>(r11.s64, r28.s64, xer);
	// blt cr6,0x82cea7b0
	if (cr6.lt) goto loc_82CEA7B0;
loc_82CEA880:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82cea8e4
	if (cr6.eq) goto loc_82CEA8E4;
	// cmpd cr6,r11,r28
	cr6.compare<int64_t>(r11.s64, r28.s64, xer);
	// bne cr6,0x82cea8cc
	if (!cr6.eq) goto loc_82CEA8CC;
	// mullw r9,r26,r10
	ctx.r9.s64 = int64_t(r26.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r8,r9,r6
	ctx.r8.u64 = ctx.r9.u64 + ctx.r6.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cea8e4
	if (cr6.eq) goto loc_82CEA8E4;
	// subf r7,r27,r8
	ctx.r7.s64 = ctx.r8.s64 - r27.s64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_82CEA8B4:
	// lfsx f13,r7,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stfs f13,0(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x82cea8b4
	if (!cr0.eq) goto loc_82CEA8B4;
	// b 0x82cea8e4
	goto loc_82CEA8E4;
loc_82CEA8CC:
	// extsw r9,r25
	ctx.r9.s64 = r25.s32;
	// cmpd cr6,r11,r9
	cr6.compare<int64_t>(r11.s64, ctx.r9.s64, xer);
	// ble cr6,0x82cea8e4
	if (!cr6.gt) goto loc_82CEA8E4;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// rldicr r9,r9,32,31
	ctx.r9.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// add r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 + ctx.r5.u64;
loc_82CEA8E4:
	// rldicr r9,r11,32,31
	ctx.r9.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF00000000;
	// rotlwi r8,r11,0
	ctx.r8.u64 = rotl32(r11.u32, 0);
	// subf r7,r9,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r9.s64;
	// mullw r5,r8,r10
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// std r7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r7.u64);
	// stvewx128 v63,r0,r21
	PPC_STORE_U32((r21.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r21.u32) & 0xF) >> 2));
	// lfd f13,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rlwinm r7,r5,2,0,29
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// rotlwi r5,r9,2
	ctx.r5.u64 = rotl32(ctx.r9.u32, 2);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// twllei r5,0
	// divwu r10,r6,r5
	ctx.r10.u32 = ctx.r6.u32 / ctx.r5.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f0
	ctx.f11.f64 = ctx.f12.f64 * f0.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r22)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// bge cr6,0x82cea940
	if (!cr6.lt) goto loc_82CEA940;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEA940:
	// lwz r10,0(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r9,0(r18)
	ctx.r9.u64 = PPC_LOAD_U32(r18.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cea964
	if (!cr6.lt) goto loc_82CEA964;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// b 0x82ca2c0c
	return;
loc_82CEA964:
	// stw r9,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82CEA600) {
	__imp__sub_82CEA600(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEA970) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r23,r3,8
	r23.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r29,r3,13
	r29.s64 = ctx.r3.s64 + 13;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r20,r3,28
	r20.s64 = ctx.r3.s64 + 28;
	// mullw r6,r4,r5
	ctx.r6.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// addi r21,r3,24
	r21.s64 = ctx.r3.s64 + 24;
	// addi r28,r3,4
	r28.s64 = ctx.r3.s64 + 4;
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// addi r22,r3,20
	r22.s64 = ctx.r3.s64 + 20;
	// addi r31,r3,52
	r31.s64 = ctx.r3.s64 + 52;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// subf r25,r5,r4
	r25.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// dcbt r0,r9
	// li r11,1
	r11.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// addi r30,r1,-144
	r30.s64 = ctx.r1.s64 + -144;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r10,4
	ctx.r10.s64 = 4;
	// lfd f0,3248(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 3248);
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// li r5,40
	ctx.r5.s64 = 40;
	// fdiv f0,f0,f11
	f0.f64 = f0.f64 / ctx.f11.f64;
	// addi r26,r3,36
	r26.s64 = ctx.r3.s64 + 36;
	// addi r6,r1,-136
	ctx.r6.s64 = ctx.r1.s64 + -136;
	// addi r27,r3,48
	r27.s64 = ctx.r3.s64 + 48;
	// li r4,0
	ctx.r4.s64 = 0;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v61,r0,r26
	temp.u32 = r0.u32 + r26.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r5,-136(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ceaae0
	if (!cr6.lt) goto loc_82CEAAE0;
loc_82CEAA80:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82ceaae0
	if (!cr6.gt) goto loc_82CEAAE0;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vsubfp128 v50,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v50.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v49,v10,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v48,v63,v50,4
	simd::store_i8(v48.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v50.u8), 12));
	// vmulfp128 v47,v49,v48
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v48.f32)));
	// vsldoi128 v46,v47,v63,8
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v47.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v45,v51,v46
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v45,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x82ceaa80
	if (cr6.lt) goto loc_82CEAA80;
loc_82CEAAE0:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r6,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = r11.s64 >> 32;
	// addi r24,r25,-1
	r24.s64 = r25.s64 + -1;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// extsw r30,r24
	r30.s64 = r24.s32;
	// cmpd cr6,r10,r30
	cr6.compare<int64_t>(ctx.r10.s64, r30.s64, xer);
	// bge cr6,0x82ceab94
	if (!cr6.lt) goto loc_82CEAB94;
loc_82CEAAFC:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ceabd0
	if (cr6.eq) goto loc_82CEABD0;
	// vspltisw128 v44,0
	simd::store_i32(v44.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r19,r10,2,0,29
	r19.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vsubfp128 v43,v44,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v43.f32, simd::sub_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r18,r10,2,0,29
	r18.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// lvlx128 v42,r19,r9
	temp.u32 = r19.u32 + ctx.r9.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vspltw128 v41,v42,0
	simd::store_i32(v41.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vcuxwfp128 v40,v10,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// lvlx128 v39,r18,r9
	temp.u32 = r18.u32 + ctx.r9.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// vrlimi128 v41,v39,8,0
	simd::store_f32(v41.f32, simd::blend_f32<8>(simd::load_f32(v41.f32), simd::permute_f32<228>(simd::load_f32(v39.f32))));
	// xor r4,r6,r4
	ctx.r4.u64 = ctx.r6.u64 ^ ctx.r4.u64;
	// rlwinm r4,r4,0,0,24
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
	// vsldoi128 v38,v63,v43,4
	simd::store_i8(v38.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v43.u8), 12));
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vmulfp128 v37,v40,v38
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v38.f32)));
	// vsldoi128 v36,v37,v63,8
	simd::store_i8(v36.u8, simd::shift_left_insert_bytes(simd::load_i8(v37.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v35,v41,v36
	simd::store_f32_aligned(v35.f32, simd::dp_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v36.f32), 0xEF));
	// stvewx128 v35,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v35.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// beq cr6,0x82ceab88
	if (cr6.eq) goto loc_82CEAB88;
	// li r4,128
	ctx.r4.s64 = 128;
	// dcbt r4,r6
loc_82CEAB88:
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// cmpd cr6,r10,r30
	cr6.compare<int64_t>(ctx.r10.s64, r30.s64, xer);
	// blt cr6,0x82ceaafc
	if (cr6.lt) goto loc_82CEAAFC;
loc_82CEAB94:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ceabd0
	if (cr6.eq) goto loc_82CEABD0;
	// cmpd cr6,r10,r30
	cr6.compare<int64_t>(ctx.r10.s64, r30.s64, xer);
	// bne cr6,0x82ceabb8
	if (!cr6.eq) goto loc_82CEABB8;
	// rlwinm r7,r24,2,0,29
	ctx.r7.u64 = rotl64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfsx f13,r7,r9
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,0(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// b 0x82ceabd0
	goto loc_82CEABD0;
loc_82CEABB8:
	// extsw r7,r25
	ctx.r7.s64 = r25.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ceabd0
	if (!cr6.gt) goto loc_82CEABD0;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CEABD0:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r26
	PPC_STORE_U32((r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r26.u32) & 0xF) >> 2));
	// lfd f13,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r11,r3,r6
	r11.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// rotlwi r9,r4,2
	ctx.r9.u64 = rotl32(ctx.r4.u32, 2);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r10.u32 / ctx.r9.u32;
	// twllei r9,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f0
	ctx.f11.f64 = ctx.f12.f64 * f0.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r27)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r27.u32 + 0, temp.u32);
	// bge cr6,0x82ceac24
	if (!cr6.lt) goto loc_82CEAC24;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEAC24:
	// lwz r10,0(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + int32_t(0) );
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82ceac48
	if (!cr6.lt) goto loc_82CEAC48;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CEAC48:
	// stw r9,0(r20)
	PPC_STORE_U32(r20.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CEA970) {
	__imp__sub_82CEA970(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEAC50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r21,r3,8
	r21.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r27,r3,13
	r27.s64 = ctx.r3.s64 + 13;
	// lwz r8,28(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r18,r3,28
	r18.s64 = ctx.r3.s64 + 28;
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r4,r4,r5
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r8,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r8.s64;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r19,r3,24
	r19.s64 = ctx.r3.s64 + 24;
	// stw r6,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r6.u32);
	// addi r26,r3,4
	r26.s64 = ctx.r3.s64 + 4;
	// addi r20,r3,20
	r20.s64 = ctx.r3.s64 + 20;
	// addi r29,r3,52
	r29.s64 = ctx.r3.s64 + 52;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// subf r22,r5,r31
	r22.s64 = r31.s64 - ctx.r5.s64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// dcbt r0,r8
	// li r11,1
	r11.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r5,40
	ctx.r5.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r7,r11,32,63
	ctx.r7.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// std r7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r7.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r7,4
	ctx.r7.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r5,r1,-144
	ctx.r5.s64 = ctx.r1.s64 + -144;
	// lvlx128 v63,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// addi r24,r3,36
	r24.s64 = ctx.r3.s64 + 36;
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// addi r10,r1,-136
	ctx.r10.s64 = ctx.r1.s64 + -136;
	// li r30,1024
	r30.s64 = 1024;
	// addi r25,r3,48
	r25.s64 = ctx.r3.s64 + 48;
	// lvlx128 v61,r0,r24
	temp.u32 = r0.u32 + r24.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r31,0
	r31.s64 = 0;
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r4)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3248);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r7,r5
	temp.u32 = ctx.r7.u32 + ctx.r5.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r4,-136(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r4,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r4.u64);
	// fdiv f0,f0,f11
	f0.f64 = f0.f64 / ctx.f11.f64;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r7,r10
	temp.u32 = ctx.r7.u32 + ctx.r10.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ceaddc
	if (!cr6.lt) goto loc_82CEADDC;
loc_82CEAD64:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82ceaddc
	if (!cr6.gt) goto loc_82CEADDC;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r29,r7
	temp.u32 = r29.u32 + ctx.r7.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r8,r7
	temp.u32 = ctx.r8.u32 + ctx.r7.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// vsubfp128 v50,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v50.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v49,v10,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v48,v63,v50,4
	simd::store_i8(v48.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v50.u8), 12));
	// vmulfp128 v47,v49,v48
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v48.f32)));
	// vsldoi128 v46,v47,v63,8
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v47.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v45,v51,v46
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v45,r9,r30
	PPC_STORE_U32((ctx.r9.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r9.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v44,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v43,r0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v42,v43,0
	simd::store_i32(v42.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v42,v44,8,0
	simd::store_f32(v42.f32, simd::blend_f32<8>(simd::load_f32(v42.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v41,v42,v46
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v41,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// blt cr6,0x82cead64
	if (cr6.lt) goto loc_82CEAD64;
loc_82CEADDC:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r5,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r5.s64 = r11.s64 >> 32;
	// addi r23,r22,-1
	r23.s64 = r22.s64 + -1;
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// extsw r28,r23
	r28.s64 = r23.s32;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// bge cr6,0x82ceaeb0
	if (!cr6.lt) goto loc_82CEAEB0;
loc_82CEADF8:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82ceaef8
	if (cr6.eq) goto loc_82CEAEF8;
	// vspltisw128 v40,0
	simd::store_i32(v40.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// vsubfp128 v39,v40,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v39.f32, simd::sub_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r5,r5,3,0,28
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lvlx128 v38,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcuxwfp128 v37,v10,31
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v36,v38,0
	simd::store_i32(v36.u32, simd::broadcast_lane_i32(simd::load_i32(v38.u32), 3));
	// lvlx128 v35,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v36,v35,8,0
	simd::store_f32(v36.f32, simd::blend_f32<8>(simd::load_f32(v36.f32), simd::permute_f32<228>(simd::load_f32(v35.f32))));
	// vsldoi128 v34,v63,v39,4
	simd::store_i8(v34.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v39.u8), 12));
	// vmulfp128 v33,v37,v34
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::load_f32_aligned(v37.f32), simd::load_f32_aligned(v34.f32)));
	// vsldoi128 v32,v33,v63,8
	simd::store_i8(v32.u8, simd::shift_left_insert_bytes(simd::load_i8(v33.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v61,v36,v32
	simd::store_f32_aligned(v61.f32, simd::dp_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(v32.f32), 0xEF));
	// stvewx128 v61,r9,r30
	PPC_STORE_U32((ctx.r9.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v61.u32), 3 - ((ctx.r9.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v60,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v59,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v58,v59,0
	simd::store_i32(v58.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// vrlimi128 v58,v60,8,0
	simd::store_f32(v58.f32, simd::blend_f32<8>(simd::load_f32(v58.f32), simd::permute_f32<228>(simd::load_f32(v60.f32))));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// rlwinm r5,r10,3,0,28
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// vmsum3fp128 v57,v58,v32
	simd::store_f32_aligned(v57.f32, simd::dp_f32(simd::load_f32_aligned(v58.f32), simd::load_f32_aligned(v32.f32), 0xEF));
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// xor r31,r5,r31
	r31.u64 = ctx.r5.u64 ^ r31.u64;
	// rlwinm r31,r31,0,0,24
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stvewx128 v57,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v57.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// beq cr6,0x82ceaea4
	if (cr6.eq) goto loc_82CEAEA4;
	// li r31,128
	r31.s64 = 128;
	// dcbt r31,r5
loc_82CEAEA4:
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// blt cr6,0x82ceadf8
	if (cr6.lt) goto loc_82CEADF8;
loc_82CEAEB0:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82ceaef8
	if (cr6.eq) goto loc_82CEAEF8;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// bne cr6,0x82ceaee0
	if (!cr6.eq) goto loc_82CEAEE0;
	// rlwinm r7,r23,3,0,28
	ctx.r7.u64 = rotl64(r23.u32 | (r23.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lfs f13,4(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r29)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r29.u32 + 4, temp.u32);
	// lfs f12,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,0(r29)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// b 0x82ceaef8
	goto loc_82CEAEF8;
loc_82CEAEE0:
	// extsw r7,r22
	ctx.r7.s64 = r22.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ceaef8
	if (!cr6.gt) goto loc_82CEAEF8;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CEAEF8:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r10,3,0,28
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r24
	PPC_STORE_U32((r24.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r24.u32) & 0xF) >> 2));
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r10,r4,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r4.s64;
	// lbz r3,0(r27)
	ctx.r3.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lfd f13,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// fmul f11,f12,f0
	ctx.f11.f64 = ctx.f12.f64 * f0.f64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rotlwi r8,r3,2
	ctx.r8.u64 = rotl32(ctx.r3.u32, 2);
	// divwu r10,r10,r8
	ctx.r10.u32 = ctx.r10.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r25)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r25.u32 + 0, temp.u32);
	// bge cr6,0x82ceaf4c
	if (!cr6.lt) goto loc_82CEAF4C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEAF4C:
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r8,0(r19)
	ctx.r8.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// rlwinm r11,r9,30,2,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82ceaf70
	if (!cr6.lt) goto loc_82CEAF70;
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CEAF70:
	// stw r8,0(r18)
	PPC_STORE_U32(r18.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CEAC50) {
	__imp__sub_82CEAC50(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEAF78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bb8
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r9,r3,28
	ctx.r9.s64 = ctx.r3.s64 + 28;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// stw r10,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, ctx.r10.u32);
	// addi r8,r3,24
	ctx.r8.s64 = ctx.r3.s64 + 24;
	// stw r9,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r9.u32);
	// addi r23,r3,13
	r23.s64 = ctx.r3.s64 + 13;
	// stw r8,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r8.u32);
	// addi r22,r3,4
	r22.s64 = ctx.r3.s64 + 4;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// addi r10,r3,20
	ctx.r10.s64 = ctx.r3.s64 + 20;
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r31,r3,52
	r31.s64 = ctx.r3.s64 + 52;
	// mullw r5,r6,r7
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// stw r10,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r10.u32);
	// lwz r30,4(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r4,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r4.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r5,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r5.u32);
	// subf r19,r7,r30
	r19.s64 = r30.s64 - ctx.r7.s64;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r11
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r7,40
	ctx.r7.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r8,r4,32,63
	ctx.r8.u64 = rotl64(ctx.r4.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r9,r1,-168
	ctx.r9.s64 = ctx.r1.s64 + -168;
	// std r8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r8.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r6,4
	ctx.r6.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v60,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r7,r1,-168
	ctx.r7.s64 = ctx.r1.s64 + -168;
	// lvlx128 v63,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// addi r20,r3,36
	r20.s64 = ctx.r3.s64 + 36;
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// addi r8,r1,-144
	ctx.r8.s64 = ctx.r1.s64 + -144;
	// li r27,3072
	r27.s64 = 3072;
	// li r28,2048
	r28.s64 = 2048;
	// lvlx128 v61,r0,r20
	temp.u32 = r0.u32 + r20.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r29,1024
	r29.s64 = 1024;
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r4)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3248);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f8.u64);
	// ld r9,-144(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// lvlx128 v55,r6,r7
	temp.u32 = ctx.r6.u32 + ctx.r7.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f7.u64);
	// ld r30,-144(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// std r30,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r30.u64);
	// addi r21,r3,48
	r21.s64 = ctx.r3.s64 + 48;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r6,r8
	temp.u32 = ctx.r6.u32 + ctx.r8.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fdiv f0,f0,f11
	ctx.fpscr.disableFlushModeUnconditional();
	f0.f64 = f0.f64 / ctx.f11.f64;
	// li r26,0
	r26.s64 = 0;
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r9,0
	cr6.compare<int64_t>(ctx.r9.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ceb154
	if (!cr6.lt) goto loc_82CEB154;
	// li r7,12
	ctx.r7.s64 = 12;
	// li r8,8
	ctx.r8.s64 = 8;
loc_82CEB0AC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82ceb154
	if (!cr6.gt) goto loc_82CEB154;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r31,r7
	temp.u32 = r31.u32 + ctx.r7.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// vsubfp128 v50,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v50.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// cmpdi cr6,r9,0
	cr6.compare<int64_t>(ctx.r9.s64, 0, xer);
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v49,v10,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v48,v63,v50,4
	simd::store_i8(v48.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v50.u8), 12));
	// vmulfp128 v47,v49,v48
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v48.f32)));
	// vsldoi128 v46,v47,v63,8
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v47.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v45,v51,v46
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v45,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v44,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v43,r31,r8
	temp.u32 = r31.u32 + ctx.r8.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v42,v43,0
	simd::store_i32(v42.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v42,v44,8,0
	simd::store_f32(v42.f32, simd::blend_f32<8>(simd::load_f32(v42.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v41,v42,v46
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v41,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v40,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v39,r31,r6
	temp.u32 = r31.u32 + ctx.r6.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v38,v39,0
	simd::store_i32(v38.u32, simd::broadcast_lane_i32(simd::load_i32(v39.u32), 3));
	// vrlimi128 v38,v40,8,0
	simd::store_f32(v38.f32, simd::blend_f32<8>(simd::load_f32(v38.f32), simd::permute_f32<228>(simd::load_f32(v40.f32))));
	// vmsum3fp128 v37,v38,v46
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v37,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v36,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v35,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v34,v35,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v35.u32), 3));
	// vrlimi128 v34,v36,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vmsum3fp128 v33,v34,v46
	simd::store_f32_aligned(v33.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v33,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v33.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82ceb0ac
	if (cr6.lt) goto loc_82CEB0AC;
loc_82CEB154:
	// sradi r8,r9,63
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 63;
	// sradi r7,r9,32
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s64 >> 32;
	// addi r24,r19,-1
	r24.s64 = r19.s64 + -1;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// extsw r25,r24
	r25.s64 = r24.s32;
	// cmpd cr6,r8,r25
	cr6.compare<int64_t>(ctx.r8.s64, r25.s64, xer);
	// bge cr6,0x82ceb280
	if (!cr6.lt) goto loc_82CEB280;
loc_82CEB170:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82ceb2d8
	if (cr6.eq) goto loc_82CEB2D8;
	// vspltisw128 v32,0
	simd::store_i32(v32.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addi r4,r8,3
	ctx.r4.s64 = ctx.r8.s64 + 3;
	// vsubfp128 v61,v32,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v61.f32, simd::sub_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// rlwinm r4,r4,2,0,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r18,r7,3
	r18.s64 = ctx.r7.s64 + 3;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r17,r8,2
	r17.s64 = ctx.r8.s64 + 2;
	// rlwinm r18,r18,2,0,29
	r18.u64 = rotl64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// vcuxwfp128 v60,v10,31
	simd::store_f32_aligned(v60.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r17,r17,2,0,29
	r17.u64 = rotl64(r17.u32 | (r17.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v59,r4,r11
	temp.u32 = ctx.r4.u32 + r11.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r7,2
	ctx.r4.s64 = ctx.r7.s64 + 2;
	// vspltw128 v58,v59,0
	simd::store_i32(v58.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r16,r4,2,0,29
	r16.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v57,r18,r11
	temp.u32 = r18.u32 + r11.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r4,r8,r11
	ctx.r4.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vrlimi128 v58,v57,8,0
	simd::store_f32(v58.f32, simd::blend_f32<8>(simd::load_f32(v58.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// add r9,r30,r9
	ctx.r9.u64 = r30.u64 + ctx.r9.u64;
	// vsldoi128 v56,v63,v61,4
	simd::store_i8(v56.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v61.u8), 12));
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vmulfp128 v55,v60,v56
	simd::store_f32_aligned(v55.f32, simd::mul_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v56.f32)));
	// vsldoi128 v54,v55,v63,8
	simd::store_i8(v54.u8, simd::shift_left_insert_bytes(simd::load_i8(v55.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v53,v58,v54
	simd::store_f32_aligned(v53.f32, simd::dp_f32(simd::load_f32_aligned(v58.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v53,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v53.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v52,r17,r11
	temp.u32 = r17.u32 + r11.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v51,v52,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// lvlx128 v50,r16,r11
	temp.u32 = r16.u32 + r11.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v51,v50,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v50.f32))));
	// vmsum3fp128 v49,v51,v54
	simd::store_f32_aligned(v49.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v49,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v49.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v48,r8,r6
	temp.u32 = ctx.r8.u32 + ctx.r6.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v47,r4,r6
	temp.u32 = ctx.r4.u32 + ctx.r6.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v46,v47,0
	simd::store_i32(v46.u32, simd::broadcast_lane_i32(simd::load_i32(v47.u32), 3));
	// vrlimi128 v46,v48,8,0
	simd::store_f32(v46.f32, simd::blend_f32<8>(simd::load_f32(v46.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vmsum3fp128 v45,v46,v54
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v45,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v44,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// sradi r8,r9,32
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 32;
	// lvlx128 v43,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v42,v43,0
	simd::store_i32(v42.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v42,v44,8,0
	simd::store_f32(v42.f32, simd::blend_f32<8>(simd::load_f32(v42.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// rlwinm r7,r8,4,0,27
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// vmsum3fp128 v41,v42,v54
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// xor r4,r7,r26
	ctx.r4.u64 = ctx.r7.u64 ^ r26.u64;
	// rlwinm r4,r4,0,0,24
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stvewx128 v41,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// beq cr6,0x82ceb274
	if (cr6.eq) goto loc_82CEB274;
	// li r4,128
	ctx.r4.s64 = 128;
	// dcbt r4,r7
loc_82CEB274:
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// cmpd cr6,r8,r25
	cr6.compare<int64_t>(ctx.r8.s64, r25.s64, xer);
	// blt cr6,0x82ceb170
	if (cr6.lt) goto loc_82CEB170;
loc_82CEB280:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82ceb2d8
	if (cr6.eq) goto loc_82CEB2D8;
	// cmpd cr6,r8,r25
	cr6.compare<int64_t>(ctx.r8.s64, r25.s64, xer);
	// bne cr6,0x82ceb2c0
	if (!cr6.eq) goto loc_82CEB2C0;
	// rlwinm r7,r24,4,0,27
	ctx.r7.u64 = rotl64(r24.u32 | (r24.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// lfs f13,12(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,12(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r31.u32 + 12, temp.u32);
	// lfs f12,8(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r31.u32 + 8, temp.u32);
	// lfs f11,4(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,4(r31)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// lfs f10,0(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,0(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// b 0x82ceb2d8
	goto loc_82CEB2D8;
loc_82CEB2C0:
	// extsw r7,r19
	ctx.r7.s64 = r19.s32;
	// cmpd cr6,r8,r7
	cr6.compare<int64_t>(ctx.r8.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ceb2d8
	if (!cr6.gt) goto loc_82CEB2D8;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
loc_82CEB2D8:
	// rldicr r7,r8,32,31
	ctx.r7.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r8,4,0,27
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r5,r7,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r7.s64;
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// stvewx128 v63,r0,r20
	PPC_STORE_U32((r20.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r20.u32) & 0xF) >> 2));
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r8,r4,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r4.s64;
	// lbz r3,0(r23)
	ctx.r3.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// lfd f13,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lwz r9,0(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + int32_t(0) );
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// fmul f11,f12,f0
	ctx.f11.f64 = ctx.f12.f64 * f0.f64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// rotlwi r8,r3,2
	ctx.r8.u64 = rotl32(ctx.r3.u32, 2);
	// divwu r11,r11,r8
	r11.u32 = r11.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r21)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// blt cr6,0x82ceb32c
	if (cr6.lt) goto loc_82CEB32C;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEB32C:
	// lwz r9,-160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// lwz r8,-156(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-156) );
	// lwz r7,-152(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-152) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82ceb360
	if (!cr6.lt) goto loc_82CEB360;
	// lwz r10,-176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c08
	return;
loc_82CEB360:
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c08
	return;
}

PPC_WEAK_FUNC(sub_82CEAF78) {
	__imp__sub_82CEAF78(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEB370) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// lbz r11,13(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r8,r3,28
	ctx.r8.s64 = ctx.r3.s64 + 28;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r7,r3,4
	ctx.r7.s64 = ctx.r3.s64 + 4;
	// stw r9,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, ctx.r9.u32);
	// addi r6,r3,24
	ctx.r6.s64 = ctx.r3.s64 + 24;
	// stw r8,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r8.u32);
	// stw r7,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r7.u32);
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r9,r3,20
	ctx.r9.s64 = ctx.r3.s64 + 20;
	// stw r6,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r6.u32);
	// mullw r5,r11,r4
	ctx.r5.s64 = int64_t(r11.s32) * int64_t(ctx.r4.s32);
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// lwz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// stw r9,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r9.u32);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r11,r3,13
	r11.s64 = ctx.r3.s64 + 13;
	// rlwinm r11,r5,2,0,29
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r4,r30
	ctx.r7.s64 = r30.s64 - ctx.r4.s64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r31,r6
	ctx.r5.s64 = ctx.r6.s64 - r31.s64;
	// stw r7,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r7.u32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r5,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r5.u32);
	// addi r4,r3,52
	ctx.r4.s64 = ctx.r3.s64 + 52;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r11
	// li r6,1
	ctx.r6.s64 = 1;
	// lfs f0,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	f0.f64 = double(temp.f32);
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// rldicr r8,r6,32,63
	ctx.r8.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// li r6,40
	ctx.r6.s64 = 40;
	// stw r9,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, ctx.r9.u32);
	// std r8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r8.u64);
	// lfd f13,-136(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// addi r7,r1,-176
	ctx.r7.s64 = ctx.r1.s64 + -176;
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmul f10,f0,f12
	ctx.f10.f64 = f0.f64 * ctx.f12.f64;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// lvlx128 v61,r3,r6
	temp.u32 = ctx.r3.u32 + ctx.r6.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r6,4
	ctx.r6.s64 = 4;
	// lvlx128 v63,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r31,r1,-136
	r31.s64 = ctx.r1.s64 + -136;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r29,r1,-176
	r29.s64 = ctx.r1.s64 + -176;
	// li r25,0
	r25.s64 = 0;
	// li r26,3072
	r26.s64 = 3072;
	// lvlx128 v60,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r27,2048
	r27.s64 = 2048;
	// fctidz f8,f10
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.f8.u64);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r7,-136(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vsubfp128 v59,v61,v60
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(v60.f32)));
	// ld r30,-176(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// lfd f0,3248(r8)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + 3248);
	// std r30,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r30.u64);
	// lvlx128 v56,r6,r31
	temp.u32 = ctx.r6.u32 + r31.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// std r7,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r7.u64);
	// lvlx128 v55,r6,r29
	temp.u32 = ctx.r6.u32 + r29.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r28,1024
	r28.s64 = 1024;
	// fdiv f0,f0,f12
	ctx.fpscr.disableFlushModeUnconditional();
	f0.f64 = f0.f64 / ctx.f12.f64;
	// stw r25,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r25.u32);
	// vspltw128 v63,v60,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ceb5a0
	if (!cr6.lt) goto loc_82CEB5A0;
	// li r29,20
	r29.s64 = 20;
	// li r31,16
	r31.s64 = 16;
	// li r8,12
	ctx.r8.s64 = 12;
	// li r9,8
	ctx.r9.s64 = 8;
loc_82CEB4BC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82ceb5a0
	if (!cr6.gt) goto loc_82CEB5A0;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r4,r29
	temp.u32 = ctx.r4.u32 + r29.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// li r25,5120
	r25.s64 = 5120;
	// li r24,4096
	r24.s64 = 4096;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsubfp128 v50,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v50.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v49,v10,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v48,v63,v50,4
	simd::store_i8(v48.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v50.u8), 12));
	// vmulfp128 v47,v49,v48
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v48.f32)));
	// vsldoi128 v46,v47,v63,8
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v47.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v45,v51,v46
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v45,r10,r25
	PPC_STORE_U32((ctx.r10.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r10.u32 + r25.u32) & 0xF) >> 2));
	// lvlx128 v44,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v43,r4,r31
	temp.u32 = ctx.r4.u32 + r31.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v42,v43,0
	simd::store_i32(v42.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v42,v44,8,0
	simd::store_f32(v42.f32, simd::blend_f32<8>(simd::load_f32(v42.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v41,v42,v46
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v41,r10,r24
	PPC_STORE_U32((ctx.r10.u32 + r24.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((ctx.r10.u32 + r24.u32) & 0xF) >> 2));
	// lvlx128 v40,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v39,r4,r8
	temp.u32 = ctx.r4.u32 + ctx.r8.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v38,v39,0
	simd::store_i32(v38.u32, simd::broadcast_lane_i32(simd::load_i32(v39.u32), 3));
	// vrlimi128 v38,v40,8,0
	simd::store_f32(v38.f32, simd::blend_f32<8>(simd::load_f32(v38.f32), simd::permute_f32<228>(simd::load_f32(v40.f32))));
	// vmsum3fp128 v37,v38,v46
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v37,r10,r26
	PPC_STORE_U32((ctx.r10.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r10.u32 + r26.u32) & 0xF) >> 2));
	// lvlx128 v35,r4,r9
	temp.u32 = ctx.r4.u32 + ctx.r9.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v34,v35,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v35.u32), 3));
	// lvlx128 v36,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v34,v36,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vmsum3fp128 v33,v34,v46
	simd::store_f32_aligned(v33.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v33,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v33.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v32,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	simd::store_shuffled(v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v61,r4,r6
	temp.u32 = ctx.r4.u32 + ctx.r6.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v60,v61,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// vrlimi128 v60,v32,8,0
	simd::store_f32(v60.f32, simd::blend_f32<8>(simd::load_f32(v60.f32), simd::permute_f32<228>(simd::load_f32(v32.f32))));
	// vmsum3fp128 v59,v60,v46
	simd::store_f32_aligned(v59.f32, simd::dp_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v59,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v59.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v58,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v57,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v56,v57,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vrlimi128 v56,v58,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v58.f32))));
	// vmsum3fp128 v55,v56,v46
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v55,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// lwz r25,-176(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82ceb4bc
	if (cr6.lt) goto loc_82CEB4BC;
loc_82CEB5A0:
	// lwz r9,-168(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-168) );
	// sradi r8,r7,63
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r7.s64 >> 63;
	// sradi r29,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	r29.s64 = ctx.r7.s64 >> 32;
	// addi r31,r9,-1
	r31.s64 = ctx.r9.s64 + -1;
	// subf r9,r8,r29
	ctx.r9.s64 = r29.s64 - ctx.r8.s64;
	// extsw r29,r31
	r29.s64 = r31.s32;
	// stw r31,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, r31.u32);
	// cmpd cr6,r9,r29
	cr6.compare<int64_t>(ctx.r9.s64, r29.s64, xer);
	// bge cr6,0x82ceb74c
	if (!cr6.lt) goto loc_82CEB74C;
loc_82CEB5C4:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82ceb7c0
	if (cr6.eq) goto loc_82CEB7C0;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r31,r9
	r31.s64 = ctx.r9.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r8,r31,1
	ctx.r8.s64 = r31.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r31,r8,1,0,30
	r31.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + r31.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r31,r9,5
	r31.s64 = ctx.r9.s64 + 5;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vcuxwfp128 v52,v10,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r31,r31,2,0,29
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r24,r8,5
	r24.s64 = ctx.r8.s64 + 5;
	// li r23,5120
	r23.s64 = 5120;
	// rlwinm r24,r24,2,0,29
	r24.u64 = rotl64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r22,r9,4
	r22.s64 = ctx.r9.s64 + 4;
	// lvlx128 v51,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r31,r8,4
	r31.s64 = ctx.r8.s64 + 4;
	// vspltw128 v50,v51,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v51.u32), 3));
	// rlwinm r22,r22,2,0,29
	r22.u64 = rotl64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// vsldoi128 v49,v63,v53,4
	simd::store_i8(v49.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// rlwinm r31,r31,2,0,29
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lvlx128 v48,r24,r11
	temp.u32 = r24.u32 + r11.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r24,r8,3
	r24.s64 = ctx.r8.s64 + 3;
	// li r21,4096
	r21.s64 = 4096;
	// vrlimi128 v50,v48,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// rlwinm r24,r24,2,0,29
	r24.u64 = rotl64(r24.u32 | (r24.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r20,r9,3
	r20.s64 = ctx.r9.s64 + 3;
	// vmulfp128 v47,v52,v49
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v49.f32)));
	// addi r19,r9,2
	r19.s64 = ctx.r9.s64 + 2;
	// rlwinm r20,r20,2,0,29
	r20.u64 = rotl64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r19,r19,2,0,29
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r18,r8,2
	r18.s64 = ctx.r8.s64 + 2;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r18,r18,2,0,29
	r18.u64 = rotl64(r18.u32 | (r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vsldoi128 v46,v47,v63,8
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v47.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v45,v50,v46
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v45,r10,r23
	PPC_STORE_U32((ctx.r10.u32 + r23.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r10.u32 + r23.u32) & 0xF) >> 2));
	// lvlx128 v44,r22,r11
	temp.u32 = r22.u32 + r11.u32;
	simd::store_shuffled(v44,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v43,v44,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v44.u32), 3));
	// lvlx128 v42,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v43,v42,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v42.f32))));
	// vmsum3fp128 v41,v43,v46
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v41,r10,r21
	PPC_STORE_U32((ctx.r10.u32 + r21.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((ctx.r10.u32 + r21.u32) & 0xF) >> 2));
	// lvlx128 v40,r24,r11
	temp.u32 = r24.u32 + r11.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v39,r20,r11
	temp.u32 = r20.u32 + r11.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v38,v39,0
	simd::store_i32(v38.u32, simd::broadcast_lane_i32(simd::load_i32(v39.u32), 3));
	// vrlimi128 v38,v40,8,0
	simd::store_f32(v38.f32, simd::blend_f32<8>(simd::load_f32(v38.f32), simd::permute_f32<228>(simd::load_f32(v40.f32))));
	// vmsum3fp128 v37,v38,v46
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v37,r10,r26
	PPC_STORE_U32((ctx.r10.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r10.u32 + r26.u32) & 0xF) >> 2));
	// lvlx128 v36,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v35,v36,0
	simd::store_i32(v35.u32, simd::broadcast_lane_i32(simd::load_i32(v36.u32), 3));
	// lvlx128 v34,r18,r11
	temp.u32 = r18.u32 + r11.u32;
	simd::store_shuffled(v34,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v35,v34,8,0
	simd::store_f32(v35.f32, simd::blend_f32<8>(simd::load_f32(v35.f32), simd::permute_f32<228>(simd::load_f32(v34.f32))));
	// vmsum3fp128 v33,v35,v46
	simd::store_f32_aligned(v33.f32, simd::dp_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v33,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v33.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v32,r8,r6
	temp.u32 = ctx.r8.u32 + ctx.r6.u32;
	simd::store_shuffled(v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v61,r9,r6
	temp.u32 = ctx.r9.u32 + ctx.r6.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v60,v61,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// vrlimi128 v60,v32,8,0
	simd::store_f32(v60.f32, simd::blend_f32<8>(simd::load_f32(v60.f32), simd::permute_f32<228>(simd::load_f32(v32.f32))));
	// vmsum3fp128 v59,v60,v46
	simd::store_f32_aligned(v59.f32, simd::dp_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v59,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v59.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v58,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v58,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v57,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v56,v57,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vrlimi128 v56,v58,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v58.f32))));
	// sradi r9,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s64 >> 32;
	// vmsum3fp128 v55,v56,v46
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v46.f32), 0xEF));
	// stvewx128 v55,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// extsw r31,r9
	r31.s64 = ctx.r9.s32;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// xor r31,r8,r25
	r31.u64 = ctx.r8.u64 ^ r25.u64;
	// rlwinm r31,r31,0,0,24
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ceb73c
	if (cr6.eq) goto loc_82CEB73C;
	// li r31,128
	r31.s64 = 128;
	// dcbt r31,r8
loc_82CEB73C:
	// mr r25,r8
	r25.u64 = ctx.r8.u64;
	// cmpd cr6,r9,r29
	cr6.compare<int64_t>(ctx.r9.s64, r29.s64, xer);
	// blt cr6,0x82ceb5c4
	if (cr6.lt) goto loc_82CEB5C4;
	// lwz r31,-176(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
loc_82CEB74C:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82ceb7c0
	if (cr6.eq) goto loc_82CEB7C0;
	// cmpd cr6,r9,r29
	cr6.compare<int64_t>(ctx.r9.s64, r29.s64, xer);
	// bne cr6,0x82ceb7a4
	if (!cr6.eq) goto loc_82CEB7A4;
	// rlwinm r8,r31,1,0,30
	ctx.r8.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// lfs f13,20(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,20(r4)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r4.u32 + 20, temp.u32);
	// lfs f12,16(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,16(r4)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r4.u32 + 16, temp.u32);
	// lfs f11,12(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,12(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lfs f10,8(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,8(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lfs f9,4(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,4(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lfs f8,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,0(r4)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// b 0x82ceb7c0
	goto loc_82CEB7C0;
loc_82CEB7A4:
	// lwz r8,-168(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-168) );
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// cmpd cr6,r9,r8
	cr6.compare<int64_t>(ctx.r9.s64, ctx.r8.s64, xer);
	// ble cr6,0x82ceb7c0
	if (!cr6.gt) goto loc_82CEB7C0;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// rldicr r8,r8,32,31
	ctx.r8.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
loc_82CEB7C0:
	// rldicr r6,r9,32,31
	ctx.r6.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// lwz r5,-160(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// rotlwi r8,r9,0
	ctx.r8.u64 = rotl32(ctx.r9.u32, 0);
	// lwz r4,-156(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-156) );
	// subf r6,r6,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r6.s64;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// std r6,-176(r1)
	PPC_STORE_U64(ctx.r1.u32 + -176, ctx.r6.u64);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stvewx128 v63,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lfd f13,-176(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// subf r8,r6,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r6.s64;
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// rotlwi r7,r9,2
	ctx.r7.u64 = rotl32(ctx.r9.u32, 2);
	// addi r9,r3,13
	ctx.r9.s64 = ctx.r3.s64 + 13;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// divwu r11,r8,r7
	r11.u32 = ctx.r8.u32 / ctx.r7.u32;
	// twllei r7,0
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// fmul f11,f12,f0
	ctx.f11.f64 = ctx.f12.f64 * f0.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// blt cr6,0x82ceb830
	if (cr6.lt) goto loc_82CEB830;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEB830:
	// lwz r9,-152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-152) );
	// lwz r8,-148(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-148) );
	// lwz r7,-144(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-144) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82ceb864
	if (!cr6.lt) goto loc_82CEB864;
	// lwz r10,-164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-164) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CEB864:
	// lwz r11,-164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-164) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CEB370) {
	__imp__sub_82CEB370(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEB870) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bbc
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r24,r3,13
	r24.s64 = ctx.r3.s64 + 13;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r20,r3,8
	r20.s64 = ctx.r3.s64 + 8;
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r17,r3,28
	r17.s64 = ctx.r3.s64 + 28;
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r9,r10,r5
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r30,4(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r31,r4,r6
	r31.s64 = ctx.r6.s64 - ctx.r4.s64;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r4,2,0,29
	ctx.r7.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r31.u32);
	// addi r18,r3,24
	r18.s64 = ctx.r3.s64 + 24;
	// addi r23,r3,4
	r23.s64 = ctx.r3.s64 + 4;
	// addi r19,r3,20
	r19.s64 = ctx.r3.s64 + 20;
	// addi r26,r3,52
	r26.s64 = ctx.r3.s64 + 52;
	// add r6,r9,r11
	ctx.r6.u64 = ctx.r9.u64 + r11.u64;
	// subf r25,r5,r30
	r25.s64 = r30.s64 - ctx.r5.s64;
	// add r4,r7,r8
	ctx.r4.u64 = ctx.r7.u64 + ctx.r8.u64;
	// dcbt r0,r6
	// li r8,1
	ctx.r8.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r9,40
	ctx.r9.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r5,r8,32,63
	ctx.r5.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r11,4
	r11.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v60,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// lvlx128 v63,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// addi r21,r3,36
	r21.s64 = ctx.r3.s64 + 36;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r7,r1,-136
	ctx.r7.s64 = ctx.r1.s64 + -136;
	// addi r22,r3,48
	r22.s64 = ctx.r3.s64 + 48;
	// li r30,0
	r30.s64 = 0;
	// lvlx128 v61,r0,r21
	temp.u32 = r0.u32 + r21.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r8)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + 3248);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r5,-136(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// lvlx128 v55,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r29,-136(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r29,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r29.u64);
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r5,0
	cr6.compare<int64_t>(ctx.r5.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ceba10
	if (!cr6.lt) goto loc_82CEBA10;
loc_82CEB980:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82ceba10
	if (!cr6.gt) goto loc_82CEBA10;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ceb9f4
	if (cr6.eq) goto loc_82CEB9F4;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v11,v0,v12
	simd::store_i8(ctx.v11.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// vcuxwfp128 v52,v11,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v11.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v51,v63,v53,4
	simd::store_i8(v51.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// vmulfp128 v50,v52,v51
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v51.f32)));
	// vsldoi128 v62,v50,v63,8
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v50.u8), simd::load_i8(v63.u8), 8));
loc_82CEB9C0:
	// lvlx128 v49,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// lvlx128 v47,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v46,v47,0
	simd::store_i32(v46.u32, simd::broadcast_lane_i32(simd::load_i32(v47.u32), 3));
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// vcsxwfp128 v45,v48,15
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vrlimi128 v46,v45,8,0
	simd::store_f32(v46.f32, simd::blend_f32<8>(simd::load_f32(v46.f32), simd::permute_f32<228>(simd::load_f32(v45.f32))));
	// vmsum3fp128 v44,v46,v62
	simd::store_f32_aligned(v44.f32, simd::dp_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v62.f32), 0xEF));
	// stvewx128 v44,r0,r7
	PPC_STORE_U32((ctx.r7.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v44.u32), 3 - ((ctx.r7.u32) & 0xF) >> 2));
	// addi r7,r7,1024
	ctx.r7.s64 = ctx.r7.s64 + 1024;
	// bne 0x82ceb9c0
	if (!cr0.eq) goto loc_82CEB9C0;
loc_82CEB9F4:
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// cmpdi cr6,r5,0
	cr6.compare<int64_t>(ctx.r5.s64, 0, xer);
	// blt cr6,0x82ceb980
	if (cr6.lt) goto loc_82CEB980;
loc_82CEBA10:
	// sradi r11,r5,63
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	r11.s64 = ctx.r5.s64 >> 63;
	// sradi r9,r5,32
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r5.s64 >> 32;
	// addi r27,r25,-1
	r27.s64 = r25.s64 + -1;
	// subf r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	// extsw r28,r27
	r28.s64 = r27.s32;
	// cmpd cr6,r11,r28
	cr6.compare<int64_t>(r11.s64, r28.s64, xer);
	// bge cr6,0x82cebb0c
	if (!cr6.lt) goto loc_82CEBB0C;
loc_82CEBA2C:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82cebb90
	if (cr6.eq) goto loc_82CEBB90;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cebac0
	if (cr6.eq) goto loc_82CEBAC0;
	// vspltisw128 v43,0
	simd::store_i32(v43.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// vsubfp128 v42,v43,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v42.f32, simd::sub_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v11,v0,v12
	simd::store_i8(ctx.v11.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// vcuxwfp128 v41,v11,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v11.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// vsldoi128 v40,v63,v42,4
	simd::store_i8(v40.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v42.u8), 12));
	// vmulfp128 v39,v41,v40
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v40.f32)));
	// rlwinm r11,r8,1,0,30
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r11,r6
	ctx.r8.u64 = r11.u64 + ctx.r6.u64;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// vsldoi128 v62,v39,v63,8
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v39.u8), simd::load_i8(v63.u8), 8));
loc_82CEBA84:
	// lvlx128 v38,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// vupkhsb128 v37,v38,v96
	simd::store_i32(v37.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// lvlx128 v36,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v35,v36,v96
	simd::store_i32(v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v36.s16)));
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// vcsxwfp128 v34,v37,15
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v33,v35,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v32,v34,0
	simd::store_i32(v32.u32, simd::broadcast_lane_i32(simd::load_i32(v34.u32), 3));
	// vrlimi128 v32,v33,8,0
	simd::store_f32(v32.f32, simd::blend_f32<8>(simd::load_f32(v32.f32), simd::permute_f32<228>(simd::load_f32(v33.f32))));
	// vmsum3fp128 v60,v32,v62
	simd::store_f32_aligned(v60.f32, simd::dp_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v62.f32), 0xEF));
	// stvewx128 v60,r0,r7
	PPC_STORE_U32((ctx.r7.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v60.u32), 3 - ((ctx.r7.u32) & 0xF) >> 2));
	// addi r7,r7,1024
	ctx.r7.s64 = ctx.r7.s64 + 1024;
	// bne 0x82ceba84
	if (!cr0.eq) goto loc_82CEBA84;
loc_82CEBAC0:
	// add r5,r29,r5
	ctx.r5.u64 = r29.u64 + ctx.r5.u64;
	// vadduwm v0,v0,v13
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// sradi r11,r5,32
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0xFFFFFFFF) != 0);
	r11.s64 = ctx.r5.s64 >> 32;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// extsw r9,r11
	ctx.r9.s64 = r11.s32;
	// mullw r8,r9,r10
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// xor r7,r9,r30
	ctx.r7.u64 = ctx.r9.u64 ^ r30.u64;
	// rlwinm r8,r7,0,0,24
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82cebb00
	if (cr6.eq) goto loc_82CEBB00;
	// li r8,128
	ctx.r8.s64 = 128;
	// dcbt r8,r9
loc_82CEBB00:
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
	// cmpd cr6,r11,r28
	cr6.compare<int64_t>(r11.s64, r28.s64, xer);
	// blt cr6,0x82ceba2c
	if (cr6.lt) goto loc_82CEBA2C;
loc_82CEBB0C:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82cebb90
	if (cr6.eq) goto loc_82CEBB90;
	// cmpd cr6,r11,r28
	cr6.compare<int64_t>(r11.s64, r28.s64, xer);
	// bne cr6,0x82cebb78
	if (!cr6.eq) goto loc_82CEBB78;
	// mullw r9,r27,r10
	ctx.r9.s64 = int64_t(r27.s32) * int64_t(ctx.r10.s32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r8,r9,r6
	ctx.r8.u64 = ctx.r9.u64 + ctx.r6.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82cebb90
	if (cr6.eq) goto loc_82CEBB90;
	// lis r31,-32256
	r31.s64 = -2113929216;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// lfs f0,3152(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 3152);
	f0.f64 = double(temp.f32);
loc_82CEBB44:
	// lhz r31,0(r8)
	r31.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// extsh r31,r31
	r31.s64 = r31.s16;
	// std r31,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r31.u64);
	// lfd f12,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r7)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82cebb44
	if (!cr0.eq) goto loc_82CEBB44;
	// b 0x82cebb90
	goto loc_82CEBB90;
loc_82CEBB78:
	// extsw r9,r25
	ctx.r9.s64 = r25.s32;
	// cmpd cr6,r11,r9
	cr6.compare<int64_t>(r11.s64, ctx.r9.s64, xer);
	// ble cr6,0x82cebb90
	if (!cr6.gt) goto loc_82CEBB90;
	// subf r9,r9,r11
	ctx.r9.s64 = r11.s64 - ctx.r9.s64;
	// rldicr r9,r9,32,31
	ctx.r9.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// add r5,r9,r5
	ctx.r5.u64 = ctx.r9.u64 + ctx.r5.u64;
loc_82CEBB90:
	// rldicr r9,r11,32,31
	ctx.r9.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF00000000;
	// rotlwi r8,r11,0
	ctx.r8.u64 = rotl32(r11.u32, 0);
	// subf r7,r9,r5
	ctx.r7.s64 = ctx.r5.s64 - ctx.r9.s64;
	// mullw r5,r8,r10
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// std r7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r7.u64);
	// stvewx128 v63,r0,r21
	PPC_STORE_U32((r21.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r21.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rlwinm r7,r5,1,0,30
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r10,r8,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r8.s64;
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// rotlwi r5,r9,1
	ctx.r5.u64 = rotl32(ctx.r9.u32, 1);
	// add r6,r10,r6
	ctx.r6.u64 = ctx.r10.u64 + ctx.r6.u64;
	// twllei r5,0
	// divwu r10,r6,r5
	ctx.r10.u32 = ctx.r6.u32 / ctx.r5.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r22)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// bge cr6,0x82cebbec
	if (!cr6.lt) goto loc_82CEBBEC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEBBEC:
	// lwz r10,0(r19)
	ctx.r10.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r9,0(r18)
	ctx.r9.u64 = PPC_LOAD_U32(r18.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cebc10
	if (!cr6.lt) goto loc_82CEBC10;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// b 0x82ca2c0c
	return;
loc_82CEBC10:
	// stw r9,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82CEB870) {
	__imp__sub_82CEB870(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEBC18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r23,r3,8
	r23.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r29,r3,13
	r29.s64 = ctx.r3.s64 + 13;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r20,r3,28
	r20.s64 = ctx.r3.s64 + 28;
	// mullw r6,r4,r5
	ctx.r6.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// addi r21,r3,24
	r21.s64 = ctx.r3.s64 + 24;
	// addi r28,r3,4
	r28.s64 = ctx.r3.s64 + 4;
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// addi r22,r3,20
	r22.s64 = ctx.r3.s64 + 20;
	// addi r31,r3,52
	r31.s64 = ctx.r3.s64 + 52;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// subf r25,r5,r4
	r25.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// dcbt r0,r9
	// li r11,1
	r11.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// addi r30,r1,-144
	r30.s64 = ctx.r1.s64 + -144;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r10,4
	ctx.r10.s64 = 4;
	// lfd f0,3248(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 3248);
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// li r5,40
	ctx.r5.s64 = 40;
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// addi r26,r3,36
	r26.s64 = ctx.r3.s64 + 36;
	// addi r6,r1,-136
	ctx.r6.s64 = ctx.r1.s64 + -136;
	// addi r27,r3,48
	r27.s64 = ctx.r3.s64 + 48;
	// li r4,0
	ctx.r4.s64 = 0;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v61,r0,r26
	temp.u32 = r0.u32 + r26.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r5,-136(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cebd90
	if (!cr6.lt) goto loc_82CEBD90;
loc_82CEBD28:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82cebd90
	if (!cr6.gt) goto loc_82CEBD90;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r0,r31
	temp.u32 = r0.u32 + r31.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v51,v53,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v50,v52,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vsubfp128 v49,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v49.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vcsxwfp128 v48,v51,15
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v47,v10,31
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v46,v63,v49,4
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v49.u8), 12));
	// vrlimi128 v50,v48,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vmulfp128 v45,v47,v46
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v46.f32)));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v50,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x82cebd28
	if (cr6.lt) goto loc_82CEBD28;
loc_82CEBD90:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r6,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = r11.s64 >> 32;
	// addi r24,r25,-1
	r24.s64 = r25.s64 + -1;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// extsw r30,r24
	r30.s64 = r24.s32;
	// cmpd cr6,r10,r30
	cr6.compare<int64_t>(ctx.r10.s64, r30.s64, xer);
	// bge cr6,0x82cebe54
	if (!cr6.lt) goto loc_82CEBE54;
loc_82CEBDAC:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82cebeb0
	if (cr6.eq) goto loc_82CEBEB0;
	// vspltisw128 v42,0
	simd::store_i32(v42.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r19,r10,1,0,30
	r19.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vsubfp128 v41,v42,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v41.f32, simd::sub_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r18,r10,1,0,30
	r18.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// lvlx128 v40,r19,r9
	temp.u32 = r19.u32 + ctx.r9.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vupkhsb128 v39,v40,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v40.s16)));
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vcuxwfp128 v38,v10,31
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// lvlx128 v37,r18,r9
	temp.u32 = r18.u32 + ctx.r9.u32;
	simd::store_shuffled(v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v36,v37,v96
	simd::store_i32(v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v37.s16)));
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// vcsxwfp128 v35,v39,15
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// xor r4,r6,r4
	ctx.r4.u64 = ctx.r6.u64 ^ ctx.r4.u64;
	// vcsxwfp128 v34,v36,15
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r4,r4,0,0,24
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
	// vsldoi128 v33,v63,v41,4
	simd::store_i8(v33.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v41.u8), 12));
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vmulfp128 v32,v38,v33
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v33.f32)));
	// vspltw128 v61,v35,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v35.u32), 3));
	// vrlimi128 v61,v34,8,0
	simd::store_f32(v61.f32, simd::blend_f32<8>(simd::load_f32(v61.f32), simd::permute_f32<228>(simd::load_f32(v34.f32))));
	// vsldoi128 v60,v32,v63,8
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v32.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v59,v61,v60
	simd::store_f32_aligned(v59.f32, simd::dp_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v59,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v59.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// beq cr6,0x82cebe48
	if (cr6.eq) goto loc_82CEBE48;
	// li r4,128
	ctx.r4.s64 = 128;
	// dcbt r4,r6
loc_82CEBE48:
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// cmpd cr6,r10,r30
	cr6.compare<int64_t>(ctx.r10.s64, r30.s64, xer);
	// blt cr6,0x82cebdac
	if (cr6.lt) goto loc_82CEBDAC;
loc_82CEBE54:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82cebeb0
	if (cr6.eq) goto loc_82CEBEB0;
	// cmpd cr6,r10,r30
	cr6.compare<int64_t>(ctx.r10.s64, r30.s64, xer);
	// bne cr6,0x82cebe98
	if (!cr6.eq) goto loc_82CEBE98;
	// rlwinm r7,r24,1,0,30
	ctx.r7.u64 = rotl64(r24.u32 | (r24.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lhzx r5,r7,r9
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r9.u32);
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// extsh r7,r5
	ctx.r7.s64 = ctx.r5.s16;
	// std r7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r7.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r31.u32 + 0, temp.u32);
	// b 0x82cebeb0
	goto loc_82CEBEB0;
loc_82CEBE98:
	// extsw r7,r25
	ctx.r7.s64 = r25.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82cebeb0
	if (!cr6.gt) goto loc_82CEBEB0;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CEBEB0:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r26
	PPC_STORE_U32((r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r26.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lbz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// rotlwi r7,r4,1
	ctx.r7.u64 = rotl32(ctx.r4.u32, 1);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// twllei r7,0
	// subf r10,r3,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r3.s64;
	// add r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
	// divwu r10,r6,r7
	ctx.r10.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r27)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r27.u32 + 0, temp.u32);
	// bge cr6,0x82cebf04
	if (!cr6.lt) goto loc_82CEBF04;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEBF04:
	// lwz r10,0(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + int32_t(0) );
	// stw r11,0(r23)
	PPC_STORE_U32(r23.u32 + 0, r11.u32);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r9,0(r21)
	ctx.r9.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cebf28
	if (!cr6.lt) goto loc_82CEBF28;
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CEBF28:
	// stw r9,0(r20)
	PPC_STORE_U32(r20.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CEBC18) {
	__imp__sub_82CEBC18(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEBF30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r21,r3,8
	r21.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r27,r3,13
	r27.s64 = ctx.r3.s64 + 13;
	// lwz r8,28(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r18,r3,28
	r18.s64 = ctx.r3.s64 + 28;
	// mullw r6,r4,r5
	ctx.r6.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r8,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r8.s64;
	// addi r19,r3,24
	r19.s64 = ctx.r3.s64 + 24;
	// addi r26,r3,4
	r26.s64 = ctx.r3.s64 + 4;
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// addi r20,r3,20
	r20.s64 = ctx.r3.s64 + 20;
	// addi r29,r3,52
	r29.s64 = ctx.r3.s64 + 52;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// subf r22,r5,r4
	r22.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// dcbt r0,r8
	// li r11,1
	r11.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r28,r1,-144
	r28.s64 = ctx.r1.s64 + -144;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r5,40
	ctx.r5.s64 = 40;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r10,4
	ctx.r10.s64 = 4;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// addi r24,r3,36
	r24.s64 = ctx.r3.s64 + 36;
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r6,r1,-136
	ctx.r6.s64 = ctx.r1.s64 + -136;
	// li r5,2
	ctx.r5.s64 = 2;
	// li r30,1024
	r30.s64 = 1024;
	// lvlx128 v61,r0,r24
	temp.u32 = r0.u32 + r24.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r25,r3,48
	r25.s64 = ctx.r3.s64 + 48;
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r4)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3248);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// li r31,0
	r31.s64 = 0;
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r10,r28
	temp.u32 = ctx.r10.u32 + r28.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r4,-136(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r4,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r4.u64);
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cec0d0
	if (!cr6.lt) goto loc_82CEC0D0;
loc_82CEC048:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82cec0d0
	if (!cr6.gt) goto loc_82CEC0D0;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r8,r5
	temp.u32 = ctx.r8.u32 + ctx.r5.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r29,r10
	temp.u32 = r29.u32 + ctx.r10.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v51,v53,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v50,v52,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// vsubfp128 v49,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v49.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vcsxwfp128 v48,v51,15
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v47,v10,31
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v46,v63,v49,4
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v49.u8), 12));
	// vrlimi128 v50,v48,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vmulfp128 v45,v47,v46
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v46.f32)));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v50,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r9,r30
	PPC_STORE_U32((ctx.r9.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((ctx.r9.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v42,r0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v41,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v40,v41,v96
	simd::store_i32(v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v39,v40,15
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v38,v42,0
	simd::store_i32(v38.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// vrlimi128 v38,v39,8,0
	simd::store_f32(v38.f32, simd::blend_f32<8>(simd::load_f32(v38.f32), simd::permute_f32<228>(simd::load_f32(v39.f32))));
	// vmsum3fp128 v37,v38,v44
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v37,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// blt cr6,0x82cec048
	if (cr6.lt) goto loc_82CEC048;
loc_82CEC0D0:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r6,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = r11.s64 >> 32;
	// addi r23,r22,-1
	r23.s64 = r22.s64 + -1;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// extsw r28,r23
	r28.s64 = r23.s32;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// bge cr6,0x82cec1c4
	if (!cr6.lt) goto loc_82CEC1C4;
loc_82CEC0EC:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82cec244
	if (cr6.eq) goto loc_82CEC244;
	// vspltisw128 v36,0
	simd::store_i32(v36.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// vsubfp128 v35,v36,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v35.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lvlx128 v34,r10,r5
	temp.u32 = ctx.r10.u32 + ctx.r5.u32;
	simd::store_shuffled(v34,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcuxwfp128 v33,v10,31
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v32,v34,v96
	simd::store_i32(v32.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v34.s16)));
	// lvlx128 v61,r6,r5
	temp.u32 = ctx.r6.u32 + ctx.r5.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v60,v61,v96
	simd::store_i32(v60.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v61.s16)));
	// vcsxwfp128 v59,v32,15
	simd::store_f32_aligned(v59.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vsldoi128 v58,v63,v35,4
	simd::store_i8(v58.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v35.u8), 12));
	// vcsxwfp128 v57,v60,15
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vmulfp128 v56,v33,v58
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(v58.f32)));
	// vspltw128 v55,v59,0
	simd::store_i32(v55.u32, simd::broadcast_lane_i32(simd::load_i32(v59.u32), 3));
	// vrlimi128 v55,v57,8,0
	simd::store_f32(v55.f32, simd::blend_f32<8>(simd::load_f32(v55.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vsldoi128 v54,v56,v63,8
	simd::store_i8(v54.u8, simd::shift_left_insert_bytes(simd::load_i8(v56.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v53,v55,v54
	simd::store_f32_aligned(v53.f32, simd::dp_f32(simd::load_f32_aligned(v55.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v53,r9,r30
	PPC_STORE_U32((ctx.r9.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v53.u32), 3 - ((ctx.r9.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v52,r0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v51,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v50,v51,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v51.s16)));
	// vupkhsb128 v49,v52,v96
	simd::store_i32(v49.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// vcsxwfp128 v48,v50,15
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// vcsxwfp128 v47,v49,15
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// xor r31,r6,r31
	r31.u64 = ctx.r6.u64 ^ r31.u64;
	// rlwinm r31,r31,0,0,24
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vspltw128 v46,v48,0
	simd::store_i32(v46.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vrlimi128 v46,v47,8,0
	simd::store_f32(v46.f32, simd::blend_f32<8>(simd::load_f32(v46.f32), simd::permute_f32<228>(simd::load_f32(v47.f32))));
	// vmsum3fp128 v45,v46,v54
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v45,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// beq cr6,0x82cec1b8
	if (cr6.eq) goto loc_82CEC1B8;
	// li r31,128
	r31.s64 = 128;
	// dcbt r31,r6
loc_82CEC1B8:
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// blt cr6,0x82cec0ec
	if (cr6.lt) goto loc_82CEC0EC;
loc_82CEC1C4:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82cec244
	if (cr6.eq) goto loc_82CEC244;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// bne cr6,0x82cec22c
	if (!cr6.eq) goto loc_82CEC22C;
	// rlwinm r7,r23,2,0,29
	ctx.r7.u64 = rotl64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// lhz r5,2(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// extsh r6,r5
	ctx.r6.s64 = ctx.r5.s16;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,4(r29)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r29.u32 + 4, temp.u32);
	// lhz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r7,r5
	ctx.r7.s64 = ctx.r5.s16;
	// std r7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r7.u64);
	// lfd f8,-136(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,0(r29)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// b 0x82cec244
	goto loc_82CEC244;
loc_82CEC22C:
	// extsw r7,r22
	ctx.r7.s64 = r22.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82cec244
	if (!cr6.gt) goto loc_82CEC244;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CEC244:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r24
	PPC_STORE_U32((r24.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r24.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r4,0(r27)
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r11,r3,r6
	r11.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// add r10,r11,r8
	ctx.r10.u64 = r11.u64 + ctx.r8.u64;
	// rotlwi r8,r4,1
	ctx.r8.u64 = rotl32(ctx.r4.u32, 1);
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// divwu r10,r10,r8
	ctx.r10.u32 = ctx.r10.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r25)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r25.u32 + 0, temp.u32);
	// bge cr6,0x82cec298
	if (!cr6.lt) goto loc_82CEC298;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEC298:
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r8,0(r19)
	ctx.r8.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// rlwinm r11,r9,30,2,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cec2bc
	if (!cr6.lt) goto loc_82CEC2BC;
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CEC2BC:
	// stw r8,0(r18)
	PPC_STORE_U32(r18.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CEBF30) {
	__imp__sub_82CEBF30(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEC2C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bbc
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r9,r3,28
	ctx.r9.s64 = ctx.r3.s64 + 28;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// stw r10,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, ctx.r10.u32);
	// addi r8,r3,24
	ctx.r8.s64 = ctx.r3.s64 + 24;
	// stw r9,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r9.u32);
	// addi r24,r3,13
	r24.s64 = ctx.r3.s64 + 13;
	// stw r8,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r8.u32);
	// addi r23,r3,4
	r23.s64 = ctx.r3.s64 + 4;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// addi r10,r3,20
	ctx.r10.s64 = ctx.r3.s64 + 20;
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// mullw r5,r6,r7
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// stw r10,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r10.u32);
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r6,r4,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r4.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r6,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r6.u32);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// subf r20,r7,r31
	r20.s64 = r31.s64 - ctx.r7.s64;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r11
	// li r4,1
	ctx.r4.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r9,r1,-168
	ctx.r9.s64 = ctx.r1.s64 + -168;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r8,r4,32,63
	ctx.r8.u64 = rotl64(ctx.r4.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r27,r1,-168
	r27.s64 = ctx.r1.s64 + -168;
	// std r8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r8.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r8,4
	ctx.r8.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// li r7,40
	ctx.r7.s64 = 40;
	// addi r21,r3,36
	r21.s64 = ctx.r3.s64 + 36;
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// li r28,3072
	r28.s64 = 3072;
	// li r29,2048
	r29.s64 = 2048;
	// lvlx128 v60,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r7,r1,-144
	ctx.r7.s64 = ctx.r1.s64 + -144;
	// lvlx128 v61,r0,r21
	temp.u32 = r0.u32 + r21.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r30,1024
	r30.s64 = 1024;
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r4)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3248);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f8.u64);
	// ld r9,-144(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r9,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r9.u64);
	// lvlx128 v55,r8,r27
	temp.u32 = ctx.r8.u32 + r27.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// stfd f7,-144(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f7.u64);
	// ld r31,-144(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// std r31,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r31.u64);
	// li r4,2
	ctx.r4.s64 = 2;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r8,r7
	temp.u32 = ctx.r8.u32 + ctx.r7.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fdiv f13,f0,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// addi r22,r3,48
	r22.s64 = ctx.r3.s64 + 48;
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// li r27,0
	r27.s64 = 0;
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// cmpdi cr6,r9,0
	cr6.compare<int64_t>(ctx.r9.s64, 0, xer);
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cec4cc
	if (!cr6.lt) goto loc_82CEC4CC;
	// li r25,6
	r25.s64 = 6;
	// li r26,12
	r26.s64 = 12;
	// li r7,8
	ctx.r7.s64 = 8;
loc_82CEC404:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82cec4cc
	if (!cr6.gt) goto loc_82CEC4CC;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r11,r25
	temp.u32 = r11.u32 + r25.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v52,r5,r26
	temp.u32 = ctx.r5.u32 + r26.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v51,v53,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v50,v52,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// vsubfp128 v49,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v49.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// cmpdi cr6,r9,0
	cr6.compare<int64_t>(ctx.r9.s64, 0, xer);
	// vcsxwfp128 v48,v51,15
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v47,v10,31
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v46,v63,v49,4
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v49.u8), 12));
	// vrlimi128 v50,v48,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vmulfp128 v45,v47,v46
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v46.f32)));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v50,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v42,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v41,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v40,v41,v96
	simd::store_i32(v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v39,v40,15
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v38,v42,0
	simd::store_i32(v38.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// vrlimi128 v38,v39,8,0
	simd::store_f32(v38.f32, simd::blend_f32<8>(simd::load_f32(v38.f32), simd::permute_f32<228>(simd::load_f32(v39.f32))));
	// vmsum3fp128 v37,v38,v44
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v37,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v36,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v35,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v34,v35,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v35.s16)));
	// vcsxwfp128 v33,v34,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v32,v36,0
	simd::store_i32(v32.u32, simd::broadcast_lane_i32(simd::load_i32(v36.u32), 3));
	// vrlimi128 v32,v33,8,0
	simd::store_f32(v32.f32, simd::blend_f32<8>(simd::load_f32(v32.f32), simd::permute_f32<228>(simd::load_f32(v33.f32))));
	// vmsum3fp128 v61,v32,v44
	simd::store_f32_aligned(v61.f32, simd::dp_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v61,r10,r30
	PPC_STORE_U32((ctx.r10.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v61.u32), 3 - ((ctx.r10.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v60,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v59,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v58,v59,v96
	simd::store_i32(v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v57,v58,15
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v56,v60,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v44
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v55,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82cec404
	if (cr6.lt) goto loc_82CEC404;
loc_82CEC4CC:
	// sradi r8,r9,63
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 63;
	// sradi r7,r9,32
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s64 >> 32;
	// addi r25,r20,-1
	r25.s64 = r20.s64 + -1;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// extsw r26,r25
	r26.s64 = r25.s32;
	// cmpd cr6,r8,r26
	cr6.compare<int64_t>(ctx.r8.s64, r26.s64, xer);
	// bge cr6,0x82cec638
	if (!cr6.lt) goto loc_82CEC638;
loc_82CEC4E8:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82cec6f8
	if (cr6.eq) goto loc_82CEC6F8;
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// addi r19,r8,3
	r19.s64 = ctx.r8.s64 + 3;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r19,r19,1,0,30
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// addi r18,r7,3
	r18.s64 = ctx.r7.s64 + 3;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r17,r8,2
	r17.s64 = ctx.r8.s64 + 2;
	// rlwinm r18,r18,1,0,30
	r18.u64 = rotl64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// vcuxwfp128 v52,v10,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r17,r17,1,0,30
	r17.u64 = rotl64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v51,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r19,r7,2
	r19.s64 = ctx.r7.s64 + 2;
	// vupkhsb128 v50,v51,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v51.s16)));
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r19,r19,1,0,30
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v49,r18,r11
	temp.u32 = r18.u32 + r11.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v47,v50,15
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// add r9,r31,r9
	ctx.r9.u64 = r31.u64 + ctx.r9.u64;
	// vsldoi128 v46,v63,v53,4
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// vcsxwfp128 v45,v48,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vmulfp128 v44,v52,v46
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v46.f32)));
	// vspltw128 v43,v47,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v47.u32), 3));
	// vrlimi128 v43,v45,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v45.f32))));
	// vsldoi128 v42,v44,v63,8
	simd::store_i8(v42.u8, simd::shift_left_insert_bytes(simd::load_i8(v44.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v41,v43,v42
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v41,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v38,r17,r11
	temp.u32 = r17.u32 + r11.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v36,v38,v96
	simd::store_i32(v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// lvlx128 v40,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v39,v40,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v40.s16)));
	// vcsxwfp128 v35,v36,15
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v37,v39,15
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v34,v35,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v35.u32), 3));
	// vrlimi128 v34,v37,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v37.f32))));
	// vmsum3fp128 v33,v34,v42
	simd::store_f32_aligned(v33.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v33,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v33.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v32,r7,r4
	temp.u32 = ctx.r7.u32 + ctx.r4.u32;
	simd::store_shuffled(v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v60,r8,r4
	temp.u32 = ctx.r8.u32 + ctx.r4.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v59,v60,v96
	simd::store_i32(v59.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v60.s16)));
	// vupkhsb128 v61,v32,v96
	simd::store_i32(v61.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v32.s16)));
	// vcsxwfp128 v58,v59,15
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v57,v61,15
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v61.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v56,v58,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v42
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v55,r10,r30
	PPC_STORE_U32((ctx.r10.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v54,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v52,v53,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vupkhsb128 v51,v54,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v54.s16)));
	// vcsxwfp128 v50,v52,15
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// sradi r8,r9,32
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 32;
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// vcsxwfp128 v49,v51,15
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// xor r27,r7,r27
	r27.u64 = ctx.r7.u64 ^ r27.u64;
	// rlwinm r27,r27,0,0,24
	r27.u64 = rotl64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// vspltw128 v48,v50,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// vrlimi128 v48,v49,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmsum3fp128 v47,v48,v42
	simd::store_f32_aligned(v47.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v47,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v47.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// beq cr6,0x82cec62c
	if (cr6.eq) goto loc_82CEC62C;
	// li r27,128
	r27.s64 = 128;
	// dcbt r27,r7
loc_82CEC62C:
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// cmpd cr6,r8,r26
	cr6.compare<int64_t>(ctx.r8.s64, r26.s64, xer);
	// blt cr6,0x82cec4e8
	if (cr6.lt) goto loc_82CEC4E8;
loc_82CEC638:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82cec6f8
	if (cr6.eq) goto loc_82CEC6F8;
	// cmpd cr6,r8,r26
	cr6.compare<int64_t>(ctx.r8.s64, r26.s64, xer);
	// bne cr6,0x82cec6e0
	if (!cr6.eq) goto loc_82CEC6E0;
	// rlwinm r7,r25,3,0,28
	ctx.r7.u64 = rotl64(r25.u32 | (r25.u64 << 32), 3) & 0xFFFFFFF8;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// lhz r4,6(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r4.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,12(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// lhz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// std r6,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r6.u64);
	// lfd f8,-144(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,8(r5)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lhz r4,2(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r4.u64);
	// lfd f4,-144(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,4(r5)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lhz r7,0(r7)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r4,r7
	ctx.r4.s64 = ctx.r7.s16;
	// std r4,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r4.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// b 0x82cec6f8
	goto loc_82CEC6F8;
loc_82CEC6E0:
	// extsw r7,r20
	ctx.r7.s64 = r20.s32;
	// cmpd cr6,r8,r7
	cr6.compare<int64_t>(ctx.r8.s64, ctx.r7.s64, xer);
	// ble cr6,0x82cec6f8
	if (!cr6.gt) goto loc_82CEC6F8;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
loc_82CEC6F8:
	// rldicr r7,r8,32,31
	ctx.r7.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r8,3,0,28
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r5,r7,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r7.s64;
	// std r5,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r5.u64);
	// stvewx128 v63,r0,r21
	PPC_STORE_U32((r21.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r21.u32) & 0xF) >> 2));
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lbz r4,0(r24)
	ctx.r4.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// rotlwi r8,r4,1
	ctx.r8.u64 = rotl32(ctx.r4.u32, 1);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// twllei r8,0
	// subf r9,r3,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r3.s64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// divwu r9,r9,r8
	ctx.r9.u32 = ctx.r9.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r22)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r22.u32 + 0, temp.u32);
	// bge cr6,0x82cec74c
	if (!cr6.lt) goto loc_82CEC74C;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEC74C:
	// lwz r9,-160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// lwz r8,-156(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-156) );
	// lwz r7,-152(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-152) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cec780
	if (!cr6.lt) goto loc_82CEC780;
	// lwz r10,-176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c0c
	return;
loc_82CEC780:
	// lwz r11,-176(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82CEC2C8) {
	__imp__sub_82CEC2C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEC790) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bb8
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r8,r3,28
	ctx.r8.s64 = ctx.r3.s64 + 28;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r5,r3,24
	ctx.r5.s64 = ctx.r3.s64 + 24;
	// stw r9,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r9.u32);
	// addi r7,r3,4
	ctx.r7.s64 = ctx.r3.s64 + 4;
	// stw r8,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r8.u32);
	// stw r5,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r5.u32);
	// addi r11,r3,13
	r11.s64 = ctx.r3.s64 + 13;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r9,r3,20
	ctx.r9.s64 = ctx.r3.s64 + 20;
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// mullw r4,r4,r6
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r6.s32);
	// lwz r5,0(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// stw r9,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r9.u32);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r7,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r7.u32);
	// rlwinm r11,r4,1,0,30
	r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r6,r30
	ctx.r4.s64 = r30.s64 - ctx.r6.s64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r31,r5
	ctx.r5.s64 = ctx.r5.s64 - r31.s64;
	// stw r4,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r4.u32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r5,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r5.u32);
	// addi r6,r3,52
	ctx.r6.s64 = ctx.r3.s64 + 52;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r11
	// li r9,1
	ctx.r9.s64 = 1;
	// lfs f0,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	f0.f64 = double(temp.f32);
	// addi r8,r3,48
	ctx.r8.s64 = ctx.r3.s64 + 48;
	// rldicr r7,r9,32,63
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// li r9,40
	ctx.r9.s64 = 40;
	// stw r8,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r8.u32);
	// std r7,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r7.u64);
	// lfd f13,-152(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// addi r4,r1,-192
	ctx.r4.s64 = ctx.r1.s64 + -192;
	// lfs f11,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// addi r8,r3,36
	ctx.r8.s64 = ctx.r3.s64 + 36;
	// fmul f10,f11,f12
	ctx.f10.f64 = ctx.f11.f64 * ctx.f12.f64;
	// lvlx128 v61,r3,r9
	temp.u32 = ctx.r3.u32 + ctx.r9.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r9,4
	ctx.r9.s64 = 4;
	// lvlx128 v63,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r31,r1,-192
	r31.s64 = ctx.r1.s64 + -192;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f0,f12
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = f0.f64 * ctx.f12.f64;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lvlx128 v60,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r1,-152
	ctx.r4.s64 = ctx.r1.s64 + -152;
	// vsubfp128 v59,v61,v60
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(v60.f32)));
	// li r24,0
	r24.s64 = 0;
	// vspltw128 v63,v60,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// li r25,5120
	r25.s64 = 5120;
	// li r26,4096
	r26.s64 = 4096;
	// lfd f0,3248(r7)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 3248);
	// li r27,3072
	r27.s64 = 3072;
	// fdiv f13,f0,f12
	ctx.f13.f64 = f0.f64 / ctx.f12.f64;
	// li r28,2048
	r28.s64 = 2048;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.f8.u64);
	// ld r8,-152(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// li r29,1024
	r29.s64 = 1024;
	// std r8,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r8.u64);
	// lvlx128 v55,r9,r31
	temp.u32 = ctx.r9.u32 + r31.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.f7.u64);
	// ld r30,-152(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// std r30,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, r30.u64);
	// li r31,2
	r31.s64 = 2;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r9,r4
	temp.u32 = ctx.r9.u32 + ctx.r4.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stw r24,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, r24.u32);
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cec9fc
	if (!cr6.lt) goto loc_82CEC9FC;
	// li r7,8
	ctx.r7.s64 = 8;
loc_82CEC8DC:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82cec9fc
	if (!cr6.gt) goto loc_82CEC9FC;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// li r4,10
	ctx.r4.s64 = 10;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// li r24,20
	r24.s64 = 20;
	// li r23,16
	r23.s64 = 16;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// li r22,12
	r22.s64 = 12;
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// li r21,6
	r21.s64 = 6;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// lvlx128 v52,r11,r4
	temp.u32 = r11.u32 + ctx.r4.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v51,v52,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// lvlx128 v50,r6,r24
	temp.u32 = ctx.r6.u32 + r24.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v49,v50,0
	simd::store_i32(v49.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vcuxwfp128 v48,v10,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// vcsxwfp128 v47,v51,15
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vsldoi128 v46,v63,v53,4
	simd::store_i8(v46.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// vmulfp128 v45,v48,v46
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v46.f32)));
	// vrlimi128 v49,v47,8,0
	simd::store_f32(v49.f32, simd::blend_f32<8>(simd::load_f32(v49.f32), simd::permute_f32<228>(simd::load_f32(v47.f32))));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v49,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r10,r25
	PPC_STORE_U32((ctx.r10.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((ctx.r10.u32 + r25.u32) & 0xF) >> 2));
	// lvlx128 v42,r6,r23
	temp.u32 = ctx.r6.u32 + r23.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v41,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v40,v41,v96
	simd::store_i32(v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v39,v40,15
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v38,v42,0
	simd::store_i32(v38.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// vrlimi128 v38,v39,8,0
	simd::store_f32(v38.f32, simd::blend_f32<8>(simd::load_f32(v38.f32), simd::permute_f32<228>(simd::load_f32(v39.f32))));
	// vmsum3fp128 v37,v38,v44
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v38.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v37,r10,r26
	PPC_STORE_U32((ctx.r10.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r10.u32 + r26.u32) & 0xF) >> 2));
	// lvlx128 v36,r6,r22
	temp.u32 = ctx.r6.u32 + r22.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v35,r11,r21
	temp.u32 = r11.u32 + r21.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v34,v35,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v35.s16)));
	// vcsxwfp128 v33,v34,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v32,v36,0
	simd::store_i32(v32.u32, simd::broadcast_lane_i32(simd::load_i32(v36.u32), 3));
	// vrlimi128 v32,v33,8,0
	simd::store_f32(v32.f32, simd::blend_f32<8>(simd::load_f32(v32.f32), simd::permute_f32<228>(simd::load_f32(v33.f32))));
	// vmsum3fp128 v61,v32,v44
	simd::store_f32_aligned(v61.f32, simd::dp_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v61,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v61.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v60,r6,r7
	temp.u32 = ctx.r6.u32 + ctx.r7.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v59,r11,r9
	temp.u32 = r11.u32 + ctx.r9.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v58,v59,v96
	simd::store_i32(v58.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v57,v58,15
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v56,v60,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v44
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v55,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v54,r6,r9
	temp.u32 = ctx.r6.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v52,v53,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vcsxwfp128 v51,v52,15
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v50,v54,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// vrlimi128 v50,v51,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v51.f32))));
	// vmsum3fp128 v49,v50,v44
	simd::store_f32_aligned(v49.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v49,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v49.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v48,r0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v47,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v46,v47,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v47.s16)));
	// vcsxwfp128 v45,v46,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v43,v48,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vrlimi128 v43,v45,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v45.f32))));
	// vmsum3fp128 v42,v43,v44
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v42,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// lwz r24,-192(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-192) );
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82cec8dc
	if (cr6.lt) goto loc_82CEC8DC;
loc_82CEC9FC:
	// lwz r9,-184(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-184) );
	// sradi r7,r8,63
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s64 >> 63;
	// sradi r4,r8,32
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFFFFFF) != 0);
	ctx.r4.s64 = ctx.r8.s64 >> 32;
	// addi r22,r9,-1
	r22.s64 = ctx.r9.s64 + -1;
	// subf r9,r7,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r7.s64;
	// extsw r23,r22
	r23.s64 = r22.s32;
	// cmpd cr6,r9,r23
	cr6.compare<int64_t>(ctx.r9.s64, r23.s64, xer);
	// bge cr6,0x82cecbf8
	if (!cr6.lt) goto loc_82CECBF8;
loc_82CECA1C:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82cecd04
	if (cr6.eq) goto loc_82CECD04;
	// extsw r4,r9
	ctx.r4.s64 = ctx.r9.s32;
	// vspltisw128 v41,0
	simd::store_i32(v41.u32, simd::set1_i32(int32_t(0x0)));
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// addi r7,r4,1
	ctx.r7.s64 = ctx.r4.s64 + 1;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// rlwinm r4,r7,1,0,30
	ctx.r4.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vsubfp128 v40,v41,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v40.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r4,r9,5
	ctx.r4.s64 = ctx.r9.s64 + 5;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vcuxwfp128 v39,v10,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// addi r21,r7,5
	r21.s64 = ctx.r7.s64 + 5;
	// addi r20,r7,4
	r20.s64 = ctx.r7.s64 + 4;
	// rlwinm r21,r21,1,0,30
	r21.u64 = rotl64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r20,r20,1,0,30
	r20.u64 = rotl64(r20.u32 | (r20.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v38,r4,r11
	temp.u32 = ctx.r4.u32 + r11.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r9,4
	ctx.r4.s64 = ctx.r9.s64 + 4;
	// vupkhsb128 v37,v38,v96
	simd::store_i32(v37.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// addi r19,r9,3
	r19.s64 = ctx.r9.s64 + 3;
	// vsldoi128 v36,v63,v40,4
	simd::store_i8(v36.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v40.u8), 12));
	// rlwinm r18,r4,1,0,30
	r18.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v35,r21,r11
	temp.u32 = r21.u32 + r11.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// rlwinm r21,r19,1,0,30
	r21.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// vupkhsb128 v34,v35,v96
	simd::store_i32(v34.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v35.s16)));
	// addi r4,r7,3
	ctx.r4.s64 = ctx.r7.s64 + 3;
	// vcsxwfp128 v33,v37,15
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// addi r19,r9,2
	r19.s64 = ctx.r9.s64 + 2;
	// rlwinm r17,r4,1,0,30
	r17.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// vmulfp128 v32,v39,v36
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v36.f32)));
	// addi r4,r7,2
	ctx.r4.s64 = ctx.r7.s64 + 2;
	// vcsxwfp128 v61,v34,15
	simd::store_f32_aligned(v61.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// rlwinm r19,r19,1,0,30
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r4,1,0,30
	r16.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r9,1,0,30
	ctx.r4.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + r11.u64;
	// add r7,r4,r11
	ctx.r7.u64 = ctx.r4.u64 + r11.u64;
	// vspltw128 v60,v33,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v33.u32), 3));
	// vsldoi128 v59,v32,v63,8
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v32.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vrlimi128 v60,v61,8,0
	simd::store_f32(v60.f32, simd::blend_f32<8>(simd::load_f32(v60.f32), simd::permute_f32<228>(simd::load_f32(v61.f32))));
	// vmsum3fp128 v58,v60,v59
	simd::store_f32_aligned(v58.f32, simd::dp_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v58,r10,r25
	PPC_STORE_U32((ctx.r10.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v58.u32), 3 - ((ctx.r10.u32 + r25.u32) & 0xF) >> 2));
	// lvlx128 v57,r20,r11
	temp.u32 = r20.u32 + r11.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v56,r18,r11
	temp.u32 = r18.u32 + r11.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v55,v56,v96
	simd::store_i32(v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v56.s16)));
	// vupkhsb128 v54,v57,v96
	simd::store_i32(v54.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v57.s16)));
	// vcsxwfp128 v53,v55,15
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vcsxwfp128 v52,v54,15
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v54.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// vmsum3fp128 v50,v51,v59
	simd::store_f32_aligned(v50.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v50,r10,r26
	PPC_STORE_U32((ctx.r10.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v50.u32), 3 - ((ctx.r10.u32 + r26.u32) & 0xF) >> 2));
	// lvlx128 v49,r21,r11
	temp.u32 = r21.u32 + r11.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// lvlx128 v47,r17,r11
	temp.u32 = r17.u32 + r11.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v45,v48,15
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v46,v47,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v47.s16)));
	// vcsxwfp128 v44,v46,15
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v43,v45,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v45.u32), 3));
	// vrlimi128 v43,v44,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v42,v43,v59
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v42,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v39,r16,r11
	temp.u32 = r16.u32 + r11.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v41,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v40,v41,v96
	simd::store_i32(v40.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v38,v40,15
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v40.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v37,v39,v96
	simd::store_i32(v37.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v39.s16)));
	// vcsxwfp128 v36,v37,15
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v35,v38,0
	simd::store_i32(v35.u32, simd::broadcast_lane_i32(simd::load_i32(v38.u32), 3));
	// vrlimi128 v35,v36,8,0
	simd::store_f32(v35.f32, simd::blend_f32<8>(simd::load_f32(v35.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vmsum3fp128 v34,v35,v59
	simd::store_f32_aligned(v34.f32, simd::dp_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v34,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v34.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v33,r7,r31
	temp.u32 = ctx.r7.u32 + r31.u32;
	simd::store_shuffled(v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v32,v33,v96
	simd::store_i32(v32.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v33.s16)));
	// lvlx128 v61,r9,r31
	temp.u32 = ctx.r9.u32 + r31.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v58,v32,15
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vupkhsb128 v60,v61,v96
	simd::store_i32(v60.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v61.s16)));
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vcsxwfp128 v57,v60,15
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// vspltw128 v56,v58,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v59
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v55,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// sradi r9,r8,32
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r8.s64 >> 32;
	// lvlx128 v53,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v52,v53,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vupkhsb128 v51,v54,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v54.s16)));
	// vcsxwfp128 v50,v52,15
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// extsw r4,r9
	ctx.r4.s64 = ctx.r9.s32;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v49,v51,15
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x38000000))));
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// xor r4,r7,r24
	ctx.r4.u64 = ctx.r7.u64 ^ r24.u64;
	// vspltw128 v48,v50,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// rlwinm r4,r4,0,0,24
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vrlimi128 v48,v49,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmsum3fp128 v47,v48,v59
	simd::store_f32_aligned(v47.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v47,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v47.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// beq cr6,0x82cecbec
	if (cr6.eq) goto loc_82CECBEC;
	// li r4,128
	ctx.r4.s64 = 128;
	// dcbt r4,r7
loc_82CECBEC:
	// mr r24,r7
	r24.u64 = ctx.r7.u64;
	// cmpd cr6,r9,r23
	cr6.compare<int64_t>(ctx.r9.s64, r23.s64, xer);
	// blt cr6,0x82ceca1c
	if (cr6.lt) goto loc_82CECA1C;
loc_82CECBF8:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82cecd04
	if (cr6.eq) goto loc_82CECD04;
	// cmpd cr6,r9,r23
	cr6.compare<int64_t>(ctx.r9.s64, r23.s64, xer);
	// bne cr6,0x82cecce8
	if (!cr6.eq) goto loc_82CECCE8;
	// rlwinm r7,r22,1,0,30
	ctx.r7.u64 = rotl64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// add r4,r22,r7
	ctx.r4.u64 = r22.u64 + ctx.r7.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// rlwinm r7,r4,2,0,29
	ctx.r7.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// lfs f0,3152(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 3152);
	f0.f64 = double(temp.f32);
	// lhz r5,10(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 10);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r5,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r5.u64);
	// lfd f12,-152(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,20(r6)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r6.u32 + 20, temp.u32);
	// lhz r4,8(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 8);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f8,-152(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,16(r6)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r6.u32 + 16, temp.u32);
	// lhz r5,6(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 6);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r5,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r5.u64);
	// lfd f4,-152(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,12(r6)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r6.u32 + 12, temp.u32);
	// lhz r4,4(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f12,-152(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,8(r6)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r6.u32 + 8, temp.u32);
	// lhz r5,2(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// extsh r5,r5
	ctx.r5.s64 = ctx.r5.s16;
	// std r5,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r5.u64);
	// lfd f8,-152(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,4(r6)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r6.u32 + 4, temp.u32);
	// lhz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// std r5,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r5.u64);
	// lfd f4,-152(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,0(r6)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r6.u32 + 0, temp.u32);
	// b 0x82cecd04
	goto loc_82CECD04;
loc_82CECCE8:
	// lwz r7,-184(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-184) );
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// cmpd cr6,r9,r7
	cr6.compare<int64_t>(ctx.r9.s64, ctx.r7.s64, xer);
	// ble cr6,0x82cecd04
	if (!cr6.gt) goto loc_82CECD04;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
loc_82CECD04:
	// rldicr r6,r9,32,31
	ctx.r6.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// lwz r5,-176(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// rotlwi r7,r9,0
	ctx.r7.u64 = rotl32(ctx.r9.u32, 0);
	// lwz r4,-172(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-172) );
	// subf r6,r6,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r6.s64;
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// std r6,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r6.u64);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// stvewx128 v63,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lfd f0,-192(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r8,r9,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r9.s64;
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// rotlwi r7,r6,1
	ctx.r7.u64 = rotl32(ctx.r6.u32, 1);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
	// addi r9,r3,13
	ctx.r9.s64 = ctx.r3.s64 + 13;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// divwu r11,r8,r7
	r11.u32 = ctx.r8.u32 / ctx.r7.u32;
	// twllei r7,0
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// blt cr6,0x82cecd74
	if (cr6.lt) goto loc_82CECD74;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CECD74:
	// lwz r9,-168(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-168) );
	// lwz r8,-164(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-164) );
	// lwz r7,-160(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cecda8
	if (!cr6.lt) goto loc_82CECDA8;
	// lwz r10,-180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-180) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c08
	return;
loc_82CECDA8:
	// lwz r11,-180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-180) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c08
	return;
}

PPC_WEAK_FUNC(sub_82CEC790) {
	__imp__sub_82CEC790(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CECDB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// lwz r4,8(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r26,r3,13
	r26.s64 = ctx.r3.s64 + 13;
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r21,r3,8
	r21.s64 = ctx.r3.s64 + 8;
	// lbz r11,13(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r18,r3,28
	r18.s64 = ctx.r3.s64 + 28;
	// lwz r5,24(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r4,r11
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(r11.s32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r5,r6,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r6.s64;
	// subf r27,r4,r31
	r27.s64 = r31.s64 - ctx.r4.s64;
	// addi r19,r3,24
	r19.s64 = ctx.r3.s64 + 24;
	// stw r5,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r5.u32);
	// addi r25,r3,4
	r25.s64 = ctx.r3.s64 + 4;
	// addi r20,r3,20
	r20.s64 = ctx.r3.s64 + 20;
	// addi r22,r3,52
	r22.s64 = ctx.r3.s64 + 52;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r4,r7,r8
	ctx.r4.u64 = ctx.r7.u64 + ctx.r8.u64;
	// dcbt r0,r6
	// li r10,1
	ctx.r10.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r7,40
	ctx.r7.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r8,r10,32,63
	ctx.r8.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// std r8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r8.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// addi r29,r1,-144
	r29.s64 = ctx.r1.s64 + -144;
	// lvlx128 v60,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fmul f9,f13,f11
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r23,r3,36
	r23.s64 = ctx.r3.s64 + 36;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// addi r8,r1,-136
	ctx.r8.s64 = ctx.r1.s64 + -136;
	// addi r24,r3,48
	r24.s64 = ctx.r3.s64 + 48;
	// li r31,0
	r31.s64 = 0;
	// lvlx128 v61,r0,r23
	temp.u32 = r0.u32 + r23.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsubfp128 v59,v60,v61
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r9)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + 3248);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// fdiv f12,f0,f11
	ctx.f12.f64 = f0.f64 / ctx.f11.f64;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r7,-136(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r7.u64);
	// lvlx128 v55,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// stfd f7,-136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r30,-136(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// std r30,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r30.u64);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cecf60
	if (!cr6.lt) goto loc_82CECF60;
loc_82CECEC4:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82cecf60
	if (!cr6.gt) goto loc_82CECF60;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82cecf44
	if (cr6.eq) goto loc_82CECF44;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// vspltisb v11,1
	simd::store_i8(ctx.v11.u8, simd::set1_i8(int8_t(0x1)));
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// vspltisb v13,7
	simd::store_i8(ctx.v13.u8, simd::set1_i8(int8_t(0x7)));
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v10,v0,v11
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v11.u8)));
	// vslb v13,v13,v13
	simd::store_shifted_i8(ctx.v13, ctx.v13, ctx.v13);
	// vcuxwfp128 v52,v10,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v51,v63,v53,4
	simd::store_i8(v51.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// vmulfp128 v50,v52,v51
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v51.f32)));
	// vsldoi128 v62,v50,v63,8
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v50.u8), simd::load_i8(v63.u8), 8));
loc_82CECF08:
	// lvlx v11,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// vaddubm v10,v11,v13
	simd::store_u8(ctx.v10.u8, simd::add_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v13.u8)));
	// lvlx128 v49,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v48,v49,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v49.u32), 3));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// vupkhsb128 v47,v10,v0
	simd::store_i16(v47.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v10.s8)));
	// vupkhsb128 v46,v47,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v47.s16)));
	// vcsxwfp128 v45,v46,7
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v48,v45,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v45.f32))));
	// vmsum3fp128 v44,v48,v62
	simd::store_f32_aligned(v44.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v62.f32), 0xEF));
	// stvewx128 v44,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v44.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,1024
	ctx.r8.s64 = ctx.r8.s64 + 1024;
	// blt cr6,0x82cecf08
	if (cr6.lt) goto loc_82CECF08;
loc_82CECF44:
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// vadduwm v0,v0,v12
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// blt cr6,0x82cecec4
	if (cr6.lt) goto loc_82CECEC4;
loc_82CECF60:
	// sradi r10,r7,63
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r7.s64 >> 63;
	// sradi r9,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s64 >> 32;
	// addi r28,r27,-1
	r28.s64 = r27.s64 + -1;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// extsw r29,r28
	r29.s64 = r28.s32;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// bge cr6,0x82ced058
	if (!cr6.lt) goto loc_82CED058;
loc_82CECF7C:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82ced0e0
	if (cr6.eq) goto loc_82CED0E0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ced010
	if (cr6.eq) goto loc_82CED010;
	// vspltisw128 v43,0
	simd::store_i32(v43.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// vspltisb v11,1
	simd::store_i8(ctx.v11.u8, simd::set1_i8(int8_t(0x1)));
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// vspltisb v13,7
	simd::store_i8(ctx.v13.u8, simd::set1_i8(int8_t(0x7)));
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// vsubfp128 v42,v43,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v42.f32, simd::sub_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v10,v0,v11
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v11.u8)));
	// vslb v13,v13,v13
	simd::store_shifted_i8(ctx.v13, ctx.v13, ctx.v13);
	// vcuxwfp128 v41,v10,31
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// vsldoi128 v40,v63,v42,4
	simd::store_i8(v40.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v42.u8), 12));
	// vmulfp128 v39,v41,v40
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v40.f32)));
	// vsldoi128 v62,v39,v63,8
	simd::store_i8(v62.u8, simd::shift_left_insert_bytes(simd::load_i8(v39.u8), simd::load_i8(v63.u8), 8));
loc_82CECFC8:
	// lvlx v11,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// vaddubm v10,v11,v13
	simd::store_u8(ctx.v10.u8, simd::add_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v13.u8)));
	// lvlx v9,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	simd::store_shuffled(ctx.v9,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v8,v9,v13
	simd::store_u8(ctx.v8.u8, simd::add_u8(simd::load_u8(ctx.v9.u8), simd::load_u8(ctx.v13.u8)));
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// vupkhsb128 v38,v10,v0
	simd::store_i16(v38.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v10.s8)));
	// vupkhsb128 v37,v8,v0
	simd::store_i16(v37.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v8.s8)));
	// vupkhsb128 v36,v38,v96
	simd::store_i32(v36.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// vupkhsb128 v35,v37,v96
	simd::store_i32(v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v37.s16)));
	// vcsxwfp128 v34,v36,7
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v36.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v33,v35,7
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v32,v34,0
	simd::store_i32(v32.u32, simd::broadcast_lane_i32(simd::load_i32(v34.u32), 3));
	// vrlimi128 v32,v33,8,0
	simd::store_f32(v32.f32, simd::blend_f32<8>(simd::load_f32(v32.f32), simd::permute_f32<228>(simd::load_f32(v33.f32))));
	// vmsum3fp128 v60,v32,v62
	simd::store_f32_aligned(v60.f32, simd::dp_f32(simd::load_f32_aligned(v32.f32), simd::load_f32_aligned(v62.f32), 0xEF));
	// stvewx128 v60,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v60.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,1024
	ctx.r8.s64 = ctx.r8.s64 + 1024;
	// bne 0x82cecfc8
	if (!cr0.eq) goto loc_82CECFC8;
loc_82CED010:
	// add r7,r30,r7
	ctx.r7.u64 = r30.u64 + ctx.r7.u64;
	// vadduwm v0,v0,v12
	simd::store_u32(ctx.v0.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v61
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// sradi r10,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r7.s64 >> 32;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// extsw r9,r10
	ctx.r9.s64 = ctx.r10.s32;
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// add r9,r9,r6
	ctx.r9.u64 = ctx.r9.u64 + ctx.r6.u64;
	// xor r8,r9,r31
	ctx.r8.u64 = ctx.r9.u64 ^ r31.u64;
	// rlwinm r8,r8,0,0,24
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82ced04c
	if (cr6.eq) goto loc_82CED04C;
	// li r8,128
	ctx.r8.s64 = 128;
	// dcbt r8,r9
loc_82CED04C:
	// mr r31,r9
	r31.u64 = ctx.r9.u64;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// blt cr6,0x82cecf7c
	if (cr6.lt) goto loc_82CECF7C;
loc_82CED058:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82ced0e0
	if (cr6.eq) goto loc_82CED0E0;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// bne cr6,0x82ced0c8
	if (!cr6.eq) goto loc_82CED0C8;
	// mullw r9,r28,r11
	ctx.r9.s64 = int64_t(r28.s32) * int64_t(r11.s32);
	// add r5,r9,r6
	ctx.r5.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ced0e0
	if (cr6.eq) goto loc_82CED0E0;
	// lis r31,-32255
	r31.s64 = -2113863680;
	// lis r30,-32255
	r30.s64 = -2113863680;
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// lfs f13,26480(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 26480);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,26484(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 26484);
	f0.f64 = double(temp.f32);
loc_82CED094:
	// lbzx r31,r9,r5
	r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r5.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// std r31,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r31.u64);
	// lfd f11,-136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fsubs f8,f9,f0
	ctx.f8.f64 = static_cast<float>(ctx.f9.f64 - f0.f64);
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f7,0(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x82ced094
	if (cr6.lt) goto loc_82CED094;
	// b 0x82ced0e0
	goto loc_82CED0E0;
loc_82CED0C8:
	// extsw r9,r27
	ctx.r9.s64 = r27.s32;
	// cmpd cr6,r10,r9
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r9.s64, xer);
	// ble cr6,0x82ced0e0
	if (!cr6.gt) goto loc_82CED0E0;
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// rldicr r9,r9,32,31
	ctx.r9.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
loc_82CED0E0:
	// rldicr r9,r10,32,31
	ctx.r9.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rotlwi r8,r10,0
	ctx.r8.u64 = rotl32(ctx.r10.u32, 0);
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// mullw r5,r8,r11
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// std r7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r7.u64);
	// stvewx128 v63,r0,r23
	PPC_STORE_U32((r23.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r23.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r10,r8,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r8.s64;
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// add r7,r10,r6
	ctx.r7.u64 = ctx.r10.u64 + ctx.r6.u64;
	// twllei r9,0
	// divwu r10,r7,r9
	ctx.r10.u32 = ctx.r7.u32 / ctx.r9.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f12,f13,f12
	ctx.f12.f64 = ctx.f13.f64 * ctx.f12.f64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stfs f11,0(r24)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r24.u32 + 0, temp.u32);
	// bge cr6,0x82ced134
	if (!cr6.lt) goto loc_82CED134;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CED134:
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r9,0(r19)
	ctx.r9.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82ced158
	if (!cr6.lt) goto loc_82CED158;
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CED158:
	// stw r9,0(r18)
	PPC_STORE_U32(r18.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CECDB8) {
	__imp__sub_82CECDB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CED160) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc4
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r22,r3,8
	r22.s64 = ctx.r3.s64 + 8;
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r28,r3,13
	r28.s64 = ctx.r3.s64 + 13;
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// addi r19,r3,28
	r19.s64 = ctx.r3.s64 + 28;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r7,r4,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r4.s64;
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stw r7,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, ctx.r7.u32);
	// addi r20,r3,24
	r20.s64 = ctx.r3.s64 + 24;
	// addi r27,r3,4
	r27.s64 = ctx.r3.s64 + 4;
	// addi r21,r3,20
	r21.s64 = ctx.r3.s64 + 20;
	// addi r30,r3,52
	r30.s64 = ctx.r3.s64 + 52;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// subf r24,r5,r31
	r24.s64 = r31.s64 - ctx.r5.s64;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// dcbt r0,r9
	// li r6,1
	ctx.r6.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r11,40
	r11.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r4,r6,32,63
	ctx.r4.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r5,r1,-128
	ctx.r5.s64 = ctx.r1.s64 + -128;
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f12,-120(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// addi r29,r1,-128
	r29.s64 = ctx.r1.s64 + -128;
	// lvlx128 v60,r3,r11
	temp.u32 = ctx.r3.u32 + r11.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fmul f9,f13,f11
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r25,r3,36
	r25.s64 = ctx.r3.s64 + 36;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// addi r31,r1,-120
	r31.s64 = ctx.r1.s64 + -120;
	// addi r26,r3,48
	r26.s64 = ctx.r3.s64 + 48;
	// li r4,0
	ctx.r4.s64 = 0;
	// lvlx128 v61,r0,r25
	temp.u32 = r0.u32 + r25.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsubfp128 v59,v60,v61
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r6)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r6.u32 + 3248);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// fdiv f12,f0,f11
	ctx.f12.f64 = f0.f64 / ctx.f11.f64;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.f8.u64);
	// ld r11,-120(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r11,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, r11.u64);
	// lvlx128 v55,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// stfd f7,-120(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.f7.u64);
	// ld r5,-120(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// std r5,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r5.u64);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ced2e4
	if (!cr6.lt) goto loc_82CED2E4;
loc_82CED26C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82ced2e4
	if (!cr6.gt) goto loc_82CED2E4;
	// vspltisb v13,7
	simd::store_i8(ctx.v13.u8, simd::set1_i8(int8_t(0x7)));
	// lvlx v11,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v10,1
	simd::store_i8(ctx.v10.u8, simd::set1_i8(int8_t(0x1)));
	// vadduwm v9,v0,v12
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// vspltw128 v52,v53,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vslb v8,v13,v13
	simd::store_shifted_i8(ctx.v8, ctx.v13, ctx.v13);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vsubfp128 v51,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v51.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vsr v7,v0,v10
	simd::store_i8(ctx.v7.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v10.u8)));
	// vor v0,v9,v9
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v9.u8));
	// vaddubm v6,v11,v8
	simd::store_u8(ctx.v6.u8, simd::add_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v8.u8)));
	// vcuxwfp128 v50,v7,31
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v7.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v49,v6,v0
	simd::store_i16(v49.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v6.s8)));
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vsldoi128 v47,v63,v51,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// vcsxwfp128 v46,v48,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v45,v50,v47
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v47.f32)));
	// vrlimi128 v52,v46,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v46.f32))));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v52,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x82ced26c
	if (cr6.lt) goto loc_82CED26C;
loc_82CED2E4:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r6,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = r11.s64 >> 32;
	// addi r23,r24,-1
	r23.s64 = r24.s64 + -1;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// extsw r29,r23
	r29.s64 = r23.s32;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// bge cr6,0x82ced3bc
	if (!cr6.lt) goto loc_82CED3BC;
	// li r31,1
	r31.s64 = 1;
loc_82CED304:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ced41c
	if (cr6.eq) goto loc_82CED41C;
	// vspltisb v13,7
	simd::store_i8(ctx.v13.u8, simd::set1_i8(int8_t(0x7)));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// vspltisw128 v42,0
	simd::store_i32(v42.u32, simd::set1_i32(int32_t(0x0)));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// add r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 + ctx.r9.u64;
	// vspltisb v11,1
	simd::store_i8(ctx.v11.u8, simd::set1_i8(int8_t(0x1)));
	// vadduwm v10,v0,v12
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vslb v13,v13,v13
	simd::store_shifted_i8(ctx.v13, ctx.v13, ctx.v13);
	// lvlx v9,r10,r9
	temp.u32 = ctx.r10.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v9,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vsubfp128 v41,v42,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v41.f32, simd::sub_f32(simd::load_f32_aligned(v42.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v8,v0,v11
	simd::store_i8(ctx.v8.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v11.u8)));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// lvlx v7,r6,r31
	temp.u32 = ctx.r6.u32 + r31.u32;
	simd::store_shuffled(ctx.v7,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor v0,v10,v10
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v10.u8));
	// vaddubm v6,v9,v13
	simd::store_u8(ctx.v6.u8, simd::add_u8(simd::load_u8(ctx.v9.u8), simd::load_u8(ctx.v13.u8)));
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vaddubm v5,v7,v13
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v7.u8), simd::load_u8(ctx.v13.u8)));
	// vcuxwfp128 v40,v8,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v8.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// vupkhsb128 v39,v6,v0
	simd::store_i16(v39.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v6.s8)));
	// xor r4,r6,r4
	ctx.r4.u64 = ctx.r6.u64 ^ ctx.r4.u64;
	// vupkhsb128 v38,v5,v0
	simd::store_i16(v38.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// rlwinm r4,r4,0,0,24
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFF80;
	// vupkhsb128 v37,v39,v96
	simd::store_i32(v37.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v39.s16)));
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// vsldoi128 v36,v63,v41,4
	simd::store_i8(v36.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v41.u8), 12));
	// vupkhsb128 v35,v38,v96
	simd::store_i32(v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// vcsxwfp128 v34,v37,7
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v33,v40,v36
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v36.f32)));
	// vcsxwfp128 v32,v35,7
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v61,v34,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v34.u32), 3));
	// vsldoi128 v60,v33,v63,8
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v33.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vrlimi128 v61,v32,8,0
	simd::store_f32(v61.f32, simd::blend_f32<8>(simd::load_f32(v61.f32), simd::permute_f32<228>(simd::load_f32(v32.f32))));
	// vmsum3fp128 v59,v61,v60
	simd::store_f32_aligned(v59.f32, simd::dp_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v59,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v59.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// beq cr6,0x82ced3b0
	if (cr6.eq) goto loc_82CED3B0;
	// li r4,128
	ctx.r4.s64 = 128;
	// dcbt r4,r6
loc_82CED3B0:
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// blt cr6,0x82ced304
	if (cr6.lt) goto loc_82CED304;
loc_82CED3BC:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ced41c
	if (cr6.eq) goto loc_82CED41C;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// bne cr6,0x82ced404
	if (!cr6.eq) goto loc_82CED404;
	// lbzx r4,r23,r9
	ctx.r4.u64 = PPC_LOAD_U8(r23.u32 + ctx.r9.u32);
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f0,-120(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfs f0,26484(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 26484);
	f0.f64 = double(temp.f32);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f13,26480(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 26480);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f10,f11,f0
	ctx.f10.f64 = static_cast<float>(ctx.f11.f64 - f0.f64);
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfs f9,0(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// b 0x82ced41c
	goto loc_82CED41C;
loc_82CED404:
	// extsw r7,r24
	ctx.r7.s64 = r24.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ced41c
	if (!cr6.gt) goto loc_82CED41C;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CED41C:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rotlwi r6,r10,0
	ctx.r6.u64 = rotl32(ctx.r10.u32, 0);
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r5.u64);
	// stvewx128 v63,r0,r25
	PPC_STORE_U32((r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r25.u32) & 0xF) >> 2));
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r11,r4,r6
	r11.s64 = ctx.r6.s64 - ctx.r4.s64;
	// lbz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// lfd f0,-128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// fmul f12,f13,f12
	ctx.f12.f64 = ctx.f13.f64 * ctx.f12.f64;
	// divwu r10,r10,r3
	ctx.r10.u32 = ctx.r10.u32 / ctx.r3.u32;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// twllei r3,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stfs f11,0(r26)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r26.u32 + 0, temp.u32);
	// bge cr6,0x82ced46c
	if (!cr6.lt) goto loc_82CED46C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CED46C:
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82ced490
	if (!cr6.lt) goto loc_82CED490;
	// stw r11,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r11.u32);
	// b 0x82ca2c14
	return;
loc_82CED490:
	// stw r9,0(r19)
	PPC_STORE_U32(r19.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c14
	return;
}

PPC_WEAK_FUNC(sub_82CED160) {
	__imp__sub_82CED160(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CED498) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc0
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r21,r3,8
	r21.s64 = ctx.r3.s64 + 8;
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r27,r3,13
	r27.s64 = ctx.r3.s64 + 13;
	// lwz r8,24(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// addi r18,r3,28
	r18.s64 = ctx.r3.s64 + 28;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r7,r4,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r4.s64;
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// addi r19,r3,24
	r19.s64 = ctx.r3.s64 + 24;
	// addi r26,r3,4
	r26.s64 = ctx.r3.s64 + 4;
	// addi r20,r3,20
	r20.s64 = ctx.r3.s64 + 20;
	// addi r29,r3,52
	r29.s64 = ctx.r3.s64 + 52;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// subf r22,r5,r31
	r22.s64 = r31.s64 - ctx.r5.s64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// dcbt r0,r8
	// li r6,1
	ctx.r6.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r11,40
	r11.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r4,r6,32,63
	ctx.r4.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r5,r1,-144
	ctx.r5.s64 = ctx.r1.s64 + -144;
	// std r4,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r4.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// addi r23,r1,-144
	r23.s64 = ctx.r1.s64 + -144;
	// lvlx128 v60,r3,r11
	temp.u32 = ctx.r3.u32 + r11.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fmul f9,f13,f11
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r24,r3,36
	r24.s64 = ctx.r3.s64 + 36;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// addi r28,r1,-136
	r28.s64 = ctx.r1.s64 + -136;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r30,1024
	r30.s64 = 1024;
	// lvlx128 v61,r0,r24
	temp.u32 = r0.u32 + r24.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r25,r3,48
	r25.s64 = ctx.r3.s64 + 48;
	// vsubfp128 v59,v60,v61
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r6)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r6.u32 + 3248);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// fdiv f12,f0,f11
	ctx.f12.f64 = f0.f64 / ctx.f11.f64;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r10,r23
	temp.u32 = ctx.r10.u32 + r23.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// stfd f7,-136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r4,-136(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// li r31,0
	r31.s64 = 0;
	// std r4,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r4.u64);
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r28
	temp.u32 = ctx.r10.u32 + r28.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ced64c
	if (!cr6.lt) goto loc_82CED64C;
loc_82CED5AC:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82ced64c
	if (!cr6.gt) goto loc_82CED64C;
	// vspltisb v13,7
	simd::store_i8(ctx.v13.u8, simd::set1_i8(int8_t(0x7)));
	// lvlx v11,r8,r5
	temp.u32 = ctx.r8.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r29,r10
	temp.u32 = r29.u32 + ctx.r10.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v10,1
	simd::store_i8(ctx.v10.u8, simd::set1_i8(int8_t(0x1)));
	// vadduwm v9,v0,v12
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// vspltw128 v52,v53,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// vslb v13,v13,v13
	simd::store_shifted_i8(ctx.v13, ctx.v13, ctx.v13);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vsubfp128 v51,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v51.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vsr v8,v0,v10
	simd::store_i8(ctx.v8.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v10.u8)));
	// vor v0,v9,v9
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v9.u8));
	// vaddubm v7,v11,v13
	simd::store_u8(ctx.v7.u8, simd::add_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v13.u8)));
	// vcuxwfp128 v50,v8,31
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v8.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v49,v7,v0
	simd::store_i16(v49.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v7.s8)));
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vsldoi128 v47,v63,v51,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// vcsxwfp128 v46,v48,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v45,v50,v47
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v47.f32)));
	// vrlimi128 v52,v46,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v46.f32))));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v52,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r9,r30
	PPC_STORE_U32((ctx.r9.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((ctx.r9.u32 + r30.u32) & 0xF) >> 2));
	// lvlx128 v42,r0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v6,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v5,v6,v13
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v13.u8)));
	// vupkhsb128 v41,v5,v0
	simd::store_i16(v41.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// vspltw128 v40,v42,0
	simd::store_i32(v40.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// vupkhsb128 v39,v41,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v38,v39,7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v40,v38,8,0
	simd::store_f32(v40.f32, simd::blend_f32<8>(simd::load_f32(v40.f32), simd::permute_f32<228>(simd::load_f32(v38.f32))));
	// vmsum3fp128 v37,v40,v44
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v37,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// blt cr6,0x82ced5ac
	if (cr6.lt) goto loc_82CED5AC;
loc_82CED64C:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r6,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = r11.s64 >> 32;
	// addi r23,r22,-1
	r23.s64 = r22.s64 + -1;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// extsw r28,r23
	r28.s64 = r23.s32;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// bge cr6,0x82ced768
	if (!cr6.lt) goto loc_82CED768;
loc_82CED668:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ced7f0
	if (cr6.eq) goto loc_82CED7F0;
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vspltisb v13,7
	simd::store_i8(ctx.v13.u8, simd::set1_i8(int8_t(0x7)));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisw128 v36,0
	simd::store_i32(v36.u32, simd::set1_i32(int32_t(0x0)));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// vspltisb v11,1
	simd::store_i8(ctx.v11.u8, simd::set1_i8(int8_t(0x1)));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// vadduwm v10,v0,v12
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v12.u32)));
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// vslb v13,v13,v13
	simd::store_shifted_i8(ctx.v13, ctx.v13, ctx.v13);
	// vsubfp128 v35,v36,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v35.f32, simd::sub_f32(simd::load_f32_aligned(v36.f32), simd::load_f32_aligned(v63.f32)));
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// vsr v9,v0,v11
	simd::store_i8(ctx.v9.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v11.u8)));
	// vor v0,v10,v10
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v10.u8));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lvlx v8,r10,r5
	temp.u32 = ctx.r10.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v8,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v7,v8,v13
	simd::store_u8(ctx.v7.u8, simd::add_u8(simd::load_u8(ctx.v8.u8), simd::load_u8(ctx.v13.u8)));
	// vcuxwfp128 v34,v9,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v9.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// lvlx v6,r6,r5
	temp.u32 = ctx.r6.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v5,v6,v13
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v13.u8)));
	// vupkhsb128 v33,v7,v0
	simd::store_i16(v33.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v7.s8)));
	// vupkhsb128 v32,v5,v0
	simd::store_i16(v32.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// vupkhsb128 v61,v33,v96
	simd::store_i32(v61.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v33.s16)));
	// vsldoi128 v60,v63,v35,4
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v35.u8), 12));
	// vupkhsb128 v59,v32,v96
	simd::store_i32(v59.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v32.s16)));
	// vcsxwfp128 v58,v61,7
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v61.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v57,v34,v60
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v60.f32)));
	// vcsxwfp128 v56,v59,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v55,v58,0
	simd::store_i32(v55.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// vsldoi128 v54,v57,v63,8
	simd::store_i8(v54.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vrlimi128 v55,v56,8,0
	simd::store_f32(v55.f32, simd::blend_f32<8>(simd::load_f32(v55.f32), simd::permute_f32<228>(simd::load_f32(v56.f32))));
	// vmsum3fp128 v53,v55,v54
	simd::store_f32_aligned(v53.f32, simd::dp_f32(simd::load_f32_aligned(v55.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v53,r9,r30
	PPC_STORE_U32((ctx.r9.u32 + r30.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v53.u32), 3 - ((ctx.r9.u32 + r30.u32) & 0xF) >> 2));
	// lvlx v4,0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v4,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v3,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v3,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v2,v3,v13
	simd::store_u8(ctx.v2.u8, simd::add_u8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v13.u8)));
	// vaddubm v1,v4,v13
	simd::store_u8(ctx.v1.u8, simd::add_u8(simd::load_u8(ctx.v4.u8), simd::load_u8(ctx.v13.u8)));
	// vupkhsb128 v52,v2,v0
	simd::store_i16(v52.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v2.s8)));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vupkhsb128 v51,v1,v0
	simd::store_i16(v51.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v1.s8)));
	// vupkhsb128 v50,v52,v96
	simd::store_i32(v50.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v52.s16)));
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// xor r31,r6,r31
	r31.u64 = ctx.r6.u64 ^ r31.u64;
	// vupkhsb128 v49,v51,v96
	simd::store_i32(v49.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v51.s16)));
	// vcsxwfp128 v48,v50,7
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// rlwinm r31,r31,0,0,24
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vcsxwfp128 v47,v49,7
	simd::store_f32_aligned(v47.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v46,v48,0
	simd::store_i32(v46.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vrlimi128 v46,v47,8,0
	simd::store_f32(v46.f32, simd::blend_f32<8>(simd::load_f32(v46.f32), simd::permute_f32<228>(simd::load_f32(v47.f32))));
	// vmsum3fp128 v45,v46,v54
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v54.f32), 0xEF));
	// stvewx128 v45,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// beq cr6,0x82ced75c
	if (cr6.eq) goto loc_82CED75C;
	// li r31,128
	r31.s64 = 128;
	// dcbt r31,r6
loc_82CED75C:
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// blt cr6,0x82ced668
	if (cr6.lt) goto loc_82CED668;
loc_82CED768:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ced7f0
	if (cr6.eq) goto loc_82CED7F0;
	// cmpd cr6,r10,r28
	cr6.compare<int64_t>(ctx.r10.s64, r28.s64, xer);
	// bne cr6,0x82ced7d8
	if (!cr6.eq) goto loc_82CED7D8;
	// rlwinm r7,r23,1,0,30
	ctx.r7.u64 = rotl64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lis r5,-32255
	ctx.r5.s64 = -2113863680;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lfs f0,26484(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 26484);
	f0.f64 = double(temp.f32);
	// lbz r6,1(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// lfs f13,26480(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 26480);
	ctx.f13.f64 = double(temp.f32);
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f11,-136(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fsubs f8,f9,f0
	ctx.f8.f64 = static_cast<float>(ctx.f9.f64 - f0.f64);
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f7,4(r29)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r29.u32 + 4, temp.u32);
	// lbz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// std r4,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r4.u64);
	// lfd f6,-136(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f0
	ctx.f3.f64 = static_cast<float>(ctx.f4.f64 - f0.f64);
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f2,0(r29)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// b 0x82ced7f0
	goto loc_82CED7F0;
loc_82CED7D8:
	// extsw r7,r22
	ctx.r7.s64 = r22.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ced7f0
	if (!cr6.gt) goto loc_82CED7F0;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CED7F0:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r24
	PPC_STORE_U32((r24.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r24.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r4,0(r27)
	ctx.r4.u64 = PPC_LOAD_U8(r27.u32 + 0);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r11,r3,r6
	r11.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// add r10,r11,r8
	ctx.r10.u64 = r11.u64 + ctx.r8.u64;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// twllei r4,0
	// divwu r10,r10,r4
	ctx.r10.u32 = ctx.r10.u32 / ctx.r4.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f12,f13,f12
	ctx.f12.f64 = ctx.f13.f64 * ctx.f12.f64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stfs f11,0(r25)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r25.u32 + 0, temp.u32);
	// bge cr6,0x82ced840
	if (!cr6.lt) goto loc_82CED840;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CED840:
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// stw r11,0(r21)
	PPC_STORE_U32(r21.u32 + 0, r11.u32);
	// subf r9,r10,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r8,0(r19)
	ctx.r8.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// rlwinm r11,r9,30,2,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82ced864
	if (!cr6.lt) goto loc_82CED864;
	// stw r11,0(r18)
	PPC_STORE_U32(r18.u32 + 0, r11.u32);
	// b 0x82ca2c10
	return;
loc_82CED864:
	// stw r8,0(r18)
	PPC_STORE_U32(r18.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82CED498) {
	__imp__sub_82CED498(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CED870) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc4
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r9,r3,28
	ctx.r9.s64 = ctx.r3.s64 + 28;
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// addi r7,r3,24
	ctx.r7.s64 = ctx.r3.s64 + 24;
	// stw r10,-132(r1)
	PPC_STORE_U32(ctx.r1.u32 + -132, ctx.r10.u32);
	// addi r8,r3,20
	ctx.r8.s64 = ctx.r3.s64 + 20;
	// stw r9,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r9.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r22,r3,13
	r22.s64 = ctx.r3.s64 + 13;
	// lwz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// addi r21,r3,4
	r21.s64 = ctx.r3.s64 + 4;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// mullw r10,r4,r5
	ctx.r10.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// stw r8,-140(r1)
	PPC_STORE_U32(ctx.r1.u32 + -140, ctx.r8.u32);
	// stw r7,-136(r1)
	PPC_STORE_U32(ctx.r1.u32 + -136, ctx.r7.u32);
	// rlwinm r8,r6,2,0,29
	ctx.r8.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r19,r5,r31
	r19.s64 = r31.s64 - ctx.r5.s64;
	// subf r6,r6,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r6.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r19,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, r19.u32);
	// stw r6,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r6.u32);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// add r11,r8,r9
	r11.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r10
	// li r8,1
	ctx.r8.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r7,r8,32,63
	ctx.r7.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// li r8,40
	ctx.r8.s64 = 40;
	// std r7,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r7.u64);
	// lfd f12,-128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// addi r4,r1,-152
	ctx.r4.s64 = ctx.r1.s64 + -152;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v60,r3,r8
	temp.u32 = ctx.r3.u32 + ctx.r8.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r8,4
	ctx.r8.s64 = 4;
	// lvlx128 v63,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r28,r1,-152
	r28.s64 = ctx.r1.s64 + -152;
	// lvlx128 v62,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v62,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v61,v62,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v61.f32, simd::cvtepi32_f32(simd::load_i32(v62.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// vsubfp128 v59,v60,v63
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v63.f32)));
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// addi r4,r1,-128
	ctx.r4.s64 = ctx.r1.s64 + -128;
	// vspltw128 v63,v63,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v63.u32), 3));
	// li r30,3
	r30.s64 = 3;
	// li r26,3072
	r26.s64 = 3072;
	// li r31,2
	r31.s64 = 2;
	// lfd f0,3248(r7)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 3248);
	// li r27,2048
	r27.s64 = 2048;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f8.u64);
	// ld r9,-128(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fdiv f12,f0,f11
	ctx.f12.f64 = f0.f64 / ctx.f11.f64;
	// std r9,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r9.u64);
	// lvlx128 v56,r8,r28
	temp.u32 = ctx.r8.u32 + r28.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v61.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f7.u64);
	// ld r29,-128(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// std r29,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, r29.u64);
	// li r28,1024
	r28.s64 = 1024;
	// vmulfp128 v55,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v55.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v57,r8,r4
	temp.u32 = ctx.r8.u32 + ctx.r4.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r4,1
	ctx.r4.s64 = 1;
	// vspltw128 v12,v57,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// addi r20,r3,48
	r20.s64 = ctx.r3.s64 + 48;
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// li r25,0
	r25.s64 = 0;
	// vspltw128 v62,v55,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// cmpdi cr6,r9,0
	cr6.compare<int64_t>(ctx.r9.s64, 0, xer);
	// bge cr6,0x82cedaa0
	if (!cr6.lt) goto loc_82CEDAA0;
	// li r24,12
	r24.s64 = 12;
	// li r7,8
	ctx.r7.s64 = 8;
loc_82CED9B0:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82cedaa0
	if (!cr6.gt) goto loc_82CEDAA0;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// lvlx v11,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v53,r5,r24
	temp.u32 = ctx.r5.u32 + r24.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v10,1
	simd::store_i8(ctx.v10.u8, simd::set1_i8(int8_t(0x1)));
	// vadduwm v9,v13,v12
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v12.u32)));
	// vspltw128 v52,v53,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// add r9,r29,r9
	ctx.r9.u64 = r29.u64 + ctx.r9.u64;
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// vsubfp128 v51,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v51.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// cmpdi cr6,r9,0
	cr6.compare<int64_t>(ctx.r9.s64, 0, xer);
	// vsr v8,v13,v10
	simd::store_i8(ctx.v8.u8, simd::vsr(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v10.u8)));
	// vor v13,v9,v9
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v9.u8));
	// vaddubm v7,v11,v0
	simd::store_u8(ctx.v7.u8, simd::add_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v0.u8)));
	// vcuxwfp128 v50,v8,31
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v8.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v49,v7,v0
	simd::store_i16(v49.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v7.s8)));
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vsldoi128 v47,v63,v51,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// vcsxwfp128 v46,v48,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v45,v50,v47
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v47.f32)));
	// vrlimi128 v52,v46,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v46.f32))));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v52,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r11,r26
	PPC_STORE_U32((r11.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((r11.u32 + r26.u32) & 0xF) >> 2));
	// lvlx128 v42,r5,r7
	temp.u32 = ctx.r5.u32 + ctx.r7.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v6,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v5,v6,v0
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v41,v5,v0
	simd::store_i16(v41.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// vspltw128 v40,v42,0
	simd::store_i32(v40.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// vupkhsb128 v39,v41,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v38,v39,7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v40,v38,8,0
	simd::store_f32(v40.f32, simd::blend_f32<8>(simd::load_f32(v40.f32), simd::permute_f32<228>(simd::load_f32(v38.f32))));
	// vmsum3fp128 v37,v40,v44
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v37,r11,r27
	PPC_STORE_U32((r11.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((r11.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v36,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v4,r10,r4
	temp.u32 = ctx.r10.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v4,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v3,v4,v0
	simd::store_u8(ctx.v3.u8, simd::add_u8(simd::load_u8(ctx.v4.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v35,v3,v0
	simd::store_i16(v35.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v3.s8)));
	// vspltw128 v34,v36,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v36.u32), 3));
	// vupkhsb128 v33,v35,v96
	simd::store_i32(v33.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v35.s16)));
	// vcsxwfp128 v32,v33,7
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v34,v32,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v32.f32))));
	// vmsum3fp128 v61,v34,v44
	simd::store_f32_aligned(v61.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v61,r11,r28
	PPC_STORE_U32((r11.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v61.u32), 3 - ((r11.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v60,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v2,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v2,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v1,v2,v0
	simd::store_u8(ctx.v1.u8, simd::add_u8(simd::load_u8(ctx.v2.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v59,v1,v0
	simd::store_i16(v59.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v1.s8)));
	// vspltw128 v58,v60,0
	simd::store_i32(v58.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vupkhsb128 v57,v59,v96
	simd::store_i32(v57.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v59.s16)));
	// vcsxwfp128 v56,v57,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v57.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v58,v56,8,0
	simd::store_f32(v58.f32, simd::blend_f32<8>(simd::load_f32(v58.f32), simd::permute_f32<228>(simd::load_f32(v56.f32))));
	// vmsum3fp128 v55,v58,v44
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v58.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v55,r0,r11
	PPC_STORE_U32((r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// blt cr6,0x82ced9b0
	if (cr6.lt) goto loc_82CED9B0;
loc_82CEDAA0:
	// sradi r8,r9,63
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 63;
	// sradi r7,r9,32
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r9.s64 >> 32;
	// addi r23,r19,-1
	r23.s64 = r19.s64 + -1;
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// extsw r24,r23
	r24.s64 = r23.s32;
	// cmpd cr6,r8,r24
	cr6.compare<int64_t>(ctx.r8.s64, r24.s64, xer);
	// bge cr6,0x82cedc30
	if (!cr6.lt) goto loc_82CEDC30;
loc_82CEDABC:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82cedcfc
	if (cr6.eq) goto loc_82CEDCFC;
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// vspltisb v11,1
	simd::store_i8(ctx.v11.u8, simd::set1_i8(int8_t(0x1)));
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vadduwm v10,v13,v12
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v12.u32)));
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// add r9,r29,r9
	ctx.r9.u64 = r29.u64 + ctx.r9.u64;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// vsr v9,v13,v11
	simd::store_i8(ctx.v9.u8, simd::vsr(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v11.u8)));
	// vor v13,v10,v10
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v10.u8));
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lvlx v8,r8,r30
	temp.u32 = ctx.r8.u32 + r30.u32;
	simd::store_shuffled(ctx.v8,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v7,v8,v0
	simd::store_u8(ctx.v7.u8, simd::add_u8(simd::load_u8(ctx.v8.u8), simd::load_u8(ctx.v0.u8)));
	// vcuxwfp128 v52,v9,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v9.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// lvlx v6,r7,r30
	temp.u32 = ctx.r7.u32 + r30.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v5,v6,v0
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v51,v7,v0
	simd::store_i16(v51.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v7.s8)));
	// vupkhsb128 v50,v5,v0
	simd::store_i16(v50.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// vupkhsb128 v49,v51,v96
	simd::store_i32(v49.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v51.s16)));
	// vsldoi128 v48,v63,v53,4
	simd::store_i8(v48.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// vupkhsb128 v47,v50,v96
	simd::store_i32(v47.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v50.s16)));
	// vcsxwfp128 v46,v49,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v45,v52,v48
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v48.f32)));
	// vcsxwfp128 v44,v47,7
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v43,v46,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v46.u32), 3));
	// vsldoi128 v42,v45,v63,8
	simd::store_i8(v42.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vrlimi128 v43,v44,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v41,v43,v42
	simd::store_f32_aligned(v41.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v41,r11,r26
	PPC_STORE_U32((r11.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v41.u32), 3 - ((r11.u32 + r26.u32) & 0xF) >> 2));
	// lvlx v4,r7,r31
	temp.u32 = ctx.r7.u32 + r31.u32;
	simd::store_shuffled(ctx.v4,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v3,r8,r31
	temp.u32 = ctx.r8.u32 + r31.u32;
	simd::store_shuffled(ctx.v3,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v2,v3,v0
	simd::store_u8(ctx.v2.u8, simd::add_u8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v1,v4,v0
	simd::store_u8(ctx.v1.u8, simd::add_u8(simd::load_u8(ctx.v4.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v40,v2,v0
	simd::store_i16(v40.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v2.s8)));
	// vupkhsb128 v39,v1,v0
	simd::store_i16(v39.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v1.s8)));
	// vupkhsb128 v38,v40,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v40.s16)));
	// vupkhsb128 v37,v39,v96
	simd::store_i32(v37.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v39.s16)));
	// vcsxwfp128 v36,v38,7
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v35,v37,7
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v34,v36,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v36.u32), 3));
	// vrlimi128 v34,v35,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v35.f32))));
	// vmsum3fp128 v33,v34,v42
	simd::store_f32_aligned(v33.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v33,r11,r27
	PPC_STORE_U32((r11.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v33.u32), 3 - ((r11.u32 + r27.u32) & 0xF) >> 2));
	// lvlx v31,r7,r4
	temp.u32 = ctx.r7.u32 + ctx.r4.u32;
	simd::store_shuffled(v31,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v30,r8,r4
	temp.u32 = ctx.r8.u32 + ctx.r4.u32;
	simd::store_shuffled(v30,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v29,v30,v0
	simd::store_u8(v29.u8, simd::add_u8(simd::load_u8(v30.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v28,v31,v0
	simd::store_u8(v28.u8, simd::add_u8(simd::load_u8(v31.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v32,v29,v0
	simd::store_i16(v32.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v29.s8)));
	// vupkhsb128 v61,v28,v0
	simd::store_i16(v61.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v28.s8)));
	// vupkhsb128 v60,v32,v96
	simd::store_i32(v60.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v32.s16)));
	// vupkhsb128 v59,v61,v96
	simd::store_i32(v59.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v61.s16)));
	// vcsxwfp128 v58,v60,7
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v57,v59,7
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v56,v58,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v42
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v55,r11,r28
	PPC_STORE_U32((r11.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((r11.u32 + r28.u32) & 0xF) >> 2));
	// lvlx v27,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v27,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v26,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v26,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v25,v26,v0
	simd::store_u8(v25.u8, simd::add_u8(simd::load_u8(v26.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v24,v27,v0
	simd::store_u8(v24.u8, simd::add_u8(simd::load_u8(v27.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v54,v25,v0
	simd::store_i16(v54.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v25.s8)));
	// sradi r8,r9,32
	xer.ca = (ctx.r9.s64 < 0) & ((ctx.r9.u64 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s64 >> 32;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// vupkhsb128 v53,v24,v0
	simd::store_i16(v53.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v24.s8)));
	// vupkhsb128 v52,v54,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v54.s16)));
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// vupkhsb128 v51,v53,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vcsxwfp128 v50,v52,7
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v49,v51,7
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v48,v50,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// vrlimi128 v48,v49,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmsum3fp128 v47,v48,v42
	simd::store_f32_aligned(v47.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v42.f32), 0xEF));
	// stvewx128 v47,r0,r11
	PPC_STORE_U32((r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v47.u32), 3 - ((r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// xor r25,r7,r25
	r25.u64 = ctx.r7.u64 ^ r25.u64;
	// rlwinm r25,r25,0,0,24
	r25.u64 = rotl64(r25.u32 | (r25.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x82cedc20
	if (cr6.eq) goto loc_82CEDC20;
	// li r25,128
	r25.s64 = 128;
	// dcbt r25,r7
loc_82CEDC20:
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// cmpd cr6,r8,r24
	cr6.compare<int64_t>(ctx.r8.s64, r24.s64, xer);
	// blt cr6,0x82cedabc
	if (cr6.lt) goto loc_82CEDABC;
	// lwz r19,-144(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-144) );
loc_82CEDC30:
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// beq cr6,0x82cedcfc
	if (cr6.eq) goto loc_82CEDCFC;
	// cmpd cr6,r8,r24
	cr6.compare<int64_t>(ctx.r8.s64, r24.s64, xer);
	// bne cr6,0x82cedce4
	if (!cr6.eq) goto loc_82CEDCE4;
	// rlwinm r7,r23,2,0,29
	ctx.r7.u64 = rotl64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// lis r4,-32255
	ctx.r4.s64 = -2113863680;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// lfs f0,26484(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 26484);
	f0.f64 = double(temp.f32);
	// lbz r6,3(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 3);
	// lfs f13,26480(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 26480);
	ctx.f13.f64 = double(temp.f32);
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// std r4,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r4.u64);
	// lfd f11,-128(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fsubs f8,f9,f0
	ctx.f8.f64 = static_cast<float>(ctx.f9.f64 - f0.f64);
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f7,12(r5)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// lbz r4,2(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 2);
	// std r4,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r4.u64);
	// lfd f6,-128(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f0
	ctx.f3.f64 = static_cast<float>(ctx.f4.f64 - f0.f64);
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f2,8(r5)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lbz r4,1(r7)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r7.u32 + 1);
	// std r4,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r4.u64);
	// lfd f1,-128(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fsubs f9,f10,f0
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - f0.f64);
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f8,4(r5)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lbz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// std r6,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.r6.u64);
	// lfd f7,-128(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fsubs f4,f5,f0
	ctx.f4.f64 = static_cast<float>(ctx.f5.f64 - f0.f64);
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// stfs f3,0(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// b 0x82cedcfc
	goto loc_82CEDCFC;
loc_82CEDCE4:
	// extsw r7,r19
	ctx.r7.s64 = r19.s32;
	// cmpd cr6,r8,r7
	cr6.compare<int64_t>(ctx.r8.s64, ctx.r7.s64, xer);
	// ble cr6,0x82cedcfc
	if (!cr6.gt) goto loc_82CEDCFC;
	// subf r7,r7,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
loc_82CEDCFC:
	// rldicr r7,r8,32,31
	ctx.r7.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r7,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r7.s64;
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// std r5,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r5.u64);
	// stvewx128 v63,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// lbz r4,0(r22)
	ctx.r4.u64 = PPC_LOAD_U8(r22.u32 + 0);
	// lfd f0,-152(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r9,r3,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// twllei r4,0
	// divwu r9,r9,r4
	ctx.r9.u32 = ctx.r9.u32 / ctx.r4.u32;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// fmul f12,f13,f12
	ctx.f12.f64 = ctx.f13.f64 * ctx.f12.f64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stfs f11,0(r20)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r20.u32 + 0, temp.u32);
	// bge cr6,0x82cedd50
	if (!cr6.lt) goto loc_82CEDD50;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82CEDD50:
	// lwz r9,-140(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-140) );
	// lwz r8,-136(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-136) );
	// lwz r7,-132(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-132) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// subf r5,r6,r11
	ctx.r5.s64 = r11.s64 - ctx.r6.s64;
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cedd84
	if (!cr6.lt) goto loc_82CEDD84;
	// lwz r10,-160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c14
	return;
loc_82CEDD84:
	// lwz r11,-160(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c14
	return;
}

PPC_WEAK_FUNC(sub_82CED870) {
	__imp__sub_82CED870(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEDD90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v16{};
	PPCVRegister v17{};
	PPCVRegister v18{};
	PPCVRegister v19{};
	PPCVRegister v20{};
	PPCVRegister v21{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bc8
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r8,r3,28
	ctx.r8.s64 = ctx.r3.s64 + 28;
	// addi r7,r3,4
	ctx.r7.s64 = ctx.r3.s64 + 4;
	// stw r9,-128(r1)
	PPC_STORE_U32(ctx.r1.u32 + -128, ctx.r9.u32);
	// addi r4,r3,24
	ctx.r4.s64 = ctx.r3.s64 + 24;
	// stw r8,-148(r1)
	PPC_STORE_U32(ctx.r1.u32 + -148, ctx.r8.u32);
	// addi r6,r3,20
	ctx.r6.s64 = ctx.r3.s64 + 20;
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r11,r3,13
	r11.s64 = ctx.r3.s64 + 13;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// stw r4,-132(r1)
	PPC_STORE_U32(ctx.r1.u32 + -132, ctx.r4.u32);
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// mullw r11,r9,r5
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r5.s32);
	// lwz r4,0(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// lwz r9,0(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// stw r6,-136(r1)
	PPC_STORE_U32(ctx.r1.u32 + -136, ctx.r6.u32);
	// subf r7,r5,r30
	ctx.r7.s64 = r30.s64 - ctx.r5.s64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r31,r4
	ctx.r4.s64 = ctx.r4.s64 - r31.s64;
	// stw r7,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r7.u32);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// stw r4,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r4.u32);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// add r11,r8,r9
	r11.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r10
	// li r6,1
	ctx.r6.s64 = 1;
	// lfs f0,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	f0.f64 = double(temp.f32);
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// rldicr r8,r6,32,63
	ctx.r8.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// li r6,40
	ctx.r6.s64 = 40;
	// stw r9,-140(r1)
	PPC_STORE_U32(ctx.r1.u32 + -140, ctx.r9.u32);
	// std r8,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r8.u64);
	// lfd f13,-120(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmul f10,f0,f12
	ctx.f10.f64 = f0.f64 * ctx.f12.f64;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// lvlx128 v61,r3,r6
	temp.u32 = ctx.r3.u32 + ctx.r6.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r6,4
	ctx.r6.s64 = 4;
	// lvlx128 v63,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r31,r1,-120
	r31.s64 = ctx.r1.s64 + -120;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r27,r1,-160
	r27.s64 = ctx.r1.s64 + -160;
	// li r22,0
	r22.s64 = 0;
	// li r28,5
	r28.s64 = 5;
	// lvlx128 v60,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r29,3
	r29.s64 = 3;
	// fctidz f8,f10
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f8.u64);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.f7.u64);
	// ld r7,-120(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// vsubfp128 v59,v61,v60
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(v60.f32)));
	// ld r26,-160(r1)
	r26.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lfd f0,3248(r8)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + 3248);
	// std r26,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, r26.u64);
	// lvlx128 v56,r6,r31
	temp.u32 = ctx.r6.u32 + r31.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// std r7,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r7.u64);
	// lvlx128 v55,r6,r27
	temp.u32 = ctx.r6.u32 + r27.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r23,3072
	r23.s64 = 3072;
	// li r30,2
	r30.s64 = 2;
	// fdiv f12,f0,f12
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = f0.f64 / ctx.f12.f64;
	// li r24,2048
	r24.s64 = 2048;
	// stw r22,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r22.u32);
	// li r31,1
	r31.s64 = 1;
	// vspltw128 v63,v60,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// li r25,1024
	r25.s64 = 1024;
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// vspltw128 v13,v55,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// vspltw128 v62,v57,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cee034
	if (!cr6.lt) goto loc_82CEE034;
loc_82CEDED8:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82cee034
	if (!cr6.gt) goto loc_82CEE034;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// lvlx v11,r10,r28
	temp.u32 = ctx.r10.u32 + r28.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisw128 v54,0
	simd::store_i32(v54.u32, simd::set1_i32(int32_t(0x0)));
	// li r9,20
	ctx.r9.s64 = 20;
	// vspltisb v10,1
	simd::store_i8(ctx.v10.u8, simd::set1_i8(int8_t(0x1)));
	// li r8,5120
	ctx.r8.s64 = 5120;
	// li r27,16
	r27.s64 = 16;
	// vadduwm v9,v13,v12
	simd::store_u32(ctx.v9.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v12.u32)));
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// li r22,4096
	r22.s64 = 4096;
	// vsubfp128 v53,v54,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v53.f32, simd::sub_f32(simd::load_f32_aligned(v54.f32), simd::load_f32_aligned(v63.f32)));
	// li r21,12
	r21.s64 = 12;
	// vsr v8,v13,v10
	simd::store_i8(ctx.v8.u8, simd::vsr(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v10.u8)));
	// lvlx128 v52,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v51,v52,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v52.u32), 3));
	// li r9,8
	ctx.r9.s64 = 8;
	// vaddubm v7,v11,v0
	simd::store_u8(ctx.v7.u8, simd::add_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v0.u8)));
	// add r7,r26,r7
	ctx.r7.u64 = r26.u64 + ctx.r7.u64;
	// vor v13,v9,v9
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v9.u8));
	// vcuxwfp128 v50,v8,31
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v8.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vupkhsb128 v49,v7,v0
	simd::store_i16(v49.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v7.s8)));
	// vupkhsb128 v48,v49,v96
	simd::store_i32(v48.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vsldoi128 v47,v63,v53,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v53.u8), 12));
	// vcsxwfp128 v46,v48,7
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v48.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vmulfp128 v45,v50,v47
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v47.f32)));
	// vrlimi128 v51,v46,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v46.f32))));
	// vsldoi128 v44,v45,v63,8
	simd::store_i8(v44.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vmsum3fp128 v43,v51,v44
	simd::store_f32_aligned(v43.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v43,r11,r8
	PPC_STORE_U32((r11.u32 + ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v43.u32), 3 - ((r11.u32 + ctx.r8.u32) & 0xF) >> 2));
	// lvlx128 v42,r5,r27
	temp.u32 = ctx.r5.u32 + r27.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v6,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v5,v6,v0
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v41,v5,v0
	simd::store_i16(v41.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// vspltw128 v40,v42,0
	simd::store_i32(v40.u32, simd::broadcast_lane_i32(simd::load_i32(v42.u32), 3));
	// vupkhsb128 v39,v41,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vcsxwfp128 v38,v39,7
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v40,v38,8,0
	simd::store_f32(v40.f32, simd::blend_f32<8>(simd::load_f32(v40.f32), simd::permute_f32<228>(simd::load_f32(v38.f32))));
	// vmsum3fp128 v37,v40,v44
	simd::store_f32_aligned(v37.f32, simd::dp_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v37,r11,r22
	PPC_STORE_U32((r11.u32 + r22.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v37.u32), 3 - ((r11.u32 + r22.u32) & 0xF) >> 2));
	// lvlx128 v35,r5,r21
	temp.u32 = ctx.r5.u32 + r21.u32;
	simd::store_shuffled(v35,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v34,v35,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v35.u32), 3));
	// lvlx v4,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	simd::store_shuffled(ctx.v4,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v3,v4,v0
	simd::store_u8(ctx.v3.u8, simd::add_u8(simd::load_u8(ctx.v4.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v36,v3,v0
	simd::store_i16(v36.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v3.s8)));
	// vupkhsb128 v33,v36,v96
	simd::store_i32(v33.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v36.s16)));
	// vcsxwfp128 v32,v33,7
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v34,v32,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v32.f32))));
	// vmsum3fp128 v61,v34,v44
	simd::store_f32_aligned(v61.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v61,r11,r23
	PPC_STORE_U32((r11.u32 + r23.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v61.u32), 3 - ((r11.u32 + r23.u32) & 0xF) >> 2));
	// lvlx128 v60,r5,r9
	temp.u32 = ctx.r5.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v2,r10,r30
	temp.u32 = ctx.r10.u32 + r30.u32;
	simd::store_shuffled(ctx.v2,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v1,v2,v0
	simd::store_u8(ctx.v1.u8, simd::add_u8(simd::load_u8(ctx.v2.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v58,v1,v0
	simd::store_i16(v58.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v1.s8)));
	// vspltw128 v59,v60,0
	simd::store_i32(v59.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vupkhsb128 v57,v58,v96
	simd::store_i32(v57.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v58.s16)));
	// vcsxwfp128 v56,v57,7
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v57.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v59,v56,8,0
	simd::store_f32(v59.f32, simd::blend_f32<8>(simd::load_f32(v59.f32), simd::permute_f32<228>(simd::load_f32(v56.f32))));
	// vmsum3fp128 v55,v59,v44
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v55,r11,r24
	PPC_STORE_U32((r11.u32 + r24.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((r11.u32 + r24.u32) & 0xF) >> 2));
	// lvlx v31,r10,r31
	temp.u32 = ctx.r10.u32 + r31.u32;
	simd::store_shuffled(v31,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v30,v31,v0
	simd::store_u8(v30.u8, simd::add_u8(simd::load_u8(v31.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v53,v30,v0
	simd::store_i16(v53.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v30.s8)));
	// lvlx128 v54,r5,r6
	temp.u32 = ctx.r5.u32 + ctx.r6.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v52,v54,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// vupkhsb128 v51,v53,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// vcsxwfp128 v50,v51,7
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v52,v50,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v50.f32))));
	// vmsum3fp128 v49,v52,v44
	simd::store_f32_aligned(v49.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v49,r11,r25
	PPC_STORE_U32((r11.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v49.u32), 3 - ((r11.u32 + r25.u32) & 0xF) >> 2));
	// lvlx128 v48,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v29,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v29,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v28,v29,v0
	simd::store_u8(v28.u8, simd::add_u8(simd::load_u8(v29.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v47,v28,v0
	simd::store_i16(v47.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v28.s8)));
	// vspltw128 v46,v48,0
	simd::store_i32(v46.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vupkhsb128 v45,v47,v96
	simd::store_i32(v45.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v47.s16)));
	// vcsxwfp128 v43,v45,7
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v45.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vrlimi128 v46,v43,8,0
	simd::store_f32(v46.f32, simd::blend_f32<8>(simd::load_f32(v46.f32), simd::permute_f32<228>(simd::load_f32(v43.f32))));
	// vmsum3fp128 v42,v46,v44
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v46.f32), simd::load_f32_aligned(v44.f32), 0xEF));
	// stvewx128 v42,r0,r11
	PPC_STORE_U32((r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r22,-160(r1)
	r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// blt cr6,0x82ceded8
	if (cr6.lt) goto loc_82CEDED8;
loc_82CEE034:
	// lwz r9,-152(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-152) );
	// sradi r8,r7,63
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r7.s64 >> 63;
	// sradi r21,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	r21.s64 = ctx.r7.s64 >> 32;
	// addi r27,r9,-1
	r27.s64 = ctx.r9.s64 + -1;
	// subf r9,r8,r21
	ctx.r9.s64 = r21.s64 - ctx.r8.s64;
	// extsw r21,r27
	r21.s64 = r27.s32;
	// stw r27,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r27.u32);
	// cmpd cr6,r9,r21
	cr6.compare<int64_t>(ctx.r9.s64, r21.s64, xer);
	// bge cr6,0x82cee260
	if (!cr6.lt) goto loc_82CEE260;
loc_82CEE058:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82cee374
	if (cr6.eq) goto loc_82CEE374;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// vspltisb v0,7
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x7)));
	// rlwinm r27,r9,1,0,30
	r27.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisw128 v41,0
	simd::store_i32(v41.u32, simd::set1_i32(int32_t(0x0)));
	// addi r9,r8,1
	ctx.r9.s64 = ctx.r8.s64 + 1;
	// vspltisb v11,1
	simd::store_i8(ctx.v11.u8, simd::set1_i8(int8_t(0x1)));
	// add r27,r8,r27
	r27.u64 = ctx.r8.u64 + r27.u64;
	// vadduwm v10,v13,v12
	simd::store_u32(ctx.v10.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v12.u32)));
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vslb v0,v0,v0
	simd::store_shifted_i8(ctx.v0, ctx.v0, ctx.v0);
	// rlwinm r27,r27,1,0,30
	r27.u64 = rotl64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// vsubfp128 v40,v41,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v40.f32, simd::sub_f32(simd::load_f32_aligned(v41.f32), simd::load_f32_aligned(v63.f32)));
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// vsr v9,v13,v11
	simd::store_i8(ctx.v9.u8, simd::vsr(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v11.u8)));
	// add r9,r27,r10
	ctx.r9.u64 = r27.u64 + ctx.r10.u64;
	// vor v13,v10,v10
	simd::store_i8(ctx.v13.u8, simd::load_i8(ctx.v10.u8));
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r27,5120
	r27.s64 = 5120;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vcuxwfp128 v39,v9,31
	simd::store_f32_aligned(v39.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v9.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// li r20,4096
	r20.s64 = 4096;
	// lvlx v8,r9,r28
	temp.u32 = ctx.r9.u32 + r28.u32;
	simd::store_shuffled(ctx.v8,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v7,v8,v0
	simd::store_u8(ctx.v7.u8, simd::add_u8(simd::load_u8(ctx.v8.u8), simd::load_u8(ctx.v0.u8)));
	// lvlx v6,r8,r28
	temp.u32 = ctx.r8.u32 + r28.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v5,v6,v0
	simd::store_u8(ctx.v5.u8, simd::add_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v38,v7,v0
	simd::store_i16(v38.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v7.s8)));
	// vsldoi128 v37,v63,v40,4
	simd::store_i8(v37.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v40.u8), 12));
	// vupkhsb128 v36,v5,v0
	simd::store_i16(v36.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v5.s8)));
	// vupkhsb128 v35,v38,v96
	simd::store_i32(v35.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v38.s16)));
	// vmulfp128 v34,v39,v37
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v37.f32)));
	// vupkhsb128 v33,v36,v96
	simd::store_i32(v33.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v36.s16)));
	// vcsxwfp128 v32,v35,7
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v61,v33,7
	simd::store_f32_aligned(v61.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v33.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vsldoi128 v60,v34,v63,8
	simd::store_i8(v60.u8, simd::shift_left_insert_bytes(simd::load_i8(v34.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v62
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// vspltw128 v59,v32,0
	simd::store_i32(v59.u32, simd::broadcast_lane_i32(simd::load_i32(v32.u32), 3));
	// vrlimi128 v59,v61,8,0
	simd::store_f32(v59.f32, simd::blend_f32<8>(simd::load_f32(v59.f32), simd::permute_f32<228>(simd::load_f32(v61.f32))));
	// vmsum3fp128 v58,v59,v60
	simd::store_f32_aligned(v58.f32, simd::dp_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v58,r11,r27
	PPC_STORE_U32((r11.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v58.u32), 3 - ((r11.u32 + r27.u32) & 0xF) >> 2));
	// lvlx v3,r9,r6
	temp.u32 = ctx.r9.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v3,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v2,v3,v0
	simd::store_u8(ctx.v2.u8, simd::add_u8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v0.u8)));
	// lvlx v4,r8,r6
	temp.u32 = ctx.r8.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v4,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vupkhsb128 v57,v2,v0
	simd::store_i16(v57.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v2.s8)));
	// vaddubm v1,v4,v0
	simd::store_u8(ctx.v1.u8, simd::add_u8(simd::load_u8(ctx.v4.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v55,v57,v96
	simd::store_i32(v55.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v57.s16)));
	// vupkhsb128 v56,v1,v0
	simd::store_i16(v56.s16, simd::extend_i8_hi_to_i16(simd::load_i8(ctx.v1.s8)));
	// vcsxwfp128 v53,v55,7
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v55.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vupkhsb128 v54,v56,v96
	simd::store_i32(v54.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v56.s16)));
	// vcsxwfp128 v52,v54,7
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v54.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// vmsum3fp128 v50,v51,v60
	simd::store_f32_aligned(v50.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v50,r11,r20
	PPC_STORE_U32((r11.u32 + r20.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v50.u32), 3 - ((r11.u32 + r20.u32) & 0xF) >> 2));
	// lvlx v31,r8,r29
	temp.u32 = ctx.r8.u32 + r29.u32;
	simd::store_shuffled(v31,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v30,r9,r29
	temp.u32 = ctx.r9.u32 + r29.u32;
	simd::store_shuffled(v30,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v29,v30,v0
	simd::store_u8(v29.u8, simd::add_u8(simd::load_u8(v30.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v28,v31,v0
	simd::store_u8(v28.u8, simd::add_u8(simd::load_u8(v31.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v49,v29,v0
	simd::store_i16(v49.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v29.s8)));
	// vupkhsb128 v48,v28,v0
	simd::store_i16(v48.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v28.s8)));
	// vupkhsb128 v47,v49,v96
	simd::store_i32(v47.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v49.s16)));
	// vupkhsb128 v46,v48,v96
	simd::store_i32(v46.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v48.s16)));
	// vcsxwfp128 v45,v47,7
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v44,v46,7
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v43,v45,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v45.u32), 3));
	// vrlimi128 v43,v44,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v42,v43,v60
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v42,r11,r23
	PPC_STORE_U32((r11.u32 + r23.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((r11.u32 + r23.u32) & 0xF) >> 2));
	// lvlx v27,r8,r30
	temp.u32 = ctx.r8.u32 + r30.u32;
	simd::store_shuffled(v27,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v26,r9,r30
	temp.u32 = ctx.r9.u32 + r30.u32;
	simd::store_shuffled(v26,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v25,v26,v0
	simd::store_u8(v25.u8, simd::add_u8(simd::load_u8(v26.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v24,v27,v0
	simd::store_u8(v24.u8, simd::add_u8(simd::load_u8(v27.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v41,v25,v0
	simd::store_i16(v41.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v25.s8)));
	// vupkhsb128 v40,v24,v0
	simd::store_i16(v40.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v24.s8)));
	// vupkhsb128 v39,v41,v96
	simd::store_i32(v39.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v41.s16)));
	// vupkhsb128 v38,v40,v96
	simd::store_i32(v38.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v40.s16)));
	// vcsxwfp128 v37,v39,7
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v36,v38,7
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v35,v37,0
	simd::store_i32(v35.u32, simd::broadcast_lane_i32(simd::load_i32(v37.u32), 3));
	// vrlimi128 v35,v36,8,0
	simd::store_f32(v35.f32, simd::blend_f32<8>(simd::load_f32(v35.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vmsum3fp128 v34,v35,v60
	simd::store_f32_aligned(v34.f32, simd::dp_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v34,r11,r24
	PPC_STORE_U32((r11.u32 + r24.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v34.u32), 3 - ((r11.u32 + r24.u32) & 0xF) >> 2));
	// lvlx v22,r9,r31
	temp.u32 = ctx.r9.u32 + r31.u32;
	simd::store_shuffled(v22,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r7,r26,r7
	ctx.r7.u64 = r26.u64 + ctx.r7.u64;
	// vaddubm v21,v22,v0
	simd::store_u8(v21.u8, simd::add_u8(simd::load_u8(v22.u8), simd::load_u8(ctx.v0.u8)));
	// lvlx v23,r8,r31
	temp.u32 = ctx.r8.u32 + r31.u32;
	simd::store_shuffled(v23,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v20,v23,v0
	simd::store_u8(v20.u8, simd::add_u8(simd::load_u8(v23.u8), simd::load_u8(ctx.v0.u8)));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// vupkhsb128 v33,v21,v0
	simd::store_i16(v33.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v21.s8)));
	// vupkhsb128 v32,v20,v0
	simd::store_i16(v32.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v20.s8)));
	// vupkhsb128 v61,v33,v96
	simd::store_i32(v61.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v33.s16)));
	// vupkhsb128 v59,v32,v96
	simd::store_i32(v59.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v32.s16)));
	// vcsxwfp128 v58,v61,7
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v61.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vcsxwfp128 v57,v59,7
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// vspltw128 v56,v58,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v58.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v60
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v55,r11,r25
	PPC_STORE_U32((r11.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((r11.u32 + r25.u32) & 0xF) >> 2));
	// lvlx v19,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v19,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v18,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v18,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vaddubm v17,v18,v0
	simd::store_u8(v17.u8, simd::add_u8(simd::load_u8(v18.u8), simd::load_u8(ctx.v0.u8)));
	// vaddubm v16,v19,v0
	simd::store_u8(v16.u8, simd::add_u8(simd::load_u8(v19.u8), simd::load_u8(ctx.v0.u8)));
	// vupkhsb128 v54,v17,v0
	simd::store_i16(v54.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v17.s8)));
	// sradi r9,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s64 >> 32;
	// extsw r27,r9
	r27.s64 = ctx.r9.s32;
	// vupkhsb128 v53,v16,v0
	simd::store_i16(v53.s16, simd::extend_i8_hi_to_i16(simd::load_i8(v16.s8)));
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// vupkhsb128 v52,v54,v96
	simd::store_i32(v52.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v54.s16)));
	// add r8,r27,r8
	ctx.r8.u64 = r27.u64 + ctx.r8.u64;
	// vupkhsb128 v51,v53,v96
	simd::store_i32(v51.s32, simd::extend_i16_hi_to_i32(simd::load_i16(v53.s16)));
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v50,v52,7
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// vcsxwfp128 v49,v51,7
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x3C000000))));
	// xor r27,r8,r22
	r27.u64 = ctx.r8.u64 ^ r22.u64;
	// rlwinm r27,r27,0,0,24
	r27.u64 = rotl64(r27.u32 | (r27.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// vspltw128 v48,v50,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// vrlimi128 v48,v49,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmsum3fp128 v47,v48,v60
	simd::store_f32_aligned(v47.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v60.f32), 0xEF));
	// stvewx128 v47,r0,r11
	PPC_STORE_U32((r11.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v47.u32), 3 - ((r11.u32) & 0xF) >> 2));
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// beq cr6,0x82cee250
	if (cr6.eq) goto loc_82CEE250;
	// li r27,128
	r27.s64 = 128;
	// dcbt r27,r8
loc_82CEE250:
	// mr r22,r8
	r22.u64 = ctx.r8.u64;
	// cmpd cr6,r9,r21
	cr6.compare<int64_t>(ctx.r9.s64, r21.s64, xer);
	// blt cr6,0x82cee058
	if (cr6.lt) goto loc_82CEE058;
	// lwz r27,-160(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
loc_82CEE260:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82cee374
	if (cr6.eq) goto loc_82CEE374;
	// cmpd cr6,r9,r21
	cr6.compare<int64_t>(ctx.r9.s64, r21.s64, xer);
	// bne cr6,0x82cee358
	if (!cr6.eq) goto loc_82CEE358;
	// rlwinm r8,r27,1,0,30
	ctx.r8.u64 = rotl64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// add r4,r27,r8
	ctx.r4.u64 = r27.u64 + ctx.r8.u64;
	// lis r31,-32255
	r31.s64 = -2113863680;
	// rlwinm r8,r4,1,0,30
	ctx.r8.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lfs f0,26484(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 26484);
	f0.f64 = double(temp.f32);
	// lfs f13,26480(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 26480);
	ctx.f13.f64 = double(temp.f32);
	// lbz r4,5(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 5);
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f11,-120(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fsubs f8,f9,f0
	ctx.f8.f64 = static_cast<float>(ctx.f9.f64 - f0.f64);
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f7,20(r5)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// lbz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 4);
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f6,-120(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f0
	ctx.f3.f64 = static_cast<float>(ctx.f4.f64 - f0.f64);
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f2,16(r5)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// lbz r4,3(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 3);
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f1,-120(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fsubs f9,f10,f0
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - f0.f64);
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f8,12(r5)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// lbz r4,2(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 2);
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f7,-120(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fsubs f4,f5,f0
	ctx.f4.f64 = static_cast<float>(ctx.f5.f64 - f0.f64);
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// stfs f3,8(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lbz r4,1(r8)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r8.u32 + 1);
	// std r4,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r4.u64);
	// lfd f2,-120(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f11,f1
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// fsubs f10,f11,f0
	ctx.f10.f64 = static_cast<float>(ctx.f11.f64 - f0.f64);
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfs f9,4(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lbz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// std r6,-120(r1)
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.r6.u64);
	// lfd f8,-120(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f0
	ctx.f5.f64 = static_cast<float>(ctx.f6.f64 - f0.f64);
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfs f4,0(r5)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// b 0x82cee374
	goto loc_82CEE374;
loc_82CEE358:
	// lwz r8,-152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-152) );
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// cmpd cr6,r9,r8
	cr6.compare<int64_t>(ctx.r9.s64, ctx.r8.s64, xer);
	// ble cr6,0x82cee374
	if (!cr6.gt) goto loc_82CEE374;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// rldicr r8,r8,32,31
	ctx.r8.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
loc_82CEE374:
	// rldicr r6,r9,32,31
	ctx.r6.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// lwz r5,-144(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-144) );
	// rotlwi r8,r9,0
	ctx.r8.u64 = rotl32(ctx.r9.u32, 0);
	// lwz r4,-140(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-140) );
	// subf r6,r6,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r6.s64;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// std r6,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r6.u64);
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stvewx128 v63,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r9,r3,13
	ctx.r9.s64 = ctx.r3.s64 + 13;
	// subf r9,r8,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r8.s64;
	// lfd f0,-160(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// add r7,r9,r10
	ctx.r7.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// twllei r6,0
	// divwu r9,r7,r6
	ctx.r9.u32 = ctx.r7.u32 / ctx.r6.u32;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// fmul f12,f13,f12
	ctx.f12.f64 = ctx.f13.f64 * ctx.f12.f64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// stfs f11,0(r4)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// bge cr6,0x82cee3e0
	if (!cr6.lt) goto loc_82CEE3E0;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82CEE3E0:
	// lwz r9,-136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-136) );
	// lwz r8,-132(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-132) );
	// lwz r7,-128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-128) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// subf r5,r6,r11
	ctx.r5.s64 = r11.s64 - ctx.r6.s64;
	// stw r10,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r10.u32);
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cee414
	if (!cr6.lt) goto loc_82CEE414;
	// lwz r10,-148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-148) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c18
	return;
loc_82CEE414:
	// lwz r11,-148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-148) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c18
	return;
}

PPC_WEAK_FUNC(sub_82CEDD90) {
	__imp__sub_82CEDD90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEE420) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bb8
	// lbz r11,13(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r24,r3,13
	r24.s64 = ctx.r3.s64 + 13;
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r19,r3,8
	r19.s64 = ctx.r3.s64 + 8;
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r6,24(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r11,r11,r5
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r5.s32);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r30,4(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r31,r4,r6
	r31.s64 = ctx.r6.s64 - ctx.r4.s64;
	// rlwinm r11,r11,1,0,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r4,2,0,29
	ctx.r8.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, r31.u32);
	// addi r16,r3,28
	r16.s64 = ctx.r3.s64 + 28;
	// addi r17,r3,24
	r17.s64 = ctx.r3.s64 + 24;
	// addi r23,r3,4
	r23.s64 = ctx.r3.s64 + 4;
	// addi r18,r3,20
	r18.s64 = ctx.r3.s64 + 20;
	// addi r25,r3,52
	r25.s64 = ctx.r3.s64 + 52;
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// subf r22,r5,r30
	r22.s64 = r30.s64 - ctx.r5.s64;
	// add r4,r8,r9
	ctx.r4.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r6
	// li r10,1
	ctx.r10.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// li r5,40
	ctx.r5.s64 = 40;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r8,r10,32,63
	ctx.r8.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r9,r1,-160
	ctx.r9.s64 = ctx.r1.s64 + -160;
	// std r8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r8.u64);
	// lfd f12,-152(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r10,4
	ctx.r10.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// addi r27,r1,-160
	r27.s64 = ctx.r1.s64 + -160;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// fmul f9,f13,f11
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r20,r3,36
	r20.s64 = ctx.r3.s64 + 36;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// addi r8,r1,-152
	ctx.r8.s64 = ctx.r1.s64 + -152;
	// lis r11,-32255
	r11.s64 = -2113863680;
	// addi r21,r3,48
	r21.s64 = ctx.r3.s64 + 48;
	// lvlx128 v61,r0,r20
	temp.u32 = r0.u32 + r20.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r29,0
	r29.s64 = 0;
	// vsubfp128 v59,v60,v61
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r9)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + 3248);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.f8.u64);
	// ld r5,-152(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r5,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r5.u64);
	// lvlx128 v55,r10,r27
	temp.u32 = ctx.r10.u32 + r27.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// stfd f7,-152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.f7.u64);
	// ld r28,-152(r1)
	r28.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// addi r30,r11,26464
	r30.s64 = r11.s64 + 26464;
	// std r28,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, r28.u64);
	// cmpdi cr6,r5,0
	cr6.compare<int64_t>(ctx.r5.s64, 0, xer);
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v12,v56,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// vspltw128 v13,v55,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v60,v57,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cee5d0
	if (!cr6.lt) goto loc_82CEE5D0;
loc_82CEE53C:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82cee5d0
	if (!cr6.gt) goto loc_82CEE5D0;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cee5b4
	if (cr6.eq) goto loc_82CEE5B4;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// vspltisb v0,1
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x1)));
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// vsubfp128 v54,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v54.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v11,v13,v0
	simd::store_i8(ctx.v11.u8, simd::vsr(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v0.u8)));
	// vcuxwfp128 v53,v11,31
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v11.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v52,v63,v54,4
	simd::store_i8(v52.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v54.u8), 12));
	// vmulfp128 v51,v53,v52
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::load_f32_aligned(v53.f32), simd::load_f32_aligned(v52.f32)));
	// vsldoi128 v61,v51,v63,8
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v51.u8), simd::load_i8(v63.u8), 8));
loc_82CEE57C:
	// lvlx128 v50,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lvx128 v0,r0,r30
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r30.u32) & ~0xF), VectorMaskL));
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// vperm128 v49,v50,v62,v0
	simd::store_i8(v49.u8, simd::permute_bytes(simd::load_i8(v50.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v0.u8)));
	// lvlx128 v48,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v47,v48,0
	simd::store_i32(v47.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// vcsxwfp128 v46,v49,31
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vrlimi128 v47,v46,8,0
	simd::store_f32(v47.f32, simd::blend_f32<8>(simd::load_f32(v47.f32), simd::permute_f32<228>(simd::load_f32(v46.f32))));
	// vmsum3fp128 v45,v47,v61
	simd::store_f32_aligned(v45.f32, simd::dp_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v61.f32), 0xEF));
	// stvewx128 v45,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v45.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,1024
	ctx.r8.s64 = ctx.r8.s64 + 1024;
	// bne 0x82cee57c
	if (!cr0.eq) goto loc_82CEE57C;
loc_82CEE5B4:
	// add r5,r28,r5
	ctx.r5.u64 = r28.u64 + ctx.r5.u64;
	// vadduwm v13,v13,v12
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v12.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v60
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v60.f32)));
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// cmpdi cr6,r5,0
	cr6.compare<int64_t>(ctx.r5.s64, 0, xer);
	// blt cr6,0x82cee53c
	if (cr6.lt) goto loc_82CEE53C;
loc_82CEE5D0:
	// sradi r11,r5,63
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	r11.s64 = ctx.r5.s64 >> 63;
	// sradi r10,r5,32
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = ctx.r5.s64 >> 32;
	// addi r26,r22,-1
	r26.s64 = r22.s64 + -1;
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// extsw r27,r26
	r27.s64 = r26.s32;
	// cmpd cr6,r11,r27
	cr6.compare<int64_t>(r11.s64, r27.s64, xer);
	// bge cr6,0x82cee6d0
	if (!cr6.lt) goto loc_82CEE6D0;
loc_82CEE5EC:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82cee75c
	if (cr6.eq) goto loc_82CEE75C;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cee684
	if (cr6.eq) goto loc_82CEE684;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r11,r11
	r11.s64 = r11.s32;
	// vspltisb v0,1
	simd::store_i8(ctx.v0.u8, simd::set1_i8(int8_t(0x1)));
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// mullw r9,r11,r7
	ctx.r9.s64 = int64_t(r11.s32) * int64_t(ctx.r7.s32);
	// vsubfp128 v44,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v44.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// vsr v11,v13,v0
	simd::store_i8(ctx.v11.u8, simd::vsr(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v0.u8)));
	// vcuxwfp128 v43,v11,31
	simd::store_f32_aligned(v43.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v11.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// vsldoi128 v42,v63,v44,4
	simd::store_i8(v42.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v44.u8), 12));
	// vmulfp128 v41,v43,v42
	simd::store_f32_aligned(v41.f32, simd::mul_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v42.f32)));
	// rlwinm r11,r9,1,0,30
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r6
	ctx.r9.u64 = r11.u64 + ctx.r6.u64;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// vsldoi128 v61,v41,v63,8
	simd::store_i8(v61.u8, simd::shift_left_insert_bytes(simd::load_i8(v41.u8), simd::load_i8(v63.u8), 8));
loc_82CEE644:
	// lvlx128 v40,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// lvx128 v0,r0,r30
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r30.u32) & ~0xF), VectorMaskL));
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// vperm128 v39,v40,v62,v0
	simd::store_i8(v39.u8, simd::permute_bytes(simd::load_i8(v40.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v0.u8)));
	// lvlx128 v38,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v38,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v37,v38,v62,v0
	simd::store_i8(v37.u8, simd::permute_bytes(simd::load_i8(v38.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v0.u8)));
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// vcsxwfp128 v36,v39,31
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v35,v37,31
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v34,v36,0
	simd::store_i32(v34.u32, simd::broadcast_lane_i32(simd::load_i32(v36.u32), 3));
	// vrlimi128 v34,v35,8,0
	simd::store_f32(v34.f32, simd::blend_f32<8>(simd::load_f32(v34.f32), simd::permute_f32<228>(simd::load_f32(v35.f32))));
	// vmsum3fp128 v33,v34,v61
	simd::store_f32_aligned(v33.f32, simd::dp_f32(simd::load_f32_aligned(v34.f32), simd::load_f32_aligned(v61.f32), 0xEF));
	// stvewx128 v33,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v33.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,1024
	ctx.r8.s64 = ctx.r8.s64 + 1024;
	// bne 0x82cee644
	if (!cr0.eq) goto loc_82CEE644;
loc_82CEE684:
	// add r5,r28,r5
	ctx.r5.u64 = r28.u64 + ctx.r5.u64;
	// vadduwm v13,v13,v12
	simd::store_u32(ctx.v13.u32, simd::add_u32(simd::load_u32(ctx.v13.u32), simd::load_u32(ctx.v12.u32)));
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// vaddfp128 v63,v63,v60
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v60.f32)));
	// sradi r11,r5,32
	xer.ca = (ctx.r5.s64 < 0) & ((ctx.r5.u64 & 0xFFFFFFFF) != 0);
	r11.s64 = ctx.r5.s64 >> 32;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// extsw r10,r11
	ctx.r10.s64 = r11.s32;
	// mullw r9,r10,r7
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// xor r8,r10,r29
	ctx.r8.u64 = ctx.r10.u64 ^ r29.u64;
	// rlwinm r9,r8,0,0,24
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82cee6c4
	if (cr6.eq) goto loc_82CEE6C4;
	// li r9,128
	ctx.r9.s64 = 128;
	// dcbt r9,r10
loc_82CEE6C4:
	// mr r29,r10
	r29.u64 = ctx.r10.u64;
	// cmpd cr6,r11,r27
	cr6.compare<int64_t>(r11.s64, r27.s64, xer);
	// blt cr6,0x82cee5ec
	if (cr6.lt) goto loc_82CEE5EC;
loc_82CEE6D0:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// beq cr6,0x82cee75c
	if (cr6.eq) goto loc_82CEE75C;
	// cmpd cr6,r11,r27
	cr6.compare<int64_t>(r11.s64, r27.s64, xer);
	// bne cr6,0x82cee744
	if (!cr6.eq) goto loc_82CEE744;
	// mullw r10,r26,r7
	ctx.r10.s64 = int64_t(r26.s32) * int64_t(ctx.r7.s32);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// add r9,r10,r6
	ctx.r9.u64 = ctx.r10.u64 + ctx.r6.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cee75c
	if (cr6.eq) goto loc_82CEE75C;
	// lis r31,-32256
	r31.s64 = -2113929216;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// lfs f0,3152(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 3152);
	f0.f64 = double(temp.f32);
loc_82CEE708:
	// lhz r31,0(r9)
	r31.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// rlwinm r30,r31,24,24,31
	r30.u64 = rotl64(r31.u32 | (r31.u64 << 32), 24) & 0xFF;
	// rlwimi r30,r31,8,16,23
	r30.u64 = (rotl32(r31.u32, 8) & 0xFF00) | (r30.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r31,r30
	r31.s64 = r30.s16;
	// std r31,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, r31.u64);
	// lfd f12,-152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne 0x82cee708
	if (!cr0.eq) goto loc_82CEE708;
	// b 0x82cee75c
	goto loc_82CEE75C;
loc_82CEE744:
	// extsw r10,r22
	ctx.r10.s64 = r22.s32;
	// cmpd cr6,r11,r10
	cr6.compare<int64_t>(r11.s64, ctx.r10.s64, xer);
	// ble cr6,0x82cee75c
	if (!cr6.gt) goto loc_82CEE75C;
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// rldicr r10,r10,32,31
	ctx.r10.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
loc_82CEE75C:
	// rldicr r10,r11,32,31
	ctx.r10.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFF00000000;
	// rotlwi r9,r11,0
	ctx.r9.u64 = rotl32(r11.u32, 0);
	// subf r8,r10,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r10.s64;
	// mullw r7,r9,r7
	ctx.r7.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// std r8,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.r8.u64);
	// stvewx128 v63,r0,r20
	PPC_STORE_U32((r20.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r20.u32) & 0xF) >> 2));
	// lfd f0,-160(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// lbz r5,0(r24)
	ctx.r5.u64 = PPC_LOAD_U8(r24.u32 + 0);
	// rlwinm r11,r7,1,0,30
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r9,r5,1
	ctx.r9.u64 = rotl32(ctx.r5.u32, 1);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// twllei r9,0
	// subf r10,r3,r11
	ctx.r10.s64 = r11.s64 - ctx.r3.s64;
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r10.u32 / ctx.r9.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r21)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// bge cr6,0x82cee7b8
	if (!cr6.lt) goto loc_82CEE7B8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEE7B8:
	// lwz r10,0(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + int32_t(0) );
	// stw r11,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r11.u32);
	// subf r8,r10,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r10.s64;
	// lwz r9,0(r17)
	ctx.r9.u64 = PPC_LOAD_U32(r17.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cee7dc
	if (!cr6.lt) goto loc_82CEE7DC;
	// stw r11,0(r16)
	PPC_STORE_U32(r16.u32 + 0, r11.u32);
	// b 0x82ca2c08
	return;
loc_82CEE7DC:
	// stw r9,0(r16)
	PPC_STORE_U32(r16.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c08
	return;
}

PPC_WEAK_FUNC(sub_82CEE420) {
	__imp__sub_82CEE420(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEE7E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bbc
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r22,r3,8
	r22.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r28,r3,13
	r28.s64 = ctx.r3.s64 + 13;
	// lwz r9,28(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r19,r3,28
	r19.s64 = ctx.r3.s64 + 28;
	// mullw r6,r4,r5
	ctx.r6.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r7,24(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// addi r20,r3,24
	r20.s64 = ctx.r3.s64 + 24;
	// addi r27,r3,4
	r27.s64 = ctx.r3.s64 + 4;
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// addi r21,r3,20
	r21.s64 = ctx.r3.s64 + 20;
	// addi r30,r3,52
	r30.s64 = ctx.r3.s64 + 52;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// subf r24,r5,r4
	r24.s64 = ctx.r4.s64 - ctx.r5.s64;
	// add r8,r6,r8
	ctx.r8.u64 = ctx.r6.u64 + ctx.r8.u64;
	// dcbt r0,r9
	// li r11,1
	r11.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r10,r1,-144
	ctx.r10.s64 = ctx.r1.s64 + -144;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r23,r1,-144
	r23.s64 = ctx.r1.s64 + -144;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r6,4
	ctx.r6.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// li r5,40
	ctx.r5.s64 = 40;
	// addi r25,r3,36
	r25.s64 = ctx.r3.s64 + 36;
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// addi r29,r1,-136
	r29.s64 = ctx.r1.s64 + -136;
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r26,r3,48
	r26.s64 = ctx.r3.s64 + 48;
	// lvlx128 v61,r0,r25
	temp.u32 = r0.u32 + r25.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r31,0
	r31.s64 = 0;
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// lfd f0,3248(r4)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 3248);
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r6,r23
	temp.u32 = ctx.r6.u32 + r23.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// stfd f7,-136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r5,-136(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r6,r29
	temp.u32 = ctx.r6.u32 + r29.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r10,26464
	ctx.r4.s64 = ctx.r10.s64 + 26464;
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cee96c
	if (!cr6.lt) goto loc_82CEE96C;
loc_82CEE900:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82cee96c
	if (!cr6.gt) goto loc_82CEE96C;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// lvlx128 v54,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvx128 v7,r0,r4
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// lvlx128 v53,r0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v52,v53,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vsubfp128 v51,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v51.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// vperm128 v50,v54,v62,v7
	simd::store_i8(v50.u8, simd::permute_bytes(simd::load_i8(v54.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vcsxwfp128 v49,v50,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcuxwfp128 v48,v10,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v47,v63,v51,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// vrlimi128 v52,v49,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmulfp128 v46,v48,v47
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v47.f32)));
	// vsldoi128 v45,v46,v63,8
	simd::store_i8(v45.u8, simd::shift_left_insert_bytes(simd::load_i8(v46.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v61
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v44,v52,v45
	simd::store_f32_aligned(v44.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v44,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v44.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// blt cr6,0x82cee900
	if (cr6.lt) goto loc_82CEE900;
loc_82CEE96C:
	// sradi r10,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 63;
	// sradi r6,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r6.s64 = r11.s64 >> 32;
	// addi r23,r24,-1
	r23.s64 = r24.s64 + -1;
	// subf r10,r10,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r10.s64;
	// extsw r29,r23
	r29.s64 = r23.s32;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// bge cr6,0x82ceea34
	if (!cr6.lt) goto loc_82CEEA34;
loc_82CEE988:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ceea98
	if (cr6.eq) goto loc_82CEEA98;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r6,r10
	ctx.r6.s64 = ctx.r10.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r18,r10,1,0,30
	r18.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r6,1
	ctx.r10.s64 = ctx.r6.s64 + 1;
	// lvx128 v7,r0,r4
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r4.u32) & ~0xF), VectorMaskL));
	// add r11,r5,r11
	r11.u64 = ctx.r5.u64 + r11.u64;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsubfp128 v43,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v43.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r17,r10,1,0,30
	r17.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// sradi r10,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r10.s64 = r11.s64 >> 32;
	// lvlx128 v42,r18,r9
	temp.u32 = r18.u32 + ctx.r9.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vperm128 v41,v42,v62,v7
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v42.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcuxwfp128 v40,v10,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// lvlx128 v39,r17,r9
	temp.u32 = r17.u32 + ctx.r9.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v38,v39,v62,v7
	simd::store_i8(v38.u8, simd::permute_bytes(simd::load_i8(v39.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// add r6,r6,r9
	ctx.r6.u64 = ctx.r6.u64 + ctx.r9.u64;
	// vcsxwfp128 v37,v41,31
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r31,r6,r31
	r31.u64 = ctx.r6.u64 ^ r31.u64;
	// vcsxwfp128 v36,v38,31
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r31,r31,0,0,24
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF80;
	// vsldoi128 v35,v63,v43,4
	simd::store_i8(v35.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v43.u8), 12));
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vmulfp128 v34,v40,v35
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v35.f32)));
	// vspltw128 v33,v37,0
	simd::store_i32(v33.u32, simd::broadcast_lane_i32(simd::load_i32(v37.u32), 3));
	// vrlimi128 v33,v36,8,0
	simd::store_f32(v33.f32, simd::blend_f32<8>(simd::load_f32(v33.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vsldoi128 v32,v34,v63,8
	simd::store_i8(v32.u8, simd::shift_left_insert_bytes(simd::load_i8(v34.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v61
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v62,v33,v32
	simd::store_f32_aligned(v62.f32, simd::dp_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(v32.f32), 0xEF));
	// stvewx128 v62,r0,r8
	PPC_STORE_U32((ctx.r8.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v62.u32), 3 - ((ctx.r8.u32) & 0xF) >> 2));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// beq cr6,0x82ceea28
	if (cr6.eq) goto loc_82CEEA28;
	// li r31,128
	r31.s64 = 128;
	// dcbt r31,r6
loc_82CEEA28:
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// blt cr6,0x82cee988
	if (cr6.lt) goto loc_82CEE988;
loc_82CEEA34:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ceea98
	if (cr6.eq) goto loc_82CEEA98;
	// cmpd cr6,r10,r29
	cr6.compare<int64_t>(ctx.r10.s64, r29.s64, xer);
	// bne cr6,0x82ceea80
	if (!cr6.eq) goto loc_82CEEA80;
	// rlwinm r7,r23,1,0,30
	ctx.r7.u64 = rotl64(r23.u32 | (r23.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lhzx r5,r7,r9
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + ctx.r9.u32);
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// rlwinm r4,r5,24,24,31
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r5,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r6,r4
	ctx.r6.s64 = ctx.r4.s16;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// b 0x82ceea98
	goto loc_82CEEA98;
loc_82CEEA80:
	// extsw r7,r24
	ctx.r7.s64 = r24.s32;
	// cmpd cr6,r10,r7
	cr6.compare<int64_t>(ctx.r10.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ceea98
	if (!cr6.gt) goto loc_82CEEA98;
	// subf r7,r7,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CEEA98:
	// rldicr r7,r10,32,31
	ctx.r7.u64 = rotl64(ctx.r10.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r10,1,0,30
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r25
	PPC_STORE_U32((r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r25.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U8(r28.u32 + 0);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r11,r3,r6
	r11.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// add r10,r11,r9
	ctx.r10.u64 = r11.u64 + ctx.r9.u64;
	// rotlwi r9,r4,1
	ctx.r9.u64 = rotl32(ctx.r4.u32, 1);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// divwu r10,r10,r9
	ctx.r10.u32 = ctx.r10.u32 / ctx.r9.u32;
	// twllei r9,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r26)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r26.u32 + 0, temp.u32);
	// bge cr6,0x82ceeaec
	if (!cr6.lt) goto loc_82CEEAEC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CEEAEC:
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// stw r11,0(r22)
	PPC_STORE_U32(r22.u32 + 0, r11.u32);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// rlwinm r11,r8,30,2,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82ceeb10
	if (!cr6.lt) goto loc_82CEEB10;
	// stw r11,0(r19)
	PPC_STORE_U32(r19.u32 + 0, r11.u32);
	// b 0x82ca2c0c
	return;
loc_82CEEB10:
	// stw r9,0(r19)
	PPC_STORE_U32(r19.u32 + 0, ctx.r9.u32);
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82CEE7E8) {
	__imp__sub_82CEE7E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEEB18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bbc
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r20,r3,8
	r20.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r26,r3,13
	r26.s64 = ctx.r3.s64 + 13;
	// lwz r8,28(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r17,r3,28
	r17.s64 = ctx.r3.s64 + 28;
	// mullw r7,r4,r5
	ctx.r7.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r4,24(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r7,r8,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r8.s64;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// addi r18,r3,24
	r18.s64 = ctx.r3.s64 + 24;
	// stw r7,-144(r1)
	PPC_STORE_U32(ctx.r1.u32 + -144, ctx.r7.u32);
	// addi r25,r3,4
	r25.s64 = ctx.r3.s64 + 4;
	// addi r19,r3,20
	r19.s64 = ctx.r3.s64 + 20;
	// addi r28,r3,52
	r28.s64 = ctx.r3.s64 + 52;
	// subf r21,r5,r31
	r21.s64 = r31.s64 - ctx.r5.s64;
	// add r10,r6,r9
	ctx.r10.u64 = ctx.r6.u64 + ctx.r9.u64;
	// dcbt r0,r8
	// li r11,1
	r11.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r9,r1,-144
	ctx.r9.s64 = ctx.r1.s64 + -144;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r6,r11,32,63
	ctx.r6.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// std r6,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r6.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// addi r30,r1,-144
	r30.s64 = ctx.r1.s64 + -144;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r9,4
	ctx.r9.s64 = 4;
	// lfd f0,3248(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 3248);
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// li r5,40
	ctx.r5.s64 = 40;
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// addi r23,r3,36
	r23.s64 = ctx.r3.s64 + 36;
	// lis r6,-32255
	ctx.r6.s64 = -2113863680;
	// li r4,2
	ctx.r4.s64 = 2;
	// li r29,1024
	r29.s64 = 1024;
	// lvlx128 v60,r3,r5
	temp.u32 = ctx.r3.u32 + ctx.r5.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r5,r1,-136
	ctx.r5.s64 = ctx.r1.s64 + -136;
	// lvlx128 v61,r0,r23
	temp.u32 = r0.u32 + r23.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r24,r3,48
	r24.s64 = ctx.r3.s64 + 48;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f8.u64);
	// ld r11,-136(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// std r11,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r11.u64);
	// lvlx128 v55,r9,r30
	temp.u32 = ctx.r9.u32 + r30.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f7.u64);
	// ld r31,-136(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// std r31,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, r31.u64);
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r9,r5
	temp.u32 = ctx.r9.u32 + ctx.r5.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r30,0
	r30.s64 = 0;
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// addi r6,r6,26464
	ctx.r6.s64 = ctx.r6.s64 + 26464;
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82ceecc8
	if (!cr6.lt) goto loc_82CEECC8;
loc_82CEEC38:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82ceecc8
	if (!cr6.gt) goto loc_82CEECC8;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// lvx128 v7,r0,r6
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v54,r8,r4
	temp.u32 = ctx.r8.u32 + ctx.r4.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r28,r9
	temp.u32 = r28.u32 + ctx.r9.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v52,v53,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// vsubfp128 v51,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v51.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// vperm128 v50,v54,v62,v7
	simd::store_i8(v50.u8, simd::permute_bytes(simd::load_i8(v54.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// vcsxwfp128 v49,v50,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcuxwfp128 v48,v10,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v47,v63,v51,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// vrlimi128 v52,v49,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmulfp128 v46,v48,v47
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v47.f32)));
	// vsldoi128 v45,v46,v63,8
	simd::store_i8(v45.u8, simd::shift_left_insert_bytes(simd::load_i8(v46.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v61
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v44,v52,v45
	simd::store_f32_aligned(v44.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v44,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v44.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v43,r0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r6
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// lvlx128 v42,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v41,v42,v62,v7
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v42.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v40,v41,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v39,v43,0
	simd::store_i32(v39.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v39,v40,8,0
	simd::store_f32(v39.f32, simd::blend_f32<8>(simd::load_f32(v39.f32), simd::permute_f32<228>(simd::load_f32(v40.f32))));
	// vmsum3fp128 v38,v39,v45
	simd::store_f32_aligned(v38.f32, simd::dp_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v38,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v38.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82ceec38
	if (cr6.lt) goto loc_82CEEC38;
loc_82CEECC8:
	// sradi r9,r11,63
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r9.s64 = r11.s64 >> 63;
	// sradi r5,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r5.s64 = r11.s64 >> 32;
	// addi r22,r21,-1
	r22.s64 = r21.s64 + -1;
	// subf r9,r9,r5
	ctx.r9.s64 = ctx.r5.s64 - ctx.r9.s64;
	// extsw r27,r22
	r27.s64 = r22.s32;
	// cmpd cr6,r9,r27
	cr6.compare<int64_t>(ctx.r9.s64, r27.s64, xer);
	// bge cr6,0x82ceedc4
	if (!cr6.lt) goto loc_82CEEDC4;
loc_82CEECE4:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ceee54
	if (cr6.eq) goto loc_82CEEE54;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// extsw r5,r9
	ctx.r5.s64 = ctx.r9.s32;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// lvx128 v7,r0,r6
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vsubfp128 v37,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v37.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lvlx128 v36,r9,r4
	temp.u32 = ctx.r9.u32 + ctx.r4.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vcuxwfp128 v35,v10,31
	simd::store_f32_aligned(v35.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v34,v36,v62,v7
	simd::store_i8(v34.u8, simd::permute_bytes(simd::load_i8(v36.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// lvlx128 v33,r5,r4
	temp.u32 = ctx.r5.u32 + ctx.r4.u32;
	simd::store_shuffled(v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v32,v33,v62,v7
	simd::store_i8(v32.u8, simd::permute_bytes(simd::load_i8(v33.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v60,v34,31
	simd::store_f32_aligned(v60.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v34.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v59,v63,v37,4
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v37.u8), 12));
	// vcsxwfp128 v58,v32,31
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v32.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vmulfp128 v57,v35,v59
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v59.f32)));
	// vspltw128 v56,v60,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vrlimi128 v56,v58,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v58.f32))));
	// vsldoi128 v55,v57,v63,8
	simd::store_i8(v55.u8, simd::shift_left_insert_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v61
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v54,v56,v55
	simd::store_f32_aligned(v54.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v55.f32), 0xEF));
	// stvewx128 v54,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v54.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v53,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r6
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r6.u32) & ~0xF), VectorMaskL));
	// lvlx128 v52,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// sradi r9,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r9.s64 = r11.s64 >> 32;
	// vperm128 v51,v52,v62,v7
	simd::store_i8(v51.u8, simd::permute_bytes(simd::load_i8(v52.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v50,v53,v62,v7
	simd::store_i8(v50.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v49,v51,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r5,r8
	ctx.r5.u64 = ctx.r5.u64 + ctx.r8.u64;
	// vcsxwfp128 v48,v50,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// xor r30,r5,r30
	r30.u64 = ctx.r5.u64 ^ r30.u64;
	// rlwinm r30,r30,0,0,24
	r30.u64 = rotl64(r30.u32 | (r30.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// vspltw128 v47,v49,0
	simd::store_i32(v47.u32, simd::broadcast_lane_i32(simd::load_i32(v49.u32), 3));
	// vrlimi128 v47,v48,8,0
	simd::store_f32(v47.f32, simd::blend_f32<8>(simd::load_f32(v47.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vmsum3fp128 v46,v47,v55
	simd::store_f32_aligned(v46.f32, simd::dp_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v55.f32), 0xEF));
	// stvewx128 v46,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v46.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// beq cr6,0x82ceedb8
	if (cr6.eq) goto loc_82CEEDB8;
	// li r30,128
	r30.s64 = 128;
	// dcbt r30,r5
loc_82CEEDB8:
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// cmpd cr6,r9,r27
	cr6.compare<int64_t>(ctx.r9.s64, r27.s64, xer);
	// blt cr6,0x82ceece4
	if (cr6.lt) goto loc_82CEECE4;
loc_82CEEDC4:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq cr6,0x82ceee54
	if (cr6.eq) goto loc_82CEEE54;
	// cmpd cr6,r9,r27
	cr6.compare<int64_t>(ctx.r9.s64, r27.s64, xer);
	// bne cr6,0x82ceee3c
	if (!cr6.eq) goto loc_82CEEE3C;
	// rlwinm r7,r22,2,0,29
	ctx.r7.u64 = rotl64(r22.u32 | (r22.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// lhz r5,2(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 2);
	// rlwinm r4,r5,24,24,31
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r5,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r4
	ctx.r5.s64 = ctx.r4.s16;
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// lfd f12,-136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,4(r28)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r28.u32 + 4, temp.u32);
	// lhz r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r7.u32 + 0);
	// rlwinm r7,r4,24,24,31
	ctx.r7.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 24) & 0xFF;
	// rlwimi r7,r4,8,16,23
	ctx.r7.u64 = (rotl32(ctx.r4.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.r5.u64);
	// lfd f8,-136(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,0(r28)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
	// b 0x82ceee54
	goto loc_82CEEE54;
loc_82CEEE3C:
	// extsw r7,r21
	ctx.r7.s64 = r21.s32;
	// cmpd cr6,r9,r7
	cr6.compare<int64_t>(ctx.r9.s64, ctx.r7.s64, xer);
	// ble cr6,0x82ceee54
	if (!cr6.gt) goto loc_82CEEE54;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rldicr r7,r7,32,31
	ctx.r7.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// add r11,r7,r11
	r11.u64 = ctx.r7.u64 + r11.u64;
loc_82CEEE54:
	// rldicr r7,r9,32,31
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r5,r7,r11
	ctx.r5.s64 = r11.s64 - ctx.r7.s64;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// stvewx128 v63,r0,r23
	PPC_STORE_U32((r23.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r23.u32) & 0xF) >> 2));
	// lfd f0,-144(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lbz r4,0(r26)
	ctx.r4.u64 = PPC_LOAD_U8(r26.u32 + 0);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r9,r3,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r3.s64;
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// rotlwi r7,r4,1
	ctx.r7.u64 = rotl32(ctx.r4.u32, 1);
	// add r6,r9,r8
	ctx.r6.u64 = ctx.r9.u64 + ctx.r8.u64;
	// twllei r7,0
	// divwu r9,r6,r7
	ctx.r9.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r24)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r24.u32 + 0, temp.u32);
	// bge cr6,0x82ceeea8
	if (!cr6.lt) goto loc_82CEEEA8;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEEEA8:
	// lwz r9,0(r19)
	ctx.r9.u64 = PPC_LOAD_U32(r19.u32 + int32_t(0) );
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r8,0(r18)
	ctx.r8.u64 = PPC_LOAD_U32(r18.u32 + int32_t(0) );
	// rlwinm r11,r7,30,2,31
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82ceeecc
	if (!cr6.lt) goto loc_82CEEECC;
	// stw r11,0(r17)
	PPC_STORE_U32(r17.u32 + 0, r11.u32);
	// b 0x82ca2c0c
	return;
loc_82CEEECC:
	// stw r8,0(r17)
	PPC_STORE_U32(r17.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82CEEB18) {
	__imp__sub_82CEEB18(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEEED8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bbc
	// addi r10,r3,8
	ctx.r10.s64 = ctx.r3.s64 + 8;
	// lbz r6,13(r3)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r9,r3,28
	ctx.r9.s64 = ctx.r3.s64 + 28;
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// stw r10,-156(r1)
	PPC_STORE_U32(ctx.r1.u32 + -156, ctx.r10.u32);
	// addi r8,r3,24
	ctx.r8.s64 = ctx.r3.s64 + 24;
	// stw r9,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r9.u32);
	// addi r23,r3,13
	r23.s64 = ctx.r3.s64 + 13;
	// stw r8,-152(r1)
	PPC_STORE_U32(ctx.r1.u32 + -152, ctx.r8.u32);
	// addi r22,r3,4
	r22.s64 = ctx.r3.s64 + 4;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// addi r10,r3,20
	ctx.r10.s64 = ctx.r3.s64 + 20;
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// mullw r6,r6,r7
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r7.s32);
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// stw r10,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r10.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r7,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r7.s64;
	// subf r5,r5,r31
	ctx.r5.s64 = r31.s64 - ctx.r5.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r4,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r4.u32);
	// stw r5,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r5.u32);
	// addi r4,r3,52
	ctx.r4.s64 = ctx.r3.s64 + 52;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r11
	// li r9,1
	ctx.r9.s64 = 1;
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	f0.f64 = double(temp.f32);
	// addi r8,r1,-168
	ctx.r8.s64 = ctx.r1.s64 + -168;
	// lfs f13,44(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// rldicr r7,r9,32,63
	ctx.r7.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r29,r1,-168
	r29.s64 = ctx.r1.s64 + -168;
	// std r7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r7.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r7,4
	ctx.r7.s64 = 4;
	// fmul f10,f0,f11
	ctx.f10.f64 = f0.f64 * ctx.f11.f64;
	// lvlx128 v63,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// fmul f9,f13,f11
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = ctx.f13.f64 * ctx.f11.f64;
	// li r6,40
	ctx.r6.s64 = 40;
	// addi r20,r3,36
	r20.s64 = ctx.r3.s64 + 36;
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// li r27,3072
	r27.s64 = 3072;
	// lfd f0,3248(r8)
	f0.u64 = PPC_LOAD_U64(ctx.r8.u32 + 3248);
	// li r28,2048
	r28.s64 = 2048;
	// lvlx128 v60,r3,r6
	temp.u32 = ctx.r3.u32 + ctx.r6.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r6,r1,-144
	ctx.r6.s64 = ctx.r1.s64 + -144;
	// lvlx128 v61,r0,r20
	temp.u32 = r0.u32 + r20.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r31,2
	r31.s64 = 2;
	// vsubfp128 v59,v60,v61
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v61.f32)));
	// vspltw128 v63,v61,0
	simd::store_i32(v63.u32, simd::broadcast_lane_i32(simd::load_i32(v61.u32), 3));
	// fctidz f8,f10
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f8.u64);
	// ld r8,-144(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fdiv f13,f0,f11
	ctx.f13.f64 = f0.f64 / ctx.f11.f64;
	// std r8,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r8.u64);
	// lvlx128 v55,r7,r29
	temp.u32 = ctx.r7.u32 + r29.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// fctidz f7,f9
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f7.u64);
	// ld r30,-144(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// std r30,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, r30.u64);
	// li r29,1024
	r29.s64 = 1024;
	// vmulfp128 v57,v59,v58
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// lvlx128 v56,r7,r6
	temp.u32 = ctx.r7.u32 + ctx.r6.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// addi r21,r3,48
	r21.s64 = ctx.r3.s64 + 48;
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// li r26,0
	r26.s64 = 0;
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// addi r9,r9,26464
	ctx.r9.s64 = ctx.r9.s64 + 26464;
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bge cr6,0x82cef0f8
	if (!cr6.lt) goto loc_82CEF0F8;
	// li r24,6
	r24.s64 = 6;
	// li r25,12
	r25.s64 = 12;
	// li r6,8
	ctx.r6.s64 = 8;
loc_82CEF020:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x82cef0f8
	if (!cr6.gt) goto loc_82CEF0F8;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvlx128 v54,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v53,r4,r25
	temp.u32 = ctx.r4.u32 + r25.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// vspltw128 v52,v53,0
	simd::store_i32(v52.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// vsubfp128 v51,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v51.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// vperm128 v50,v54,v62,v7
	simd::store_i8(v50.u8, simd::permute_bytes(simd::load_i8(v54.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// vcsxwfp128 v49,v50,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcuxwfp128 v48,v10,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vsldoi128 v47,v63,v51,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v51.u8), 12));
	// vrlimi128 v52,v49,8,0
	simd::store_f32(v52.f32, simd::blend_f32<8>(simd::load_f32(v52.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmulfp128 v46,v48,v47
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v47.f32)));
	// vsldoi128 v45,v46,v63,8
	simd::store_i8(v45.u8, simd::shift_left_insert_bytes(simd::load_i8(v46.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v61
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v44,v52,v45
	simd::store_f32_aligned(v44.f32, simd::dp_f32(simd::load_f32_aligned(v52.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v44,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v44.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v43,r4,r6
	temp.u32 = ctx.r4.u32 + ctx.r6.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v42,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v41,v42,v62,v7
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v42.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v40,v41,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v39,v43,0
	simd::store_i32(v39.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v39,v40,8,0
	simd::store_f32(v39.f32, simd::blend_f32<8>(simd::load_f32(v39.f32), simd::permute_f32<228>(simd::load_f32(v40.f32))));
	// vmsum3fp128 v38,v39,v45
	simd::store_f32_aligned(v38.f32, simd::dp_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v38,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v38.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v37,r4,r7
	temp.u32 = ctx.r4.u32 + ctx.r7.u32;
	simd::store_shuffled(v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v36,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v35,v36,v62,v7
	simd::store_i8(v35.u8, simd::permute_bytes(simd::load_i8(v36.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v34,v35,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v33,v37,0
	simd::store_i32(v33.u32, simd::broadcast_lane_i32(simd::load_i32(v37.u32), 3));
	// vrlimi128 v33,v34,8,0
	simd::store_f32(v33.f32, simd::blend_f32<8>(simd::load_f32(v33.f32), simd::permute_f32<228>(simd::load_f32(v34.f32))));
	// vmsum3fp128 v32,v33,v45
	simd::store_f32_aligned(v32.f32, simd::dp_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v32,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v32.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v60,r0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v59,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v58,v59,v62,v7
	simd::store_i8(v58.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v57,v58,31
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v56,v60,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v45
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v55,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82cef020
	if (cr6.lt) goto loc_82CEF020;
loc_82CEF0F8:
	// lwz r7,-176(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// sradi r6,r8,63
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r8.s64 >> 63;
	// sradi r25,r8,32
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFFFFFF) != 0);
	r25.s64 = ctx.r8.s64 >> 32;
	// addi r24,r7,-1
	r24.s64 = ctx.r7.s64 + -1;
	// subf r7,r6,r25
	ctx.r7.s64 = r25.s64 - ctx.r6.s64;
	// extsw r25,r24
	r25.s64 = r24.s32;
	// cmpd cr6,r7,r25
	cr6.compare<int64_t>(ctx.r7.s64, r25.s64, xer);
	// bge cr6,0x82cef278
	if (!cr6.lt) goto loc_82CEF278;
loc_82CEF118:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82cef35c
	if (cr6.eq) goto loc_82CEF35C;
	// extsw r6,r7
	ctx.r6.s64 = ctx.r7.s32;
	// vspltisw128 v62,0
	simd::store_i32(v62.u32, simd::set1_i32(int32_t(0x0)));
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// addi r19,r7,3
	r19.s64 = ctx.r7.s64 + 3;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// vsubfp128 v54,v62,v63
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v54.f32, simd::sub_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v63.f32)));
	// rlwinm r19,r19,1,0,30
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// addi r18,r6,3
	r18.s64 = ctx.r6.s64 + 3;
	// addi r17,r6,2
	r17.s64 = ctx.r6.s64 + 2;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// rlwinm r18,r18,1,0,30
	r18.u64 = rotl64(r18.u32 | (r18.u64 << 32), 1) & 0xFFFFFFFE;
	// vcuxwfp128 v53,v10,31
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r17,r17,1,0,30
	r17.u64 = rotl64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v52,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r19,r7,2
	r19.s64 = ctx.r7.s64 + 2;
	// vperm128 v51,v52,v62,v7
	simd::store_i8(v51.u8, simd::permute_bytes(simd::load_i8(v52.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r19,r19,1,0,30
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v50,r18,r11
	temp.u32 = r18.u32 + r11.u32;
	simd::store_shuffled(v50,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// vperm128 v49,v50,v62,v7
	simd::store_i8(v49.u8, simd::permute_bytes(simd::load_i8(v50.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v48,v51,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r8,r30,r8
	ctx.r8.u64 = r30.u64 + ctx.r8.u64;
	// vsldoi128 v47,v63,v54,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v63.u8), simd::load_i8(v54.u8), 12));
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + r11.u64;
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// vcsxwfp128 v46,v49,31
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v49.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vmulfp128 v45,v53,v47
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::load_f32_aligned(v53.f32), simd::load_f32_aligned(v47.f32)));
	// vspltw128 v44,v48,0
	simd::store_i32(v44.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vrlimi128 v44,v46,8,0
	simd::store_f32(v44.f32, simd::blend_f32<8>(simd::load_f32(v44.f32), simd::permute_f32<228>(simd::load_f32(v46.f32))));
	// vsldoi128 v43,v45,v63,8
	simd::store_i8(v43.u8, simd::shift_left_insert_bytes(simd::load_i8(v45.u8), simd::load_i8(v63.u8), 8));
	// vaddfp128 v63,v63,v61
	simd::store_f32_aligned(v63.f32, simd::add_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v42,v44,v43
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v44.f32), simd::load_f32_aligned(v43.f32), 0xEF));
	// stvewx128 v42,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v41,r17,r11
	temp.u32 = r17.u32 + r11.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v40,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v39,v40,v62,v7
	simd::store_i8(v39.u8, simd::permute_bytes(simd::load_i8(v40.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v38,v41,v62,v7
	simd::store_i8(v38.u8, simd::permute_bytes(simd::load_i8(v41.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v37,v39,31
	simd::store_f32_aligned(v37.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v36,v38,31
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v35,v37,0
	simd::store_i32(v35.u32, simd::broadcast_lane_i32(simd::load_i32(v37.u32), 3));
	// vrlimi128 v35,v36,8,0
	simd::store_f32(v35.f32, simd::blend_f32<8>(simd::load_f32(v35.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vmsum3fp128 v34,v35,v43
	simd::store_f32_aligned(v34.f32, simd::dp_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v43.f32), 0xEF));
	// stvewx128 v34,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v34.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v32,r6,r31
	temp.u32 = ctx.r6.u32 + r31.u32;
	simd::store_shuffled(v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v33,r7,r31
	temp.u32 = ctx.r7.u32 + r31.u32;
	simd::store_shuffled(v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v59,v33,v62,v7
	simd::store_i8(v59.u8, simd::permute_bytes(simd::load_i8(v33.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v60,v32,v62,v7
	simd::store_i8(v60.u8, simd::permute_bytes(simd::load_i8(v32.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v57,v59,31
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v59.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v58,v60,31
	simd::store_f32_aligned(v58.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v56,v57,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vrlimi128 v56,v58,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v58.f32))));
	// vmsum3fp128 v55,v56,v43
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v43.f32), 0xEF));
	// stvewx128 v55,r10,r29
	PPC_STORE_U32((ctx.r10.u32 + r29.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32 + r29.u32) & 0xF) >> 2));
	// lvlx128 v54,r0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v53,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v52,v53,v62,v7
	simd::store_i8(v52.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v51,v54,v62,v7
	simd::store_i8(v51.u8, simd::permute_bytes(simd::load_i8(v54.u8), simd::load_i8(v62.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v50,v52,31
	simd::store_f32_aligned(v50.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// sradi r7,r8,32
	xer.ca = (ctx.r8.s64 < 0) & ((ctx.r8.u64 & 0xFFFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s64 >> 32;
	// rlwinm r6,r7,3,0,28
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// vcsxwfp128 v49,v51,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// xor r26,r6,r26
	r26.u64 = ctx.r6.u64 ^ r26.u64;
	// rlwinm r26,r26,0,0,24
	r26.u64 = rotl64(r26.u32 | (r26.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// vspltw128 v48,v50,0
	simd::store_i32(v48.u32, simd::broadcast_lane_i32(simd::load_i32(v50.u32), 3));
	// vrlimi128 v48,v49,8,0
	simd::store_f32(v48.f32, simd::blend_f32<8>(simd::load_f32(v48.f32), simd::permute_f32<228>(simd::load_f32(v49.f32))));
	// vmsum3fp128 v47,v48,v43
	simd::store_f32_aligned(v47.f32, simd::dp_f32(simd::load_f32_aligned(v48.f32), simd::load_f32_aligned(v43.f32), 0xEF));
	// stvewx128 v47,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v47.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// beq cr6,0x82cef26c
	if (cr6.eq) goto loc_82CEF26C;
	// li r26,128
	r26.s64 = 128;
	// dcbt r26,r6
loc_82CEF26C:
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// cmpd cr6,r7,r25
	cr6.compare<int64_t>(ctx.r7.s64, r25.s64, xer);
	// blt cr6,0x82cef118
	if (cr6.lt) goto loc_82CEF118;
loc_82CEF278:
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x82cef35c
	if (cr6.eq) goto loc_82CEF35C;
	// cmpd cr6,r7,r25
	cr6.compare<int64_t>(ctx.r7.s64, r25.s64, xer);
	// bne cr6,0x82cef340
	if (!cr6.eq) goto loc_82CEF340;
	// rlwinm r9,r24,3,0,28
	ctx.r9.u64 = rotl64(r24.u32 | (r24.u64 << 32), 3) & 0xFFFFFFF8;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// lhz r5,6(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// rlwinm r6,r5,24,24,31
	ctx.r6.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r6,r5,8,16,23
	ctx.r6.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r6.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// std r6,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r6.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,12(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 12, temp.u32);
	// lhz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// rlwinm r6,r5,24,24,31
	ctx.r6.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r6,r5,8,16,23
	ctx.r6.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r6.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// std r6,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r6.u64);
	// lfd f8,-144(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,8(r4)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// lhz r5,2(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// rlwinm r6,r5,24,24,31
	ctx.r6.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r6,r5,8,16,23
	ctx.r6.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r6.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r6,r6
	ctx.r6.s64 = ctx.r6.s16;
	// std r6,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r6.u64);
	// lfd f4,-144(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,4(r4)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// lhz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// rlwinm r9,r5,24,24,31
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r9,r5,8,16,23
	ctx.r9.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r9.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r9
	ctx.r5.s64 = ctx.r9.s16;
	// std r5,-144(r1)
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.r5.u64);
	// lfd f12,-144(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r4)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// b 0x82cef35c
	goto loc_82CEF35C;
loc_82CEF340:
	// lwz r9,-176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// cmpd cr6,r7,r9
	cr6.compare<int64_t>(ctx.r7.s64, ctx.r9.s64, xer);
	// ble cr6,0x82cef35c
	if (!cr6.gt) goto loc_82CEF35C;
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// rldicr r9,r9,32,31
	ctx.r9.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
loc_82CEF35C:
	// rldicr r9,r7,32,31
	ctx.r9.u64 = rotl64(ctx.r7.u64, 32) & 0xFFFFFFFF00000000;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r6,r9,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r9.s64;
	// std r6,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.r6.u64);
	// stvewx128 v63,r0,r20
	PPC_STORE_U32((r20.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v63.u32), 3 - ((r20.u32) & 0xF) >> 2));
	// lfd f0,-168(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lbz r5,0(r23)
	ctx.r5.u64 = PPC_LOAD_U8(r23.u32 + 0);
	// lwz r9,0(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + int32_t(0) );
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r8,r4,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r4.s64;
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// rotlwi r3,r5,1
	ctx.r3.u64 = rotl32(ctx.r5.u32, 1);
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// twllei r3,0
	// divwu r11,r11,r3
	r11.u32 = r11.u32 / ctx.r3.u32;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r21)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r21.u32 + 0, temp.u32);
	// blt cr6,0x82cef3b0
	if (cr6.lt) goto loc_82CEF3B0;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEF3B0:
	// lwz r9,-160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// lwz r8,-156(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-156) );
	// lwz r7,-152(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-152) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cef3e4
	if (!cr6.lt) goto loc_82CEF3E4;
	// lwz r10,-172(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-172) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c0c
	return;
loc_82CEF3E4:
	// lwz r11,-172(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-172) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82CEEED8) {
	__imp__sub_82CEEED8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEF3F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCVRegister v32{};
	PPCVRegister v33{};
	PPCVRegister v34{};
	PPCVRegister v35{};
	PPCVRegister v36{};
	PPCVRegister v37{};
	PPCVRegister v38{};
	PPCVRegister v39{};
	PPCVRegister v40{};
	PPCVRegister v41{};
	PPCVRegister v42{};
	PPCVRegister v43{};
	PPCVRegister v44{};
	PPCVRegister v45{};
	PPCVRegister v46{};
	PPCVRegister v47{};
	PPCVRegister v48{};
	PPCVRegister v49{};
	PPCVRegister v50{};
	PPCVRegister v51{};
	PPCVRegister v52{};
	PPCVRegister v53{};
	PPCVRegister v54{};
	PPCVRegister v55{};
	PPCVRegister v56{};
	PPCVRegister v57{};
	PPCVRegister v58{};
	PPCVRegister v59{};
	PPCVRegister v60{};
	PPCVRegister v61{};
	PPCVRegister v62{};
	PPCVRegister v63{};
	PPCRegister temp{};
	// mflr r12
	// bl 0x82ca2bb4
	// addi r9,r3,8
	ctx.r9.s64 = ctx.r3.s64 + 8;
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// addi r8,r3,28
	ctx.r8.s64 = ctx.r3.s64 + 28;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r7,r3,4
	ctx.r7.s64 = ctx.r3.s64 + 4;
	// stw r9,-164(r1)
	PPC_STORE_U32(ctx.r1.u32 + -164, ctx.r9.u32);
	// addi r6,r3,24
	ctx.r6.s64 = ctx.r3.s64 + 24;
	// stw r8,-180(r1)
	PPC_STORE_U32(ctx.r1.u32 + -180, ctx.r8.u32);
	// addi r11,r3,13
	r11.s64 = ctx.r3.s64 + 13;
	// stw r7,-176(r1)
	PPC_STORE_U32(ctx.r1.u32 + -176, ctx.r7.u32);
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r9,r3,20
	ctx.r9.s64 = ctx.r3.s64 + 20;
	// stw r6,-160(r1)
	PPC_STORE_U32(ctx.r1.u32 + -160, ctx.r6.u32);
	// mullw r4,r4,r5
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r31,0(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// lwz r6,0(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// lwz r30,0(r7)
	r30.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// stw r9,-168(r1)
	PPC_STORE_U32(ctx.r1.u32 + -168, ctx.r9.u32);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// rlwinm r11,r4,1,0,30
	r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r7,r5,r30
	ctx.r7.s64 = r30.s64 - ctx.r5.s64;
	// rlwinm r8,r31,2,0,29
	ctx.r8.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r4,r31,r6
	ctx.r4.s64 = ctx.r6.s64 - r31.s64;
	// stw r7,-184(r1)
	PPC_STORE_U32(ctx.r1.u32 + -184, ctx.r7.u32);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r4,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, ctx.r4.u32);
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// dcbt r0,r11
	// li r6,1
	ctx.r6.s64 = 1;
	// lfs f0,44(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	f0.f64 = double(temp.f32);
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// rldicr r8,r6,32,63
	ctx.r8.u64 = rotl64(ctx.r6.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// addi r7,r1,-192
	ctx.r7.s64 = ctx.r1.s64 + -192;
	// stw r9,-172(r1)
	PPC_STORE_U32(ctx.r1.u32 + -172, ctx.r9.u32);
	// std r8,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r8.u64);
	// lfd f13,-152(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// li r6,40
	ctx.r6.s64 = 40;
	// lfs f11,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fmul f10,f0,f12
	ctx.f10.f64 = f0.f64 * ctx.f12.f64;
	// fmul f9,f11,f12
	ctx.f9.f64 = ctx.f11.f64 * ctx.f12.f64;
	// lvlx128 v63,r0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v63,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lvlx128 v61,r3,r6
	temp.u32 = ctx.r3.u32 + ctx.r6.u32;
	simd::store_shuffled(v61,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r8,4
	ctx.r8.s64 = 4;
	// vcsxwfp128 v62,v63,0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v62.f32, simd::cvtepi32_f32(simd::load_i32(v63.s32)));
	// addi r6,r1,-152
	ctx.r6.s64 = ctx.r1.s64 + -152;
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// addi r31,r1,-192
	r31.s64 = ctx.r1.s64 + -192;
	// lfd f0,3248(r7)
	ctx.fpscr.disableFlushModeUnconditional();
	f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 3248);
	// li r24,0
	r24.s64 = 0;
	// li r25,4096
	r25.s64 = 4096;
	// fdiv f13,f0,f12
	ctx.f13.f64 = f0.f64 / ctx.f12.f64;
	// li r26,3072
	r26.s64 = 3072;
	// fctidz f8,f10
	ctx.f8.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f8,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.f8.u64);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f9.f64);
	// stfd f7,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.f7.u64);
	// ld r7,-152(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// lvlx128 v60,r0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// ld r29,-192(r1)
	r29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// vsubfp128 v59,v61,v60
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v59.f32, simd::sub_f32(simd::load_f32_aligned(v61.f32), simd::load_f32_aligned(v60.f32)));
	// std r29,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, r29.u64);
	// lvlx128 v56,r8,r6
	temp.u32 = ctx.r8.u32 + ctx.r6.u32;
	simd::store_shuffled(v56,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrefp128 v58,v62
	simd::store_f32(v58.f32, simd::reciprocal_f32(simd::load_f32(v62.f32)));
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// std r7,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r7.u64);
	// lvlx128 v55,r8,r31
	temp.u32 = ctx.r8.u32 + r31.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r27,2048
	r27.s64 = 2048;
	// stw r24,-192(r1)
	PPC_STORE_U32(ctx.r1.u32 + -192, r24.u32);
	// li r30,2
	r30.s64 = 2;
	// vspltw128 v62,v60,0
	simd::store_i32(v62.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// li r28,1024
	r28.s64 = 1024;
	// vspltw128 v13,v56,0
	simd::store_i32(ctx.v13.u32, simd::broadcast_lane_i32(simd::load_i32(v56.u32), 3));
	// addi r9,r9,26464
	ctx.r9.s64 = ctx.r9.s64 + 26464;
	// vspltw128 v0,v55,0
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(v55.u32), 3));
	// vmulfp128 v57,v59,v58
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::load_f32_aligned(v59.f32), simd::load_f32_aligned(v58.f32)));
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// vspltw128 v61,v57,0
	simd::store_i32(v61.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// bge cr6,0x82cef67c
	if (!cr6.lt) goto loc_82CEF67C;
	// li r6,8
	ctx.r6.s64 = 8;
loc_82CEF540:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// ble cr6,0x82cef67c
	if (!cr6.gt) goto loc_82CEF67C;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// li r31,10
	r31.s64 = 10;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// li r24,20
	r24.s64 = 20;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// li r23,5120
	r23.s64 = 5120;
	// vsubfp128 v54,v63,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v54.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// li r22,16
	r22.s64 = 16;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// lvlx128 v53,r11,r31
	temp.u32 = r11.u32 + r31.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v52,v53,v63,v7
	simd::store_i8(v52.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// li r31,12
	r31.s64 = 12;
	// lvlx128 v51,r5,r24
	temp.u32 = ctx.r5.u32 + r24.u32;
	simd::store_shuffled(v51,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r24,6
	r24.s64 = 6;
	// vspltw128 v50,v51,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v51.u32), 3));
	// add r7,r29,r7
	ctx.r7.u64 = r29.u64 + ctx.r7.u64;
	// vcuxwfp128 v49,v10,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// vcsxwfp128 v48,v52,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// cmpdi cr6,r7,0
	cr6.compare<int64_t>(ctx.r7.s64, 0, xer);
	// vsldoi128 v47,v62,v54,4
	simd::store_i8(v47.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v54.u8), 12));
	// vmulfp128 v46,v49,v47
	simd::store_f32_aligned(v46.f32, simd::mul_f32(simd::load_f32_aligned(v49.f32), simd::load_f32_aligned(v47.f32)));
	// vrlimi128 v50,v48,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vsldoi128 v45,v46,v62,8
	simd::store_i8(v45.u8, simd::shift_left_insert_bytes(simd::load_i8(v46.u8), simd::load_i8(v62.u8), 8));
	// vaddfp128 v62,v62,v61
	simd::store_f32_aligned(v62.f32, simd::add_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v61.f32)));
	// vmsum3fp128 v44,v50,v45
	simd::store_f32_aligned(v44.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v44,r10,r23
	PPC_STORE_U32((ctx.r10.u32 + r23.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v44.u32), 3 - ((ctx.r10.u32 + r23.u32) & 0xF) >> 2));
	// lvlx128 v43,r5,r22
	temp.u32 = ctx.r5.u32 + r22.u32;
	simd::store_shuffled(v43,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v42,r11,r6
	temp.u32 = r11.u32 + ctx.r6.u32;
	simd::store_shuffled(v42,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v41,v42,v63,v7
	simd::store_i8(v41.u8, simd::permute_bytes(simd::load_i8(v42.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v40,v41,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v41.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v39,v43,0
	simd::store_i32(v39.u32, simd::broadcast_lane_i32(simd::load_i32(v43.u32), 3));
	// vrlimi128 v39,v40,8,0
	simd::store_f32(v39.f32, simd::blend_f32<8>(simd::load_f32(v39.f32), simd::permute_f32<228>(simd::load_f32(v40.f32))));
	// vmsum3fp128 v38,v39,v45
	simd::store_f32_aligned(v38.f32, simd::dp_f32(simd::load_f32_aligned(v39.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v38,r10,r25
	PPC_STORE_U32((ctx.r10.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v38.u32), 3 - ((ctx.r10.u32 + r25.u32) & 0xF) >> 2));
	// lvlx128 v37,r5,r31
	temp.u32 = ctx.r5.u32 + r31.u32;
	simd::store_shuffled(v37,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v36,r11,r24
	temp.u32 = r11.u32 + r24.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v35,v36,v63,v7
	simd::store_i8(v35.u8, simd::permute_bytes(simd::load_i8(v36.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v34,v35,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v33,v37,0
	simd::store_i32(v33.u32, simd::broadcast_lane_i32(simd::load_i32(v37.u32), 3));
	// vrlimi128 v33,v34,8,0
	simd::store_f32(v33.f32, simd::blend_f32<8>(simd::load_f32(v33.f32), simd::permute_f32<228>(simd::load_f32(v34.f32))));
	// vmsum3fp128 v32,v33,v45
	simd::store_f32_aligned(v32.f32, simd::dp_f32(simd::load_f32_aligned(v33.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v32,r10,r26
	PPC_STORE_U32((ctx.r10.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v32.u32), 3 - ((ctx.r10.u32 + r26.u32) & 0xF) >> 2));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v60,r5,r6
	temp.u32 = ctx.r5.u32 + ctx.r6.u32;
	simd::store_shuffled(v60,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw128 v56,v60,0
	simd::store_i32(v56.u32, simd::broadcast_lane_i32(simd::load_i32(v60.u32), 3));
	// lvlx128 v59,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_shuffled(v59,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v58,v59,v63,v7
	simd::store_i8(v58.u8, simd::permute_bytes(simd::load_i8(v59.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v57,v58,31
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vrlimi128 v56,v57,8,0
	simd::store_f32(v56.f32, simd::blend_f32<8>(simd::load_f32(v56.f32), simd::permute_f32<228>(simd::load_f32(v57.f32))));
	// vmsum3fp128 v55,v56,v45
	simd::store_f32_aligned(v55.f32, simd::dp_f32(simd::load_f32_aligned(v56.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v55,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v55.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v54,r5,r8
	temp.u32 = ctx.r5.u32 + ctx.r8.u32;
	simd::store_shuffled(v54,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v53,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v52,v53,v63,v7
	simd::store_i8(v52.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v51,v52,31
	simd::store_f32_aligned(v51.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v52.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v50,v54,0
	simd::store_i32(v50.u32, simd::broadcast_lane_i32(simd::load_i32(v54.u32), 3));
	// vrlimi128 v50,v51,8,0
	simd::store_f32(v50.f32, simd::blend_f32<8>(simd::load_f32(v50.f32), simd::permute_f32<228>(simd::load_f32(v51.f32))));
	// vmsum3fp128 v49,v50,v45
	simd::store_f32_aligned(v49.f32, simd::dp_f32(simd::load_f32_aligned(v50.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v49,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v49.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v48,r0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v47,r0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(v47,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v46,v47,v63,v7
	simd::store_i8(v46.u8, simd::permute_bytes(simd::load_i8(v47.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v44,v46,31
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v43,v48,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v48.u32), 3));
	// vrlimi128 v43,v44,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v42,v43,v45
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v45.f32), 0xEF));
	// stvewx128 v42,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// lwz r24,-192(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-192) );
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x82cef540
	if (cr6.lt) goto loc_82CEF540;
loc_82CEF67C:
	// lwz r8,-184(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-184) );
	// sradi r6,r7,63
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0x7FFFFFFFFFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s64 >> 63;
	// sradi r31,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	r31.s64 = ctx.r7.s64 >> 32;
	// addi r22,r8,-1
	r22.s64 = ctx.r8.s64 + -1;
	// subf r8,r6,r31
	ctx.r8.s64 = r31.s64 - ctx.r6.s64;
	// extsw r23,r22
	r23.s64 = r22.s32;
	// cmpd cr6,r8,r23
	cr6.compare<int64_t>(ctx.r8.s64, r23.s64, xer);
	// bge cr6,0x82cef894
	if (!cr6.lt) goto loc_82CEF894;
loc_82CEF69C:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82cef9d0
	if (cr6.eq) goto loc_82CEF9D0;
	// extsw r31,r8
	r31.s64 = ctx.r8.s32;
	// vspltisw128 v63,0
	simd::store_i32(v63.u32, simd::set1_i32(int32_t(0x0)));
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vspltisb v12,1
	simd::store_i8(ctx.v12.u8, simd::set1_i8(int8_t(0x1)));
	// addi r6,r31,1
	ctx.r6.s64 = r31.s64 + 1;
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// add r8,r31,r8
	ctx.r8.u64 = r31.u64 + ctx.r8.u64;
	// vadduwm v11,v0,v13
	simd::store_u32(ctx.v11.u32, simd::add_u32(simd::load_u32(ctx.v0.u32), simd::load_u32(ctx.v13.u32)));
	// rlwinm r31,r6,1,0,30
	r31.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// vsubfp128 v41,v63,v62
	ctx.fpscr.enableFlushMode();
	simd::store_f32_aligned(v41.f32, simd::sub_f32(simd::load_f32_aligned(v63.f32), simd::load_f32_aligned(v62.f32)));
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vsr v10,v0,v12
	simd::store_i8(ctx.v10.u8, simd::vsr(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// add r6,r6,r31
	ctx.r6.u64 = ctx.r6.u64 + r31.u64;
	// addi r31,r8,5
	r31.s64 = ctx.r8.s64 + 5;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r31,1,0,30
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// vcuxwfp128 v40,v10,31
	simd::store_f32_aligned(v40.f32, simd::mul_f32(simd::cvtepu32_f32(simd::load_u32(ctx.v10.u32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// addi r21,r6,5
	r21.s64 = ctx.r6.s64 + 5;
	// li r20,5120
	r20.s64 = 5120;
	// rlwinm r21,r21,1,0,30
	r21.u64 = rotl64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r19,r8,4
	r19.s64 = ctx.r8.s64 + 4;
	// lvlx128 v39,r31,r11
	temp.u32 = r31.u32 + r11.u32;
	simd::store_shuffled(v39,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r31,r6,4
	r31.s64 = ctx.r6.s64 + 4;
	// vperm128 v38,v39,v63,v7
	simd::store_i8(v38.u8, simd::permute_bytes(simd::load_i8(v39.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// rlwinm r19,r19,1,0,30
	r19.u64 = rotl64(r19.u32 | (r19.u64 << 32), 1) & 0xFFFFFFFE;
	// vsldoi128 v37,v62,v41,4
	simd::store_i8(v37.u8, simd::shift_left_insert_bytes(simd::load_i8(v62.u8), simd::load_i8(v41.u8), 12));
	// rlwinm r18,r31,1,0,30
	r18.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lvlx128 v36,r21,r11
	temp.u32 = r21.u32 + r11.u32;
	simd::store_shuffled(v36,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r31,r6,3
	r31.s64 = ctx.r6.s64 + 3;
	// vperm128 v35,v36,v63,v7
	simd::store_i8(v35.u8, simd::permute_bytes(simd::load_i8(v36.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// addi r21,r8,3
	r21.s64 = ctx.r8.s64 + 3;
	// vcsxwfp128 v34,v38,31
	simd::store_f32_aligned(v34.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v38.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// rlwinm r17,r31,1,0,30
	r17.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r21,r21,1,0,30
	r21.u64 = rotl64(r21.u32 | (r21.u64 << 32), 1) & 0xFFFFFFFE;
	// vmulfp128 v33,v40,v37
	simd::store_f32_aligned(v33.f32, simd::mul_f32(simd::load_f32_aligned(v40.f32), simd::load_f32_aligned(v37.f32)));
	// addi r31,r6,2
	r31.s64 = ctx.r6.s64 + 2;
	// vcsxwfp128 v32,v35,31
	simd::store_f32_aligned(v32.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v35.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// addi r16,r8,2
	r16.s64 = ctx.r8.s64 + 2;
	// rlwinm r15,r31,1,0,30
	r15.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r16,r16,1,0,30
	r16.u64 = rotl64(r16.u32 | (r16.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r8,1,0,30
	r31.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r6,r11
	ctx.r8.u64 = ctx.r6.u64 + r11.u64;
	// add r6,r31,r11
	ctx.r6.u64 = r31.u64 + r11.u64;
	// vspltw128 v60,v34,0
	simd::store_i32(v60.u32, simd::broadcast_lane_i32(simd::load_i32(v34.u32), 3));
	// vsldoi128 v59,v33,v62,8
	simd::store_i8(v59.u8, simd::shift_left_insert_bytes(simd::load_i8(v33.u8), simd::load_i8(v62.u8), 8));
	// vaddfp128 v62,v62,v61
	simd::store_f32_aligned(v62.f32, simd::add_f32(simd::load_f32_aligned(v62.f32), simd::load_f32_aligned(v61.f32)));
	// vrlimi128 v60,v32,8,0
	simd::store_f32(v60.f32, simd::blend_f32<8>(simd::load_f32(v60.f32), simd::permute_f32<228>(simd::load_f32(v32.f32))));
	// vmsum3fp128 v58,v60,v59
	simd::store_f32_aligned(v58.f32, simd::dp_f32(simd::load_f32_aligned(v60.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v58,r10,r20
	PPC_STORE_U32((ctx.r10.u32 + r20.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v58.u32), 3 - ((ctx.r10.u32 + r20.u32) & 0xF) >> 2));
	// lvlx128 v55,r18,r11
	temp.u32 = r18.u32 + r11.u32;
	simd::store_shuffled(v55,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v57,r19,r11
	temp.u32 = r19.u32 + r11.u32;
	simd::store_shuffled(v57,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v56,v57,v63,v7
	simd::store_i8(v56.u8, simd::permute_bytes(simd::load_i8(v57.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v54,v55,v63,v7
	simd::store_i8(v54.u8, simd::permute_bytes(simd::load_i8(v55.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v53,v56,31
	simd::store_f32_aligned(v53.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v56.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v52,v54,31
	simd::store_f32_aligned(v52.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v54.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v51,v53,0
	simd::store_i32(v51.u32, simd::broadcast_lane_i32(simd::load_i32(v53.u32), 3));
	// vrlimi128 v51,v52,8,0
	simd::store_f32(v51.f32, simd::blend_f32<8>(simd::load_f32(v51.f32), simd::permute_f32<228>(simd::load_f32(v52.f32))));
	// vmsum3fp128 v50,v51,v59
	simd::store_f32_aligned(v50.f32, simd::dp_f32(simd::load_f32_aligned(v51.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v50,r10,r25
	PPC_STORE_U32((ctx.r10.u32 + r25.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v50.u32), 3 - ((ctx.r10.u32 + r25.u32) & 0xF) >> 2));
	// lvlx128 v49,r17,r11
	temp.u32 = r17.u32 + r11.u32;
	simd::store_shuffled(v49,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v48,r21,r11
	temp.u32 = r21.u32 + r11.u32;
	simd::store_shuffled(v48,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v47,v48,v63,v7
	simd::store_i8(v47.u8, simd::permute_bytes(simd::load_i8(v48.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v46,v49,v63,v7
	simd::store_i8(v46.u8, simd::permute_bytes(simd::load_i8(v49.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v45,v47,31
	simd::store_f32_aligned(v45.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v47.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v44,v46,31
	simd::store_f32_aligned(v44.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v46.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v43,v45,0
	simd::store_i32(v43.u32, simd::broadcast_lane_i32(simd::load_i32(v45.u32), 3));
	// vrlimi128 v43,v44,8,0
	simd::store_f32(v43.f32, simd::blend_f32<8>(simd::load_f32(v43.f32), simd::permute_f32<228>(simd::load_f32(v44.f32))));
	// vmsum3fp128 v42,v43,v59
	simd::store_f32_aligned(v42.f32, simd::dp_f32(simd::load_f32_aligned(v43.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v42,r10,r26
	PPC_STORE_U32((ctx.r10.u32 + r26.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v42.u32), 3 - ((ctx.r10.u32 + r26.u32) & 0xF) >> 2));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// lvlx128 v41,r15,r11
	temp.u32 = r15.u32 + r11.u32;
	simd::store_shuffled(v41,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v40,r16,r11
	temp.u32 = r16.u32 + r11.u32;
	simd::store_shuffled(v40,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vperm128 v39,v40,v63,v7
	simd::store_i8(v39.u8, simd::permute_bytes(simd::load_i8(v40.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v38,v39,31
	simd::store_f32_aligned(v38.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v39.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vperm128 v37,v41,v63,v7
	simd::store_i8(v37.u8, simd::permute_bytes(simd::load_i8(v41.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v36,v37,31
	simd::store_f32_aligned(v36.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v37.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v35,v38,0
	simd::store_i32(v35.u32, simd::broadcast_lane_i32(simd::load_i32(v38.u32), 3));
	// vrlimi128 v35,v36,8,0
	simd::store_f32(v35.f32, simd::blend_f32<8>(simd::load_f32(v35.f32), simd::permute_f32<228>(simd::load_f32(v36.f32))));
	// vmsum3fp128 v34,v35,v59
	simd::store_f32_aligned(v34.f32, simd::dp_f32(simd::load_f32_aligned(v35.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// add r7,r29,r7
	ctx.r7.u64 = r29.u64 + ctx.r7.u64;
	// vor v0,v11,v11
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v11.u8));
	// addi r4,r4,-1
	ctx.r4.s64 = ctx.r4.s64 + -1;
	// stvewx128 v34,r10,r27
	PPC_STORE_U32((ctx.r10.u32 + r27.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v34.u32), 3 - ((ctx.r10.u32 + r27.u32) & 0xF) >> 2));
	// lvlx128 v33,r8,r30
	temp.u32 = ctx.r8.u32 + r30.u32;
	simd::store_shuffled(v33,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx128 v32,r6,r30
	temp.u32 = ctx.r6.u32 + r30.u32;
	simd::store_shuffled(v32,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// vperm128 v60,v32,v63,v7
	simd::store_i8(v60.u8, simd::permute_bytes(simd::load_i8(v32.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v58,v33,v63,v7
	simd::store_i8(v58.u8, simd::permute_bytes(simd::load_i8(v33.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v57,v60,31
	simd::store_f32_aligned(v57.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v60.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vcsxwfp128 v56,v58,31
	simd::store_f32_aligned(v56.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v58.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// vspltw128 v55,v57,0
	simd::store_i32(v55.u32, simd::broadcast_lane_i32(simd::load_i32(v57.u32), 3));
	// vrlimi128 v55,v56,8,0
	simd::store_f32(v55.f32, simd::blend_f32<8>(simd::load_f32(v55.f32), simd::permute_f32<228>(simd::load_f32(v56.f32))));
	// vmsum3fp128 v54,v55,v59
	simd::store_f32_aligned(v54.f32, simd::dp_f32(simd::load_f32_aligned(v55.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v54,r10,r28
	PPC_STORE_U32((ctx.r10.u32 + r28.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v54.u32), 3 - ((ctx.r10.u32 + r28.u32) & 0xF) >> 2));
	// lvlx128 v53,r0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v53,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// sradi r8,r7,32
	xer.ca = (ctx.r7.s64 < 0) & ((ctx.r7.u64 & 0xFFFFFFFF) != 0);
	ctx.r8.s64 = ctx.r7.s64 >> 32;
	// lvlx128 v52,r0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(v52,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v7,r0,r9
	simd::store_shuffled(ctx.v7, simd::load_and_shuffle(base + ((ctx.r9.u32) & ~0xF), VectorMaskL));
	// vperm128 v51,v52,v63,v7
	simd::store_i8(v51.u8, simd::permute_bytes(simd::load_i8(v52.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vperm128 v50,v53,v63,v7
	simd::store_i8(v50.u8, simd::permute_bytes(simd::load_i8(v53.u8), simd::load_i8(v63.u8), simd::load_i8(ctx.v7.u8)));
	// vcsxwfp128 v49,v51,31
	simd::store_f32_aligned(v49.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v51.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// extsw r31,r8
	r31.s64 = ctx.r8.s32;
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// vcsxwfp128 v48,v50,31
	simd::store_f32_aligned(v48.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(v50.s32)), simd::bitcast_f32(simd::set1_i32(0x30000000))));
	// add r6,r31,r6
	ctx.r6.u64 = r31.u64 + ctx.r6.u64;
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + r11.u64;
	// xor r31,r6,r24
	r31.u64 = ctx.r6.u64 ^ r24.u64;
	// vspltw128 v47,v49,0
	simd::store_i32(v47.u32, simd::broadcast_lane_i32(simd::load_i32(v49.u32), 3));
	// rlwinm r31,r31,0,0,24
	r31.u64 = rotl64(r31.u32 | (r31.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// vrlimi128 v47,v48,8,0
	simd::store_f32(v47.f32, simd::blend_f32<8>(simd::load_f32(v47.f32), simd::permute_f32<228>(simd::load_f32(v48.f32))));
	// vmsum3fp128 v46,v47,v59
	simd::store_f32_aligned(v46.f32, simd::dp_f32(simd::load_f32_aligned(v47.f32), simd::load_f32_aligned(v59.f32), 0xEF));
	// stvewx128 v46,r0,r10
	PPC_STORE_U32((ctx.r10.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v46.u32), 3 - ((ctx.r10.u32) & 0xF) >> 2));
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// beq cr6,0x82cef888
	if (cr6.eq) goto loc_82CEF888;
	// li r31,128
	r31.s64 = 128;
	// dcbt r31,r6
loc_82CEF888:
	// mr r24,r6
	r24.u64 = ctx.r6.u64;
	// cmpd cr6,r8,r23
	cr6.compare<int64_t>(ctx.r8.s64, r23.s64, xer);
	// blt cr6,0x82cef69c
	if (cr6.lt) goto loc_82CEF69C;
loc_82CEF894:
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82cef9d0
	if (cr6.eq) goto loc_82CEF9D0;
	// cmpd cr6,r8,r23
	cr6.compare<int64_t>(ctx.r8.s64, r23.s64, xer);
	// bne cr6,0x82cef9b4
	if (!cr6.eq) goto loc_82CEF9B4;
	// rlwinm r9,r22,1,0,30
	ctx.r9.u64 = rotl64(r22.u32 | (r22.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// add r4,r22,r9
	ctx.r4.u64 = r22.u64 + ctx.r9.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// lfs f0,3152(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3152);
	f0.f64 = double(temp.f32);
	// lhz r6,10(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 10);
	// rlwinm r4,r6,24,24,31
	ctx.r4.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r6,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f12,-152(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,20(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 20, temp.u32);
	// lhz r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// rlwinm r4,r6,24,24,31
	ctx.r4.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r6,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f8,-152(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,16(r5)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r5.u32 + 16, temp.u32);
	// lhz r6,6(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 6);
	// rlwinm r4,r6,24,24,31
	ctx.r4.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r6,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f4,-152(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,12(r5)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + 12, temp.u32);
	// lhz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 4);
	// rlwinm r4,r6,24,24,31
	ctx.r4.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r6,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f12,-152(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,8(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 8, temp.u32);
	// lhz r6,2(r9)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r9.u32 + 2);
	// rlwinm r4,r6,24,24,31
	ctx.r4.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r6,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r4,r4
	ctx.r4.s64 = ctx.r4.s16;
	// std r4,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r4.u64);
	// lfd f8,-152(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,4(r5)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// lhz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 0);
	// rlwinm r6,r9,24,24,31
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFF;
	// rlwimi r6,r9,8,16,23
	ctx.r6.u64 = (rotl32(ctx.r9.u32, 8) & 0xFF00) | (ctx.r6.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r9,r6
	ctx.r9.s64 = ctx.r6.s16;
	// std r9,-152(r1)
	PPC_STORE_U64(ctx.r1.u32 + -152, ctx.r9.u64);
	// lfd f4,-152(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -152);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,0(r5)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// b 0x82cef9d0
	goto loc_82CEF9D0;
loc_82CEF9B4:
	// lwz r9,-184(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-184) );
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// cmpd cr6,r8,r9
	cr6.compare<int64_t>(ctx.r8.s64, ctx.r9.s64, xer);
	// ble cr6,0x82cef9d0
	if (!cr6.gt) goto loc_82CEF9D0;
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rldicr r9,r9,32,31
	ctx.r9.u64 = rotl64(ctx.r9.u64, 32) & 0xFFFFFFFF00000000;
	// add r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
loc_82CEF9D0:
	// rldicr r6,r8,32,31
	ctx.r6.u64 = rotl64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000;
	// lwz r5,-176(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-176) );
	// addi r9,r3,36
	ctx.r9.s64 = ctx.r3.s64 + 36;
	// lwz r4,-172(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-172) );
	// subf r7,r6,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r6.s64;
	// rotlwi r6,r8,0
	ctx.r6.u64 = rotl32(ctx.r8.u32, 0);
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// std r7,-192(r1)
	PPC_STORE_U64(ctx.r1.u32 + -192, ctx.r7.u64);
	// stvewx128 v62,r0,r9
	PPC_STORE_U32((ctx.r9.u32) & ~0x3, simd::extract_u32(*reinterpret_cast<const simd::vec128i*>(&v62.u32), 3 - ((ctx.r9.u32) & 0xF) >> 2));
	// add r6,r6,r8
	ctx.r6.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lfd f0,-192(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -192);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// rlwinm r6,r6,2,0,29
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// fmul f11,f12,f13
	ctx.f11.f64 = ctx.f12.f64 * ctx.f13.f64;
	// addi r9,r3,13
	ctx.r9.s64 = ctx.r3.s64 + 13;
	// subf r9,r8,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r8.s64;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + r11.u64;
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// rotlwi r9,r7,1
	ctx.r9.u64 = rotl32(ctx.r7.u32, 1);
	// twllei r9,0
	// divwu r9,r3,r9
	ctx.r9.u32 = ctx.r3.u32 / ctx.r9.u32;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r4)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r4.u32 + 0, temp.u32);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x82cefa40
	if (!cr6.lt) goto loc_82CEFA40;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82CEFA40:
	// lwz r9,-168(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-168) );
	// lwz r8,-164(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-164) );
	// lwz r7,-160(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-160) );
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// subf r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	// lwz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// rlwinm r11,r5,30,2,31
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cefa74
	if (!cr6.lt) goto loc_82CEFA74;
	// lwz r10,-180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-180) );
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x82ca2c04
	return;
loc_82CEFA74:
	// lwz r11,-180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-180) );
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// b 0x82ca2c04
	return;
}

PPC_WEAK_FUNC(sub_82CEF3F0) {
	__imp__sub_82CEF3F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEFA80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r8,r5,r11
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r4,r31
	r11.s64 = r31.s64 - ctx.r4.s64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// blt cr6,0x82cefad0
	if (cr6.lt) goto loc_82CEFAD0;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_82CEFAD0:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CEFAF4:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82cefb24
	if (cr6.eq) goto loc_82CEFB24;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_82CEFB08:
	// lfs f12,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f11,0(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r9,r9,1024
	ctx.r9.s64 = ctx.r9.s64 + 1024;
	// bne 0x82cefb08
	if (!cr0.eq) goto loc_82CEFB08;
loc_82CEFB24:
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// fadds f0,f13,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82cefaf4
	if (!cr0.eq) goto loc_82CEFAF4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rotlwi r8,r10,2
	ctx.r8.u64 = rotl32(ctx.r10.u32, 2);
	// divwu r10,r9,r8
	ctx.r10.u32 = ctx.r9.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cefb60
	if (cr6.lt) goto loc_82CEFB60;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CEFB60:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cefb8c
	if (!cr6.lt) goto loc_82CEFB8C;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CEFB8C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CEFA80) {
	__imp__sub_82CEFA80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEFB98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r4,r10,r11
	ctx.r4.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// blt cr6,0x82cefbe8
	if (cr6.lt) goto loc_82CEFBE8;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CEFBE8:
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,0
	r11.s64 = 0;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82cefc14
	if (cr6.eq) goto loc_82CEFC14;
loc_82CEFC00:
	// rlwinm r5,r11,7,0,24
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r10
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x82cefc00
	if (cr6.lt) goto loc_82CEFC00;
loc_82CEFC14:
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CEFC34:
	// lfs f12,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f11,0(r7)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82cefc34
	if (!cr0.eq) goto loc_82CEFC34;
	// lbz r11,13(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r8,r11,2
	ctx.r8.u64 = rotl32(r11.u32, 2);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r10,r6,r8
	ctx.r10.u32 = ctx.r6.u32 / ctx.r8.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cefc7c
	if (cr6.lt) goto loc_82CEFC7C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CEFC7C:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cefca8
	if (!cr6.lt) goto loc_82CEFCA8;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CEFCA8:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CEFB98) {
	__imp__sub_82CEFB98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEFCB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r4,r9,r10
	ctx.r4.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r4,2,0,29
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82cefd08
	if (cr6.lt) goto loc_82CEFD08;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CEFD08:
	// rlwinm r7,r9,3,0,28
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cefd34
	if (cr6.eq) goto loc_82CEFD34;
loc_82CEFD20:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cefd20
	if (cr6.lt) goto loc_82CEFD20;
loc_82CEFD34:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CEFD54:
	// lfs f12,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f11,1024(r8)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// bne 0x82cefd54
	if (!cr0.eq) goto loc_82CEFD54;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,2
	ctx.r7.u64 = rotl32(ctx.r10.u32, 2);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r11
	ctx.r6.s64 = r11.s64 - ctx.r9.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cefda8
	if (!cr6.lt) goto loc_82CEFDA8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CEFDA8:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cefdd4
	if (!cr6.lt) goto loc_82CEFDD4;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CEFDD4:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CEFCB8) {
	__imp__sub_82CEFCB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEFDE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r5,r9,r10
	ctx.r5.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r9,r7,r8
	ctx.r9.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// blt cr6,0x82cefe30
	if (cr6.lt) goto loc_82CEFE30;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CEFE30:
	// rlwinm r7,r8,4,0,27
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cefe5c
	if (cr6.eq) goto loc_82CEFE5C;
loc_82CEFE48:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cefe48
	if (cr6.lt) goto loc_82CEFE48;
loc_82CEFE5C:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CEFE7C:
	// lfs f12,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f11,3072(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 3072, temp.u32);
	// lfs f10,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,2048(r9)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2048, temp.u32);
	// lfs f8,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,1024(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// lfs f6,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,0(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// bne 0x82cefe7c
	if (!cr0.eq) goto loc_82CEFE7C;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,2
	ctx.r7.u64 = rotl32(ctx.r10.u32, 2);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cefee8
	if (!cr6.lt) goto loc_82CEFEE8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CEFEE8:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82ceff14
	if (!cr6.lt) goto loc_82CEFF14;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CEFF14:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CEFDE0) {
	__imp__sub_82CEFDE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CEFF20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r5,r7,r8
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r8.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// blt cr6,0x82ceff6c
	if (cr6.lt) goto loc_82CEFF6C;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CEFF6C:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ceffa0
	if (cr6.eq) goto loc_82CEFFA0;
loc_82CEFF8C:
	// rlwinm r5,r9,7,0,24
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x82ceff8c
	if (cr6.lt) goto loc_82CEFF8C;
loc_82CEFFA0:
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r9,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r9.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fdivs f13,f12,f9
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CEFFC0:
	// lfs f12,20(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f11,5120(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lfs f10,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,4096(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lfs f8,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,3072(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lfs f6,8(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,2048(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lfs f4,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,1024(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lfs f2,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,0(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// bne 0x82ceffc0
	if (!cr0.eq) goto loc_82CEFFC0;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r9,2
	ctx.r7.u64 = rotl32(ctx.r9.u32, 2);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cf0044
	if (!cr6.lt) goto loc_82CF0044;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CF0044:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf0070
	if (!cr6.lt) goto loc_82CF0070;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF0070:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CEFF20) {
	__imp__sub_82CEFF20(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0080) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r8,r5,r11
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r4,r31
	r11.s64 = r31.s64 - ctx.r4.s64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// blt cr6,0x82cf00d0
	if (cr6.lt) goto loc_82CF00D0;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_82CF00D0:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF00FC:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82cf0144
	if (cr6.eq) goto loc_82CF0144;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_82CF0110:
	// lhz r31,0(r10)
	r31.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r31,r31
	r31.s64 = r31.s16;
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// lfd f11,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,0(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,1024
	ctx.r9.s64 = ctx.r9.s64 + 1024;
	// bne 0x82cf0110
	if (!cr0.eq) goto loc_82CF0110;
loc_82CF0144:
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// fadds f0,f12,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82cf00fc
	if (!cr0.eq) goto loc_82CF00FC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rotlwi r8,r10,1
	ctx.r8.u64 = rotl32(ctx.r10.u32, 1);
	// divwu r10,r9,r8
	ctx.r10.u32 = ctx.r9.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf0180
	if (cr6.lt) goto loc_82CF0180;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF0180:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf01ac
	if (!cr6.lt) goto loc_82CF01AC;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF01AC:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0080) {
	__imp__sub_82CF0080(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF01B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r4,r10,r11
	ctx.r4.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// blt cr6,0x82cf0208
	if (cr6.lt) goto loc_82CF0208;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CF0208:
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r11,0
	r11.s64 = 0;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82cf0234
	if (cr6.eq) goto loc_82CF0234;
loc_82CF0220:
	// rlwinm r5,r11,7,0,24
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r10
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x82cf0220
	if (cr6.lt) goto loc_82CF0220;
loc_82CF0234:
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF025C:
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// extsh r6,r11
	ctx.r6.s64 = r11.s16;
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f11,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,0(r7)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf025c
	if (!cr0.eq) goto loc_82CF025C;
	// lbz r11,13(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r10,r6,r8
	ctx.r10.u32 = ctx.r6.u32 / ctx.r8.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf02bc
	if (cr6.lt) goto loc_82CF02BC;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF02BC:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf02e8
	if (!cr6.lt) goto loc_82CF02E8;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF02E8:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF01B8) {
	__imp__sub_82CF01B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF02F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r4,r9,r10
	ctx.r4.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82cf0348
	if (cr6.lt) goto loc_82CF0348;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CF0348:
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf0374
	if (cr6.eq) goto loc_82CF0374;
loc_82CF0360:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf0360
	if (cr6.lt) goto loc_82CF0360;
loc_82CF0374:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF039C:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// std r6,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r6.u64);
	// lfd f11,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,1024(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lhz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// std r10,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r10.u64);
	// lfd f6,-24(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f2,0(r8)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf039c
	if (!cr0.eq) goto loc_82CF039C;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,1
	ctx.r7.u64 = rotl32(ctx.r10.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r11
	ctx.r6.s64 = r11.s64 - ctx.r9.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cf0420
	if (!cr6.lt) goto loc_82CF0420;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF0420:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf044c
	if (!cr6.lt) goto loc_82CF044C;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF044C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF02F8) {
	__imp__sub_82CF02F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0458) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r5,r9,r10
	ctx.r5.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r9,r7,r8
	ctx.r9.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// blt cr6,0x82cf04a8
	if (cr6.lt) goto loc_82CF04A8;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CF04A8:
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf04d4
	if (cr6.eq) goto loc_82CF04D4;
loc_82CF04C0:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf04c0
	if (cr6.lt) goto loc_82CF04C0;
loc_82CF04D4:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r10,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r10.u64);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF04FC:
	// lhz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// std r6,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r6.u64);
	// lfd f11,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,3072(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 3072, temp.u32);
	// lhz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// std r10,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r10.u64);
	// lfd f6,-40(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f2,2048(r9)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2048, temp.u32);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r5.u64);
	// lfd f1,-32(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * f0.f64));
	// stfs f8,1024(r9)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// std r7,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r7.u64);
	// lfd f7,-24(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,0(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf04fc
	if (!cr0.eq) goto loc_82CF04FC;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,1
	ctx.r7.u64 = rotl32(ctx.r10.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cf05c8
	if (!cr6.lt) goto loc_82CF05C8;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF05C8:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf05f4
	if (!cr6.lt) goto loc_82CF05F4;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF05F4:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0458) {
	__imp__sub_82CF0458(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0600) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r5,r7,r8
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r8.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// blt cr6,0x82cf064c
	if (cr6.lt) goto loc_82CF064C;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CF064C:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf0680
	if (cr6.eq) goto loc_82CF0680;
loc_82CF066C:
	// rlwinm r5,r9,7,0,24
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf066c
	if (cr6.lt) goto loc_82CF066C;
loc_82CF0680:
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r9,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r9.u64);
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF06A8:
	// lhz r9,10(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// std r6,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r6.u64);
	// lfd f11,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,5120(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lhz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// std r9,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r9.u64);
	// lfd f6,-56(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f2,4096(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lhz r7,6(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r5.u64);
	// lfd f1,-48(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * f0.f64));
	// stfs f8,3072(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// std r7,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r7.u64);
	// lfd f7,-40(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,2048(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lhz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r4.u64);
	// lfd f2,-32(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f11,f1
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,1024(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// std r6,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r6.u64);
	// lfd f8,-24(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * f0.f64));
	// stfs f4,0(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf06a8
	if (!cr0.eq) goto loc_82CF06A8;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r9,1
	ctx.r7.u64 = rotl32(ctx.r9.u32, 1);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cf07bc
	if (!cr6.lt) goto loc_82CF07BC;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CF07BC:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf07e8
	if (!cr6.lt) goto loc_82CF07E8;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF07E8:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0600) {
	__imp__sub_82CF0600(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF07F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r7,r9,r11
	ctx.r7.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r11,r11,r4
	r11.s64 = ctx.r4.s64 - r11.s64;
	// subf r10,r10,r31
	ctx.r10.s64 = r31.s64 - ctx.r10.s64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// blt cr6,0x82cf0844
	if (cr6.lt) goto loc_82CF0844;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
loc_82CF0844:
	// extsw r11,r10
	r11.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r11,-32255
	r11.s64 = -2113863680;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,26484(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 26484);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,26480(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 26480);
	ctx.f12.f64 = double(temp.f32);
loc_82CF0874:
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82cf08b8
	if (cr6.eq) goto loc_82CF08B8;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_82CF0884:
	// lbzx r4,r11,r8
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + ctx.r8.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f10,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = static_cast<float>(ctx.f8.f64 - ctx.f13.f64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,0(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// blt cr6,0x82cf0884
	if (cr6.lt) goto loc_82CF0884;
loc_82CF08B8:
	// addic. r7,r7,-1
	xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// fadds f0,f11,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bne 0x82cf0874
	if (!cr0.eq) goto loc_82CF0874;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// twllei r10,0
	// divwu r10,r9,r10
	ctx.r10.u32 = ctx.r9.u32 / ctx.r10.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf08f0
	if (cr6.lt) goto loc_82CF08F0;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF08F0:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r6
	ctx.r10.s64 = ctx.r6.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf091c
	if (!cr6.lt) goto loc_82CF091C;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF091C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF07F8) {
	__imp__sub_82CF07F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0928) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r9,r5,r10
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82cf0974
	if (cr6.lt) goto loc_82CF0974;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CF0974:
	// addi r8,r9,127
	ctx.r8.s64 = ctx.r9.s64 + 127;
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82cf099c
	if (cr6.eq) goto loc_82CF099C;
loc_82CF0988:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// blt cr6,0x82cf0988
	if (cr6.lt) goto loc_82CF0988;
loc_82CF099C:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,26484(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 26484);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,26480(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 26480);
	ctx.f12.f64 = double(temp.f32);
loc_82CF09CC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// std r8,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r8.u64);
	// lfd f10,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = static_cast<float>(ctx.f8.f64 - ctx.f13.f64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,0(r7)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// bne 0x82cf09cc
	if (!cr0.eq) goto loc_82CF09CC;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// twllei r9,0
	// divwu r10,r8,r9
	ctx.r10.u32 = ctx.r8.u32 / ctx.r9.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf0a28
	if (cr6.lt) goto loc_82CF0A28;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF0A28:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf0a54
	if (!cr6.lt) goto loc_82CF0A54;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF0A54:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0928) {
	__imp__sub_82CF0928(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0A60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// mullw r9,r5,r10
	ctx.r9.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r10.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82cf0aac
	if (cr6.lt) goto loc_82CF0AAC;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CF0AAC:
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf0ad8
	if (cr6.eq) goto loc_82CF0AD8;
loc_82CF0AC4:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf0ac4
	if (cr6.lt) goto loc_82CF0AC4;
loc_82CF0AD8:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,26484(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 26484);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,26480(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 26480);
	ctx.f12.f64 = double(temp.f32);
loc_82CF0B08:
	// lbz r7,1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// std r7,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r7.u64);
	// lfd f10,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = static_cast<float>(ctx.f8.f64 - ctx.f13.f64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,1024(r8)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lbz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// std r5,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r5.u64);
	// lfd f4,-24(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f13
	ctx.f1.f64 = static_cast<float>(ctx.f2.f64 - ctx.f13.f64);
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,0(r8)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// bne 0x82cf0b08
	if (!cr0.eq) goto loc_82CF0B08;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r10,r11
	ctx.r7.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// twllei r9,0
	// divwu r10,r7,r9
	ctx.r10.u32 = ctx.r7.u32 / ctx.r9.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf0b88
	if (cr6.lt) goto loc_82CF0B88;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF0B88:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf0bb4
	if (!cr6.lt) goto loc_82CF0BB4;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF0BB4:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0A60) {
	__imp__sub_82CF0A60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0BC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r4,24(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r6,r6,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r9,r7,r8
	ctx.r9.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// blt cr6,0x82cf0c08
	if (cr6.lt) goto loc_82CF0C08;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CF0C08:
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf0c34
	if (cr6.eq) goto loc_82CF0C34;
loc_82CF0C20:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf0c20
	if (cr6.lt) goto loc_82CF0C20;
loc_82CF0C34:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32255
	ctx.r10.s64 = -2113863680;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,26484(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 26484);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,26480(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 26480);
	ctx.f12.f64 = double(temp.f32);
loc_82CF0C64:
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// std r7,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r7.u64);
	// lfd f10,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = static_cast<float>(ctx.f8.f64 - ctx.f13.f64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,3072(r9)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r9.u32 + 3072, temp.u32);
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// std r5,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r5.u64);
	// lfd f4,-24(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f13
	ctx.f1.f64 = static_cast<float>(ctx.f2.f64 - ctx.f13.f64);
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,2048(r9)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2048, temp.u32);
	// lbz r10,1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// std r10,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r10.u64);
	// lfd f8,-16(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = static_cast<float>(ctx.f6.f64 - ctx.f13.f64);
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,1024(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// lbz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// std r6,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r6.u64);
	// lfd f2,-8(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f10,f1
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// fsubs f9,f10,f13
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fmuls f8,f9,f12
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,0(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// bne 0x82cf0c64
	if (!cr0.eq) goto loc_82CF0C64;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r10,r11
	ctx.r7.s64 = r11.s64 - ctx.r10.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// twllei r8,0
	// divwu r10,r7,r8
	ctx.r10.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf0d2c
	if (cr6.lt) goto loc_82CF0D2C;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF0D2C:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf0d54
	if (!cr6.lt) goto loc_82CF0D54;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// blr 
	return;
loc_82CF0D54:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0BC0) {
	__imp__sub_82CF0BC0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0D60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r7,r8
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r4,24(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r8.s64;
	// subf r6,r6,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r6.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// blt cr6,0x82cf0da4
	if (cr6.lt) goto loc_82CF0DA4;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CF0DA4:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf0dd8
	if (cr6.eq) goto loc_82CF0DD8;
loc_82CF0DC4:
	// rlwinm r5,r9,7,0,24
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf0dc4
	if (cr6.lt) goto loc_82CF0DC4;
loc_82CF0DD8:
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lis r7,-32255
	ctx.r7.s64 = -2113863680;
	// std r9,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r9.u64);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,26484(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 26484);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// lfs f12,26480(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 26480);
	ctx.f12.f64 = double(temp.f32);
loc_82CF0E08:
	// lbz r7,5(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// std r7,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r7.u64);
	// lfd f10,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = static_cast<float>(ctx.f8.f64 - ctx.f13.f64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,5120(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lbz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// std r5,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r5.u64);
	// lfd f4,-40(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f3,f4
	ctx.f3.f64 = double(ctx.f4.s64);
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// fsubs f1,f2,f13
	ctx.f1.f64 = static_cast<float>(ctx.f2.f64 - ctx.f13.f64);
	// fmuls f10,f1,f12
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,4096(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lbz r9,3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// std r9,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r9.u64);
	// lfd f8,-32(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fsubs f5,f6,f13
	ctx.f5.f64 = static_cast<float>(ctx.f6.f64 - ctx.f13.f64);
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,3072(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lbz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// std r6,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r6.u64);
	// lfd f2,-24(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f10,f1
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// fsubs f9,f10,f13
	ctx.f9.f64 = static_cast<float>(ctx.f10.f64 - ctx.f13.f64);
	// fmuls f8,f9,f12
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,2048(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lbz r4,1(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// std r4,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r4.u64);
	// lfd f6,-16(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fsubs f3,f4,f13
	ctx.f3.f64 = static_cast<float>(ctx.f4.f64 - ctx.f13.f64);
	// fmuls f2,f3,f12
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// fmuls f1,f2,f0
	ctx.f1.f64 = double(float(ctx.f2.f64 * f0.f64));
	// stfs f1,1024(r10)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,6
	r11.s64 = r11.s64 + 6;
	// std r7,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r7.u64);
	// lfd f10,-8(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fsubs f7,f8,f13
	ctx.f7.f64 = static_cast<float>(ctx.f8.f64 - ctx.f13.f64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// stfs f5,0(r10)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// bne 0x82cf0e08
	if (!cr0.eq) goto loc_82CF0E08;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r7,r9,r11
	ctx.r7.s64 = r11.s64 - ctx.r9.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// twllei r8,0
	// divwu r9,r7,r8
	ctx.r9.u32 = ctx.r7.u32 / ctx.r8.u32;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82cf0f18
	if (cr6.lt) goto loc_82CF0F18;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CF0F18:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf0f40
	if (!cr6.lt) goto loc_82CF0F40;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// blr 
	return;
loc_82CF0F40:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0D60) {
	__imp__sub_82CF0D60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF0F48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r5,13(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r4,28(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r8,r5,r11
	ctx.r8.s64 = int64_t(ctx.r5.s32) * int64_t(r11.s32);
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r6,r4,2,0,29
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r4,r31
	r11.s64 = r31.s64 - ctx.r4.s64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// blt cr6,0x82cf0f9c
	if (cr6.lt) goto loc_82CF0F9C;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_82CF0F9C:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// std r11,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, r11.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF0FC8:
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82cf1018
	if (cr6.eq) goto loc_82CF1018;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_82CF0FDC:
	// lhz r31,0(r10)
	r31.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r30,r31,24,24,31
	r30.u64 = rotl64(r31.u32 | (r31.u64 << 32), 24) & 0xFF;
	// rlwimi r30,r31,8,16,23
	r30.u64 = (rotl32(r31.u32, 8) & 0xFF00) | (r30.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r31,r30
	r31.s64 = r30.s16;
	// std r31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, r31.u64);
	// lfd f11,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,0(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,1024
	ctx.r9.s64 = ctx.r9.s64 + 1024;
	// bne 0x82cf0fdc
	if (!cr0.eq) goto loc_82CF0FDC;
loc_82CF1018:
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// fadds f0,f12,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82cf0fc8
	if (!cr0.eq) goto loc_82CF0FC8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rotlwi r8,r10,1
	ctx.r8.u64 = rotl32(ctx.r10.u32, 1);
	// divwu r10,r9,r8
	ctx.r10.u32 = ctx.r9.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf1054
	if (cr6.lt) goto loc_82CF1054;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF1054:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf1084
	if (!cr6.lt) goto loc_82CF1084;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF1084:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF0F48) {
	__imp__sub_82CF0F48(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF1098) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r4,r10,r11
	ctx.r4.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r5
	r11.s64 = ctx.r5.s64 - r11.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r11,r6
	cr6.compare<int32_t>(r11.s32, ctx.r6.s32, xer);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// blt cr6,0x82cf10e8
	if (cr6.lt) goto loc_82CF10E8;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CF10E8:
	// rlwinm r8,r9,1,0,30
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// li r11,0
	r11.s64 = 0;
	// addi r8,r8,127
	ctx.r8.s64 = ctx.r8.s64 + 127;
	// rlwinm r8,r8,25,7,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82cf1114
	if (cr6.eq) goto loc_82CF1114;
loc_82CF1100:
	// rlwinm r5,r11,7,0,24
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r10
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x82cf1100
	if (cr6.lt) goto loc_82CF1100;
loc_82CF1114:
	// extsw r11,r6
	r11.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF113C:
	// lhz r11,0(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// rlwinm r8,r11,24,24,31
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 24) & 0xFF;
	// rlwimi r8,r11,8,16,23
	ctx.r8.u64 = (rotl32(r11.u32, 8) & 0xFF00) | (ctx.r8.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r8
	ctx.r5.s64 = ctx.r8.s16;
	// std r5,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r5.u64);
	// lfd f11,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,0(r7)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf113c
	if (!cr0.eq) goto loc_82CF113C;
	// lbz r11,13(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r8,r11,1
	ctx.r8.u64 = rotl32(r11.u32, 1);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// twllei r8,0
	// divwu r10,r6,r8
	ctx.r10.u32 = ctx.r6.u32 / ctx.r8.u32;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf11a4
	if (cr6.lt) goto loc_82CF11A4;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF11A4:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf11d0
	if (!cr6.lt) goto loc_82CF11D0;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF11D0:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF1098) {
	__imp__sub_82CF1098(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF11E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r4,r9,r10
	ctx.r4.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// blt cr6,0x82cf1230
	if (cr6.lt) goto loc_82CF1230;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
loc_82CF1230:
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf125c
	if (cr6.eq) goto loc_82CF125C;
loc_82CF1248:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf1248
	if (cr6.lt) goto loc_82CF1248;
loc_82CF125C:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r10,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r10.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF1284:
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// rlwinm r7,r10,24,24,31
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFF;
	// rlwimi r7,r10,8,16,23
	ctx.r7.u64 = (rotl32(ctx.r10.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r5.u64);
	// lfd f11,-32(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,1024(r8)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r8.u32 + 1024, temp.u32);
	// lhz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r10,r4,24,24,31
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 24) & 0xFF;
	// rlwimi r10,r4,8,16,23
	ctx.r10.u64 = (rotl32(ctx.r4.u32, 8) & 0xFF00) | (ctx.r10.u64 & 0xFFFFFFFFFFFF00FF);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// std r6,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r6.u64);
	// lfd f6,-24(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f2,0(r8)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf1284
	if (!cr0.eq) goto loc_82CF1284;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,1
	ctx.r7.u64 = rotl32(ctx.r10.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r11
	ctx.r6.s64 = r11.s64 - ctx.r9.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cf1318
	if (!cr6.lt) goto loc_82CF1318;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF1318:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf1344
	if (!cr6.lt) goto loc_82CF1344;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF1344:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF11E0) {
	__imp__sub_82CF11E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF1350) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r5,r9,r10
	ctx.r5.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r9,r5,1,0,30
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r4
	ctx.r10.s64 = ctx.r4.s64 - ctx.r10.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// add r9,r7,r8
	ctx.r9.u64 = ctx.r7.u64 + ctx.r8.u64;
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// blt cr6,0x82cf13a0
	if (cr6.lt) goto loc_82CF13A0;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CF13A0:
	// rlwinm r7,r8,3,0,28
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r7,127
	ctx.r7.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r7,25,7,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf13cc
	if (cr6.eq) goto loc_82CF13CC;
loc_82CF13B8:
	// rlwinm r5,r10,7,0,24
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf13b8
	if (cr6.lt) goto loc_82CF13B8;
loc_82CF13CC:
	// extsw r10,r6
	ctx.r10.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r10,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r10.u64);
	// lfd f11,-48(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF13F4:
	// lhz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r7,r10,24,24,31
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFF;
	// rlwimi r7,r10,8,16,23
	ctx.r7.u64 = (rotl32(ctx.r10.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r5.u64);
	// lfd f11,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,3072(r9)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r9.u32 + 3072, temp.u32);
	// lhz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// rlwinm r10,r4,24,24,31
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 24) & 0xFF;
	// rlwimi r10,r4,8,16,23
	ctx.r10.u64 = (rotl32(ctx.r4.u32, 8) & 0xFF00) | (ctx.r10.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r6,r10
	ctx.r6.s64 = ctx.r10.s16;
	// std r6,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r6.u64);
	// lfd f6,-40(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f2,2048(r9)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r9.u32 + 2048, temp.u32);
	// lhz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// rlwinm r4,r5,24,24,31
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r5,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// std r7,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r7.u64);
	// lfd f1,-32(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * f0.f64));
	// stfs f8,1024(r9)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + 1024, temp.u32);
	// lhz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r5,r6,24,24,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r5,r6,8,16,23
	ctx.r5.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r5.u64 & 0xFFFFFFFFFFFF00FF);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// extsh r10,r5
	ctx.r10.s64 = ctx.r5.s16;
	// std r10,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r10.u64);
	// lfd f7,-24(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,0(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf13f4
	if (!cr0.eq) goto loc_82CF13F4;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,1
	ctx.r7.u64 = rotl32(ctx.r10.u32, 1);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cf14e0
	if (!cr6.lt) goto loc_82CF14E0;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF14E0:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf150c
	if (!cr6.lt) goto loc_82CF150C;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF150C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF1350) {
	__imp__sub_82CF1350(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF1518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mullw r5,r7,r8
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r31,24(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,20(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r8,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r8.s64;
	// subf r6,r6,r31
	ctx.r6.s64 = r31.s64 - ctx.r6.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// cmpw cr6,r8,r6
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, xer);
	// blt cr6,0x82cf1564
	if (cr6.lt) goto loc_82CF1564;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_82CF1564:
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r7,127
	ctx.r5.s64 = ctx.r7.s64 + 127;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82cf1598
	if (cr6.eq) goto loc_82CF1598;
loc_82CF1584:
	// rlwinm r5,r9,7,0,24
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r11
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf1584
	if (cr6.lt) goto loc_82CF1584;
loc_82CF1598:
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// std r9,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r9.u64);
	// lfd f11,-64(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3152(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3152);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
loc_82CF15C0:
	// lhz r9,10(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 10);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r7,r9,24,24,31
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFF;
	// rlwimi r7,r9,8,16,23
	ctx.r7.u64 = (rotl32(ctx.r9.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.r5.u64);
	// lfd f11,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * f0.f64));
	// stfs f7,5120(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 5120, temp.u32);
	// lhz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 8);
	// rlwinm r9,r4,24,24,31
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 24) & 0xFF;
	// rlwimi r9,r4,8,16,23
	ctx.r9.u64 = (rotl32(ctx.r4.u32, 8) & 0xFF00) | (ctx.r9.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// std r6,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.r6.u64);
	// lfd f6,-56(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fmuls f3,f4,f13
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f2,4096(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4096, temp.u32);
	// lhz r5,6(r11)
	ctx.r5.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// rlwinm r4,r5,24,24,31
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 24) & 0xFF;
	// rlwimi r4,r5,8,16,23
	ctx.r4.u64 = (rotl32(ctx.r5.u32, 8) & 0xFF00) | (ctx.r4.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r7,r4
	ctx.r7.s64 = ctx.r4.s16;
	// std r7,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.r7.u64);
	// lfd f1,-48(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// fcfid f11,f1
	ctx.f11.f64 = double(ctx.f1.s64);
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// fmuls f9,f10,f13
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// fmuls f8,f9,f0
	ctx.f8.f64 = double(float(ctx.f9.f64 * f0.f64));
	// stfs f8,3072(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 3072, temp.u32);
	// lhz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// rlwinm r5,r6,24,24,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 24) & 0xFF;
	// rlwimi r5,r6,8,16,23
	ctx.r5.u64 = (rotl32(ctx.r6.u32, 8) & 0xFF00) | (ctx.r5.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r9,r5
	ctx.r9.s64 = ctx.r5.s16;
	// std r9,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r9.u64);
	// lfd f7,-40(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * f0.f64));
	// stfs f3,2048(r10)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r10.u32 + 2048, temp.u32);
	// lhz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// rlwinm r6,r7,24,24,31
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 24) & 0xFF;
	// rlwimi r6,r7,8,16,23
	ctx.r6.u64 = (rotl32(ctx.r7.u32, 8) & 0xFF00) | (ctx.r6.u64 & 0xFFFFFFFFFFFF00FF);
	// extsh r4,r6
	ctx.r4.s64 = ctx.r6.s16;
	// std r4,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r4.u64);
	// lfd f2,-32(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// frsp f11,f1
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f9,1024(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r7,r9,24,24,31
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFF;
	// rlwimi r7,r9,8,16,23
	ctx.r7.u64 = (rotl32(ctx.r9.u32, 8) & 0xFF00) | (ctx.r7.u64 & 0xFFFFFFFFFFFF00FF);
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// extsh r5,r7
	ctx.r5.s64 = ctx.r7.s16;
	// std r5,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r5.u64);
	// lfd f8,-24(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// fcfid f7,f8
	ctx.f7.f64 = double(ctx.f8.s64);
	// frsp f6,f7
	ctx.f6.f64 = double(float(ctx.f7.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * f0.f64));
	// stfs f4,0(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// bne 0x82cf15c0
	if (!cr0.eq) goto loc_82CF15C0;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r9,1
	ctx.r7.u64 = rotl32(ctx.r9.u32, 1);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cf1704
	if (!cr6.lt) goto loc_82CF1704;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CF1704:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf1730
	if (!cr6.lt) goto loc_82CF1730;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
loc_82CF1730:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF1518) {
	__imp__sub_82CF1518(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF1740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// addi r31,r3,52
	r31.s64 = ctx.r3.s64 + 52;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lbz r4,13(r3)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r8,r4,r5
	ctx.r8.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r5.s32);
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// subf r10,r5,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r5.s64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// blt cr6,0x82cf17a0
	if (cr6.lt) goto loc_82CF17A0;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
loc_82CF17A0:
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// std r11,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, r11.u64);
	// lfd f11,-32(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// lfs f13,3140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3140);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f11,f12,f9
	ctx.f11.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f10,f11,f13
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// beq cr6,0x82cf17e8
	if (cr6.eq) goto loc_82CF17E8;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82CF17DC:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x82cf17dc
	if (!cr0.eq) goto loc_82CF17DC;
loc_82CF17E8:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// rlwinm r30,r4,2,0,29
	r30.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f12,3056(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3056);
	ctx.f12.f64 = double(temp.f32);
loc_82CF17F4:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82cf1844
	if (cr6.eq) goto loc_82CF1844;
	// fadds f13,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f11.f64 + f0.f64));
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
	// mr r11,r31
	r11.u64 = r31.u64;
	// subf r8,r31,r5
	ctx.r8.s64 = ctx.r5.s64 - r31.s64;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
loc_82CF1810:
	// lfsx f9,r8,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + r11.u32);
	ctx.f9.f64 = double(temp.f32);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lfs f8,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f13,f9
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// fadds f6,f9,f8
	ctx.f6.f64 = double(float(ctx.f9.f64 + ctx.f8.f64));
	// stfs f9,0(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * f0.f64));
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f4,0(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f7,4(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// bne 0x82cf1810
	if (!cr0.eq) goto loc_82CF1810;
loc_82CF1844:
	// addic. r6,r6,-1
	xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// fadds f0,f10,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f10.f64 + f0.f64));
	// add r5,r30,r5
	ctx.r5.u64 = r30.u64 + ctx.r5.u64;
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// bne 0x82cf17f4
	if (!cr0.eq) goto loc_82CF17F4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r9,r11,r5
	ctx.r9.s64 = ctx.r5.s64 - r11.s64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rotlwi r8,r10,2
	ctx.r8.u64 = rotl32(ctx.r10.u32, 2);
	// divwu r10,r9,r8
	ctx.r10.u32 = ctx.r9.u32 / ctx.r8.u32;
	// twllei r8,0
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82cf1880
	if (cr6.lt) goto loc_82CF1880;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF1880:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf18a0
	if (!cr6.lt) goto loc_82CF18A0;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CF18A0:
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82cf18c4
	if (cr6.eq) goto loc_82CF18C4;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82CF18B8:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x82cf18b8
	if (!cr0.eq) goto loc_82CF18B8;
loc_82CF18C4:
	// ld r30,-16(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF1740) {
	__imp__sub_82CF1740(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF18D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r5,r11,r10
	ctx.r5.s64 = ctx.r10.s64 - r11.s64;
	// lwz r4,4(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// srawi r5,r5,1
	xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r9.s64;
	// addze r10,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r10.s64 = temp.s64;
	// add r11,r8,r11
	r11.u64 = ctx.r8.u64 + r11.u64;
	// add r8,r6,r7
	ctx.r8.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x82cf1920
	if (cr6.lt) goto loc_82CF1920;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
loc_82CF1920:
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f11,f13,f0
	ctx.f11.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// std r7,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r7.u64);
	// lfd f10,-16(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// addi r5,r10,127
	ctx.r5.s64 = ctx.r10.s64 + 127;
	// lfs f12,3140(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3140);
	ctx.f12.f64 = double(temp.f32);
	// li r10,0
	ctx.r10.s64 = 0;
	// rlwinm r7,r5,25,7,31
	ctx.r7.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// fdivs f13,f11,f8
	ctx.f13.f64 = double(float(ctx.f11.f64 / ctx.f8.f64));
	// fmuls f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// beq cr6,0x82cf1978
	if (cr6.eq) goto loc_82CF1978;
loc_82CF1964:
	// rlwinm r6,r10,7,0,24
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r6,r11
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf1964
	if (cr6.lt) goto loc_82CF1964;
loc_82CF1978:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f11,3056(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3056);
	ctx.f11.f64 = double(temp.f32);
loc_82CF1980:
	// lfs f10,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 + f0.f64));
	// lfs f8,52(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	ctx.f8.f64 = double(temp.f32);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// fadds f7,f10,f8
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
	// stfs f10,52(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 52, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// fmuls f6,f9,f10
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * f0.f64));
	// fadds f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 + f0.f64));
	// fmuls f4,f5,f11
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// stfs f4,0(r8)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r8.u32 + 0, temp.u32);
	// stfs f6,4(r8)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r8.u32 + 4, temp.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne 0x82cf1980
	if (!cr0.eq) goto loc_82CF1980;
	// lbz r10,13(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r10,2
	ctx.r7.u64 = rotl32(ctx.r10.u32, 2);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r9,r11
	ctx.r6.s64 = r11.s64 - ctx.r9.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82cf19e4
	if (!cr6.lt) goto loc_82CF19E4;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82CF19E4:
	// lwz r11,20(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// subf r10,r11,r8
	ctx.r10.s64 = ctx.r8.s64 - r11.s64;
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf1a0c
	if (!cr6.lt) goto loc_82CF1A0C;
	// stw r10,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r10.u32);
	// blr 
	return;
loc_82CF1A0C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF18D0) {
	__imp__sub_82CF18D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF1A18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// addi r5,r3,52
	ctx.r5.s64 = ctx.r3.s64 + 52;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r4,r11,r10
	ctx.r4.s64 = ctx.r10.s64 - r11.s64;
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// srawi r4,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r8,r9,r31
	ctx.r8.s64 = r31.s64 - ctx.r9.s64;
	// addze r9,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r9.s64 = temp.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// add r10,r6,r7
	ctx.r10.u64 = ctx.r6.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// blt cr6,0x82cf1a70
	if (cr6.lt) goto loc_82CF1A70;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_82CF1A70:
	// extsw r7,r9
	ctx.r7.s64 = ctx.r9.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// rlwinm r9,r8,3,0,28
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// std r7,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r7.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// addi r4,r9,127
	ctx.r4.s64 = ctx.r9.s64 + 127;
	// lfs f13,3140(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3140);
	ctx.f13.f64 = double(temp.f32);
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r7,r4,25,7,31
	ctx.r7.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// beq cr6,0x82cf1ac8
	if (cr6.eq) goto loc_82CF1AC8;
loc_82CF1AB4:
	// rlwinm r6,r9,7,0,24
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r6,r11
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x82cf1ab4
	if (cr6.lt) goto loc_82CF1AB4;
loc_82CF1AC8:
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// li r9,2
	ctx.r9.s64 = 2;
loc_82CF1AD0:
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82cf1ad0
	if (!cr0.eq) goto loc_82CF1AD0;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f13,3056(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3056);
	ctx.f13.f64 = double(temp.f32);
loc_82CF1AE4:
	// lfs f10,4(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 + f0.f64));
	// lfs f8,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// fadds f7,f10,f8
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
	// stfs f10,4(r5)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r5.u32 + 4, temp.u32);
	// fmuls f6,f9,f10
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * f0.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfs f4,1024(r10)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1024, temp.u32);
	// stfs f6,1028(r10)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r10.u32 + 1028, temp.u32);
	// lfs f1,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f9,f3
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// fadds f10,f3,f1
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f1.f64));
	// stfs f3,0(r5)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * f0.f64));
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// fmuls f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f8,0(r10)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f2,4(r10)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r10.u32 + 4, temp.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne 0x82cf1ae4
	if (!cr0.eq) goto loc_82CF1AE4;
	// lbz r9,13(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r7,r9,2
	ctx.r7.u64 = rotl32(ctx.r9.u32, 2);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r6,r8,r11
	ctx.r6.s64 = r11.s64 - ctx.r8.s64;
	// twllei r7,0
	// divwu r11,r6,r7
	r11.u32 = ctx.r6.u32 / ctx.r7.u32;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82cf1b6c
	if (!cr6.lt) goto loc_82CF1B6C;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_82CF1B6C:
	// lwz r8,20(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// subf r7,r8,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r8.s64;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// rlwinm r10,r7,30,2,31
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82cf1b8c
	if (!cr6.lt) goto loc_82CF1B8C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CF1B8C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// li r11,2
	r11.s64 = 2;
loc_82CF1B98:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// bne 0x82cf1b98
	if (!cr0.eq) goto loc_82CF1B98;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF1A18) {
	__imp__sub_82CF1A18(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82CF1BB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	f0.f64 = double(temp.f32);
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// addi r9,r3,52
	ctx.r9.s64 = ctx.r3.s64 + 52;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r7,13(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// subf r4,r11,r10
	ctx.r4.s64 = ctx.r10.s64 - r11.s64;
	// lwz r31,4(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mullw r10,r7,r8
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r6,20(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// srawi r4,r4,1
	xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// subf r7,r8,r31
	ctx.r7.s64 = r31.s64 - ctx.r8.s64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r8,r4
	temp.s64 = ctx.r4.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r8.s64 = temp.s64;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r11,r5,r6
	r11.u64 = ctx.r5.u64 + ctx.r6.u64;
	// cmpw cr6,r7,r8
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r8.s32, xer);
	// blt cr6,0x82cf1c08
	if (cr6.lt) goto loc_82CF1C08;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_82CF1C08:
	// extsw r6,r8
	ctx.r6.s64 = ctx.r8.s32;
	// lfs f13,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// rlwinm r8,r7,4,0,27
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f11,-16(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// addi r4,r8,127
	ctx.r4.s64 = ctx.r8.s64 + 127;
	// lfs f13,3140(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 3140);
	ctx.f13.f64 = double(temp.f32);
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm r6,r4,25,7,31
	ctx.r6.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// fdivs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 / ctx.f9.f64));
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// beq cr6,0x82cf1c60
	if (cr6.eq) goto loc_82CF1C60;
loc_82CF1C4C:
	// rlwinm r5,r8,7,0,24
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
	// dcbt r5,r10
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// cmplw cr6,r8,r6
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, xer);
	// blt cr6,0x82cf1c4c
	if (cr6.lt) goto loc_82CF1C4C;
loc_82CF1C60:
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// li r8,4
	ctx.r8.s64 = 4;
loc_82CF1C68:
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// bne 0x82cf1c68
	if (!cr0.eq) goto loc_82CF1C68;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfs f13,3056(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 3056);
	ctx.f13.f64 = double(temp.f32);
loc_82CF1C7C:
	// lfs f10,12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// fadds f9,f12,f0
	ctx.f9.f64 = double(float(ctx.f12.f64 + f0.f64));
	// lfs f8,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// addic. r7,r7,-1
	xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// fadds f7,f10,f8
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
	// stfs f10,12(r9)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r9.u32 + 12, temp.u32);
	// fmuls f6,f9,f10
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmuls f5,f7,f0
	ctx.f5.f64 = double(float(ctx.f7.f64 * f0.f64));
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfs f4,3072(r11)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(r11.u32 + 3072, temp.u32);
	// stfs f6,3076(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r11.u32 + 3076, temp.u32);
	// lfs f3,8(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f9,f3
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// lfs f1,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// fadds f10,f3,f1
	ctx.f10.f64 = double(float(ctx.f3.f64 + ctx.f1.f64));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * f0.f64));
	// stfs f3,8(r9)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r9.u32 + 8, temp.u32);
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f7,2048(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 2048, temp.u32);
	// stfs f2,2052(r11)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(r11.u32 + 2052, temp.u32);
	// lfs f6,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f9,f6
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// lfs f4,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fadds f3,f6,f4
	ctx.f3.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// fmuls f2,f3,f0
	ctx.f2.f64 = double(float(ctx.f3.f64 * f0.f64));
	// stfs f6,4(r9)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r9.u32 + 4, temp.u32);
	// fmuls f1,f2,f13
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f1,1024(r11)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r11.u32 + 1024, temp.u32);
	// stfs f5,1028(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r11.u32 + 1028, temp.u32);
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f9,f10
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f8,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fadds f7,f10,f8
	ctx.f7.f64 = double(float(ctx.f10.f64 + ctx.f8.f64));
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * f0.f64));
	// stfs f10,0(r9)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// fmuls f5,f6,f13
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// stfs f5,0(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stfs f9,4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x82cf1c7c
	if (!cr0.eq) goto loc_82CF1C7C;
	// lbz r8,13(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 13);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rotlwi r6,r8,2
	ctx.r6.u64 = rotl32(ctx.r8.u32, 2);
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r5,r7,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r7.s64;
	// twllei r6,0
	// divwu r10,r5,r6
	ctx.r10.u32 = ctx.r5.u32 / ctx.r6.u32;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bge cr6,0x82cf1d4c
	if (!cr6.lt) goto loc_82CF1D4C;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_82CF1D4C:
	// lwz r7,20(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// lwz r10,24(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// subf r6,r7,r11
	ctx.r6.s64 = r11.s64 - ctx.r7.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// rlwinm r11,r6,30,2,31
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82cf1d6c
	if (cr6.lt) goto loc_82CF1D6C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82CF1D6C:
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stfs f0,36(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// li r11,4
	r11.s64 = 4;
loc_82CF1D78:
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x82cf1d78
	if (!cr0.eq) goto loc_82CF1D78;
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82CF1BB0) {
	__imp__sub_82CF1BB0(ctx, base);
}


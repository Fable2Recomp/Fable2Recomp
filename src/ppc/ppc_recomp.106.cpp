#include "ppc_recomp_shared.h"

PPC_FUNC_IMPL(__imp__sub_82BA0E80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,-18016
	ctx.r3.s64 = r11.s64 + -18016;
	// bl 0x832b26cc
	__imp__DbgPrint(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,13432(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(13432) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba0b30
	sub_82BA0B30(ctx, base);
	// lwz r11,13432(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(13432) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba0f50
	if (cr6.eq) goto loc_82BA0F50;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r11,28540(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28540) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x82ba0ed8
	if (cr6.eq) goto loc_82BA0ED8;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ba0f70
	goto loc_82BA0F70;
loc_82BA0ED8:
	// lbz r9,10941(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,10908(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10908) );
	// li r5,255
	ctx.r5.s64 = 255;
	// lwz r8,10896(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10896) );
	// ori r9,r9,3
	ctx.r9.u64 = ctx.r9.u64 | 3;
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// stb r9,10941(r31)
	PPC_STORE_U8(r31.u32 + 10941, ctx.r9.u8);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,81
	ctx.r3.s64 = ctx.r1.s64 + 81;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stb r10,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r10.u8);
	// stw r10,11008(r31)
	PPC_STORE_U32(r31.u32 + 11008, ctx.r10.u32);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r5,256
	ctx.r5.s64 = 256;
	// addi r4,r11,-18032
	ctx.r4.s64 = r11.s64 + -18032;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba0f6c
	if (cr6.eq) goto loc_82BA0F6C;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,84
	ctx.r3.s64 = 84;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba0f6c
	goto loc_82BA0F6C;
loc_82BA0F50:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r3,r11,-17936
	ctx.r3.s64 = r11.s64 + -17936;
	// bl 0x832b26cc
	__imp__DbgPrint(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r3,r11,-17712
	ctx.r3.s64 = r11.s64 + -17712;
	// bl 0x832b26cc
	__imp__DbgPrint(ctx, base);
	// twi 31,r0,22
loc_82BA0F6C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82BA0F70:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA0E80) {
	__imp__sub_82BA0E80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA0F88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,8084(r10)
	PPC_STORE_U8(ctx.r10.u32 + 8084, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA0F88) {
	__imp__sub_82BA0F88(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA0FA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,0
	r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,8084(r10)
	PPC_STORE_U8(ctx.r10.u32 + 8084, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA0FA0) {
	__imp__sub_82BA0FA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA0FB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// lis r11,1
	r11.s64 = 65536;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// subf r11,r3,r11
	r11.s64 = r11.s64 - ctx.r3.s64;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA0FB8) {
	__imp__sub_82BA0FB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA0FF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// lis r11,2
	r11.s64 = 131072;
	// ori r11,r11,32728
	r11.u64 = r11.u64 | 32728;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// bge cr6,0x82ba1024
	if (!cr6.lt) goto loc_82BA1024;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// extsw r11,r3
	r11.s64 = ctx.r3.s32;
loc_82BA1024:
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,22116(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22116, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA0FF0) {
	__imp__sub_82BA0FF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA1048) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r31,-31927
	r31.s64 = -2092367872;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r11,r31,8085
	r11.s64 = r31.s64 + 8085;
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r3,r11,16387
	ctx.r3.s64 = r11.s64 + 16387;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,8085(r31)
	PPC_STORE_U8(r31.u32 + 8085, r11.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA1048) {
	__imp__sub_82BA1048(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA1090) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,987(r10)
	PPC_STORE_U8(ctx.r10.u32 + 987, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA1090) {
	__imp__sub_82BA1090(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA10A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// li r11,1
	r11.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r11,986(r10)
	PPC_STORE_U8(ctx.r10.u32 + 986, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA10A8) {
	__imp__sub_82BA10A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA10C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-688(r1)
	ea = -688 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r9,r10,-17616
	ctx.r9.s64 = ctx.r10.s64 + -17616;
	// addi r29,r11,6
	r29.s64 = r11.s64 + 6;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// lwz r10,-17616(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-17616) );
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// lis r27,-32038
	r27.s64 = -2099642368;
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// li r5,54
	ctx.r5.s64 = 54;
	// lhz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,90
	ctx.r3.s64 = ctx.r1.s64 + 90;
	// ori r27,r27,7
	r27.u64 = r27.u64 | 7;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// sth r9,88(r1)
	PPC_STORE_U16(ctx.r1.u32 + 88, ctx.r9.u16);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// addi r9,r11,-17604
	ctx.r9.s64 = r11.s64 + -17604;
	// addi r10,r10,3976
	ctx.r10.s64 = ctx.r10.s64 + 3976;
	// li r5,51
	ctx.r5.s64 = 51;
	// stw r10,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r10.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,-17604(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-17604) );
	// addi r3,r1,161
	ctx.r3.s64 = ctx.r1.s64 + 161;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// lbz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 12);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// stw r10,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r10.u32);
	// stw r8,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r8.u32);
	// stb r9,160(r1)
	PPC_STORE_U8(ctx.r1.u32 + 160, ctx.r9.u8);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// addi r9,r11,-17588
	ctx.r9.s64 = r11.s64 + -17588;
	// addi r10,r10,4000
	ctx.r10.s64 = ctx.r10.s64 + 4000;
	// li r5,54
	ctx.r5.s64 = 54;
	// stw r10,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r10.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,-17588(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-17588) );
	// addi r3,r1,226
	ctx.r3.s64 = ctx.r1.s64 + 226;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// lhz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r9.u32 + 8);
	// stw r11,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r11.u32);
	// stw r10,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r10.u32);
	// sth r9,224(r1)
	PPC_STORE_U16(ctx.r1.u32 + 224, ctx.r9.u16);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r10,r10,4024
	ctx.r10.s64 = ctx.r10.s64 + 4024;
	// addi r11,r11,-17576
	r11.s64 = r11.s64 + -17576;
	// stw r10,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, ctx.r10.u32);
	// addi r9,r1,284
	ctx.r9.s64 = ctx.r1.s64 + 284;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,19
	r11.s64 = 19;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82BA11B8:
	// lbz r11,0(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stb r11,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, r11.u8);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// bdnz 0x82ba11b8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA11B8;
	// li r5,45
	ctx.r5.s64 = 45;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,303
	ctx.r3.s64 = ctx.r1.s64 + 303;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// addi r4,r11,-17556
	ctx.r4.s64 = r11.s64 + -17556;
	// addi r11,r10,4080
	r11.s64 = ctx.r10.s64 + 4080;
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// li r5,27
	ctx.r5.s64 = 27;
	// stw r11,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r11.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// li r5,37
	ctx.r5.s64 = 37;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,379
	ctx.r3.s64 = ctx.r1.s64 + 379;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// addi r4,r11,-17528
	ctx.r4.s64 = r11.s64 + -17528;
	// addi r11,r10,4168
	r11.s64 = ctx.r10.s64 + 4168;
	// addi r3,r1,420
	ctx.r3.s64 = ctx.r1.s64 + 420;
	// li r5,25
	ctx.r5.s64 = 25;
	// stw r11,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, r11.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// li r5,39
	ctx.r5.s64 = 39;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,445
	ctx.r3.s64 = ctx.r1.s64 + 445;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// addi r9,r11,-17500
	ctx.r9.s64 = r11.s64 + -17500;
	// addi r10,r10,4264
	ctx.r10.s64 = ctx.r10.s64 + 4264;
	// li r5,49
	ctx.r5.s64 = 49;
	// stw r10,484(r1)
	PPC_STORE_U32(ctx.r1.u32 + 484, ctx.r10.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,-17500(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-17500) );
	// addi r3,r1,503
	ctx.r3.s64 = ctx.r1.s64 + 503;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// lhz r7,12(r9)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r9.u32 + 12);
	// lbz r9,14(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 14);
	// stw r11,488(r1)
	PPC_STORE_U32(ctx.r1.u32 + 488, r11.u32);
	// stw r10,492(r1)
	PPC_STORE_U32(ctx.r1.u32 + 492, ctx.r10.u32);
	// stw r8,496(r1)
	PPC_STORE_U32(ctx.r1.u32 + 496, ctx.r8.u32);
	// sth r7,500(r1)
	PPC_STORE_U16(ctx.r1.u32 + 500, ctx.r7.u16);
	// stb r9,502(r1)
	PPC_STORE_U8(ctx.r1.u32 + 502, ctx.r9.u8);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// addi r9,r11,-17484
	ctx.r9.s64 = r11.s64 + -17484;
	// addi r10,r10,4240
	ctx.r10.s64 = ctx.r10.s64 + 4240;
	// li r5,51
	ctx.r5.s64 = 51;
	// stw r10,552(r1)
	PPC_STORE_U32(ctx.r1.u32 + 552, ctx.r10.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,-17484(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-17484) );
	// addi r3,r1,569
	ctx.r3.s64 = ctx.r1.s64 + 569;
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// lbz r9,12(r9)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + 12);
	// stw r11,556(r1)
	PPC_STORE_U32(ctx.r1.u32 + 556, r11.u32);
	// stw r10,560(r1)
	PPC_STORE_U32(ctx.r1.u32 + 560, ctx.r10.u32);
	// stw r8,564(r1)
	PPC_STORE_U32(ctx.r1.u32 + 564, ctx.r8.u32);
	// stb r9,568(r1)
	PPC_STORE_U8(ctx.r1.u32 + 568, ctx.r9.u8);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32070
	r11.s64 = -2101739520;
	// li r28,0
	r28.s64 = 0;
	// addi r11,r11,4264
	r11.s64 = r11.s64 + 4264;
	// addi r31,r1,80
	r31.s64 = ctx.r1.s64 + 80;
	// stw r11,620(r1)
	PPC_STORE_U32(ctx.r1.u32 + 620, r11.u32);
loc_82BA12E0:
	// mr r11,r31
	r11.u64 = r31.u64;
loc_82BA12E4:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ba12e4
	if (!cr6.eq) goto loc_82BA12E4;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rotlwi r30,r11,0
	r30.u64 = rotl32(r11.u32, 0);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ca3920
	sub_82CA3920(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba1348
	if (cr0.eq) goto loc_82BA1348;
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r31,r31,68
	r31.s64 = r31.s64 + 68;
	// cmplwi cr6,r28,8
	cr6.compare<uint32_t>(r28.u32, 8, xer);
	// blt cr6,0x82ba12e0
	if (cr6.lt) goto loc_82BA12E0;
loc_82BA1328:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r4,r11,-17460
	ctx.r4.s64 = r11.s64 + -17460;
loc_82BA1330:
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,688
	ctx.r1.s64 = ctx.r1.s64 + 688;
	// b 0x82ca2c2c
	return;
loc_82BA1348:
	// mulli r10,r28,68
	ctx.r10.s64 = r28.s64 * 68;
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// add r11,r30,r29
	r11.u64 = r30.u64 + r29.u64;
	// addi r3,r11,1
	ctx.r3.s64 = r11.s64 + 1;
	// lwzx r11,r10,r9
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r27,730
	r27.s64 = 47841280;
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba1328
	if (cr0.eq) goto loc_82BA1328;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r4,r11,-17468
	ctx.r4.s64 = r11.s64 + -17468;
	// b 0x82ba1330
	goto loc_82BA1330;
}

PPC_WEAK_FUNC(sub_82BA10C0) {
	__imp__sub_82BA10C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA1380) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,23360
	r31.s64 = ctx.r3.s64 + 23360;
	// lwz r3,23952(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(23952) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba13c8
	if (cr6.eq) goto loc_82BA13C8;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// rlwinm. r11,r11,0,4,4
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x8000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba13c0
	if (!cr0.eq) goto loc_82BA13C0;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x832b266c
	__imp__MmFreePhysicalMemory(ctx, base);
	// b 0x82ba13c8
	goto loc_82BA13C8;
loc_82BA13C0:
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// bl 0x822023f0
	sub_822023F0(ctx, base);
loc_82BA13C8:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r30,-20096
	r30.s64 = -1317011456;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba1404
	if (cr6.eq) goto loc_82BA1404;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// srawi r11,r11,30
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 30;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x82ba13f8
	if (cr6.lt) goto loc_82BA13F8;
	// bne cr6,0x82ba1404
	if (!cr6.eq) goto loc_82BA1404;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x822023f0
	sub_822023F0(ctx, base);
	// b 0x82ba1404
	goto loc_82BA1404;
loc_82BA13F8:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x832b266c
	__imp__MmFreePhysicalMemory(ctx, base);
loc_82BA1404:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba1440
	if (cr6.eq) goto loc_82BA1440;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r11,r11,30
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 30;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x82ba1434
	if (cr6.lt) goto loc_82BA1434;
	// bne cr6,0x82ba1440
	if (!cr6.eq) goto loc_82BA1440;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x822023f0
	sub_822023F0(ctx, base);
	// b 0x82ba1440
	goto loc_82BA1440;
loc_82BA1434:
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x832b266c
	__imp__MmFreePhysicalMemory(ctx, base);
loc_82BA1440:
	// li r5,620
	ctx.r5.s64 = 620;
	// lwz r30,616(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(616) );
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r11,r31,20
	r11.s64 = r31.s64 + 20;
	// stw r30,616(r31)
	PPC_STORE_U32(r31.u32 + 616, r30.u32);
	// li r9,-1
	ctx.r9.s64 = -1;
	// li r10,41
	ctx.r10.s64 = 41;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82BA1468:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82ba1468
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA1468;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA1380) {
	__imp__sub_82BA1380(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA1490) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-544(r1)
	ea = -544 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,23360
	r31.s64 = ctx.r3.s64 + 23360;
	// stw r5,580(r1)
	PPC_STORE_U32(ctx.r1.u32 + 580, ctx.r5.u32);
	// addi r7,r31,368
	ctx.r7.s64 = r31.s64 + 368;
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r31.u32);
	// addi r11,r31,372
	r11.s64 = r31.s64 + 372;
	// stw r7,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r7.u32);
	// lwz r10,604(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(604) );
	// clrlwi r10,r10,3
	ctx.r10.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// stw r10,604(r31)
	PPC_STORE_U32(r31.u32 + 604, ctx.r10.u32);
	// srawi. r8,r10,29
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 29;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lwz r6,36(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(36) );
	// lwz r10,40(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(40) );
	// rlwinm r10,r10,2,30,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x2;
	// clrlwi r9,r6,19
	ctx.r9.u64 = ctx.r6.u32 & 0x1FFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r6,r6,19,19,31
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 19) & 0x1FFF;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// addi r6,r9,31
	ctx.r6.s64 = ctx.r9.s64 + 31;
	// sth r9,0(r7)
	PPC_STORE_U16(ctx.r7.u32 + 0, ctx.r9.u16);
	// addi r5,r10,31
	ctx.r5.s64 = ctx.r10.s64 + 31;
	// sth r10,370(r31)
	PPC_STORE_U16(r31.u32 + 370, ctx.r10.u16);
	// rlwinm r6,r6,0,16,26
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFE0;
	// rlwinm r9,r5,0,16,26
	ctx.r9.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFE0;
	// sth r6,372(r31)
	PPC_STORE_U16(r31.u32 + 372, ctx.r6.u16);
	// sth r9,374(r31)
	PPC_STORE_U16(r31.u32 + 374, ctx.r9.u16);
	// bne 0x82ba1824
	if (!cr0.eq) goto loc_82BA1824;
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r6,-32247
	ctx.r6.s64 = -2113339392;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// addi r9,r1,100
	ctx.r9.s64 = ctx.r1.s64 + 100;
	// std r11,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, r11.u64);
	// lfd f0,120(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// lfs f0,3216(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3216);
	f0.f64 = double(temp.f32);
	// frsp f10,f13
	ctx.f10.f64 = double(float(ctx.f13.f64));
	// addi r11,r6,24724
	r11.s64 = ctx.r6.s64 + 24724;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stfs f0,100(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lvlx v0,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lfs f13,13356(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 13356);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// lfs f12,3144(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3144);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r1,100
	ctx.r7.s64 = ctx.r1.s64 + 100;
	// lfs f0,21580(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 21580);
	f0.f64 = double(temp.f32);
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// lfs f11,19348(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 19348);
	ctx.f11.f64 = double(temp.f32);
	// addi r30,r1,100
	r30.s64 = ctx.r1.s64 + 100;
	// li r10,16
	ctx.r10.s64 = 16;
	// fdivs f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 / ctx.f10.f64));
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// stfs f10,112(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lvlx v13,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r31,408
	ctx.r9.s64 = r31.s64 + 408;
	// stfs f0,112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r29,r11,-860
	r29.s64 = r11.s64 + -860;
	// lvlx v10,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r28,-860
	r28.s64 = -860;
	// stfs f9,112(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r5,r11,-860
	ctx.r5.s64 = r11.s64 + -860;
	// lvlx v8,0,r3
	temp.u32 = r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v8,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r3,48
	ctx.r3.s64 = 48;
	// lvlx v12,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v12,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// stfs f11,100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// lvlx v11,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r8,r11,-860
	ctx.r8.s64 = r11.s64 + -860;
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// vrlimi128 v10,v11,4,3
	simd::store_f32(ctx.v10.f32, simd::blend_f32<4>(simd::load_f32(ctx.v10.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// lvlx v9,0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v9,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v8,v9,4,3
	simd::store_f32(ctx.v8.f32, simd::blend_f32<4>(simd::load_f32(ctx.v8.f32), simd::permute_f32<57>(simd::load_f32(ctx.v9.f32))));
	// stfs f12,100(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// li r4,12
	ctx.r4.s64 = 12;
	// lvlx v9,0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(ctx.v9,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v9,v0,4,3
	simd::store_f32(ctx.v9.f32, simd::blend_f32<4>(simd::load_f32(ctx.v9.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v8,v9,3,2
	simd::store_f32(ctx.v8.f32, simd::blend_f32<3>(simd::load_f32(ctx.v8.f32), simd::permute_f32<78>(simd::load_f32(ctx.v9.f32))));
	// stw r4,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r4.u32);
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// vrlimi128 v13,v10,3,2
	simd::store_f32(ctx.v13.f32, simd::blend_f32<3>(simd::load_f32(ctx.v13.f32), simd::permute_f32<78>(simd::load_f32(ctx.v10.f32))));
	// addi r7,r11,-860
	ctx.r7.s64 = r11.s64 + -860;
	// addi r30,r11,-860
	r30.s64 = r11.s64 + -860;
	// stvlx v8,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v8), 15 - i));
}
	// addi r7,r7,32
	ctx.r7.s64 = ctx.r7.s64 + 32;
	// stvrx v8,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v8), i));
}
	// addi r9,r31,424
	ctx.r9.s64 = r31.s64 + 424;
	// lvlx v12,r11,r28
	temp.u32 = r11.u32 + r28.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r4,32
	ctx.r4.s64 = 32;
	// lvrx v0,r10,r29
	temp.u32 = ctx.r10.u32 + r29.u32;
	simd::store_i8(ctx.v0.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor v0,v12,v0
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stvrx v0,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r31,440
	ctx.r9.s64 = r31.s64 + 440;
	// lvrx v0,r10,r8
	temp.u32 = ctx.r10.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v0.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r27,r1,84
	r27.s64 = ctx.r1.s64 + 84;
	// lvlx v12,r5,r10
	temp.u32 = ctx.r5.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vor v0,v12,v0
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// stvrx v0,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r31,456
	ctx.r9.s64 = r31.s64 + 456;
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// lvrx v12,r10,r7
	temp.u32 = ctx.r10.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v12.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r16,r1,84
	r16.s64 = ctx.r1.s64 + 84;
	// lvlx v0,r30,r4
	temp.u32 = r30.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// vor v0,v0,v12
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// stw r16,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r16.u32);
	// addi r28,r1,96
	r28.s64 = ctx.r1.s64 + 96;
	// addi r26,r1,84
	r26.s64 = ctx.r1.s64 + 84;
	// addi r25,r1,96
	r25.s64 = ctx.r1.s64 + 96;
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// addi r24,r1,84
	r24.s64 = ctx.r1.s64 + 84;
	// stvrx v0,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r31,472
	ctx.r9.s64 = r31.s64 + 472;
	// lfs f9,1940(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 1940);
	ctx.f9.f64 = double(temp.f32);
	// addi r23,r1,96
	r23.s64 = ctx.r1.s64 + 96;
	// lfs f10,1936(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1936);
	ctx.f10.f64 = double(temp.f32);
	// addi r22,r1,84
	r22.s64 = ctx.r1.s64 + 84;
	// lfs f0,13344(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 13344);
	f0.f64 = double(temp.f32);
	// addi r21,r1,96
	r21.s64 = ctx.r1.s64 + 96;
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r20,r1,84
	r20.s64 = ctx.r1.s64 + 84;
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r19,r1,96
	r19.s64 = ctx.r1.s64 + 96;
	// lvlx v0,0,r3
	temp.u32 = r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r17,r1,84
	r17.s64 = ctx.r1.s64 + 84;
	// lvlx v12,0,r27
	temp.u32 = r0.u32 + r27.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lis r18,-32246
	r18.s64 = -2113273856;
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r15,r1,96
	r15.s64 = ctx.r1.s64 + 96;
	// lvlx v0,0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r14,-32256
	r14.s64 = -2113929216;
	// lvlx v11,0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v0,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v11,v12,3,2
	simd::store_f32(ctx.v11.f32, simd::blend_f32<3>(simd::load_f32(ctx.v11.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// addi r18,r18,-2164
	r18.s64 = r18.s64 + -2164;
	// addi r16,r1,92
	r16.s64 = ctx.r1.s64 + 92;
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// stvlx v11,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v11), 15 - i));
}
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// stvrx v11,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v11), i));
}
	// addi r9,r31,488
	ctx.r9.s64 = r31.s64 + 488;
	// lfs f9,21572(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 21572);
	ctx.f9.f64 = double(temp.f32);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lfs f10,21560(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 21560);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r26
	temp.u32 = r0.u32 + r26.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v12,0,r25
	temp.u32 = r0.u32 + r25.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v10,0,r24
	temp.u32 = r0.u32 + r24.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// lvlx v11,0,r23
	temp.u32 = r0.u32 + r23.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v10,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v10.f32))));
	// vrlimi128 v11,v12,3,2
	simd::store_f32(ctx.v11.f32, simd::blend_f32<3>(simd::load_f32(ctx.v11.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v11,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v11), 15 - i));
}
	// stvrx v11,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v11), i));
}
	// addi r9,r31,504
	ctx.r9.s64 = r31.s64 + 504;
	// lfs f10,23068(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 23068);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,24724(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 24724);
	ctx.f8.f64 = double(temp.f32);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lfs f9,22708(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 22708);
	ctx.f9.f64 = double(temp.f32);
	// addi r11,r31,520
	r11.s64 = r31.s64 + 520;
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r22
	temp.u32 = r0.u32 + r22.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v12,0,r21
	temp.u32 = r0.u32 + r21.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// stfs f8,84(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v0,0,r20
	temp.u32 = r0.u32 + r20.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v11,0,r19
	temp.u32 = r0.u32 + r19.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v0,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v11,v12,3,2
	simd::store_f32(ctx.v11.f32, simd::blend_f32<3>(simd::load_f32(ctx.v11.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v11,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v11), 15 - i));
}
	// stvrx v11,r9,r10
{
	uint32_t addr = 
ctx.r9.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v11), i));
}
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r17
	temp.u32 = r0.u32 + r17.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f11,3128(r14)
	temp.u32 = PPC_LOAD_U32(r14.u32 + 3128);
	ctx.f11.f64 = double(temp.f32);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v12,0,r15
	temp.u32 = r0.u32 + r15.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f10,-25236(r18)
	temp.u32 = PPC_LOAD_U32(r18.u32 + -25236);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r16
	temp.u32 = r0.u32 + r16.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v12,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// lvlx v12,0,r3
	temp.u32 = r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v11,v12,3,2
	simd::store_f32(ctx.v11.f32, simd::blend_f32<3>(simd::load_f32(ctx.v11.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v11,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v11), 15 - i));
}
	// stvrx v11,r11,r10
{
	uint32_t addr = 
r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v11), i));
}
	// addi r11,r31,536
	r11.s64 = r31.s64 + 536;
	// stvlx v13,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r11,r10
{
	uint32_t addr = 
r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r11,r31,552
	r11.s64 = r31.s64 + 552;
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f13,1076(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1076);
	ctx.f13.f64 = double(temp.f32);
	// lvlx v13,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// lvlx v0,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v12,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v12,v13,3,2
	simd::store_f32(ctx.v12.f32, simd::blend_f32<3>(simd::load_f32(ctx.v12.f32), simd::permute_f32<78>(simd::load_f32(ctx.v13.f32))));
	// stvlx v12,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v12), 15 - i));
}
	// stvrx v12,r11,r10
{
	uint32_t addr = 
r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v12), i));
}
	// lhz r11,372(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 372);
	// lhz r10,374(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 374);
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r10,r11,27,5,26
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFE0;
	// b 0x82ba1b5c
	goto loc_82BA1B5C;
loc_82BA1824:
	// cmpwi cr6,r8,1
	cr6.compare<int32_t>(ctx.r8.s32, 1, xer);
	// bne cr6,0x82ba1b68
	if (!cr6.eq) goto loc_82BA1B68;
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lis r8,-32247
	ctx.r8.s64 = -2113339392;
	// addi r28,r1,100
	r28.s64 = ctx.r1.s64 + 100;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// std r11,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, r11.u64);
	// addi r7,r8,24724
	ctx.r7.s64 = ctx.r8.s64 + 24724;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// addi r6,r11,-2164
	ctx.r6.s64 = r11.s64 + -2164;
	// lfs f11,19348(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 19348);
	ctx.f11.f64 = double(temp.f32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stfs f11,100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// addi r30,r1,92
	r30.s64 = ctx.r1.s64 + 92;
	// lvlx v12,0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f10,3216(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3216);
	ctx.f10.f64 = double(temp.f32);
	// addi r10,r1,84
	ctx.r10.s64 = ctx.r1.s64 + 84;
	// lfs f0,13344(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 13344);
	f0.f64 = double(temp.f32);
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r27,r1,96
	r27.s64 = ctx.r1.s64 + 96;
	// lfs f12,-25688(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -25688);
	ctx.f12.f64 = double(temp.f32);
	// addi r9,r31,408
	ctx.r9.s64 = r31.s64 + 408;
	// stfs f12,92(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// li r11,16
	r11.s64 = 16;
	// lvlx v0,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// li r26,3
	r26.s64 = 3;
	// lfd f9,120(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// addi r10,r10,2024
	ctx.r10.s64 = ctx.r10.s64 + 2024;
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lvlx v13,0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f13,13356(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 13356);
	ctx.f13.f64 = double(temp.f32);
	// vrlimi128 v12,v13,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lvlx v13,0,r3
	temp.u32 = r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// li r3,12
	ctx.r3.s64 = 12;
	// lvlx v11,0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// vrlimi128 v11,v13,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// addi r29,r10,16
	r29.s64 = ctx.r10.s64 + 16;
	// addi r30,r10,32
	r30.s64 = ctx.r10.s64 + 32;
	// li r3,32
	ctx.r3.s64 = 32;
	// lis r28,-32240
	r28.s64 = -2112880640;
	// lis r26,-32240
	r26.s64 = -2112880640;
	// fmuls f10,f9,f10
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmuls f12,f10,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fdivs f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 / ctx.f10.f64));
	// stfs f9,80(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fdivs f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 / ctx.f10.f64));
	// stfs f9,92(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lvlx v13,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v10,0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v10,v13,4,3
	simd::store_f32(ctx.v10.f32, simd::blend_f32<4>(simd::load_f32(ctx.v10.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// vrlimi128 v10,v11,3,2
	simd::store_f32(ctx.v10.f32, simd::blend_f32<3>(simd::load_f32(ctx.v10.f32), simd::permute_f32<78>(simd::load_f32(ctx.v11.f32))));
	// lvlx v13,0,r27
	temp.u32 = r0.u32 + r27.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// stvlx v10,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v10), 15 - i));
}
	// stvrx v10,r9,r11
{
	uint32_t addr = 
ctx.r9.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v10), i));
}
	// vrlimi128 v13,v12,3,2
	simd::store_f32(ctx.v13.f32, simd::blend_f32<3>(simd::load_f32(ctx.v13.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// lvlx v12,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r31,424
	ctx.r9.s64 = r31.s64 + 424;
	// lvrx v0,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v0.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor v0,v12,v0
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// stvrx v0,r9,r11
{
	uint32_t addr = 
ctx.r9.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r31,440
	ctx.r9.s64 = r31.s64 + 440;
	// lvlx v0,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r10,r31,456
	ctx.r10.s64 = r31.s64 + 456;
	// lvrx v12,r11,r29
	temp.u32 = r11.u32 + r29.u32;
	simd::store_i8(ctx.v12.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor v0,v0,v12
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8)));
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// stvrx v0,r9,r11
{
	uint32_t addr = 
ctx.r9.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lis r14,-32256
	r14.s64 = -2113929216;
	// lvlx v12,r5,r3
	temp.u32 = ctx.r5.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx v0,r11,r30
	temp.u32 = r11.u32 + r30.u32;
	simd::store_i8(ctx.v0.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// stw r14,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r14.u32);
	// addi r14,r1,80
	r14.s64 = ctx.r1.s64 + 80;
	// vor v0,v12,v0
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v0.u8)));
	// addi r29,r1,80
	r29.s64 = ctx.r1.s64 + 80;
	// stw r14,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r14.u32);
	// lis r14,-32256
	r14.s64 = -2113929216;
	// addi r27,r1,80
	r27.s64 = ctx.r1.s64 + 80;
	// stw r14,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r14.u32);
	// addi r25,r1,80
	r25.s64 = ctx.r1.s64 + 80;
	// stvlx v0,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// addi r23,r1,92
	r23.s64 = ctx.r1.s64 + 92;
	// stvrx v0,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r10,r31,472
	ctx.r10.s64 = r31.s64 + 472;
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r24,r1,84
	r24.s64 = ctx.r1.s64 + 84;
	// addi r22,r1,80
	r22.s64 = ctx.r1.s64 + 80;
	// addi r21,r1,92
	r21.s64 = ctx.r1.s64 + 92;
	// addi r18,r1,92
	r18.s64 = ctx.r1.s64 + 92;
	// addi r19,r1,80
	r19.s64 = ctx.r1.s64 + 80;
	// addi r20,r1,84
	r20.s64 = ctx.r1.s64 + 84;
	// addi r17,r1,92
	r17.s64 = ctx.r1.s64 + 92;
	// addi r16,r1,80
	r16.s64 = ctx.r1.s64 + 80;
	// addi r15,r1,80
	r15.s64 = ctx.r1.s64 + 80;
	// addi r14,r1,84
	r14.s64 = ctx.r1.s64 + 84;
	// lis r30,-32240
	r30.s64 = -2112880640;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f12,1940(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 1940);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// lvlx v0,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f12,1936(r28)
	temp.u32 = PPC_LOAD_U32(r28.u32 + 1936);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v10,0,r27
	temp.u32 = r0.u32 + r27.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v11,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// vrlimi128 v0,v10,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v10.f32))));
	// vrlimi128 v0,v12,3,2
	simd::store_f32(ctx.v0.f32, simd::blend_f32<3>(simd::load_f32(ctx.v0.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v0,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// stvrx v0,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r10,r31,488
	ctx.r10.s64 = r31.s64 + 488;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f13,92(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f12,21572(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 21572);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v11,0,r23
	temp.u32 = r0.u32 + r23.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v0,0,r25
	temp.u32 = r0.u32 + r25.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f12,21560(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 21560);
	ctx.f12.f64 = double(temp.f32);
	// vrlimi128 v0,v11,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r22
	temp.u32 = r0.u32 + r22.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v12,0,r24
	temp.u32 = r0.u32 + r24.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v11,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// vrlimi128 v0,v12,3,2
	simd::store_f32(ctx.v0.f32, simd::blend_f32<3>(simd::load_f32(ctx.v0.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v0,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// stvrx v0,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r10,r31,504
	ctx.r10.s64 = r31.s64 + 504;
	// lfs f12,24724(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 24724);
	ctx.f12.f64 = double(temp.f32);
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f12,23068(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 23068);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,92(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lvlx v12,0,r21
	temp.u32 = r0.u32 + r21.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lvlx v10,0,r18
	temp.u32 = r0.u32 + r18.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f12,22708(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 22708);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// lvlx v11,0,r19
	temp.u32 = r0.u32 + r19.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r20
	temp.u32 = r0.u32 + r20.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v0,v12,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// vrlimi128 v10,v11,4,3
	simd::store_f32(ctx.v10.f32, simd::blend_f32<4>(simd::load_f32(ctx.v10.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// vrlimi128 v10,v0,3,2
	simd::store_f32(ctx.v10.f32, simd::blend_f32<3>(simd::load_f32(ctx.v10.f32), simd::permute_f32<78>(simd::load_f32(ctx.v0.f32))));
	// stvlx v10,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v10), 15 - i));
}
	// stvrx v10,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v10), i));
}
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r10,r31,520
	ctx.r10.s64 = r31.s64 + 520;
	// lfs f13,-25236(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -25236);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// lvlx v12,0,r15
	temp.u32 = r0.u32 + r15.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f13,3128(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3128);
	ctx.f13.f64 = double(temp.f32);
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(104) );
	// lvlx v11,0,r16
	temp.u32 = r0.u32 + r16.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v10,0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lvlx v0,0,r17
	temp.u32 = r0.u32 + r17.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v0,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v10,v12,4,3
	simd::store_f32(ctx.v10.f32, simd::blend_f32<4>(simd::load_f32(ctx.v10.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// vrlimi128 v10,v11,3,2
	simd::store_f32(ctx.v10.f32, simd::blend_f32<3>(simd::load_f32(ctx.v10.f32), simd::permute_f32<78>(simd::load_f32(ctx.v11.f32))));
	// stvlx v10,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v10), 15 - i));
}
	// stvrx v10,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v10), i));
}
	// addi r11,r31,536
	r11.s64 = r31.s64 + 536;
	// stvlx v13,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r11,r9
{
	uint32_t addr = 
r11.u32 + ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r11,r31,552
	r11.s64 = r31.s64 + 552;
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v11,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// lfs f0,1076(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 1076);
	f0.f64 = double(temp.f32);
	// lvlx v13,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f13,3144(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 3144);
	ctx.f13.f64 = double(temp.f32);
	// lvlx v12,0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v11,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r14
	temp.u32 = r0.u32 + r14.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v0,v13,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// vrlimi128 v12,v0,3,2
	simd::store_f32(ctx.v12.f32, simd::blend_f32<3>(simd::load_f32(ctx.v12.f32), simd::permute_f32<78>(simd::load_f32(ctx.v0.f32))));
	// stvlx v12,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v12), 15 - i));
}
	// stvrx v12,r11,r8
{
	uint32_t addr = 
r11.u32 + ctx.r8.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v12), i));
}
	// lhz r10,372(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 372);
	// lhz r11,374(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 374);
	// mullw r11,r11,r10
	r11.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,23,9,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 23) & 0x7FFFFE;
loc_82BA1B5C:
	// addi r11,r31,372
	r11.s64 = r31.s64 + 372;
	// stw r10,376(r31)
	PPC_STORE_U32(r31.u32 + 376, ctx.r10.u32);
	// b 0x82ba1ebc
	goto loc_82BA1EBC;
loc_82BA1B68:
	// cmpwi cr6,r8,2
	cr6.compare<int32_t>(ctx.r8.s32, 2, xer);
	// bne cr6,0x82ba1ebc
	if (!cr6.eq) goto loc_82BA1EBC;
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// li r11,16
	r11.s64 = 16;
	// addi r10,r10,2120
	ctx.r10.s64 = ctx.r10.s64 + 2120;
	// addi r9,r31,424
	ctx.r9.s64 = r31.s64 + 424;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// lis r26,-32246
	r26.s64 = -2113273856;
	// li r21,3
	r21.s64 = 3;
	// lvlx v13,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r24,r26,-2164
	r24.s64 = r26.s64 + -2164;
	// lvrx v0,r11,r10
	temp.u32 = r11.u32 + ctx.r10.u32;
	simd::store_i8(ctx.v0.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// li r20,6
	r20.s64 = 6;
	// vor v0,v13,v0
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v13.u8), simd::load_i8(ctx.v0.u8)));
	// addi r7,r10,32
	ctx.r7.s64 = ctx.r10.s64 + 32;
	// li r6,32
	ctx.r6.s64 = 32;
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// lis r3,-32247
	ctx.r3.s64 = -2113339392;
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// addi r30,r1,88
	r30.s64 = ctx.r1.s64 + 88;
	// stvrx v0,r9,r11
{
	uint32_t addr = 
ctx.r9.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r31,440
	ctx.r9.s64 = r31.s64 + 440;
	// stw r26,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r26.u32);
	// addi r3,r3,24724
	ctx.r3.s64 = ctx.r3.s64 + 24724;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r24.u32);
	// addi r27,r1,92
	r27.s64 = ctx.r1.s64 + 92;
	// stw r21,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r21.u32);
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// stw r20,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r20.u32);
	// addi r29,r1,104
	r29.s64 = ctx.r1.s64 + 104;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// addi r25,r1,88
	r25.s64 = ctx.r1.s64 + 88;
	// addi r26,r1,104
	r26.s64 = ctx.r1.s64 + 104;
	// addi r23,r1,88
	r23.s64 = ctx.r1.s64 + 88;
	// addi r22,r1,104
	r22.s64 = ctx.r1.s64 + 104;
	// mr r21,r11
	r21.u64 = r11.u64;
	// mr r20,r11
	r20.u64 = r11.u64;
	// lvlx v0,r10,r11
	temp.u32 = ctx.r10.u32 + r11.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvrx v13,r11,r8
	temp.u32 = r11.u32 + ctx.r8.u32;
	simd::store_i8(ctx.v13.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// vor v0,v0,v13
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v13.u8)));
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// stvrx v0,r9,r11
{
	uint32_t addr = 
ctx.r9.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// addi r9,r31,456
	ctx.r9.s64 = r31.s64 + 456;
	// lvlx v0,r10,r6
	temp.u32 = ctx.r10.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r10,r31,472
	ctx.r10.s64 = r31.s64 + 472;
	// lvrx v13,r11,r7
	temp.u32 = r11.u32 + ctx.r7.u32;
	simd::store_i8(ctx.v13.u8, simd::load_unaligned_vector_right(base, temp.u32));
	// vor v0,v0,v13
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v13.u8)));
	// stvlx v0,0,r9
{
	uint32_t addr = 
ctx.r9.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// stvrx v0,r9,r11
{
	uint32_t addr = 
ctx.r9.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// lfs f13,1936(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 1936);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,88(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v0,0,r30
	temp.u32 = r0.u32 + r30.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f0,13344(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 13344);
	f0.f64 = double(temp.f32);
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lvlx v12,0,r27
	temp.u32 = r0.u32 + r27.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f13,1940(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 1940);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,104(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lvlx v13,0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// lvlx v0,0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v13,v12,3,2
	simd::store_f32(ctx.v13.f32, simd::blend_f32<3>(simd::load_f32(ctx.v13.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v13,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// lfs f12,23068(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 23068);
	ctx.f12.f64 = double(temp.f32);
	// lwz r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// lfs f13,-16400(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + -16400);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r31,488
	ctx.r10.s64 = r31.s64 + 488;
	// stfs f13,104(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lfs f12,13356(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 13356);
	ctx.f12.f64 = double(temp.f32);
	// lvlx v13,0,r26
	temp.u32 = r0.u32 + r26.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,104(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lvlx v0,0,r25
	temp.u32 = r0.u32 + r25.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v12,0,r22
	temp.u32 = r0.u32 + r22.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v11,0,r23
	temp.u32 = r0.u32 + r23.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v11,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v11.f32))));
	// vrlimi128 v12,v13,3,2
	simd::store_f32(ctx.v12.f32, simd::blend_f32<3>(simd::load_f32(ctx.v12.f32), simd::permute_f32<78>(simd::load_f32(ctx.v13.f32))));
	// stvlx v12,0,r10
{
	uint32_t addr = 
ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v12), 15 - i));
}
	// mr r31,r7
	r31.u64 = ctx.r7.u64;
	// stvrx v12,r10,r11
{
	uint32_t addr = 
ctx.r10.u32 + r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v12), i));
}
	// addi r7,r7,504
	ctx.r7.s64 = ctx.r7.s64 + 504;
	// addi r6,r1,104
	ctx.r6.s64 = ctx.r1.s64 + 104;
	// stw r7,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r7.u32);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r30,r11
	r30.u64 = r11.u64;
	// lis r29,-32240
	r29.s64 = -2112880640;
	// lis r28,-32256
	r28.s64 = -2113929216;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// addi r25,r1,92
	r25.s64 = ctx.r1.s64 + 92;
	// addi r27,r1,104
	r27.s64 = ctx.r1.s64 + 104;
	// addi r26,r1,80
	r26.s64 = ctx.r1.s64 + 80;
	// addi r24,r1,92
	r24.s64 = ctx.r1.s64 + 92;
	// addi r23,r1,84
	r23.s64 = ctx.r1.s64 + 84;
	// addi r22,r1,88
	r22.s64 = ctx.r1.s64 + 88;
	// addi r19,r1,96
	r19.s64 = ctx.r1.s64 + 96;
	// addi r18,r1,88
	r18.s64 = ctx.r1.s64 + 88;
	// addi r17,r1,92
	r17.s64 = ctx.r1.s64 + 92;
	// addi r16,r1,84
	r16.s64 = ctx.r1.s64 + 84;
	// lfs f11,21580(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 21580);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,-15748(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -15748);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// lvlx v0,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// lfs f12,21560(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 21560);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f11,104(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lfs f12,22708(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 22708);
	ctx.f12.f64 = double(temp.f32);
	// lis r3,-32240
	ctx.r3.s64 = -2112880640;
	// stfs f12,80(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v13,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lvlx v12,0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r6,r1,108
	ctx.r6.s64 = ctx.r1.s64 + 108;
	// vrlimi128 v13,v12,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// lvlx v12,0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v13,v12,3,2
	simd::store_f32(ctx.v13.f32, simd::blend_f32<3>(simd::load_f32(ctx.v13.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v13,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r11,r21
{
	uint32_t addr = 
r11.u32 + r21.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r11,r31,520
	r11.s64 = r31.s64 + 520;
	// lfs f10,1076(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 1076);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,88(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lfs f10,3144(r28)
	temp.u32 = PPC_LOAD_U32(r28.u32 + 3144);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lfs f10,3128(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3128);
	ctx.f10.f64 = double(temp.f32);
	// li r10,48
	ctx.r10.s64 = 48;
	// stfs f10,80(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f10,3216(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 3216);
	ctx.f10.f64 = double(temp.f32);
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// stfs f10,92(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lvlx v12,0,r25
	temp.u32 = r0.u32 + r25.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v0,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lvlx v13,0,r27
	temp.u32 = r0.u32 + r27.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// lvlx v0,0,r26
	temp.u32 = r0.u32 + r26.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v0,v12,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// vrlimi128 v13,v0,3,2
	simd::store_f32(ctx.v13.f32, simd::blend_f32<3>(simd::load_f32(ctx.v13.f32), simd::permute_f32<78>(simd::load_f32(ctx.v0.f32))));
	// stvlx v13,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r11,r20
{
	uint32_t addr = 
r11.u32 + r20.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r11,r31,552
	r11.s64 = r31.s64 + 552;
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f0,92(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f12,-25688(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25688);
	ctx.f12.f64 = double(temp.f32);
	// lwz r7,108(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(108) );
	// lvlx v13,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// lfs f10,1932(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 1932);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,88(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r24
	temp.u32 = r0.u32 + r24.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v11,0,r22
	temp.u32 = r0.u32 + r22.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// lvlx v12,0,r23
	temp.u32 = r0.u32 + r23.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v12,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// vrlimi128 v11,v13,3,2
	simd::store_f32(ctx.v11.f32, simd::blend_f32<3>(simd::load_f32(ctx.v11.f32), simd::permute_f32<78>(simd::load_f32(ctx.v13.f32))));
	// li r10,16
	ctx.r10.s64 = 16;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r28,r1,108
	r28.s64 = ctx.r1.s64 + 108;
	// addi r29,r1,104
	r29.s64 = ctx.r1.s64 + 104;
	// stvlx v11,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v11), 15 - i));
}
	// stvrx v11,r11,r30
{
	uint32_t addr = 
r11.u32 + r30.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v11), i));
}
	// addi r11,r31,408
	r11.s64 = r31.s64 + 408;
	// stfs f11,88(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v0,0,r18
	temp.u32 = r0.u32 + r18.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f12,-25700(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25700);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,92(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f12,-25436(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25436);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r17
	temp.u32 = r0.u32 + r17.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lfs f12,1928(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 1928);
	ctx.f12.f64 = double(temp.f32);
	// lvlx v12,0,r16
	temp.u32 = r0.u32 + r16.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v13,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v13,0,r19
	temp.u32 = r0.u32 + r19.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v0,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v13,v12,3,2
	simd::store_f32(ctx.v13.f32, simd::blend_f32<3>(simd::load_f32(ctx.v13.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// stvlx v13,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v13), 15 - i));
}
	// stvrx v13,r11,r10
{
	uint32_t addr = 
r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v13), i));
}
	// addi r11,r31,536
	r11.s64 = r31.s64 + 536;
	// stfs f0,108(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f13,104(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lfs f0,-2164(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -2164);
	f0.f64 = double(temp.f32);
	// lvlx v0,0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,108(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// lvlx v13,0,r3
	temp.u32 = r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v0,v13,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// lvlx v13,0,r28
	temp.u32 = r0.u32 + r28.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v12,0,r29
	temp.u32 = r0.u32 + r29.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v12,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// vrlimi128 v0,v13,3,2
	simd::store_f32(ctx.v0.f32, simd::blend_f32<3>(simd::load_f32(ctx.v0.f32), simd::permute_f32<78>(simd::load_f32(ctx.v13.f32))));
	// stvlx v0,0,r11
{
	uint32_t addr = 
r11.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < (16 - tmp_off); i++)
		PPC_STORE_U8(addr + i, simd::extract_u8(simd::to_vec128i(ctx.v0), 15 - i));
}
	// stvrx v0,r11,r10
{
	uint32_t addr = 
r11.u32 + ctx.r10.u32;
	uint32_t tmp_off = addr & 0xF;
	for (size_t i = 0; i < tmp_off; i++)
		PPC_STORE_U8(addr - i - 1, simd::extract_u8(simd::to_vec128i(ctx.v0), i));
}
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(120) );
	// lhz r10,2(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// addi r10,r10,-48
	ctx.r10.s64 = ctx.r10.s64 + -48;
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// addi r11,r11,-80
	r11.s64 = r11.s64 + -80;
	// divwu r11,r11,r4
	r11.u32 = r11.u32 / ctx.r4.u32;
	// divwu r10,r10,r5
	ctx.r10.u32 = ctx.r10.u32 / ctx.r5.u32;
	// mullw r11,r10,r11
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// rlwinm r11,r11,6,0,25
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 6) & 0xFFFFFFC0;
	// stw r11,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r11.u32);
loc_82BA1EBC:
	// lwz r9,376(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(376) );
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(112) );
	// li r5,260
	ctx.r5.s64 = 260;
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// mullw r11,r9,r11
	r11.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// lwz r4,580(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(580) );
	// lwz r10,22116(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(22116) );
	// addi r11,r11,511
	r11.s64 = r11.s64 + 511;
	// mullw r9,r9,r8
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// stw r9,388(r31)
	PPC_STORE_U32(r31.u32 + 388, ctx.r9.u32);
	// rlwinm r11,r11,23,16,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 23) & 0xFFFF;
	// rldicr r30,r10,20,63
	r30.u64 = rotl64(ctx.r10.u64, 20) & 0xFFFFFFFFFFFFFFFF;
	// sth r11,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r11.u16);
	// addi r11,r11,172
	r11.s64 = r11.s64 + 172;
	// rlwinm r25,r11,9,0,22
	r25.u64 = rotl64(r11.u32 | (r11.u64 << 32), 9) & 0xFFFFFE00;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// li r26,0
	r26.s64 = 0;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// stb r26,387(r1)
	PPC_STORE_U8(ctx.r1.u32 + 387, r26.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BA1F14:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ba1f14
	if (!cr6.eq) goto loc_82BA1F14;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ba1f64
	if (cr6.lt) goto loc_82BA1F64;
loc_82BA1F48:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi cr6,r10,92
	cr6.compare<uint32_t>(ctx.r10.u32, 92, xer);
	// beq cr6,0x82ba1f64
	if (cr6.eq) goto loc_82BA1F64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ba1f48
	if (!cr6.lt) goto loc_82BA1F48;
loc_82BA1F64:
	// stb r26,1(r11)
	PPC_STORE_U8(r11.u32 + 1, r26.u8);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,120
	ctx.r4.s64 = ctx.r1.s64 + 120;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x83004d18
	sub_83004D18(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba1fb0
	if (cr0.eq) goto loc_82BA1FB0;
	// lis r10,640
	ctx.r10.s64 = 41943040;
	// ld r11,120(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// add r9,r30,r10
	ctx.r9.u64 = r30.u64 + ctx.r10.u64;
	// cmpld cr6,r11,r9
	cr6.compare<uint64_t>(r11.u64, ctx.r9.u64, xer);
	// bge cr6,0x82ba1fb0
	if (!cr6.lt) goto loc_82BA1FB0;
	// cmpld cr6,r11,r10
	cr6.compare<uint64_t>(r11.u64, ctx.r10.u64, xer);
	// ble cr6,0x82ba1fac
	if (!cr6.gt) goto loc_82BA1FAC;
	// lis r10,-640
	ctx.r10.s64 = -41943040;
	// add r30,r11,r10
	r30.u64 = r11.u64 + ctx.r10.u64;
	// b 0x82ba1fb0
	goto loc_82BA1FB0;
loc_82BA1FAC:
	// mr r30,r26
	r30.u64 = r26.u64;
loc_82BA1FB0:
	// addi r11,r25,2048
	r11.s64 = r25.s64 + 2048;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// cmpld cr6,r11,r30
	cr6.compare<uint64_t>(r11.u64, r30.u64, xer);
	// ble cr6,0x82ba1fcc
	if (!cr6.gt) goto loc_82BA1FCC;
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// b 0x82ba2138
	goto loc_82BA2138;
loc_82BA1FCC:
	// lis r11,-17
	r11.s64 = -1114112;
	// lwz r9,596(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// oris r8,r10,65520
	ctx.r8.u64 = ctx.r10.u64 | 4293918720;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// rotlwi r10,r30,0
	ctx.r10.u64 = rotl32(r30.u32, 0);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// mr r28,r26
	r28.u64 = r26.u64;
	// divdu r11,r11,r8
	r11.u64 = r11.u64 / ctx.r8.u64;
	// li r27,-1
	r27.s64 = -1;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// rlwimi r9,r11,14,12,17
	ctx.r9.u64 = (rotl32(r11.u32, 14) & 0xFC000) | (ctx.r9.u64 & 0xFFFFFFFFFFF03FFF);
	// rlwinm r11,r9,18,26,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 18) & 0x3F;
	// stw r9,596(r31)
	PPC_STORE_U32(r31.u32 + 596, ctx.r9.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rlwinm r11,r11,20,0,11
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 20) & 0xFFF00000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r11.u32);
loc_82BA2018:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ba2034
	if (cr6.eq) goto loc_82BA2034;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// mr r29,r26
	r29.u64 = r26.u64;
	// srawi r11,r11,30
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 30;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82ba2038
	if (cr6.eq) goto loc_82BA2038;
loc_82BA2034:
	// lwz r29,616(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(616) );
loc_82BA2038:
	// li r8,4096
	ctx.r8.s64 = 4096;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1028
	ctx.r5.s64 = 1028;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// mr r30,r26
	r30.u64 = r26.u64;
	// bl 0x832b263c
	__imp__MmAllocatePhysicalMemoryEx(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x82ba208c
	if (!cr0.eq) goto loc_82BA208C;
	// lis r4,-18048
	ctx.r4.s64 = -1182793728;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8222ca28
	sub_8222CA28(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82ba207c
	if (cr0.eq) goto loc_82BA207C;
	// li r30,1
	r30.s64 = 1;
	// b 0x82ba208c
	goto loc_82BA208C;
loc_82BA207C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba208c
	if (cr6.eq) goto loc_82BA208C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// li r30,2
	r30.s64 = 2;
loc_82BA208C:
	// rlwinm r11,r28,2,0,29
	r11.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stwx r3,r11,r31
	PPC_STORE_U32(r11.u32 + r31.u32, ctx.r3.u32);
	// beq cr6,0x82ba2130
	if (cr6.eq) goto loc_82BA2130;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82ba20b0
	if (!cr6.eq) goto loc_82BA20B0;
	// rlwimi r11,r30,30,0,1
	r11.u64 = (rotl32(r30.u32, 30) & 0xC0000000) | (r11.u64 & 0xFFFFFFFF3FFFFFFF);
	// b 0x82ba20b4
	goto loc_82BA20B4;
loc_82BA20B0:
	// rlwimi r11,r30,28,2,3
	r11.u64 = (rotl32(r30.u32, 28) & 0x30000000) | (r11.u64 & 0xFFFFFFFFCFFFFFFF);
loc_82BA20B4:
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// stw r11,612(r31)
	PPC_STORE_U32(r31.u32 + 612, r11.u32);
	// cmplwi cr6,r28,2
	cr6.compare<uint32_t>(r28.u32, 2, xer);
	// blt cr6,0x82ba2018
	if (cr6.lt) goto loc_82BA2018;
	// li r8,4096
	ctx.r8.s64 = 4096;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,1536
	ctx.r4.s64 = 1536;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x832b263c
	__imp__MmAllocatePhysicalMemoryEx(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,592(r31)
	PPC_STORE_U32(r31.u32 + 592, ctx.r3.u32);
	// beq 0x82ba20f8
	if (cr0.eq) goto loc_82BA20F8;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// rlwinm r11,r11,0,5,3
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFF7FFFFFF;
	// b 0x82ba2118
	goto loc_82BA2118;
loc_82BA20F8:
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// li r3,1536
	ctx.r3.s64 = 1536;
	// bl 0x8222ca28
	sub_8222CA28(ctx, base);
	// stw r3,592(r31)
	PPC_STORE_U32(r31.u32 + 592, ctx.r3.u32);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82ba2130
	if (cr0.eq) goto loc_82BA2130;
	// lwz r11,612(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(612) );
	// oris r11,r11,2048
	r11.u64 = r11.u64 | 134217728;
loc_82BA2118:
	// stw r11,612(r31)
	PPC_STORE_U32(r31.u32 + 612, r11.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// ori r11,r11,16
	r11.u64 = r11.u64 | 16;
	// stb r11,608(r31)
	PPC_STORE_U8(r31.u32 + 608, r11.u8);
	// b 0x82ba2138
	goto loc_82BA2138;
loc_82BA2130:
	// lis r3,-32761
	ctx.r3.s64 = -2147024896;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
loc_82BA2138:
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BA1490) {
	__imp__sub_82BA1490(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA2140) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lhz r11,14(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 14);
	// lwz r8,596(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(596) );
	// addi r10,r11,172
	ctx.r10.s64 = r11.s64 + 172;
	// lwz r11,380(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(380) );
	// rlwinm r9,r8,18,26,31
	ctx.r9.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 18) & 0x3F;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// rlwinm r10,r10,9,0,22
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 9) & 0xFFFFFE00;
	// rlwinm r8,r8,12,26,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0x3F;
	// stw r11,380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 380, r11.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ba217c
	if (!cr6.eq) goto loc_82BA217C;
	// lwz r9,384(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(384) );
	// b 0x82ba2180
	goto loc_82BA2180;
loc_82BA217C:
	// lis r9,-16
	ctx.r9.s64 = -1048576;
loc_82BA2180:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ba2198
	if (cr6.lt) goto loc_82BA2198;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
loc_82BA2198:
	// addi r10,r8,46
	ctx.r10.s64 = ctx.r8.s64 + 46;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r11,r10,r3
	PPC_STORE_U32(ctx.r10.u32 + ctx.r3.u32, r11.u32);
	// lwz r10,596(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(596) );
	// rlwinm r11,r10,12,26,31
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3F;
	// rlwinm r9,r10,18,26,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 18) & 0x3F;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// twllei r9,0
	// divwu r8,r11,r9
	ctx.r8.u32 = r11.u32 / ctx.r9.u32;
	// mullw r9,r8,r9
	ctx.r9.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// rlwimi r10,r11,20,6,11
	ctx.r10.u64 = (rotl32(r11.u32, 20) & 0x3F00000) | (ctx.r10.u64 & 0xFFFFFFFFFC0FFFFF);
	// rlwinm. r11,r10,0,6,11
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3F00000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,596(r3)
	PPC_STORE_U32(ctx.r3.u32 + 596, ctx.r10.u32);
	// bne 0x82ba21ec
	if (!cr0.eq) goto loc_82BA21EC;
	// lbz r11,608(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 608);
	// li r10,2048
	ctx.r10.s64 = 2048;
	// ori r11,r11,32
	r11.u64 = r11.u64 | 32;
	// stw r10,380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 380, ctx.r10.u32);
	// stb r11,608(r3)
	PPC_STORE_U8(ctx.r3.u32 + 608, r11.u8);
	// blr 
	return;
loc_82BA21EC:
	// li r11,0
	r11.s64 = 0;
	// stw r11,380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 380, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA2140) {
	__imp__sub_82BA2140(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA21F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-816(r1)
	ea = -816 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r30,r24,23360
	r30.s64 = r24.s64 + 23360;
	// lbz r11,23968(r24)
	r11.u64 = PPC_LOAD_U8(r24.u32 + 23968);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba222c
	if (cr0.eq) goto loc_82BA222C;
loc_82BA221C:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82ba2568
	sub_82BA2568(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ba2560
	goto loc_82BA2560;
loc_82BA222C:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r4,r11,24472
	ctx.r4.s64 = r11.s64 + 24472;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// li r25,0
	r25.s64 = 0;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stb r25,371(r1)
	PPC_STORE_U8(ctx.r1.u32 + 371, r25.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BA2250:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ba2250
	if (!cr6.eq) goto loc_82BA2250;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r31,r11,-1
	r31.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ba22a0
	if (cr6.lt) goto loc_82BA22A0;
loc_82BA2284:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,92
	cr6.compare<uint32_t>(r11.u32, 92, xer);
	// beq cr6,0x82ba22a0
	if (cr6.eq) goto loc_82BA22A0;
	// addi r31,r31,-1
	r31.s64 = r31.s64 + -1;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bge cr6,0x82ba2284
	if (!cr6.lt) goto loc_82BA2284;
loc_82BA22A0:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,92
	cr6.compare<uint32_t>(r11.u32, 92, xer);
	// bne cr6,0x82ba221c
	if (!cr6.eq) goto loc_82BA221C;
	// stb r25,0(r31)
	PPC_STORE_U8(r31.u32 + 0, r25.u8);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x832b21fc
	__imp__RtlInitAnsiString(ctx, base);
	// lis r11,-31951
	r11.s64 = -2093940736;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r11,23276
	ctx.r3.s64 = r11.s64 + 23276;
	// bl 0x832b287c
	__imp__ObCreateSymbolicLink(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x82ba221c
	if (cr0.lt) goto loc_82BA221C;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r5,r11,-17452
	ctx.r5.s64 = r11.s64 + -17452;
	// addi r4,r10,-17444
	ctx.r4.s64 = ctx.r10.s64 + -17444;
	// addi r6,r31,1
	ctx.r6.s64 = r31.s64 + 1;
	// addi r3,r1,480
	ctx.r3.s64 = ctx.r1.s64 + 480;
	// bl 0x832b301c
	__imp__sprintf(ctx, base);
	// addi r5,r1,480
	ctx.r5.s64 = ctx.r1.s64 + 480;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82ba1490
	sub_82BA1490(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x82ba221c
	if (cr0.lt) goto loc_82BA221C;
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r4,r1,480
	ctx.r4.s64 = ctx.r1.s64 + 480;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// stb r25,371(r1)
	PPC_STORE_U8(ctx.r1.u32 + 371, r25.u8);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BA2324:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ba2324
	if (!cr6.eq) goto loc_82BA2324;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x82ba2384
	if (cr6.lt) goto loc_82BA2384;
loc_82BA235C:
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// cmpwi cr6,r10,46
	cr6.compare<int32_t>(ctx.r10.s32, 46, xer);
	// beq cr6,0x82ba2384
	if (cr6.eq) goto loc_82BA2384;
	// cmpwi cr6,r10,92
	cr6.compare<int32_t>(ctx.r10.s32, 92, xer);
	// beq cr6,0x82ba2384
	if (cr6.eq) goto loc_82BA2384;
	// addi r29,r29,-1
	r29.s64 = r29.s64 + -1;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ba235c
	if (!cr6.lt) goto loc_82BA235C;
loc_82BA2384:
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(r29.u32 + 0);
	// cmplwi cr6,r10,46
	cr6.compare<uint32_t>(ctx.r10.u32, 46, xer);
	// beq cr6,0x82ba2394
	if (cr6.eq) goto loc_82BA2394;
	// mr r29,r11
	r29.u64 = r11.u64;
loc_82BA2394:
	// lwz r10,596(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r31,r25
	r31.u64 = r25.u64;
	// rlwinm. r10,r10,0,12,17
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFC000;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r27,r11,-17436
	r27.s64 = r11.s64 + -17436;
	// beq 0x82ba247c
	if (cr0.eq) goto loc_82BA247C;
	// addi r28,r30,20
	r28.s64 = r30.s64 + 20;
	// lis r26,-31946
	r26.s64 = -2093613056;
loc_82BA23B4:
	// lbz r11,986(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 986);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// bne 0x82ba221c
	if (!cr0.eq) goto loc_82BA221C;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,480
	ctx.r10.s64 = ctx.r1.s64 + 480;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b301c
	__imp__sprintf(ctx, base);
	// li r9,0
	ctx.r9.s64 = 0;
	// lis r8,26624
	ctx.r8.s64 = 1744830464;
	// li r7,2
	ctx.r7.s64 = 2;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// lis r4,16384
	ctx.r4.s64 = 1073741824;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82cbcc00
	sub_82CBCC00(ctx, base);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x82ba221c
	if (cr6.eq) goto loc_82BA221C;
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// rlwinm r11,r11,18,26,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x82ba2440
	if (!cr6.eq) goto loc_82BA2440;
	// lwz r11,384(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(384) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba2434
	if (cr6.eq) goto loc_82BA2434;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// b 0x82ba2448
	goto loc_82BA2448;
loc_82BA2434:
	// li r11,1
	r11.s64 = 1;
	// rldicr r11,r11,32,63
	r11.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// b 0x82ba2448
	goto loc_82BA2448;
loc_82BA2440:
	// li r11,0
	r11.s64 = 0;
	// oris r11,r11,65520
	r11.u64 = r11.u64 | 4293918720;
loc_82BA2448:
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// bl 0x82cc0fc0
	sub_82CC0FC0(ctx, base);
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// bl 0x82cc14d0
	sub_82CC14D0(ctx, base);
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// rlwinm r11,r11,18,26,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x82ba23b4
	if (cr6.lt) goto loc_82BA23B4;
loc_82BA247C:
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,480
	ctx.r10.s64 = ctx.r1.s64 + 480;
	// subf r11,r11,r29
	r11.s64 = r29.s64 - r11.s64;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
loc_82BA248C:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b301c
	__imp__sprintf(ctx, base);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bl 0x82cc0e68
	sub_82CC0E68(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x82ba248c
	if (!cr0.eq) goto loc_82BA248C;
	// stw r25,348(r30)
	PPC_STORE_U32(r30.u32 + 348, r25.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r25,352(r30)
	PPC_STORE_U32(r30.u32 + 352, r25.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r25,356(r30)
	PPC_STORE_U32(r30.u32 + 356, r25.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r25,360(r30)
	PPC_STORE_U32(r30.u32 + 360, r25.u32);
	// addi r31,r30,348
	r31.s64 = r30.s64 + 348;
	// stw r25,364(r30)
	PPC_STORE_U32(r30.u32 + 364, r25.u32);
	// bl 0x82cbc4b8
	sub_82CBC4B8(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,364(r30)
	PPC_STORE_U32(r30.u32 + 364, ctx.r3.u32);
	// beq 0x82ba221c
	if (cr0.eq) goto loc_82BA221C;
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r25,356(r30)
	PPC_STORE_U32(r30.u32 + 356, r25.u32);
	// stw r25,360(r30)
	PPC_STORE_U32(r30.u32 + 360, r25.u32);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,2048
	ctx.r5.s64 = 2048;
	// lwz r3,20(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// bl 0x82cc0760
	sub_82CC0760(ctx, base);
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,364(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(364) );
	// bl 0x82196c58
	sub_82196C58(ctx, base);
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x832b30cc
	__imp__VdGetCurrentDisplayInformation(ctx, base);
	// li r11,2048
	r11.s64 = 2048;
	// lbz r10,600(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 600);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,380(r30)
	PPC_STORE_U32(r30.u32 + 380, r11.u32);
	// lwz r9,596(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// stw r25,584(r30)
	PPC_STORE_U32(r30.u32 + 584, r25.u32);
	// rlwinm r9,r9,0,12,5
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFC0FFFFF;
	// stw r9,596(r30)
	PPC_STORE_U32(r30.u32 + 596, ctx.r9.u32);
	// lbz r11,389(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 389);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwimi r10,r11,2,24,24
	ctx.r10.u64 = (rotl32(r11.u32, 2) & 0x80) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF7F);
	// stb r10,600(r30)
	PPC_STORE_U8(r30.u32 + 600, ctx.r10.u8);
loc_82BA2560:
	// addi r1,r1,816
	ctx.r1.s64 = ctx.r1.s64 + 816;
	// b 0x82ca2c28
	return;
}

PPC_WEAK_FUNC(sub_82BA21F8) {
	__imp__sub_82BA21F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA2568) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stwu r1,-2320(r1)
	ea = -2320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// addi r31,r26,23360
	r31.s64 = r26.s64 + 23360;
	// lbz r11,23968(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 23968);
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82ba2924
	if (cr0.eq) goto loc_82BA2924;
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(364) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba28fc
	if (cr6.eq) goto loc_82BA28FC;
	// rlwinm. r10,r11,0,0,24
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r28,-1
	r28.s64 = -1;
	// beq 0x82ba26e8
	if (cr0.eq) goto loc_82BA26E8;
	// rlwinm. r11,r11,0,25,25
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba25b4
	if (cr0.eq) goto loc_82BA25B4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82196c58
	sub_82196C58(ctx, base);
loc_82BA25B4:
	// lwz r29,16(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r30,588(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(588) );
	// bl 0x832b225c
	__imp__KeGetCurrentProcessType(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// bne cr6,0x82ba25d4
	if (!cr6.eq) goto loc_82BA25D4;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2472) );
	// b 0x82ba25dc
	goto loc_82BA25DC;
loc_82BA25D4:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2476(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2476) );
loc_82BA25DC:
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ba25f8
	if (cr6.eq) goto loc_82BA25F8;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,6
	ctx.r5.s64 = 6;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82242628
	sub_82242628(ctx, base);
loc_82BA25F8:
	// lis r10,-31920
	ctx.r10.s64 = -2091909120;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// addi r10,r10,-18432
	ctx.r10.s64 = ctx.r10.s64 + -18432;
	// addi r30,r11,5528
	r30.s64 = r11.s64 + 5528;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
loc_82BA2610:
	// mfmsr r8
	// mtmsrd r13,1
	// lwarx r9,0,r7
	reserved.u32 = *(uint32_t*)(base + ctx.r7.u32);
	ctx.r9.u64 = __builtin_bswap32(reserved.u32);
	// stwcx. r11,0,r7
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r7.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r8,1
	// bne 0x82ba2610
	if (!cr0.eq) goto loc_82BA2610;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r10,6144
	ctx.r10.s64 = 6144;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// divwu r10,r11,r10
	ctx.r10.u32 = r11.u32 / ctx.r10.u32;
	// cmplwi cr6,r10,14
	cr6.compare<uint32_t>(ctx.r10.u32, 14, xer);
	// blt cr6,0x82ba2648
	if (cr6.lt) goto loc_82BA2648;
	// li r10,14
	ctx.r10.s64 = 14;
loc_82BA2648:
	// lwz r11,584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(584) );
	// mulli r9,r10,12
	ctx.r9.s64 = ctx.r10.s64 * 12;
	// lwz r6,596(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// lhz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U16(r31.u32 + 12);
	// lwz r5,380(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(380) );
	// addi r4,r11,-1
	ctx.r4.s64 = r11.s64 + -1;
	// rlwinm r11,r6,12,26,31
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 12) & 0x3F;
	// rlwinm r4,r4,2,29,29
	ctx.r4.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0x4;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r7,-25768
	ctx.r7.s64 = -1688731648;
	// lwzx r4,r4,r31
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + r31.u32);
	// rlwinm r24,r6,4,30,31
	r24.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0x3;
	// ori r25,r7,59162
	r25.u64 = ctx.r7.u64 | 59162;
	// stw r5,356(r31)
	PPC_STORE_U32(r31.u32 + 356, ctx.r5.u32);
	// addi r11,r4,-4
	r11.s64 = ctx.r4.s64 + -4;
	// stw r27,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r27.u32);
	// rlwinm r23,r9,9,0,22
	r23.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 9) & 0xFFFFFE00;
	// lwzx r3,r8,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + r31.u32);
	// addi r7,r31,348
	ctx.r7.s64 = r31.s64 + 348;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stwu r25,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r25.u32);
	r11.u32 = ea;
	// stwu r24,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r24.u32);
	r11.u32 = ea;
	// stwu r29,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r29.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// bl 0x82cc0760
	sub_82CC0760(ctx, base);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba2140
	sub_82BA2140(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(364) );
	// bl 0x82196c58
	sub_82196C58(ctx, base);
	// b 0x82ba26dc
	goto loc_82BA26DC;
loc_82BA26D4:
	// li r3,6
	ctx.r3.s64 = 6;
	// bl 0x82cbc6b0
	sub_82CBC6B0(ctx, base);
loc_82BA26DC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// cmplwi cr6,r11,6144
	cr6.compare<uint32_t>(r11.u32, 6144, xer);
	// bne cr6,0x82ba26d4
	if (!cr6.eq) goto loc_82BA26D4;
loc_82BA26E8:
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// mr r30,r27
	r30.u64 = r27.u64;
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
	// rlwinm. r11,r11,0,12,17
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFC000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba2764
	if (cr0.eq) goto loc_82BA2764;
	// addi r29,r31,20
	r29.s64 = r31.s64 + 20;
loc_82BA2700:
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba2730
	if (!cr0.eq) goto loc_82BA2730;
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// rlwinm r11,r11,12,26,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 12) & 0x3F;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x82ba2730
	if (cr6.lt) goto loc_82BA2730;
	// ble cr6,0x82ba2728
	if (!cr6.gt) goto loc_82BA2728;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// b 0x82ba2734
	goto loc_82BA2734;
loc_82BA2728:
	// lwz r4,380(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(380) );
	// b 0x82ba2734
	goto loc_82BA2734;
loc_82BA2730:
	// lwz r4,164(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + int32_t(164) );
loc_82BA2734:
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// bl 0x82cc0fc0
	sub_82CC0FC0(ctx, base);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// bl 0x82cc14d0
	sub_82CC14D0(ctx, base);
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// rlwinm r11,r11,18,26,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 18) & 0x3F;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x82ba2700
	if (cr6.lt) goto loc_82BA2700;
loc_82BA2764:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x832b30cc
	__imp__VdGetCurrentDisplayInformation(ctx, base);
	// lwz r11,604(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(604) );
	// srawi. r11,r11,29
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1FFFFFFF) != 0);
	r11.s64 = r11.s32 >> 29;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba2784
	if (!cr0.eq) goto loc_82BA2784;
	// lis r10,21415
	ctx.r10.s64 = 1403453440;
	// ori r10,r10,8884
	ctx.r10.u64 = ctx.r10.u64 | 8884;
	// b 0x82ba27a8
	goto loc_82BA27A8;
loc_82BA2784:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82ba2798
	if (!cr6.eq) goto loc_82BA2798;
	// lis r10,21415
	ctx.r10.s64 = 1403453440;
	// ori r10,r10,8885
	ctx.r10.u64 = ctx.r10.u64 | 8885;
	// b 0x82ba27a8
	goto loc_82BA27A8;
loc_82BA2798:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82ba27ac
	if (!cr6.eq) goto loc_82BA27AC;
	// lis r10,21415
	ctx.r10.s64 = 1403453440;
	// ori r10,r10,8886
	ctx.r10.u64 = ctx.r10.u64 | 8886;
loc_82BA27A8:
	// stw r10,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r10.u32);
loc_82BA27AC:
	// lis r10,1
	ctx.r10.s64 = 65536;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 | 1;
	// stw r10,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r10.u32);
	// beq cr6,0x82ba27ec
	if (cr6.eq) goto loc_82BA27EC;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82ba27ec
	if (cr6.eq) goto loc_82BA27EC;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82ba280c
	if (!cr6.eq) goto loc_82BA280C;
	// li r10,400
	ctx.r10.s64 = 400;
	// li r11,224
	r11.s64 = 224;
	// stw r10,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r10.u32);
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r11.u32);
	// stw r10,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r10.u32);
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
	// b 0x82ba280c
	goto loc_82BA280C;
loc_82BA27EC:
	// lhz r11,368(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 368);
	// lhz r10,370(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 370);
	// lhz r9,168(r1)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r1.u32 + 168);
	// lhz r8,170(r1)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r1.u32 + 170);
	// stw r11,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r11.u32);
	// stw r10,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r10.u32);
	// stw r9,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r9.u32);
	// stw r8,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r8.u32);
loc_82BA280C:
	// bl 0x832b29ac
	__imp__KeQueryPerformanceFrequency(ctx, base);
	// lwz r11,596(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// stw r3,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r3.u32);
	// rlwinm. r8,r11,6,31,31
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// rlwinm r10,r11,6,26,31
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 6) & 0x3F;
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
	// beq 0x82ba2834
	if (cr0.eq) goto loc_82BA2834;
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
loc_82BA2834:
	// rlwinm. r10,r10,0,30,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82ba2844
	if (cr0.eq) goto loc_82BA2844;
	// ori r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 2;
	// stw r9,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r9.u32);
loc_82BA2844:
	// rlwinm. r11,r11,0,0,0
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x80000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba2854
	if (cr0.eq) goto loc_82BA2854;
	// lwz r4,592(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(592) );
	// b 0x82ba2858
	goto loc_82BA2858;
loc_82BA2854:
	// addi r4,r26,15004
	ctx.r4.s64 = r26.s64 + 15004;
loc_82BA2858:
	// lbz r11,101(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 101);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x82ba287c
	if (!cr6.eq) goto loc_82BA287C;
	// ori r11,r9,4
	r11.u64 = ctx.r9.u64 | 4;
	// stw r11,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r11.u32);
	// bl 0x82b9df08
	sub_82B9DF08(ctx, base);
	// b 0x82ba2880
	goto loc_82BA2880;
loc_82BA287C:
	// bl 0x82b9de70
	sub_82B9DE70(ctx, base);
loc_82BA2880:
	// lwz r30,596(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(596) );
	// addi r3,r1,232
	ctx.r3.s64 = ctx.r1.s64 + 232;
	// li r5,56
	ctx.r5.s64 = 56;
	// rlwinm. r11,r30,0,5,5
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 0) & 0x4000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// addi r4,r26,13724
	ctx.r4.s64 = r26.s64 + 13724;
	// bne 0x82ba289c
	if (!cr0.eq) goto loc_82BA289C;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
loc_82BA289C:
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lbz r11,608(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 608);
	// rlwinm. r11,r11,0,26,26
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba28bc
	if (cr0.eq) goto loc_82BA28BC;
	// rlwinm r10,r30,12,26,31
	ctx.r10.u64 = rotl64(r30.u32 | (r30.u64 << 32), 12) & 0x3F;
	// lwz r11,380(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(380) );
	// stw r10,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r10.u32);
	// b 0x82ba28c4
	goto loc_82BA28C4;
loc_82BA28BC:
	// li r11,2048
	r11.s64 = 2048;
	// stw r27,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r27.u32);
loc_82BA28C4:
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, r11.u32);
	// addi r7,r31,348
	ctx.r7.s64 = r31.s64 + 348;
	// stw r27,356(r31)
	PPC_STORE_U32(r31.u32 + 356, r27.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stw r27,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r27.u32);
	// li r5,2048
	ctx.r5.s64 = 2048;
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// bl 0x82cc0760
	sub_82CC0760(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(364) );
	// bl 0x82196c58
	sub_82196C58(ctx, base);
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(364) );
	// bl 0x82cbbf60
	sub_82CBBF60(ctx, base);
loc_82BA28FC:
	// mr r30,r27
	r30.u64 = r27.u64;
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
loc_82BA2904:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x82ba2924
	if (cr6.eq) goto loc_82BA2924;
	// bl 0x82cbbf60
	sub_82CBBF60(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplwi cr6,r30,41
	cr6.compare<uint32_t>(r30.u32, 41, xer);
	// blt cr6,0x82ba2904
	if (cr6.lt) goto loc_82BA2904;
loc_82BA2924:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2504(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2504) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba2948
	if (cr6.eq) goto loc_82BA2948;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba2974
	if (cr6.eq) goto loc_82BA2974;
	// b 0x82ba2960
	goto loc_82BA2960;
loc_82BA2948:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba2974
	if (cr6.eq) goto loc_82BA2974;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
loc_82BA2960:
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,27
	ctx.r3.s64 = 27;
	// addi r4,r10,-17428
	ctx.r4.s64 = ctx.r10.s64 + -17428;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA2974:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82ba1380
	sub_82BA1380(ctx, base);
	// lis r11,-31951
	r11.s64 = -2093940736;
	// addi r3,r11,23276
	ctx.r3.s64 = r11.s64 + 23276;
	// bl 0x832b288c
	__imp__ObDeleteSymbolicLink(ctx, base);
	// lis r8,-31946
	ctx.r8.s64 = -2093613056;
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// addi r6,r8,987
	ctx.r6.s64 = ctx.r8.s64 + 987;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// stb r10,987(r8)
	PPC_STORE_U8(ctx.r8.u32 + 987, ctx.r10.u8);
	// stb r11,-1(r6)
	PPC_STORE_U8(ctx.r6.u32 + -1, r11.u8);
	// stb r9,8085(r7)
	PPC_STORE_U8(ctx.r7.u32 + 8085, ctx.r9.u8);
	// addi r1,r1,2320
	ctx.r1.s64 = ctx.r1.s64 + 2320;
	// b 0x82ca2c24
	return;
}

PPC_WEAK_FUNC(sub_82BA2568) {
	__imp__sub_82BA2568(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA29B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd0
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r6,16
	ctx.r6.s64 = 16;
	// li r5,112
	ctx.r5.s64 = 112;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r30,r31,23360
	r30.s64 = r31.s64 + 23360;
	// bl 0x8219ce10
	sub_8219CE10(ctx, base);
	// ld r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// cmpldi cr6,r4,0
	cr6.compare<uint64_t>(ctx.r4.u64, 0, xer);
	// beq cr6,0x82ba2b00
	if (cr6.eq) goto loc_82BA2B00;
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// and r11,r11,r4
	r11.u64 = r11.u64 & ctx.r4.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba2a0c
	if (cr6.eq) goto loc_82BA2A0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,10560(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10560) );
	// bl 0x822155e0
	sub_822155E0(ctx, base);
	// std r3,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r3.u64);
loc_82BA2A0C:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// clrldi r10,r11,52
	ctx.r10.u64 = r11.u64 & 0xFFF;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba2a3c
	if (cr6.eq) goto loc_82BA2A3C;
	// addi r6,r31,10548
	ctx.r6.s64 = r31.s64 + 10548;
	// li r5,8704
	ctx.r5.s64 = 8704;
	// rldicr r4,r11,52,11
	ctx.r4.u64 = rotl64(r11.u64, 52) & 0xFFF0000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// rldicr r11,r11,0,51
	r11.u64 = rotl64(r11.u64, 0) & 0xFFFFFFFFFFFFF000;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA2A3C:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// rlwinm r10,r11,0,15,19
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1F000;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba2a74
	if (cr6.eq) goto loc_82BA2A74;
	// addi r6,r31,10528
	ctx.r6.s64 = r31.s64 + 10528;
	// li r5,8576
	ctx.r5.s64 = 8576;
	// rldicr r4,r11,47,4
	ctx.r4.u64 = rotl64(r11.u64, 47) & 0xF800000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-2
	r12.s64 = -131072;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,4095
	r12.u64 = r12.u64 | 4095;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA2A74:
	// lis r12,0
	r12.s64 = 0;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// rldicr r12,r12,42,21
	r12.u64 = rotl64(r12.u64, 42) & 0xFFFFFC0000000000;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba2abc
	if (cr6.eq) goto loc_82BA2ABC;
	// addi r6,r31,10368
	ctx.r6.s64 = r31.s64 + 10368;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// rldicr r4,r11,6,15
	ctx.r4.u64 = rotl64(r11.u64, 6) & 0xFFFF000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-1
	r12.s64 = -65536;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,42,63
	r12.u64 = rotl64(r12.u64, 42) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA2ABC:
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// clrldi r12,r12,22
	r12.u64 = r12.u64 & 0x3FFFFFFFFFF;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba2b00
	if (cr6.eq) goto loc_82BA2B00;
	// addi r6,r31,10444
	ctx.r6.s64 = r31.s64 + 10444;
	// li r5,8448
	ctx.r5.s64 = 8448;
	// rldicr r4,r11,22,20
	ctx.r4.u64 = rotl64(r11.u64, 22) & 0xFFFFF80000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,21,63
	r12.u64 = rotl64(r12.u64, 21) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA2B00:
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba2b50
	if (cr6.eq) goto loc_82BA2B50;
	// lis r12,31
	r12.s64 = 2031616;
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// rldicr r12,r12,34,29
	r12.u64 = rotl64(r12.u64, 34) & 0xFFFFFFFC00000000;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba2b50
	if (cr6.eq) goto loc_82BA2B50;
	// addi r6,r31,10596
	ctx.r6.s64 = r31.s64 + 10596;
	// li r5,8832
	ctx.r5.s64 = 8832;
	// rldicr r4,r11,9,20
	ctx.r4.u64 = rotl64(r11.u64, 9) & 0xFFFFF80000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,34,63
	r12.u64 = rotl64(r12.u64, 34) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
loc_82BA2B50:
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba2b88
	if (cr6.eq) goto loc_82BA2B88;
	// clrldi r10,r11,26
	ctx.r10.u64 = r11.u64 & 0x3FFFFFFFFF;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba2b88
	if (cr6.eq) goto loc_82BA2B88;
	// addi r6,r31,10680
	ctx.r6.s64 = r31.s64 + 10680;
	// li r5,8960
	ctx.r5.s64 = 8960;
	// rldicr r4,r11,26,37
	ctx.r4.u64 = rotl64(r11.u64, 26) & 0xFFFFFFFFFC000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// rldicr r11,r11,0,25
	r11.u64 = rotl64(r11.u64, 0) & 0xFFFFFFC000000000;
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
loc_82BA2B88:
	// lis r11,-32247
	r11.s64 = -2113339392;
	// lwz r10,604(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(604) );
	// addi r24,r11,15524
	r24.s64 = r11.s64 + 15524;
	// srawi. r11,r10,29
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1FFFFFFF) != 0);
	r11.s64 = ctx.r10.s32 >> 29;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba2bb0
	if (!cr0.eq) goto loc_82BA2BB0;
	// lis r25,1792
	r25.s64 = 117440512;
	// addi r26,r24,1548
	r26.s64 = r24.s64 + 1548;
	// li r29,525
	r29.s64 = 525;
	// ori r25,r25,21
	r25.u64 = r25.u64 | 21;
	// b 0x82ba2bf4
	goto loc_82BA2BF4;
loc_82BA2BB0:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x82ba2bcc
	if (!cr6.eq) goto loc_82BA2BCC;
	// lis r25,1792
	r25.s64 = 117440512;
	// addi r26,r24,3684
	r26.s64 = r24.s64 + 3684;
	// li r29,933
	r29.s64 = 933;
	// ori r25,r25,19
	r25.u64 = r25.u64 | 19;
	// b 0x82ba2bf4
	goto loc_82BA2BF4;
loc_82BA2BCC:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82ba2be8
	if (!cr6.eq) goto loc_82BA2BE8;
	// lis r25,1792
	r25.s64 = 117440512;
	// addi r26,r24,7452
	r26.s64 = r24.s64 + 7452;
	// li r29,210
	r29.s64 = 210;
	// ori r25,r25,15
	r25.u64 = r25.u64 | 15;
	// b 0x82ba2bf4
	goto loc_82BA2BF4;
loc_82BA2BE8:
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// lwz r26,80(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82BA2BF4:
	// addi r4,r29,5
	ctx.r4.s64 = r29.s64 + 5;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// lis r11,-16384
	r11.s64 = -1073741824;
	// li r10,768
	ctx.r10.s64 = 768;
	// ori r11,r11,15104
	r11.u64 = r11.u64 | 15104;
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// ori r9,r9,11008
	ctx.r9.u64 = ctx.r9.u64 | 11008;
	// li r22,0
	r22.s64 = 0;
	// rlwimi r9,r11,16,2,15
	ctx.r9.u64 = (rotl32(r11.u32, 16) & 0x3FFF0000) | (ctx.r9.u64 & 0xFFFFFFFFC000FFFF);
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// clrlwi r11,r29,18
	r11.u64 = r29.u32 & 0x3FFF;
	// rlwinm r27,r29,2,0,29
	r27.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stwu r11,4(r29)
	ea = 4 + r29.u32;
	PPC_STORE_U32(ea, r11.u32);
	r29.u32 = ea;
	// addi r3,r29,4
	ctx.r3.s64 = r29.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// add r3,r27,r29
	ctx.r3.u64 = r27.u64 + r29.u64;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba2c70
	if (!cr6.gt) goto loc_82BA2C70;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA2C70:
	// lis r11,-16368
	r11.s64 = -1072693248;
	// li r10,1
	ctx.r10.s64 = 1;
	// ori r11,r11,11008
	r11.u64 = r11.u64 | 11008;
	// li r9,15
	ctx.r9.s64 = 15;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// li r5,60
	ctx.r5.s64 = 60;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stwu r9,4(r29)
	ea = 4 + r29.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r29.u32 = ea;
	// addi r3,r29,4
	ctx.r3.s64 = r29.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r11,r29,60
	r11.s64 = r29.s64 + 60;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// oris r9,r25,4096
	ctx.r9.u64 = r25.u64 | 268435456;
	// ori r10,r10,8576
	ctx.r10.u64 = ctx.r10.u64 | 8576;
	// mr r8,r22
	ctx.r8.u64 = r22.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x82ba2cdc
	if (!cr6.gt) goto loc_82BA2CDC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA2CDC:
	// lis r11,2
	r11.s64 = 131072;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r11,r11,8448
	r11.u64 = r11.u64 | 8448;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r11,r22
	r11.u64 = r22.u64;
	// mr r9,r22
	ctx.r9.u64 = r22.u64;
	// li r8,8851
	ctx.r8.s64 = 8851;
	// mr r7,r22
	ctx.r7.u64 = r22.u64;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// ori r10,r6,8708
	ctx.r10.u64 = ctx.r6.u64 | 8708;
	// lis r6,1
	ctx.r6.s64 = 65536;
	// li r4,768
	ctx.r4.s64 = 768;
	// li r29,8978
	r29.s64 = 8978;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lis r27,0
	r27.s64 = 0;
	// li r26,8205
	r26.s64 = 8205;
	// ori r11,r27,65535
	r11.u64 = r27.u64 | 65535;
	// mr r27,r22
	r27.u64 = r22.u64;
	// li r25,8704
	r25.s64 = 8704;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// mr r24,r22
	r24.u64 = r22.u64;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// stwu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// stwu r25,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r25.u32);
	ctx.r3.u32 = ea;
	// stwu r24,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r24.u32);
	ctx.r3.u32 = ea;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// ble cr6,0x82ba2d84
	if (!cr6.gt) goto loc_82BA2D84;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA2D84:
	// lis r11,5
	r11.s64 = 327680;
	// li r9,1
	ctx.r9.s64 = 1;
	// ori r11,r11,18432
	r11.u64 = r11.u64 | 18432;
	// addi r29,r30,392
	r29.s64 = r30.s64 + 392;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28) );
	// lhz r10,372(r30)
	ctx.r10.u64 = PPC_LOAD_U16(r30.u32 + 372);
	// rlwinm r11,r11,0,22,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x3FC;
	// rlwinm r10,r10,17,0,9
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 17) & 0xFFC00000;
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// oris r11,r11,32768
	r11.u64 = r11.u64 | 2147483648;
	// ori r11,r11,18434
	r11.u64 = r11.u64 | 18434;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r10,32(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(32) );
	// rlwinm r11,r10,12,20,31
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r10,3
	ctx.r10.u64 = ctx.r10.u32 & 0x1FFFFFFF;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,36(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(36) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,40(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(40) );
	// rlwinm r10,r11,13,0,18
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 13) & 0xFFFFE000;
	// rlwinm r11,r11,0,19,7
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFF001FFF;
	// srawi r10,r10,13
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1FFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 13;
	// rlwinm r11,r11,0,7,0
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFF81FFFFFF;
	// rlwimi r10,r9,24,19,12
	ctx.r10.u64 = (rotl32(ctx.r9.u32, 24) & 0xFFFFFFFFFFF81FFF) | (ctx.r10.u64 & 0x7E000);
	// or r11,r10,r11
	r11.u64 = ctx.r10.u64 | r11.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,44(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(44) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,48(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(48) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// lwz r8,604(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(604) );
	// lwz r10,584(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(584) );
	// rlwinm r10,r10,2,29,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x4;
	// rlwinm r27,r11,2,30,31
	r27.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0x3;
	// lwzx r25,r10,r30
	r25.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// clrlwi r23,r27,31
	r23.u64 = r27.u32 & 0x1;
	// rlwinm r10,r27,31,31,31
	ctx.r10.u64 = rotl64(r27.u32 | (r27.u64 << 32), 31) & 0x1;
	// mulli r11,r23,56
	r11.s64 = r23.s64 * 56;
	// addi r11,r11,527
	r11.s64 = r11.s64 + 527;
	// mulli r9,r10,1536
	ctx.r9.s64 = ctx.r10.s64 * 1536;
	// rlwinm r10,r11,0,0,22
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFE00;
	// srawi. r11,r8,29
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1FFFFFFF) != 0);
	r11.s64 = ctx.r8.s32 >> 29;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r28,r10,r25
	r28.u64 = ctx.r10.u64 + r25.u64;
	// bne 0x82ba2e64
	if (!cr0.eq) goto loc_82BA2E64;
loc_82BA2E50:
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// andi. r9,r9,49400
	ctx.r9.u64 = ctx.r9.u64 & 49400;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// oris r9,r9,19200
	ctx.r9.u64 = ctx.r9.u64 | 1258291200;
	// ori r9,r9,1536
	ctx.r9.u64 = ctx.r9.u64 | 1536;
	// b 0x82ba2e84
	goto loc_82BA2E84;
loc_82BA2E64:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x82ba2e50
	if (cr6.eq) goto loc_82BA2E50;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82ba2ec0
	if (!cr6.eq) goto loc_82BA2EC0;
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// andi. r9,r9,49400
	ctx.r9.u64 = ctx.r9.u64 & 49400;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// oris r9,r9,19200
	ctx.r9.u64 = ctx.r9.u64 | 1258291200;
	// ori r9,r9,2560
	ctx.r9.u64 = ctx.r9.u64 | 2560;
loc_82BA2E84:
	// rlwinm r11,r28,12,20,31
	r11.u64 = rotl64(r28.u32 | (r28.u64 << 32), 12) & 0xFFF;
	// lwz r8,388(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(388) );
	// rlwinm r10,r28,0,3,29
	ctx.r10.u64 = rotl64(r28.u32 | (r28.u64 << 32), 0) & 0x1FFFFFFC;
	// stw r9,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r9.u32);
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// lis r7,16384
	ctx.r7.s64 = 1073741824;
	// rlwinm r11,r11,0,19,19
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// li r10,75
	ctx.r10.s64 = 75;
	// rlwimi r7,r11,30,2,31
	ctx.r7.u64 = (rotl32(r11.u32, 30) & 0x3FFFFFFF) | (ctx.r7.u64 & 0xFFFFFFFFC0000000);
	// lis r11,19200
	r11.s64 = 1258291200;
	// rlwimi r8,r10,24,0,8
	ctx.r8.u64 = (rotl32(ctx.r10.u32, 24) & 0xFF800000) | (ctx.r8.u64 & 0xFFFFFFFF007FFFFF);
	// stw r7,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r7.u32);
	// stw r11,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r11.u32);
	// stw r8,12(r29)
	PPC_STORE_U32(r29.u32 + 12, ctx.r8.u32);
loc_82BA2EC0:
	// li r4,49
	ctx.r4.s64 = 49;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// lis r11,47
	r11.s64 = 3080192;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// ori r11,r11,16384
	r11.u64 = r11.u64 | 16384;
	// li r5,192
	ctx.r5.s64 = 192;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r3,r29,4
	ctx.r3.s64 = r29.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r11,r29,192
	r11.s64 = r29.s64 + 192;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// lbz r11,608(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// rlwinm. r11,r11,0,25,25
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba2f0c
	if (cr0.eq) goto loc_82BA2F0C;
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,364(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(364) );
	// bl 0x82196c58
	sub_82196C58(ctx, base);
loc_82BA2F0C:
	// lhz r11,14(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// rotlwi r11,r11,9
	r11.u64 = rotl32(r11.u32, 9);
	// addi r7,r10,5528
	ctx.r7.s64 = ctx.r10.s64 + 5528;
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
loc_82BA2F20:
	// mfmsr r8
	// mtmsrd r13,1
	// lwarx r9,0,r7
	reserved.u32 = *(uint32_t*)(base + ctx.r7.u32);
	ctx.r9.u64 = __builtin_bswap32(reserved.u32);
	// stwcx. r10,0,r7
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r7.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r8,1
	// bne 0x82ba2f20
	if (!cr0.eq) goto loc_82BA2F20;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// li r8,6144
	ctx.r8.s64 = 6144;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// divwu r11,r11,r8
	r11.u32 = r11.u32 / ctx.r8.u32;
	// cmplwi cr6,r11,14
	cr6.compare<uint32_t>(r11.u32, 14, xer);
	// mr r24,r11
	r24.u64 = r11.u64;
	// blt cr6,0x82ba2f5c
	if (cr6.lt) goto loc_82BA2F5C;
	// li r24,14
	r24.s64 = 14;
loc_82BA2F5C:
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(584) );
	// lis r9,-25768
	ctx.r9.s64 = -1688731648;
	// stw r10,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r10.u32);
	// rlwinm r10,r28,12,20,31
	ctx.r10.u64 = rotl64(r28.u32 | (r28.u64 << 32), 12) & 0xFFF;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// lwz r8,596(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// ori r7,r9,59162
	ctx.r7.u64 = ctx.r9.u64 | 59162;
	// lwz r6,16(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// rlwinm r11,r11,2,29,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0x4;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// rlwinm r8,r8,4,30,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0x3;
	// lwz r5,56(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// addi r9,r10,512
	ctx.r9.s64 = ctx.r10.s64 + 512;
	// clrlwi r10,r28,3
	ctx.r10.u64 = r28.u32 & 0x1FFFFFFF;
	// rlwinm r9,r9,0,19,19
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1000;
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// cmplw cr6,r3,r5
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r5.u32, xer);
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,10392(r31)
	PPC_STORE_U32(r31.u32 + 10392, ctx.r10.u32);
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// stwu r7,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	r11.u32 = ea;
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// stwu r6,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	r11.u32 = ea;
	// stwu r24,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r24.u32);
	r11.u32 = ea;
	// ble cr6,0x82ba2fc8
	if (!cr6.gt) goto loc_82BA2FC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA2FC8:
	// li r11,8198
	r11.s64 = 8198;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r28,r22
	r28.u64 = r22.u64;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lwz r11,13504(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(13504) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// stw r11,13504(r31)
	PPC_STORE_U32(r31.u32 + 13504, r11.u32);
	// lwz r29,376(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(376) );
loc_82BA2FF0:
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba3008
	if (!cr6.gt) goto loc_82BA3008;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA3008:
	// li r11,8450
	r11.s64 = 8450;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// cmplwi cr6,r29,65535
	cr6.compare<uint32_t>(r29.u32, 65535, xer);
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// ble cr6,0x82ba302c
	if (!cr6.gt) goto loc_82BA302C;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,65534
	ctx.r10.u64 = ctx.r10.u64 | 65534;
loc_82BA302C:
	// lis r9,-16384
	ctx.r9.s64 = -1073741824;
	// li r8,129
	ctx.r8.s64 = 129;
	// ori r9,r9,13825
	ctx.r9.u64 = ctx.r9.u64 | 13825;
	// rlwimi r8,r10,16,0,15
	ctx.r8.u64 = (rotl32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r8.u64 & 0xFFFFFFFF0000FFFF);
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// subf. r29,r10,r29
	r29.s64 = r29.s64 - ctx.r10.s64;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// stwu r8,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r11.u32 = ea;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// beq 0x82ba305c
	if (cr0.eq) goto loc_82BA305C;
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// b 0x82ba2ff0
	goto loc_82BA2FF0;
loc_82BA305C:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// stw r22,10392(r31)
	PPC_STORE_U32(r31.u32 + 10392, r22.u32);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba3074
	if (!cr6.gt) goto loc_82BA3074;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA3074:
	// li r11,8198
	r11.s64 = 8198;
	// mr r10,r22
	ctx.r10.u64 = r22.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lwz r10,13504(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(13504) );
	// lwz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// stw r10,13504(r31)
	PPC_STORE_U32(r31.u32 + 13504, ctx.r10.u32);
	// ble cr6,0x82ba30b0
	if (!cr6.gt) goto loc_82BA30B0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82BA30B0:
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// li r9,6
	ctx.r9.s64 = 6;
	// ori r10,r10,17920
	ctx.r10.u64 = ctx.r10.u64 | 17920;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r11,r11,8
	r11.u64 = r11.u64 | 8;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// oris r11,r11,8
	r11.u64 = r11.u64 | 524288;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// oris r11,r11,16
	r11.u64 = r11.u64 | 1048576;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// oris r11,r11,65024
	r11.u64 = r11.u64 | 4261412864;
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r11,r11,2048
	r11.u64 = r11.u64 | 2048;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// ori r11,r11,256
	r11.u64 = r11.u64 | 256;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
	// lwz r29,10908(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10908) );
	// lwz r26,588(r30)
	r26.u64 = PPC_LOAD_U32(r30.u32 + int32_t(588) );
	// stw r29,10928(r31)
	PPC_STORE_U32(r31.u32 + 10928, r29.u32);
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// stw r29,588(r30)
	PPC_STORE_U32(r30.u32 + 588, r29.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ce10
	sub_8219CE10(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ce10
	sub_8219CE10(ctx, base);
	// lwz r11,596(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// clrlwi r28,r27,30
	r28.u64 = r27.u32 & 0x3;
	// rlwimi r11,r27,28,2,3
	r11.u64 = (rotl32(r27.u32, 28) & 0x30000000) | (r11.u64 & 0xFFFFFFFFCFFFFFFF);
	// addi r29,r25,16
	r29.s64 = r25.s64 + 16;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// stw r11,596(r30)
	PPC_STORE_U32(r30.u32 + 596, r11.u32);
	// beq cr6,0x82ba3174
	if (cr6.eq) goto loc_82BA3174;
	// li r5,56
	ctx.r5.s64 = 56;
	// addi r4,r31,13724
	ctx.r4.s64 = r31.s64 + 13724;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r29,r29,56
	r29.s64 = r29.s64 + 56;
loc_82BA3174:
	// subf r11,r25,r29
	r11.s64 = r29.s64 - r25.s64;
	// rlwinm. r10,r27,0,30,30
	ctx.r10.u64 = rotl64(r27.u32 | (r27.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,511
	r11.s64 = r11.s64 + 511;
	// rlwinm r11,r11,0,0,22
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFE00;
	// add r31,r11,r25
	r31.u64 = r11.u64 + r25.u64;
	// beq 0x82ba31c4
	if (cr0.eq) goto loc_82BA31C4;
	// lbz r11,600(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 600);
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r3,592(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(592) );
	// rlwinm. r11,r11,0,0,24
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// beq 0x82ba31ac
	if (cr0.eq) goto loc_82BA31AC;
	// bl 0x82b9df08
	sub_82B9DF08(ctx, base);
	// b 0x82ba31b0
	goto loc_82BA31B0;
loc_82BA31AC:
	// bl 0x82b9de70
	sub_82B9DE70(ctx, base);
loc_82BA31B0:
	// li r5,1536
	ctx.r5.s64 = 1536;
	// lwz r4,592(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(592) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r31,r31,1536
	r31.s64 = r31.s64 + 1536;
loc_82BA31C4:
	// lwz r8,584(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(584) );
	// mulli r10,r24,12
	ctx.r10.s64 = r24.s64 * 12;
	// lhz r11,12(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 12);
	// lwz r7,596(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// lhz r9,14(r30)
	ctx.r9.u64 = PPC_LOAD_U16(r30.u32 + 14);
	// lbz r6,608(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// rlwinm r8,r8,2,29,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0x4;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r10,r28,26,0,5
	ctx.r10.u64 = rotl64(r28.u32 | (r28.u64 << 32), 26) & 0xFC000000;
	// rlwinm r29,r11,9,0,22
	r29.u64 = rotl64(r11.u32 | (r11.u64 << 32), 9) & 0xFFFFFE00;
	// or r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 | ctx.r7.u64;
	// lwzx r8,r8,r30
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r30.u32);
	// rlwinm. r6,r6,0,0,24
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFF80;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// clrlwi r11,r10,2
	r11.u64 = ctx.r10.u32 & 0x3FFFFFFF;
	// subf r10,r8,r31
	ctx.r10.s64 = r31.s64 - ctx.r8.s64;
	// stw r11,596(r30)
	PPC_STORE_U32(r30.u32 + 596, r11.u32);
	// rlwinm r11,r10,23,9,31
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 23) & 0x7FFFFF;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// sth r11,12(r30)
	PPC_STORE_U16(r30.u32 + 12, r11.u16);
	// beq 0x82ba32a8
	if (cr0.eq) goto loc_82BA32A8;
	// lwz r11,380(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(380) );
	// stw r22,360(r30)
	PPC_STORE_U32(r30.u32 + 360, r22.u32);
	// stw r11,356(r30)
	PPC_STORE_U32(r30.u32 + 356, r11.u32);
	// bl 0x832b225c
	__imp__KeGetCurrentProcessType(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// bne cr6,0x82ba3238
	if (!cr6.eq) goto loc_82BA3238;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2472) );
	// b 0x82ba3240
	goto loc_82BA3240;
loc_82BA3238:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2476(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2476) );
loc_82BA3240:
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x82ba325c
	if (cr6.eq) goto loc_82BA325C;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,6
	ctx.r5.s64 = 6;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82242628
	sub_82242628(ctx, base);
loc_82BA325C:
	// lwz r10,596(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(596) );
	// addi r7,r30,348
	ctx.r7.s64 = r30.s64 + 348;
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(584) );
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// rlwinm r10,r10,12,26,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 12) & 0x3F;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,5
	ctx.r10.s64 = ctx.r10.s64 + 5;
	// rlwinm r11,r11,2,29,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0x4;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwzx r4,r11,r30
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// lwzx r3,r10,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// bl 0x82cc0760
	sub_82CC0760(ctx, base);
	// lbz r11,608(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// ori r11,r11,64
	r11.u64 = r11.u64 | 64;
	// stb r11,608(r30)
	PPC_STORE_U8(r30.u32 + 608, r11.u8);
	// bl 0x82ba2140
	sub_82BA2140(ctx, base);
loc_82BA32A8:
	// lbz r10,608(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 608);
	// lwz r11,584(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(584) );
	// ori r10,r10,128
	ctx.r10.u64 = ctx.r10.u64 | 128;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r10,608(r30)
	PPC_STORE_U8(r30.u32 + 608, ctx.r10.u8);
	// stw r11,584(r30)
	PPC_STORE_U32(r30.u32 + 584, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c20
	return;
}

PPC_WEAK_FUNC(sub_82BA29B8) {
	__imp__sub_82BA29B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA32C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r5,r11,24472
	ctx.r5.s64 = r11.s64 + 24472;
	// lbz r11,-16387(r5)
	r11.u64 = PPC_LOAD_U8(ctx.r5.u32 + -16387);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x82ba33a4
	if (cr0.eq) goto loc_82BA33A4;
	// lbz r11,23968(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 23968);
	// rlwinm. r11,r11,0,27,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba3378
	if (!cr0.eq) goto loc_82BA3378;
	// bl 0x82ba21f8
	sub_82BA21F8(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba3308
	if (cr0.eq) goto loc_82BA3308;
	// li r6,0
	ctx.r6.s64 = 0;
	// b 0x82ba3310
	goto loc_82BA3310;
loc_82BA3308:
	// lis r6,-32768
	ctx.r6.s64 = -2147483648;
	// ori r6,r6,16389
	ctx.r6.u64 = ctx.r6.u64 | 16389;
loc_82BA3310:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r5,r11,-17408
	ctx.r5.s64 = r11.s64 + -17408;
	// addi r4,r10,-17376
	ctx.r4.s64 = ctx.r10.s64 + -17376;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x832b301c
	__imp__sprintf(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2504(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2504) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba334c
	if (cr6.eq) goto loc_82BA334C;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba33a4
	if (cr6.eq) goto loc_82BA33A4;
	// b 0x82ba3364
	goto loc_82BA3364;
loc_82BA334C:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba33a4
	if (cr6.eq) goto loc_82BA33A4;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
loc_82BA3364:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,27
	ctx.r3.s64 = 27;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba33a4
	goto loc_82BA33A4;
loc_82BA3378:
	// lis r11,-31946
	r11.s64 = -2093613056;
	// addi r11,r11,987
	r11.s64 = r11.s64 + 987;
	// lbz r10,-1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// cmplwi r10,0
	cr0.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq 0x82ba3394
	if (cr0.eq) goto loc_82BA3394;
	// bl 0x82ba2568
	sub_82BA2568(ctx, base);
	// b 0x82ba33a4
	goto loc_82BA33A4;
loc_82BA3394:
	// lbz r11,0(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// cmplwi r11,0
	cr0.compare<uint32_t>(r11.u32, 0, xer);
	// beq 0x82ba33a4
	if (cr0.eq) goto loc_82BA33A4;
	// bl 0x82ba29b8
	sub_82BA29B8(ctx, base);
loc_82BA33A4:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA32C8) {
	__imp__sub_82BA32C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA33B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,292
	r11.s64 = 19136512;
	// stfs f1,-32(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// lwz r10,-32(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-32) );
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// ori r11,r11,16237
	r11.u64 = r11.u64 | 16237;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// rlwimi r10,r11,30,1,31
	ctx.r10.u64 = (rotl32(r11.u32, 30) & 0x7FFFFFFF) | (ctx.r10.u64 & 0xFFFFFFFF80000000);
	// stw r10,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r10.u32);
	// lfs f12,-32(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -32);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// fadds f12,f12,f1
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f1.f64));
	// lfs f0,420(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 420);
	f0.f64 = double(temp.f32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f13,-28508(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -28508);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// addi r6,r1,-16
	ctx.r6.s64 = ctx.r1.s64 + -16;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// addi r8,r8,4800
	ctx.r8.s64 = ctx.r8.s64 + 4800;
	// addi r7,r7,4752
	ctx.r7.s64 = ctx.r7.s64 + 4752;
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * f0.f64));
	// lfs f0,-27456(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27456);
	f0.f64 = double(temp.f32);
	// stfs f0,-16(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -16, temp.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lvx128 v12,r0,r8
	simd::store_shuffled(ctx.v12, simd::load_and_shuffle(base + ((ctx.r8.u32) & ~0xF), VectorMaskL));
	// addi r11,r11,4720
	r11.s64 = r11.s64 + 4720;
	// lvx128 v11,r0,r7
	simd::store_shuffled(ctx.v11, simd::load_and_shuffle(base + ((ctx.r7.u32) & ~0xF), VectorMaskL));
	// lvx128 v13,r0,r11
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// fctiwz f0,f12
	f0.u64 = uint64_t(int32_t(std::trunc(ctx.f12.f64)));
	// stfd f0,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, f0.u64);
	// lwa r5,-28(r1)
	ctx.r5.s64 = int32_t(PPC_LOAD_U32(ctx.r1.u32 + -28));
	// std r5,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r5.u64);
	// lfd f0,-32(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fnmsubs f0,f0,f13,f1
	f0.f64 = -double(std::fma(float(f0.f64), float(ctx.f13.f64), -float(ctx.f1.f64)));
	// stfs f0,-12(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -12, temp.u32);
	// fmuls f13,f0,f0
	ctx.f13.f64 = double(float(f0.f64 * f0.f64));
	// stfs f13,-8(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + -8, temp.u32);
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f0,-4(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -4, temp.u32);
	// lvx128 v0,r0,r10
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// vmulfp128 v10,v0,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vspltw v0,v0,1
	simd::store_i32(ctx.v0.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 2));
	// vmulfp128 v10,v10,v0
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vspltw v9,v10,3
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v10.u32), 0));
	// vmsum4fp128 v13,v10,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::dp_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v13.f32), 0xFF));
	// vmulfp128 v0,v9,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// stvx128 v13,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v13), &VectorMaskL[(ea & 0xF) * 16]);
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	f0.f64 = double(temp.f32);
	// vmulfp128 v13,v10,v0
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmulfp128 v0,v13,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmsum4fp128 v13,v13,v12
	simd::store_f32_aligned(ctx.v13.f32, simd::dp_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v12.f32), 0xFF));
	// vmsum4fp128 v0,v0,v11
	simd::store_f32_aligned(ctx.v0.f32, simd::dp_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v11.f32), 0xFF));
	// stvx128 v13,r0,r11
	ea = (r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v13), &VectorMaskL[(ea & 0xF) * 16]);
	// lfs f13,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f13.f64 = double(temp.f32);
	// stvx128 v0,r0,r6
	ea = (ctx.r6.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v0), &VectorMaskL[(ea & 0xF) * 16]);
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fadds f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fadds f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 + f0.f64));
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA33B8) {
	__imp__sub_82BA33B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA34B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stfs f1,20(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 20, temp.u32);
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// addi r9,r1,-28
	ctx.r9.s64 = ctx.r1.s64 + -28;
	// addi r8,r1,20
	ctx.r8.s64 = ctx.r1.s64 + 20;
	// addi r7,r1,-28
	ctx.r7.s64 = ctx.r1.s64 + -28;
	// lfs f0,-27468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27468);
	f0.f64 = double(temp.f32);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// stfs f0,-32(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lvlx v0,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r10,r6,-17344
	ctx.r10.s64 = ctx.r6.s64 + -17344;
	// stfs f0,-28(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// addi r11,r11,-17360
	r11.s64 = r11.s64 + -17360;
	// lvlx v11,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// stfs f0,-28(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// lvlx v12,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v0,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// lvlx v13,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v12,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// vor v10,v13,v13
	simd::store_i8(ctx.v10.u8, simd::load_i8(ctx.v13.u8));
	// lvx128 v13,r0,r10
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// vspltw v6,v13,1
	simd::store_i32(ctx.v6.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 2));
	// lvx128 v0,r0,r11
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// vspltw v5,v13,2
	simd::store_i32(ctx.v5.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 1));
	// vspltw v4,v13,3
	simd::store_i32(ctx.v4.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 0));
	// vrlimi128 v10,v11,3,2
	simd::store_f32(ctx.v10.f32, simd::blend_f32<3>(simd::load_f32(ctx.v10.f32), simd::permute_f32<78>(simd::load_f32(ctx.v11.f32))));
	// vspltw v11,v13,0
	simd::store_i32(ctx.v11.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 3));
	// vspltw v12,v0,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 3));
	// vspltw v8,v0,2
	simd::store_i32(ctx.v8.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 1));
	// vspltw v9,v0,1
	simd::store_i32(ctx.v9.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 2));
	// vrfim v13,v10
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32(ctx.v13.f32, simd::round_f32(simd::load_f32(ctx.v10.f32), simd::round_to_neg_inf));
	// vspltw v7,v0,3
	simd::store_i32(ctx.v7.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 0));
	// vsubfp v0,v10,v13
	simd::store_f32_aligned(ctx.v0.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vexptefp v10,v13
	simd::store_f32(ctx.v10.f32, simd::log2_f32(simd::to_vec128f(ctx.v13)));
	// vmulfp128 v13,v0,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v9,v0,v9,v12
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v11,v0,v6,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v6.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v0,v0,v13
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v9,v13,v8,v9
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v8.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v11,v13,v5,v11
	simd::store_f32_aligned(ctx.v11.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v5.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmulfp128 v13,v13,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v9,v0,v7,v9
	simd::store_f32_aligned(ctx.v9.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vmaddfp v0,v0,v4,v11
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v4.f32)), simd::load_f32_aligned(ctx.v11.f32)));
	// vmaddfp v13,v13,v0,v9
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v9.f32)));
	// vrefp v0,v13
	simd::store_f32(ctx.v0.f32, simd::reciprocal_f32(simd::load_f32(ctx.v13.f32)));
	// vnmsubfp v11,v13,v0,v12
	simd::store_f32_aligned(ctx.v11.f32, simd::xor_f32(simd::sub_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)), simd::bitcast_f32(simd::set1_i32(0x80000000))));
	// vor v8,v0,v0
	simd::store_i8(ctx.v8.u8, simd::load_i8(ctx.v0.u8));
	// vmaddfp v0,v0,v11,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v11.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vnmsubfp v13,v13,v0,v12
	simd::store_f32_aligned(ctx.v13.f32, simd::xor_f32(simd::sub_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)), simd::bitcast_f32(simd::set1_i32(0x80000000))));
	// vcmpeqfp v9,v0,v0
	simd::store_f32_aligned(ctx.v9.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v0,v0,v13,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vsel v0,v8,v0,v9
	simd::store_i8(ctx.v0.u8, simd::select_i8(simd::load_i8(ctx.v8.u8), simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v9.u8)));
	// vmulfp128 v0,v10,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// stvx128 v0,r0,r9
	ea = (ctx.r9.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v0), &VectorMaskL[(ea & 0xF) * 16]);
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA34B0) {
	__imp__sub_82BA34B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA3598) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCVRegister v22{};
	PPCVRegister v23{};
	PPCVRegister v24{};
	PPCVRegister v25{};
	PPCVRegister v26{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	PPCVRegister vTemp{};
	uint32_t ea{};
	// lis r11,-32246
	r11.s64 = -2113273856;
	// vspltisw v10,0
	simd::store_i32(ctx.v10.u32, simd::set1_i32(int32_t(0x0)));
	// addi r7,r1,-40
	ctx.r7.s64 = ctx.r1.s64 + -40;
	// stfs f2,28(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 28, temp.u32);
	// addi r6,r1,28
	ctx.r6.s64 = ctx.r1.s64 + 28;
	// stfs f1,20(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 20, temp.u32);
	// addi r10,r1,20
	ctx.r10.s64 = ctx.r1.s64 + 20;
	// vspltisw v11,-1
	simd::store_i32(ctx.v11.u32, simd::set1_i32(int32_t(0xFFFFFFFF)));
	// vupkd3d128 v0,v10,4
	temp.f32 = 3.0f;
	temp.s32 += ctx.v10.s16[1];
	vTemp.f32[3] = temp.f32;
	temp.f32 = 3.0f;
	temp.s32 += ctx.v10.s16[0];
	vTemp.f32[2] = temp.f32;
	vTemp.f32[1] = 0.0f;
	vTemp.f32[0] = 1.0f;
	ctx.v0 = vTemp;
	// addi r9,r1,-48
	ctx.r9.s64 = ctx.r1.s64 + -48;
	// lfs f0,-27468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27468);
	f0.f64 = double(temp.f32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// stfs f0,-40(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -40, temp.u32);
	// addi r8,r1,-44
	ctx.r8.s64 = ctx.r1.s64 + -44;
	// addi r11,r11,-17328
	r11.s64 = r11.s64 + -17328;
	// lvlx v23,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(v23,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw v5,v0,3
	simd::store_i32(ctx.v5.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 0));
	// lvlx v6,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v6,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// stfs f0,-48(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -48, temp.u32);
	// addi r5,r1,-36
	ctx.r5.s64 = ctx.r1.s64 + -36;
	// lvlx v25,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(v25,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// addi r4,r1,-32
	ctx.r4.s64 = ctx.r1.s64 + -32;
	// vrlimi128 v6,v25,4,3
	simd::store_f32(ctx.v6.f32, simd::blend_f32<4>(simd::load_f32(ctx.v6.f32), simd::permute_f32<57>(simd::load_f32(v25.f32))));
	// lvx128 v0,r0,r11
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// addi r11,r10,-17312
	r11.s64 = ctx.r10.s64 + -17312;
	// vspltw v3,v0,0
	simd::store_i32(ctx.v3.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 3));
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// vspltw v2,v0,1
	simd::store_i32(ctx.v2.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 2));
	// addi r3,r1,-28
	ctx.r3.s64 = ctx.r1.s64 + -28;
	// vspltw v1,v0,2
	simd::store_i32(ctx.v1.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 1));
	// addi r10,r10,-17360
	ctx.r10.s64 = ctx.r10.s64 + -17360;
	// vspltw v31,v0,3
	simd::store_i32(v31.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 0));
	// lvlx v0,0,r6
	temp.u32 = r0.u32 + ctx.r6.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvx128 v13,r0,r11
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// vslw v8,v11,v11
	ctx.fpscr.enableFlushModeUnconditional();
	simd::to_vec128i(ctx.v8) = simd::shift_left_variable_i32(simd::to_vec128i(ctx.v11), simd::to_vec128i(ctx.v11));
	// vspltw v30,v13,0
	simd::store_i32(v30.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 3));
	// stfs f0,-44(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -44, temp.u32);
	// vspltw v29,v13,1
	simd::store_i32(v29.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 2));
	// lvlx v25,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(v25,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vspltw v28,v13,2
	simd::store_i32(v28.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 1));
	// vrlimi128 v23,v25,4,3
	simd::store_f32(v23.f32, simd::blend_f32<4>(simd::load_f32(v23.f32), simd::permute_f32<57>(simd::load_f32(v25.f32))));
	// vspltw v27,v13,3
	simd::store_i32(v27.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 0));
	// lvx128 v13,r0,r10
	simd::store_shuffled(ctx.v13, simd::load_and_shuffle(base + ((ctx.r10.u32) & ~0xF), VectorMaskL));
	// vspltw v12,v13,0
	simd::store_i32(ctx.v12.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 3));
	// stfs f0,-36(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -36, temp.u32);
	// vspltw v26,v13,1
	simd::store_i32(v26.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 2));
	// stfs f0,-32(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// vspltw v24,v13,2
	simd::store_i32(v24.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 1));
	// stfs f0,-28(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -28, temp.u32);
	// vspltw v22,v13,3
	simd::store_i32(v22.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v13.u32), 0));
	// lvlx v13,0,r5
	temp.u32 = r0.u32 + ctx.r5.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v0,v13,4,3
	simd::store_f32(ctx.v0.f32, simd::blend_f32<4>(simd::load_f32(ctx.v0.f32), simd::permute_f32<57>(simd::load_f32(ctx.v13.f32))));
	// lvlx v25,0,r4
	temp.u32 = r0.u32 + ctx.r4.u32;
	simd::store_shuffled(v25,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v13,0,r3
	temp.u32 = r0.u32 + ctx.r3.u32;
	simd::store_shuffled(ctx.v13,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v13,v25,4,3
	simd::store_f32(ctx.v13.f32, simd::blend_f32<4>(simd::load_f32(ctx.v13.f32), simd::permute_f32<57>(simd::load_f32(v25.f32))));
	// vrlimi128 v6,v23,3,2
	simd::store_f32(ctx.v6.f32, simd::blend_f32<3>(simd::load_f32(ctx.v6.f32), simd::permute_f32<78>(simd::load_f32(v23.f32))));
	// vspltisw v9,-9
	simd::store_i32(ctx.v9.u32, simd::set1_i32(int32_t(0xFFFFFFF7)));
	// vrlimi128 v0,v13,3,2
	simd::store_f32(ctx.v0.f32, simd::blend_f32<3>(simd::load_f32(ctx.v0.f32), simd::permute_f32<78>(simd::load_f32(ctx.v13.f32))));
	// vandc v13,v6,v8
	simd::store_u8(ctx.v13.u8, simd::andnot_u8(simd::load_u8(ctx.v8.u8), simd::load_u8(ctx.v6.u8)));
	// vor v25,v0,v0
	simd::store_i8(v25.u8, simd::load_i8(ctx.v0.u8));
	// vslw v7,v11,v9
	ctx.fpscr.enableFlushModeUnconditional();
	simd::to_vec128i(ctx.v7) = simd::shift_left_variable_i32(simd::to_vec128i(ctx.v11), simd::to_vec128i(ctx.v9));
	// vor v0,v13,v13
	simd::store_i8(ctx.v0.u8, simd::load_i8(ctx.v13.u8));
	// vlogefp v13,v13
	simd::store_f32(ctx.v13.f32, simd::log2_f32(simd::to_vec128f(ctx.v13)));
	// vsel v0,v0,v5,v7
	simd::store_i8(ctx.v0.u8, simd::select_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v5.u8), simd::load_i8(ctx.v7.u8)));
	// vsubfp v0,v0,v5
	simd::store_f32_aligned(ctx.v0.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v5.f32)));
	// vrfim v7,v13
	simd::store_f32(ctx.v7.f32, simd::round_f32(simd::load_f32(ctx.v13.f32), simd::round_to_neg_inf));
	// vmaddfp v4,v0,v2,v3
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v2.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmulfp128 v13,v0,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v3,v0,v29,v30
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v29.f32)), simd::load_f32_aligned(v30.f32)));
	// vmulfp128 v2,v0,v25
	simd::store_f32_aligned(ctx.v2.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v25.f32)));
	// vmulfp128 v7,v7,v25
	simd::store_f32_aligned(ctx.v7.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(v25.f32)));
	// vmulfp128 v0,v0,v13
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v4,v13,v1,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v1.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v3,v13,v28,v3
	simd::store_f32_aligned(ctx.v3.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v28.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmulfp128 v13,v13,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v4,v0,v31,v4
	simd::store_f32_aligned(ctx.v4.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v31.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vmaddfp v0,v0,v27,v3
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v27.f32)), simd::load_f32_aligned(ctx.v3.f32)));
	// vmaddfp v0,v13,v0,v4
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v4.f32)));
	// vspltisw v4,1
	simd::store_i32(ctx.v4.u32, simd::set1_i32(int32_t(0x1)));
	// vmaddfp v7,v2,v0,v7
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v2.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vctsxs v3,v25,0
	simd::store_i32(ctx.v3.s32, simd::vctsxs(simd::load_f32_aligned(v25.f32)));
	// lis r11,-32240
	r11.s64 = -2112880640;
	// vrfiz v31,v25
	simd::store_f32(v31.f32, simd::round_f32(simd::load_f32(v25.f32), simd::round_to_zero));
	// vand v8,v6,v8
	simd::store_u8(ctx.v8.u8, simd::and_u8(simd::load_u8(ctx.v6.u8), simd::load_u8(ctx.v8.u8)));
	// addi r11,r11,-17344
	r11.s64 = r11.s64 + -17344;
	// vcmpeqfp v2,v6,v10
	simd::store_f32_aligned(ctx.v2.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v6.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// vcmpgtfp v1,v10,v25
	simd::store_f32_aligned(ctx.v1.f32, simd::cmpgt_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(v25.f32)));
	// vslw v9,v11,v9
	simd::to_vec128i(ctx.v9) = simd::shift_left_variable_i32(simd::to_vec128i(ctx.v11), simd::to_vec128i(ctx.v9));
	// vcmpeqfp v13,v25,v10
	simd::store_f32_aligned(ctx.v13.f32, simd::cmpeq_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(ctx.v10.f32)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vcmpgtfp v6,v10,v6
	simd::store_f32_aligned(ctx.v6.f32, simd::cmpgt_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v6.f32)));
	// lvx128 v0,r0,r11
	simd::store_shuffled(ctx.v0, simd::load_and_shuffle(base + ((r11.u32) & ~0xF), VectorMaskL));
	// vsrw v9,v9,v4
simd::store_shuffled(ctx.v9, simd::shift_right_logical_i32(simd::to_vec128i(ctx.v9), simd::and_u32(simd::to_vec128i(ctx.v4), simd::set1_i32(0x1F))));
	// vspltw v30,v0,0
	simd::store_i32(v30.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 3));
	// vspltw v29,v0,1
	simd::store_i32(v29.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 2));
	// vspltw v28,v0,2
	simd::store_i32(v28.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 1));
	// vspltw v27,v0,3
	simd::store_i32(v27.u32, simd::broadcast_lane_i32(simd::load_i32(ctx.v0.u32), 0));
	// vand v0,v3,v4
	simd::store_u8(ctx.v0.u8, simd::and_u8(simd::load_u8(ctx.v3.u8), simd::load_u8(ctx.v4.u8)));
	// vrfim v3,v7
	simd::store_f32(ctx.v3.f32, simd::round_f32(simd::load_f32(ctx.v7.f32), simd::round_to_neg_inf));
	// vandc v4,v2,v1
	simd::store_u8(ctx.v4.u8, simd::andnot_u8(simd::load_u8(ctx.v1.u8), simd::load_u8(ctx.v2.u8)));
	// vslw v0,v0,v11
	simd::to_vec128i(ctx.v0) = simd::shift_left_variable_i32(simd::to_vec128i(ctx.v0), simd::to_vec128i(ctx.v11));
	// vcmpeqfp v11,v25,v31
	simd::store_f32_aligned(ctx.v11.f32, simd::cmpeq_f32(simd::load_f32_aligned(v25.f32), simd::load_f32_aligned(v31.f32)));
	// vor v2,v2,v13
	simd::store_i8(ctx.v2.u8, simd::or_i8(simd::load_i8(ctx.v2.u8), simd::load_i8(ctx.v13.u8)));
	// vand v8,v8,v0
	simd::store_u8(ctx.v8.u8, simd::and_u8(simd::load_u8(ctx.v8.u8), simd::load_u8(ctx.v0.u8)));
	// vsubfp v0,v7,v3
	simd::store_f32_aligned(ctx.v0.f32, simd::sub_f32(simd::load_f32_aligned(ctx.v7.f32), simd::load_f32_aligned(ctx.v3.f32)));
	// vor v10,v10,v8
	simd::store_i8(ctx.v10.u8, simd::or_i8(simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v8.u8)));
	// vsel v10,v9,v10,v4
	simd::store_i8(ctx.v10.u8, simd::select_i8(simd::load_i8(ctx.v9.u8), simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v4.u8)));
	// vexptefp v9,v3
	simd::store_f32(ctx.v9.f32, simd::log2_f32(simd::to_vec128f(ctx.v3)));
	// vandc v11,v6,v11
	simd::store_u8(ctx.v11.u8, simd::andnot_u8(simd::load_u8(ctx.v11.u8), simd::load_u8(ctx.v6.u8)));
	// vsel v10,v10,v5,v13
	simd::store_i8(ctx.v10.u8, simd::select_i8(simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v5.u8), simd::load_i8(ctx.v13.u8)));
	// vor v11,v11,v2
	simd::store_i8(ctx.v11.u8, simd::or_i8(simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v2.u8)));
	// vmulfp128 v13,v0,v0
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v7,v0,v26,v12
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v26.f32)), simd::load_f32_aligned(ctx.v12.f32)));
	// vmaddfp v6,v0,v29,v30
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v29.f32)), simd::load_f32_aligned(v30.f32)));
	// vmulfp128 v0,v0,v13
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v7,v13,v24,v7
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v24.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v6,v13,v28,v6
	simd::store_f32_aligned(ctx.v6.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(v28.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// vmulfp128 v13,v13,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmaddfp v7,v0,v22,v7
	simd::store_f32_aligned(ctx.v7.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v22.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vmaddfp v0,v0,v27,v6
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(v27.f32)), simd::load_f32_aligned(ctx.v6.f32)));
	// vmaddfp v13,v13,v0,v7
	simd::store_f32_aligned(ctx.v13.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v7.f32)));
	// vrefp v0,v13
	simd::store_f32(ctx.v0.f32, simd::reciprocal_f32(simd::load_f32(ctx.v13.f32)));
	// vnmsubfp v7,v13,v0,v12
	simd::store_f32_aligned(ctx.v7.f32, simd::xor_f32(simd::sub_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)), simd::bitcast_f32(simd::set1_i32(0x80000000))));
	// vor v5,v0,v0
	simd::store_i8(ctx.v5.u8, simd::load_i8(ctx.v0.u8));
	// vmaddfp v0,v0,v7,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v7.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vnmsubfp v13,v13,v0,v12
	simd::store_f32_aligned(ctx.v13.f32, simd::xor_f32(simd::sub_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v0.f32)), simd::load_f32_aligned(ctx.v12.f32)), simd::bitcast_f32(simd::set1_i32(0x80000000))));
	// vcmpeqfp v6,v0,v0
	simd::store_f32_aligned(ctx.v6.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vmaddfp v0,v0,v13,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vsel v0,v5,v0,v6
	simd::store_i8(ctx.v0.u8, simd::select_i8(simd::load_i8(ctx.v5.u8), simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v6.u8)));
	// vmulfp128 v0,v9,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v9.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vor v0,v0,v8
	simd::store_i8(ctx.v0.u8, simd::or_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v8.u8)));
	// vsel v0,v0,v10,v11
	simd::store_i8(ctx.v0.u8, simd::select_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v10.u8), simd::load_i8(ctx.v11.u8)));
	// stvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v0), &VectorMaskL[(ea & 0xF) * 16]);
	// lfs f1,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA3598) {
	__imp__sub_82BA3598(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA37B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-976(r1)
	ea = -976 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// rlwinm. r11,r30,4,0,27
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x82ba37e4
	if (!cr0.gt) goto loc_82BA37E4;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ca30e8
	sub_82CA30E8(ctx, base);
loc_82BA37E4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82ba380c
	if (!cr6.gt) goto loc_82BA380C;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// li r10,-1
	ctx.r10.s64 = -1;
	// cmplwi r30,0
	cr0.compare<uint32_t>(r30.u32, 0, xer);
	// beq 0x82ba380c
	if (cr0.eq) goto loc_82BA380C;
	// mtctr r30
	ctr.u64 = r30.u64;
loc_82BA3800:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82ba3800
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA3800;
loc_82BA380C:
	// srawi r11,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	r11.s64 = r30.s32 >> 1;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// li r29,0
	r29.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stwx r29,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, r29.u32);
	// cmpwi cr6,r30,2
	cr6.compare<int32_t>(r30.s32, 2, xer);
	// stwx r29,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, r29.u32);
	// beq cr6,0x82ba3878
	if (cr6.eq) goto loc_82BA3878;
	// addi r11,r30,2
	r11.s64 = r30.s64 + 2;
	// addi r10,r30,-3
	ctx.r10.s64 = r30.s64 + -3;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addze r11,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	r11.s64 = temp.s64;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r3,r11,r9
	PPC_STORE_U32(r11.u32 + ctx.r9.u32, ctx.r3.u32);
	// stwx r3,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r3.u32);
loc_82BA3878:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82ba38a8
	if (!cr6.gt) goto loc_82BA38A8;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_82BA3888:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmpwi cr6,r9,-1
	cr6.compare<int32_t>(ctx.r9.s32, -1, xer);
	// bne cr6,0x82ba389c
	if (!cr6.eq) goto loc_82BA389C;
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
loc_82BA389C:
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x82ba3888
	if (!cr0.eq) goto loc_82BA3888;
loc_82BA38A8:
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// subf r31,r31,r11
	r31.s64 = r11.s64 - r31.s64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r9,r11,-17748
	ctx.r9.s64 = r11.s64 + -17748;
	// li r28,16
	r28.s64 = 16;
	// li r5,4
	ctx.r5.s64 = 4;
	// lfs f12,-17748(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17748);
	ctx.f12.f64 = double(temp.f32);
	// lfd f10,3368(r10)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3368);
	// lfs f11,-9720(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -9720);
	ctx.f11.f64 = double(temp.f32);
loc_82BA38D4:
	// li r7,512
	ctx.r7.s64 = 512;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82ba39a4
	if (!cr6.gt) goto loc_82BA39A4;
	// addi r9,r1,240
	ctx.r9.s64 = ctx.r1.s64 + 240;
	// add r10,r31,r4
	ctx.r10.u64 = r31.u64 + ctx.r4.u64;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82BA38EC:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne 0x82ba38ec
	if (!cr0.eq) goto loc_82BA38EC;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
loc_82BA390C:
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// addi r9,r10,7
	ctx.r9.s64 = ctx.r10.s64 + 7;
	// slw r9,r3,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r9.u8 & 0x3F));
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// ble cr6,0x82ba3958
	if (!cr6.gt) goto loc_82BA3958;
	// std r9,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r9.u64);
	// lfd f13,176(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fadd f0,f0,f10
	f0.f64 = f0.f64 + ctx.f10.f64;
	// fctiwz f0,f0
	f0.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, f0.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// b 0x82ba397c
	goto loc_82BA397C;
loc_82BA3958:
	// std r9,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, ctx.r9.u64);
	// lfd f13,152(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fsub f0,f0,f10
	f0.f64 = f0.f64 - ctx.f10.f64;
	// fctiwz f0,f0
	f0.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, f0.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
loc_82BA397C:
	// addi r6,r1,192
	ctx.r6.s64 = ctx.r1.s64 + 192;
	// subfic r10,r10,2
	xer.ca = ctx.r10.u32 <= 2;
	ctx.r10.s64 = 2 - ctx.r10.s64;
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// stwx r9,r11,r6
	PPC_STORE_U32(r11.u32 + ctx.r6.u32, ctx.r9.u32);
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x82ba390c
	if (!cr0.eq) goto loc_82BA390C;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// ble cr6,0x82ba3a94
	if (!cr6.gt) goto loc_82BA3A94;
loc_82BA39A4:
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82ba3a68
	if (!cr6.gt) goto loc_82BA3A68;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82BA39BC:
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// fabs f9,f0
	ctx.f9.u64 = f0.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f9,f11
	cr6.compare(ctx.f9.f64, ctx.f11.f64);
	// ble cr6,0x82ba3a20
	if (!cr6.gt) goto loc_82BA3A20;
	// addi r6,r1,192
	ctx.r6.s64 = ctx.r1.s64 + 192;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwax r6,r11,r6
	ctx.r6.s64 = int32_t(PPC_LOAD_U32(r11.u32 + ctx.r6.u32));
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// std r6,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r6.u64);
	// addi r10,r10,7
	ctx.r10.s64 = ctx.r10.s64 + 7;
	// slw r10,r3,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r10.u8 & 0x3F));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lfd f9,168(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 168);
	// std r10,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r10.u64);
	// lfd f8,160(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fmuls f0,f8,f0
	f0.f64 = double(float(ctx.f8.f64 * f0.f64));
	// fsubs f9,f9,f0
	ctx.f9.f64 = static_cast<float>(ctx.f9.f64 - f0.f64);
	// fdivs f0,f9,f0
	f0.f64 = double(float(ctx.f9.f64 / f0.f64));
	// fabs f0,f0
	f0.u64 = f0.u64 & 0x7FFFFFFFFFFFFFFF;
	// b 0x82ba3a24
	goto loc_82BA3A24;
loc_82BA3A20:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64;
loc_82BA3A24:
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r10,255
	cr6.compare<int32_t>(ctx.r10.s32, 255, xer);
	// bge cr6,0x82ba3a58
	if (!cr6.lt) goto loc_82BA3A58;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// sraw r10,r5,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r10.s64 = ctx.r5.s32 >> temp.u32;
	// cmpw cr6,r7,r10
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, xer);
	// blt cr6,0x82ba3a58
	if (cr6.lt) goto loc_82BA3A58;
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82ba3a58
	if (!cr6.lt) goto loc_82BA3A58;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
loc_82BA3A58:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r9,r30
	cr6.compare<int32_t>(ctx.r9.s32, r30.s32, xer);
	// blt cr6,0x82ba39bc
	if (cr6.lt) goto loc_82BA39BC;
loc_82BA3A68:
	// rlwinm r11,r8,2,0,29
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r8,r11,r9
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// sraw r8,r5,r8
	temp.u32 = ctx.r8.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r8.s64 = ctx.r5.s32 >> temp.u32;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf. r7,r8,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r8.s64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// bgt 0x82ba39a4
	if (cr0.gt) goto loc_82BA39A4;
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
loc_82BA3A94:
	// bge cr6,0x82ba3bb0
	if (!cr6.lt) goto loc_82BA3BB0;
loc_82BA3A98:
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82ba3b88
	if (!cr6.gt) goto loc_82BA3B88;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_82BA3AB0:
	// addi r10,r1,240
	ctx.r10.s64 = ctx.r1.s64 + 240;
	// lfsx f0,r11,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	f0.f64 = double(temp.f32);
	// fabs f9,f0
	ctx.f9.u64 = f0.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f9,f11
	cr6.compare(ctx.f9.f64, ctx.f11.f64);
	// ble cr6,0x82ba3b1c
	if (!cr6.gt) goto loc_82BA3B1C;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// addi r27,r1,96
	r27.s64 = ctx.r1.s64 + 96;
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r10,r11,r27
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r10,r10,7
	ctx.r10.s64 = ctx.r10.s64 + 7;
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// std r9,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r9.u64);
	// slw r10,r3,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r3.u32 << (ctx.r10.u8 & 0x3F));
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// lfd f9,136(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfd f8,144(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// fmuls f0,f9,f0
	f0.f64 = double(float(ctx.f9.f64 * f0.f64));
	// frsp f9,f8
	ctx.f9.f64 = double(float(ctx.f8.f64));
	// fsubs f9,f9,f0
	ctx.f9.f64 = static_cast<float>(ctx.f9.f64 - f0.f64);
	// fdivs f0,f9,f0
	f0.f64 = double(float(ctx.f9.f64 / f0.f64));
	// fabs f0,f0
	f0.u64 = f0.u64 & 0x7FFFFFFFFFFFFFFF;
	// b 0x82ba3b20
	goto loc_82BA3B20;
loc_82BA3B1C:
	// fmr f0,f12
	ctx.fpscr.disableFlushMode();
	f0.f64 = ctx.f12.f64;
loc_82BA3B20:
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// sraw r9,r5,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r9.s64 = ctx.r5.s32 >> temp.u32;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
	// cmpw cr6,r7,r9
	cr6.compare<int32_t>(ctx.r7.s32, ctx.r9.s32, xer);
	// bgt cr6,0x82ba3b78
	if (cr6.gt) goto loc_82BA3B78;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82ba3b54
	if (!cr6.eq) goto loc_82BA3B54;
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bgt cr6,0x82ba3b68
	if (cr6.gt) goto loc_82BA3B68;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
loc_82BA3B54:
	// ble cr6,0x82ba3b78
	if (!cr6.gt) goto loc_82BA3B78;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r10,-256
	cr6.compare<int32_t>(ctx.r10.s32, -256, xer);
	// ble cr6,0x82ba3b78
	if (!cr6.gt) goto loc_82BA3B78;
loc_82BA3B68:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82ba3b78
	if (!cr6.lt) goto loc_82BA3B78;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
loc_82BA3B78:
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmpw cr6,r8,r30
	cr6.compare<int32_t>(ctx.r8.s32, r30.s32, xer);
	// blt cr6,0x82ba3ab0
	if (cr6.lt) goto loc_82BA3AB0;
loc_82BA3B88:
	// rlwinm r11,r6,2,0,29
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,192
	ctx.r10.s64 = ctx.r1.s64 + 192;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lwzx r8,r11,r10
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// sraw r9,r5,r9
	temp.u32 = ctx.r9.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	xer.ca = (ctx.r5.s32 < 0) & (((ctx.r5.s32 >> temp.u32) << temp.u32) != ctx.r5.s32);
	ctx.r9.s64 = ctx.r5.s32 >> temp.u32;
	// add. r7,r9,r7
	ctx.r7.u64 = ctx.r9.u64 + ctx.r7.u64;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// stwx r8,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r8.u32);
	// blt 0x82ba3a98
	if (cr0.lt) goto loc_82BA3A98;
loc_82BA3BB0:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82ba3bf0
	if (!cr6.gt) goto loc_82BA3BF0;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// mr r11,r29
	r11.u64 = r29.u64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
loc_82BA3BC4:
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r7,r1,192
	ctx.r7.s64 = ctx.r1.s64 + 192;
	// lwzx r8,r11,r8
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// lwzx r7,r11,r7
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + ctx.r7.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// subfic r8,r8,2
	xer.ca = ctx.r8.u32 <= 2;
	ctx.r8.s64 = 2 - ctx.r8.s64;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// slw r8,r7,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r7.u32 << (ctx.r8.u8 & 0x3F));
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// bne 0x82ba3bc4
	if (!cr0.eq) goto loc_82BA3BC4;
loc_82BA3BF0:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r4,r4,4
	ctx.r4.s64 = ctx.r4.s64 + 4;
	// bne 0x82ba38d4
	if (!cr0.eq) goto loc_82BA38D4;
	// addi r1,r1,976
	ctx.r1.s64 = ctx.r1.s64 + 976;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA37B8) {
	__imp__sub_82BA37B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA3C08) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stfd f31,-16(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -16, f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fabs f0,f1
	f0.u64 = ctx.f1.u64 & 0x7FFFFFFFFFFFFFFF;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f13,904(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 904);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x82ba3c98
	if (!cr6.lt) goto loc_82BA3C98;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lfs f0,900(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 900);
	f0.f64 = double(temp.f32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// fmuls f7,f1,f0
	ctx.f7.f64 = double(float(ctx.f1.f64 * f0.f64));
	// lfs f0,896(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 896);
	f0.f64 = double(temp.f32);
	// lfs f13,892(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 892);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lfs f12,888(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 888);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lfs f11,884(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 884);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,880(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 880);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r9,-27468
	r11.s64 = ctx.r9.s64 + -27468;
	// lfs f9,876(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 876);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f7,f7
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f7.f64));
	// fmadds f0,f7,f0,f13
	f0.f64 = double(std::fma(float(ctx.f7.f64), float(f0.f64), float(ctx.f13.f64)));
	// fmadds f0,f0,f7,f12
	f0.f64 = double(std::fma(float(f0.f64), float(ctx.f7.f64), float(ctx.f12.f64)));
	// fmadds f0,f0,f7,f11
	f0.f64 = double(std::fma(float(f0.f64), float(ctx.f7.f64), float(ctx.f11.f64)));
	// fmadds f0,f0,f7,f10
	f0.f64 = double(std::fma(float(f0.f64), float(ctx.f7.f64), float(ctx.f10.f64)));
	// fmadds f0,f0,f7,f9
	f0.f64 = double(std::fma(float(f0.f64), float(ctx.f7.f64), float(ctx.f9.f64)));
	// fmadds f1,f0,f7,f8
	ctx.f1.f64 = double(std::fma(float(f0.f64), float(ctx.f7.f64), float(ctx.f8.f64)));
	// b 0x82ba3d90
	goto loc_82BA3D90;
loc_82BA3C98:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// fdivs f31,f13,f0
	ctx.fpscr.disableFlushMode();
	f31.f64 = double(float(ctx.f13.f64 / f0.f64));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// fmr f1,f0
	ctx.f1.f64 = f0.f64;
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// vspltisw v0,1
	simd::store_i32(ctx.v0.u32, simd::set1_i32(int32_t(0x1)));
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,84
	ctx.r10.s64 = ctx.r1.s64 + 84;
	// lfs f0,-27468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27468);
	f0.f64 = double(temp.f32);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// stfs f0,88(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// vcfsx v13,v0,1
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(ctx.v13.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v0.s32)), simd::bitcast_f32(simd::set1_i32(0x3F000000))));
	// lvlx v12,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,88(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v10,0,r7
	temp.u32 = r0.u32 + ctx.r7.u32;
	simd::store_shuffled(ctx.v10,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v11,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// stfs f0,84(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v11,v10,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v10.f32))));
	// vrlimi128 v11,v12,3,2
	simd::store_f32(ctx.v11.f32, simd::blend_f32<3>(simd::load_f32(ctx.v11.f32), simd::permute_f32<78>(simd::load_f32(ctx.v12.f32))));
	// vrsqrtefp v0,v11
	ctx.fpscr.enableFlushModeUnconditional();
simd::store_shuffled(ctx.v0, simd::rsqrt_f32(simd::to_vec128f(ctx.v11)));
	// vmulfp128 v10,v11,v13
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v9,v0,v0
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vcmpeqfp v8,v0,v0
	simd::store_f32_aligned(ctx.v8.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vnmsubfp v13,v10,v9,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::xor_f32(simd::sub_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v13.f32)), simd::bitcast_f32(simd::set1_i32(0x80000000))));
	// vmaddfp v0,v0,v13,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vcmpeqfp v13,v13,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v0,v11,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v11.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vxor v13,v13,v8
	simd::store_u8(ctx.v13.u8, simd::xor_i8(simd::load_u8(ctx.v13.u8), simd::load_u8(ctx.v8.u8)));
	// vsel v0,v0,v11,v13
	simd::store_i8(ctx.v0.u8, simd::select_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v11.u8), simd::load_i8(ctx.v13.u8)));
	// stvx128 v0,r0,r11
	ea = (r11.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v0), &VectorMaskL[(ea & 0xF) * 16]);
	// bl 0x82ba34b0
	sub_82BA34B0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f0,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	f0.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// fdivs f7,f1,f0
	ctx.f7.f64 = double(float(ctx.f1.f64 / f0.f64));
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lfs f0,872(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 872);
	f0.f64 = double(temp.f32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,868(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 868);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// fmsubs f6,f31,f0,f13
	ctx.f6.f64 = double(std::fma(float(f31.f64), float(f0.f64), -float(ctx.f13.f64)));
	// lfs f0,864(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 864);
	f0.f64 = double(temp.f32);
	// lfs f13,860(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 860);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lfs f12,856(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 856);
	ctx.f12.f64 = double(temp.f32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lfs f11,852(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 852);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,848(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 848);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,844(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 844);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,840(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 840);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f0,f6,f31,f0
	f0.f64 = double(std::fma(float(ctx.f6.f64), float(f31.f64), float(f0.f64)));
	// fmsubs f0,f0,f31,f13
	f0.f64 = double(std::fma(float(f0.f64), float(f31.f64), -float(ctx.f13.f64)));
	// fmadds f0,f0,f31,f12
	f0.f64 = double(std::fma(float(f0.f64), float(f31.f64), float(ctx.f12.f64)));
	// fmsubs f0,f0,f31,f11
	f0.f64 = double(std::fma(float(f0.f64), float(f31.f64), -float(ctx.f11.f64)));
	// fmadds f0,f0,f31,f10
	f0.f64 = double(std::fma(float(f0.f64), float(f31.f64), float(ctx.f10.f64)));
	// fmadds f0,f0,f31,f9
	f0.f64 = double(std::fma(float(f0.f64), float(f31.f64), float(ctx.f9.f64)));
	// fmadds f0,f0,f31,f8
	f0.f64 = double(std::fma(float(f0.f64), float(f31.f64), float(ctx.f8.f64)));
	// fmuls f1,f7,f0
	ctx.f1.f64 = double(float(ctx.f7.f64 * f0.f64));
loc_82BA3D90:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// lfd f31,-16(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA3C08) {
	__imp__sub_82BA3C08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA3DA8) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, f29.u64);
	// stfd f30,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, f30.u64);
	// stfd f31,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	f31.f64 = ctx.f1.f64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r31,r11,-27468
	r31.s64 = r11.s64 + -27468;
	// lfs f0,12(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 12);
	f0.f64 = double(temp.f32);
	// fabs f13,f31
	ctx.f13.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x82ba3e18
	if (!cr6.lt) goto loc_82BA3E18;
	// lfs f2,9556(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 9556);
	ctx.f2.f64 = double(temp.f32);
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	f30.f64 = ctx.f1.f64;
	// lfs f2,8236(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 8236);
	ctx.f2.f64 = double(temp.f32);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// lfs f0,9724(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 9724);
	f0.f64 = double(temp.f32);
	// fmuls f12,f1,f0
	ctx.f12.f64 = double(float(ctx.f1.f64 * f0.f64));
	// lfs f0,14432(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 14432);
	f0.f64 = double(temp.f32);
	// lfs f13,9716(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 9716);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f0,f30,f0,f12
	f0.f64 = double(std::fma(float(f30.f64), float(f0.f64), -float(ctx.f12.f64)));
	// fadds f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 + ctx.f13.f64));
	// b 0x82ba3e70
	goto loc_82BA3E70;
loc_82BA3E18:
	// fabs f0,f31
	ctx.fpscr.disableFlushMode();
	f0.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// lfs f30,8236(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 8236);
	f30.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	cr6.compare(f0.f64, f30.f64);
	// bge cr6,0x82ba3e6c
	if (!cr6.lt) goto loc_82BA3E6C;
	// fmr f2,f30
	ctx.f2.f64 = f30.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	f29.f64 = ctx.f1.f64;
	// lfs f2,9556(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 9556);
	ctx.f2.f64 = double(temp.f32);
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmuls f11,f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64 * f30.f64));
	// lfs f0,9724(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 9724);
	f0.f64 = double(temp.f32);
	// fabs f10,f31
	ctx.f10.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// lfs f13,23104(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 23104);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// lfs f12,27524(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 27524);
	ctx.f12.f64 = double(temp.f32);
	// fmsubs f0,f29,f0,f11
	f0.f64 = double(std::fma(float(f29.f64), float(f0.f64), -float(ctx.f11.f64)));
	// fnmsubs f0,f10,f13,f0
	f0.f64 = -double(std::fma(float(ctx.f10.f64), float(ctx.f13.f64), -float(f0.f64)));
	// fadds f1,f0,f12
	ctx.f1.f64 = double(float(f0.f64 + ctx.f12.f64));
	// b 0x82ba3e70
	goto loc_82BA3E70;
loc_82BA3E6C:
	// lfs f1,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
loc_82BA3E70:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// lfd f29,-40(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f30,-32(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// lfd f31,-24(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA3DA8) {
	__imp__sub_82BA3DA8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA3E90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCVRegister v127{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// addi r12,r1,-48
	r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82ca74e4
	// li r12,-176
	r12.s64 = -176;
	// stvx128 v127,r1,r12
	ea = (ctx.r1.u32 + r12.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(v127), &VectorMaskL[(ea & 0xF) * 16]);
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// fmr f20,f1
	ctx.fpscr.disableFlushMode();
	f20.f64 = ctx.f1.f64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// fmr f31,f2
	f31.f64 = ctx.f2.f64;
	// addi r11,r11,-17744
	r11.s64 = r11.s64 + -17744;
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// rlwinm r31,r27,4,0,27
	r31.u64 = rotl64(r27.u32 | (r27.u64 << 32), 4) & 0xFFFFFFF0;
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// lfs f21,-9724(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9724);
	f21.f64 = double(temp.f32);
	// lfs f22,-9712(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9712);
	f22.f64 = double(temp.f32);
	// beq cr6,0x82ba42c4
	if (cr6.eq) goto loc_82BA42C4;
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x82ba4194
	if (cr6.eq) goto loc_82BA4194;
	// cmpwi cr6,r3,3
	cr6.compare<int32_t>(ctx.r3.s32, 3, xer);
	// beq cr6,0x82ba4118
	if (cr6.eq) goto loc_82BA4118;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// beq cr6,0x82ba40b4
	if (cr6.eq) goto loc_82BA40B4;
	// cmpwi cr6,r3,5
	cr6.compare<int32_t>(ctx.r3.s32, 5, xer);
	// beq cr6,0x82ba4000
	if (cr6.eq) goto loc_82BA4000;
	// cmpwi cr6,r3,6
	cr6.compare<int32_t>(ctx.r3.s32, 6, xer);
	// beq cr6,0x82ba3f70
	if (cr6.eq) goto loc_82BA3F70;
	// cmpwi cr6,r3,7
	cr6.compare<int32_t>(ctx.r3.s32, 7, xer);
	// bne cr6,0x82ba3f70
	if (!cr6.eq) goto loc_82BA3F70;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba3f30
	if (cr6.eq) goto loc_82BA3F30;
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi r31,0
	cr0.compare<uint32_t>(r31.u32, 0, xer);
	// beq 0x82ba3f30
	if (cr0.eq) goto loc_82BA3F30;
	// mtctr r31
	ctr.u64 = r31.u64;
loc_82BA3F24:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82ba3f24
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA3F24;
loc_82BA3F30:
	// addi r11,r31,-16
	r11.s64 = r31.s64 + -16;
	// addi r10,r31,16
	ctx.r10.s64 = r31.s64 + 16;
	// rlwinm r11,r11,31,1,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r9,r10,31,1,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82ba43b8
	if (!cr6.lt) goto loc_82BA43B8;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r11,r9
	r11.s64 = ctx.r9.s64 - r11.s64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + r28.u64;
	// lis r9,16256
	ctx.r9.s64 = 1065353216;
	// beq 0x82ba43b8
	if (cr0.eq) goto loc_82BA43B8;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82BA3F60:
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ba3f60
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA3F60;
	// b 0x82ba43b8
	goto loc_82BA43B8;
loc_82BA3F70:
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba43b8
	if (cr6.eq) goto loc_82BA43B8;
	// rlwinm r10,r31,31,1,31
	ctx.r10.u64 = rotl64(r31.u32 | (r31.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// addi r6,r10,-16
	ctx.r6.s64 = ctx.r10.s64 + -16;
	// subfic r7,r10,16
	xer.ca = ctx.r10.u32 <= 16;
	ctx.r7.s64 = 16 - ctx.r10.s64;
loc_82BA3F8C:
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// blt cr6,0x82ba3fb0
	if (cr6.lt) goto loc_82BA3FB0;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ba3fb0
	if (!cr6.lt) goto loc_82BA3FB0;
	// add r8,r7,r11
	ctx.r8.u64 = ctx.r7.u64 + r11.u64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// b 0x82ba3fd8
	goto loc_82BA3FD8;
loc_82BA3FB0:
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bge cr6,0x82ba3fe8
	if (!cr6.lt) goto loc_82BA3FE8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ba3fe8
	if (cr6.lt) goto loc_82BA3FE8;
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// std r8,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r8.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_82BA3FD8:
	// fcfid f0,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// b 0x82ba3fec
	goto loc_82BA3FEC;
loc_82BA3FE8:
	// stfs f21,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
loc_82BA3FEC:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// blt cr6,0x82ba3f8c
	if (cr6.lt) goto loc_82BA3F8C;
	// b 0x82ba43b8
	goto loc_82BA43B8;
loc_82BA4000:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba43b8
	if (cr6.eq) goto loc_82BA43B8;
	// clrldi r10,r31,32
	ctx.r10.u64 = r31.u64 & 0xFFFFFFFF;
	// lfs f0,-10108(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10108);
	f0.f64 = double(temp.f32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// fdivs f28,f22,f31
	f28.f64 = double(float(f22.f64 / f31.f64));
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// mr r30,r28
	r30.u64 = r28.u64;
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// lfs f29,-28512(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -28512);
	f29.f64 = double(temp.f32);
	// fmuls f27,f13,f0
	f27.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_82BA4038:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f27
	f0.f64 = static_cast<float>(f0.f64 - f27.f64);
	// fmuls f31,f0,f20
	f31.f64 = double(float(f0.f64 * f20.f64));
	// fcmpu cr6,f31,f21
	cr6.compare(f31.f64, f21.f64);
	// bne cr6,0x82ba4064
	if (!cr6.eq) goto loc_82BA4064;
	// fmr f30,f22
	f30.f64 = f22.f64;
	// b 0x82ba4074
	goto loc_82BA4074;
loc_82BA4064:
	// fmuls f30,f31,f29
	ctx.fpscr.disableFlushMode();
	f30.f64 = double(float(f31.f64 * f29.f64));
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// bl 0x82ba33b8
	sub_82BA33B8(ctx, base);
	// fdivs f30,f1,f30
	ctx.fpscr.disableFlushMode();
	f30.f64 = double(float(ctx.f1.f64 / f30.f64));
loc_82BA4074:
	// fmuls f0,f28,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f28.f64 * f31.f64));
	// fcmpu cr6,f0,f21
	cr6.compare(f0.f64, f21.f64);
	// bne cr6,0x82ba4088
	if (!cr6.eq) goto loc_82BA4088;
	// fmr f0,f22
	f0.f64 = f22.f64;
	// b 0x82ba4098
	goto loc_82BA4098;
loc_82BA4088:
	// fmuls f31,f0,f29
	ctx.fpscr.disableFlushMode();
	f31.f64 = double(float(f0.f64 * f29.f64));
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82ba33b8
	sub_82BA33B8(ctx, base);
	// fdivs f0,f1,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 / f31.f64));
loc_82BA4098:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// fmuls f0,f0,f30
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * f30.f64));
	// stfs f0,0(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x82ba4038
	if (cr6.lt) goto loc_82BA4038;
	// b 0x82ba43b8
	goto loc_82BA43B8;
loc_82BA40B4:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba43b8
	if (cr6.eq) goto loc_82BA43B8;
	// clrldi r10,r31,32
	ctx.r10.u64 = r31.u64 & 0xFFFFFFFF;
	// lfs f0,-10108(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10108);
	f0.f64 = double(temp.f32);
	// mr r30,r28
	r30.u64 = r28.u64;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_82BA40E0:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f31
	f0.f64 = static_cast<float>(f0.f64 - f31.f64);
	// fmuls f1,f0,f20
	ctx.f1.f64 = double(float(f0.f64 * f20.f64));
	// bl 0x82ba3da8
	sub_82BA3DA8(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stfs f1,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x82ba40e0
	if (cr6.lt) goto loc_82BA40E0;
	// b 0x82ba43b8
	goto loc_82BA43B8;
loc_82BA4118:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba43b8
	if (cr6.eq) goto loc_82BA43B8;
	// clrldi r10,r31,32
	ctx.r10.u64 = r31.u64 & 0xFFFFFFFF;
	// lfs f0,-10108(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10108);
	f0.f64 = double(temp.f32);
	// lfs f29,-1488(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -1488);
	f29.f64 = double(temp.f32);
	// fdivs f31,f22,f31
	f31.f64 = double(float(f22.f64 / f31.f64));
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// mr r30,r28
	r30.u64 = r28.u64;
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f30,f13,f0
	f30.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_82BA414C:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// fmr f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f29.f64;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f30
	f0.f64 = static_cast<float>(f0.f64 - f30.f64);
	// fmuls f0,f0,f31
	f0.f64 = double(float(f0.f64 * f31.f64));
	// fmuls f0,f0,f20
	f0.f64 = double(float(f0.f64 * f20.f64));
	// fmuls f0,f0,f0
	f0.f64 = double(float(f0.f64 * f0.f64));
	// fneg f2,f0
	ctx.f2.u64 = f0.u64 ^ 0x8000000000000000;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stfs f1,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x82ba414c
	if (cr6.lt) goto loc_82BA414C;
	// b 0x82ba43b8
	goto loc_82BA43B8;
loc_82BA4194:
	// clrldi r10,r31,32
	ctx.r10.u64 = r31.u64 & 0xFFFFFFFF;
	// lfs f0,-10108(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10108);
	f0.f64 = double(temp.f32);
	// li r30,0
	r30.s64 = 0;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f30,f13,f0
	f30.f64 = double(float(ctx.f13.f64 * f0.f64));
	// beq cr6,0x82ba43b8
	if (cr6.eq) goto loc_82BA43B8;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// vspltisw v0,1
	simd::store_i32(ctx.v0.u32, simd::set1_i32(int32_t(0x1)));
	// stfs f21,84(r1)
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// mr r29,r28
	r29.u64 = r28.u64;
	// stfs f21,88(r1)
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fdivs f28,f22,f30
	f28.f64 = double(float(f22.f64 / f30.f64));
	// stfs f21,80(r1)
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vcsxwfp128 v127,v0,1
	ctx.fpscr.enableFlushModeUnconditional();
	simd::store_f32_aligned(v127.f32, simd::mul_f32(simd::cvtepi32_f32(simd::load_i32(ctx.v0.s32)), simd::bitcast_f32(simd::set1_i32(0x3F000000))));
	// lfs f27,-28512(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -28512);
	f27.f64 = double(temp.f32);
loc_82BA41E0:
	// clrldi r11,r30,32
	r11.u64 = r30.u64 & 0xFFFFFFFF;
	// vor128 v13,v127,v127
	simd::store_i8(ctx.v13.u8, simd::load_i8(v127.u8));
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// lvlx v12,0,r9
	temp.u32 = r0.u32 + ctx.r9.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v11,0,r8
	temp.u32 = r0.u32 + ctx.r8.u32;
	simd::store_shuffled(ctx.v11,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// lvlx v0,0,r10
	temp.u32 = r0.u32 + ctx.r10.u32;
	simd::store_shuffled(ctx.v0,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v11,v12,4,3
	simd::store_f32(ctx.v11.f32, simd::blend_f32<4>(simd::load_f32(ctx.v11.f32), simd::permute_f32<57>(simd::load_f32(ctx.v12.f32))));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f29,f0
	f29.f64 = double(float(f0.f64));
	// fsubs f0,f29,f30
	f0.f64 = static_cast<float>(f29.f64 - f30.f64);
	// fmuls f0,f0,f28
	f0.f64 = double(float(f0.f64 * f28.f64));
	// fnmsubs f0,f0,f0,f22
	f0.f64 = -double(std::fma(float(f0.f64), float(f0.f64), -float(f22.f64)));
	// stfs f0,96(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v12,0,r11
	temp.u32 = r0.u32 + r11.u32;
	simd::store_shuffled(ctx.v12,
		simde_mm_shuffle_epi8(
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(base + ((temp).u32 & ~0xF))),
			simde_mm_load_si128(reinterpret_cast<const simde__m128i*>(&VectorMaskL[((temp).u32 & 0xF) * 16]))
		));
	// vrlimi128 v12,v0,4,3
	simd::store_f32(ctx.v12.f32, simd::blend_f32<4>(simd::load_f32(ctx.v12.f32), simd::permute_f32<57>(simd::load_f32(ctx.v0.f32))));
	// vrlimi128 v12,v11,3,2
	simd::store_f32(ctx.v12.f32, simd::blend_f32<3>(simd::load_f32(ctx.v12.f32), simd::permute_f32<78>(simd::load_f32(ctx.v11.f32))));
	// vrsqrtefp v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
simd::store_shuffled(ctx.v0, simd::rsqrt_f32(simd::to_vec128f(ctx.v12)));
	// vmulfp128 v10,v12,v127
	simd::store_f32_aligned(ctx.v10.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(v127.f32)));
	// vmulfp128 v9,v0,v0
	simd::store_f32_aligned(ctx.v9.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vcmpeqfp v8,v0,v0
	simd::store_f32_aligned(ctx.v8.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vnmsubfp128 v13,v10,v9,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::xor_f32(simd::sub_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v10.f32), simd::load_f32_aligned(ctx.v9.f32)), simd::load_f32_aligned(ctx.v13.f32)), simd::bitcast_f32(simd::set1_i32(0x80000000))));
	// vmaddfp v0,v0,v13,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::add_f32(simd::mul_f32(simd::load_f32_aligned(ctx.v0.f32), simd::load_f32_aligned(ctx.v13.f32)), simd::load_f32_aligned(ctx.v0.f32)));
	// vcmpeqfp v13,v13,v13
	simd::store_f32_aligned(ctx.v13.f32, simd::cmpeq_f32(simd::load_f32_aligned(ctx.v13.f32), simd::load_f32_aligned(ctx.v13.f32)));
	// vmulfp128 v0,v12,v0
	simd::store_f32_aligned(ctx.v0.f32, simd::mul_f32(simd::load_f32_aligned(ctx.v12.f32), simd::load_f32_aligned(ctx.v0.f32)));
	// vxor v13,v13,v8
	simd::store_u8(ctx.v13.u8, simd::xor_i8(simd::load_u8(ctx.v13.u8), simd::load_u8(ctx.v8.u8)));
	// vsel v0,v0,v12,v13
	simd::store_i8(ctx.v0.u8, simd::select_i8(simd::load_i8(ctx.v0.u8), simd::load_i8(ctx.v12.u8), simd::load_i8(ctx.v13.u8)));
	// stvx128 v0,r0,r10
	ea = (ctx.r10.u32) & ~0xF;
	simd::store_shuffled(base + ea, simd::to_vec128i(ctx.v0), &VectorMaskL[(ea & 0xF) * 16]);
	// lfs f0,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	f0.f64 = double(temp.f32);
	// fmuls f1,f0,f31
	ctx.f1.f64 = double(float(f0.f64 * f31.f64));
	// bl 0x82ba3c08
	sub_82BA3C08(ctx, base);
	// fmr f26,f1
	ctx.fpscr.disableFlushMode();
	f26.f64 = ctx.f1.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82ba3c08
	sub_82BA3C08(ctx, base);
	// fsubs f0,f29,f30
	ctx.fpscr.disableFlushMode();
	f0.f64 = static_cast<float>(f29.f64 - f30.f64);
	// fdivs f29,f26,f1
	f29.f64 = double(float(f26.f64 / ctx.f1.f64));
	// fmuls f0,f0,f20
	f0.f64 = double(float(f0.f64 * f20.f64));
	// fcmpu cr6,f0,f21
	cr6.compare(f0.f64, f21.f64);
	// bne cr6,0x82ba4298
	if (!cr6.eq) goto loc_82BA4298;
	// fmr f0,f22
	f0.f64 = f22.f64;
	// b 0x82ba42a8
	goto loc_82BA42A8;
loc_82BA4298:
	// fmuls f26,f0,f27
	ctx.fpscr.disableFlushMode();
	f26.f64 = double(float(f0.f64 * f27.f64));
	// fmr f1,f26
	ctx.f1.f64 = f26.f64;
	// bl 0x82ba33b8
	sub_82BA33B8(ctx, base);
	// fdivs f0,f1,f26
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 / f26.f64));
loc_82BA42A8:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// fmuls f0,f0,f29
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 * f29.f64));
	// stfs f0,0(r29)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r29.u32 + 0, temp.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// blt cr6,0x82ba41e0
	if (cr6.lt) goto loc_82BA41E0;
	// b 0x82ba43b8
	goto loc_82BA43B8;
loc_82BA42C4:
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba43b8
	if (cr6.eq) goto loc_82BA43B8;
	// clrldi r10,r31,32
	ctx.r10.u64 = r31.u64 & 0xFFFFFFFF;
	// lfs f0,-10108(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10108);
	f0.f64 = double(temp.f32);
	// lfs f29,-1488(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -1488);
	f29.f64 = double(temp.f32);
	// mr r30,r28
	r30.u64 = r28.u64;
	// std r10,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r10.u64);
	// lfs f30,-168(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -168);
	f30.f64 = double(temp.f32);
	// lfs f27,-10128(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10128);
	f27.f64 = double(temp.f32);
	// lfs f28,-9596(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9596);
	f28.f64 = double(temp.f32);
	// lfs f26,-10120(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -10120);
	f26.f64 = double(temp.f32);
	// lfs f25,-820(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -820);
	f25.f64 = double(temp.f32);
	// lfs f24,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	f24.f64 = double(temp.f32);
	// lfd f13,104(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f23,f13,f0
	f23.f64 = double(float(ctx.f13.f64 * f0.f64));
loc_82BA430C:
	// clrldi r11,r29,32
	r11.u64 = r29.u64 & 0xFFFFFFFF;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fsubs f0,f0,f23
	f0.f64 = static_cast<float>(f0.f64 - f23.f64);
	// fmuls f31,f0,f20
	f31.f64 = double(float(f0.f64 * f20.f64));
	// fabs f0,f31
	f0.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f0,f22
	cr6.compare(f0.f64, f22.f64);
	// bge cr6,0x82ba4360
	if (!cr6.lt) goto loc_82BA4360;
	// fmr f2,f30
	ctx.f2.f64 = f30.f64;
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmr f19,f1
	ctx.fpscr.disableFlushMode();
	f19.f64 = ctx.f1.f64;
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmuls f0,f1,f28
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 * f28.f64));
	// fmsubs f0,f19,f27,f0
	f0.f64 = double(std::fma(float(f19.f64), float(f27.f64), -float(f0.f64)));
	// fadds f0,f0,f26
	f0.f64 = double(float(f0.f64 + f26.f64));
	// b 0x82ba43a4
	goto loc_82BA43A4;
loc_82BA4360:
	// fabs f0,f31
	ctx.fpscr.disableFlushMode();
	f0.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f0,f29
	cr6.compare(f0.f64, f29.f64);
	// bge cr6,0x82ba43a0
	if (!cr6.lt) goto loc_82BA43A0;
	// fmr f2,f29
	ctx.f2.f64 = f29.f64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmr f19,f1
	ctx.fpscr.disableFlushMode();
	f19.f64 = ctx.f1.f64;
	// fmr f2,f30
	ctx.f2.f64 = f30.f64;
	// fabs f1,f31
	ctx.f1.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// bl 0x82ba3598
	sub_82BA3598(ctx, base);
	// fmuls f0,f1,f30
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 * f30.f64));
	// fabs f13,f31
	ctx.f13.u64 = f31.u64 & 0x7FFFFFFFFFFFFFFF;
	// fmsubs f0,f19,f28,f0
	f0.f64 = double(std::fma(float(f19.f64), float(f28.f64), -float(f0.f64)));
	// fnmsubs f0,f13,f25,f0
	f0.f64 = -double(std::fma(float(ctx.f13.f64), float(f25.f64), -float(f0.f64)));
	// fadds f0,f0,f24
	f0.f64 = double(float(f0.f64 + f24.f64));
	// b 0x82ba43a4
	goto loc_82BA43A4;
loc_82BA43A0:
	// fmr f0,f21
	ctx.fpscr.disableFlushMode();
	f0.f64 = f21.f64;
loc_82BA43A4:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// stfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// blt cr6,0x82ba430c
	if (cr6.lt) goto loc_82BA430C;
loc_82BA43B8:
	// stfs f21,0(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f21.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// li r8,16
	ctx.r8.s64 = 16;
loc_82BA43C4:
	// fmr f0,f21
	ctx.fpscr.disableFlushMode();
	f0.f64 = f21.f64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ba4410
	if (cr6.eq) goto loc_82BA4410;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_82BA43D8:
	// lfs f13,0(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// fadds f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 + f0.f64));
	// addi r9,r9,64
	ctx.r9.s64 = ctx.r9.s64 + 64;
	// bne 0x82ba43d8
	if (!cr0.eq) goto loc_82BA43D8;
	// fdivs f0,f22,f0
	f0.f64 = double(float(f22.f64 / f0.f64));
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
loc_82BA43F8:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f13,0(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// bne 0x82ba43f8
	if (!cr0.eq) goto loc_82BA43F8;
loc_82BA4410:
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x82ba43c4
	if (!cr0.eq) goto loc_82BA43C4;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// li r0,-176
	r0.s64 = -176;
	// lvx128 v127,r1,r0
	simd::store_shuffled(v127, simd::load_and_shuffle(base + ((ctx.r1.u32 + r0.u32) & ~0xF), VectorMaskL));
	// addi r12,r1,-48
	r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82ca7530
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA3E90) {
	__imp__sub_82BA3E90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA4438) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-784(r1)
	ea = -784 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r8
	r30.u64 = ctx.r8.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x82ba4474
	if (!cr6.eq) goto loc_82BA4474;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
loc_82BA4474:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lfs f0,0(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r10,r11,-27468
	ctx.r10.s64 = r11.s64 + -27468;
	// lfs f9,18768(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 18768);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x82ba4490
	if (!cr6.lt) goto loc_82BA4490;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_82BA4490:
	// lfs f11,12(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x82ba44a0
	if (!cr6.gt) goto loc_82BA44A0;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA44A0:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,10664(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 10664);
	ctx.f13.f64 = double(temp.f32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// lfs f12,1784(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 1784);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f13,f0,f12,f13
	ctx.f13.f64 = -double(std::fma(float(f0.f64), float(ctx.f12.f64), -float(ctx.f13.f64)));
	// fmadds f10,f13,f0,f11
	ctx.f10.f64 = double(std::fma(float(ctx.f13.f64), float(f0.f64), float(ctx.f11.f64)));
	// beq cr6,0x82ba454c
	if (cr6.eq) goto loc_82BA454C;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x82ba454c
	if (cr6.eq) goto loc_82BA454C;
	// lfs f0,4(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x82ba44d4
	if (!cr6.lt) goto loc_82BA44D4;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_82BA44D4:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x82ba44e0
	if (!cr6.gt) goto loc_82BA44E0;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA44E0:
	// lfs f13,18760(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 18760);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// fmadds f8,f0,f12,f13
	ctx.f8.f64 = double(std::fma(float(f0.f64), float(ctx.f12.f64), float(ctx.f13.f64)));
	// lfs f13,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfd f12,1776(r11)
	ctx.f12.u64 = PPC_LOAD_U64(r11.u32 + 1776);
	// fcmpu cr6,f1,f12
	cr6.compare(ctx.f1.f64, ctx.f12.f64);
	// fnmsubs f13,f8,f0,f13
	ctx.f13.f64 = -double(std::fma(float(ctx.f8.f64), float(f0.f64), -float(ctx.f13.f64)));
	// ble cr6,0x82ba4510
	if (!cr6.gt) goto loc_82BA4510;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f0,1768(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 1768);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x82ba4514
	if (cr6.lt) goto loc_82BA4514;
loc_82BA4510:
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_82BA4514:
	// fcmpu cr6,f10,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f10.f64, ctx.f11.f64);
	// bge cr6,0x82ba4520
	if (!cr6.lt) goto loc_82BA4520;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_82BA4520:
	// fcmpu cr6,f13,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f13.f64, ctx.f11.f64);
	// bge cr6,0x82ba452c
	if (!cr6.lt) goto loc_82BA452C;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_82BA452C:
	// lfs f0,8236(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8236);
	f0.f64 = double(temp.f32);
	// fdivs f0,f0,f1
	f0.f64 = double(float(f0.f64 / ctx.f1.f64));
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// blt cr6,0x82ba4540
	if (cr6.lt) goto loc_82BA4540;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA4540:
	// fmuls f13,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f0.f64 * ctx.f13.f64));
	// fmuls f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// b 0x82ba4590
	goto loc_82BA4590;
loc_82BA454C:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f0,1760(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 1760);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x82ba456c
	if (!cr6.gt) goto loc_82BA456C;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f0,1752(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 1752);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// blt cr6,0x82ba4570
	if (cr6.lt) goto loc_82BA4570;
loc_82BA456C:
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_82BA4570:
	// fcmpu cr6,f10,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f10.f64, ctx.f11.f64);
	// bge cr6,0x82ba457c
	if (!cr6.lt) goto loc_82BA457C;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
loc_82BA457C:
	// fdivs f0,f11,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f11.f64 / ctx.f1.f64));
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// blt cr6,0x82ba458c
	if (cr6.lt) goto loc_82BA458C;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA458C:
	// fmuls f13,f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f0.f64 * ctx.f10.f64));
loc_82BA4590:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f10,-384(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -384);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lfs f0,3216(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3216);
	f0.f64 = double(temp.f32);
	// lis r11,31
	r11.s64 = 2031616;
	// fmuls f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 * f0.f64));
	// stfs f0,4(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 4, temp.u32);
	// lfs f0,1744(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1744);
	f0.f64 = double(temp.f32);
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// fmadds f0,f1,f0,f10
	f0.f64 = double(std::fma(float(ctx.f1.f64), float(f0.f64), float(ctx.f10.f64)));
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, f0.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// stw r9,648(r31)
	PPC_STORE_U32(r31.u32 + 648, ctx.r9.u32);
	// ble cr6,0x82ba45d4
	if (!cr6.gt) goto loc_82BA45D4;
	// stw r11,648(r31)
	PPC_STORE_U32(r31.u32 + 648, r11.u32);
loc_82BA45D4:
	// lfs f1,4(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// cmpwi cr6,r5,2
	cr6.compare<int32_t>(ctx.r5.s32, 2, xer);
	// bne cr6,0x82ba4614
	if (!cr6.eq) goto loc_82BA4614;
	// lfs f0,8(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x82ba45f4
	if (!cr6.lt) goto loc_82BA45F4;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_82BA45F4:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x82ba4600
	if (!cr6.gt) goto loc_82BA4600;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA4600:
	// lfs f13,9556(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 9556);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8228(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8228);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f0,f13,f12
	ctx.f12.f64 = double(std::fma(float(f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// lfs f13,9716(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 9716);
	ctx.f13.f64 = double(temp.f32);
	// b 0x82ba4680
	goto loc_82BA4680;
loc_82BA4614:
	// cmpwi cr6,r5,3
	cr6.compare<int32_t>(ctx.r5.s32, 3, xer);
	// bne cr6,0x82ba464c
	if (!cr6.eq) goto loc_82BA464C;
	// lfs f0,8(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x82ba462c
	if (!cr6.lt) goto loc_82BA462C;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_82BA462C:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x82ba4638
	if (!cr6.gt) goto loc_82BA4638;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA4638:
	// lfs f13,-380(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -380);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,9560(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 9560);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f0,f13,f12
	ctx.f13.f64 = double(std::fma(float(f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// fmadds f2,f13,f0,f10
	ctx.f2.f64 = double(std::fma(float(ctx.f13.f64), float(f0.f64), float(ctx.f10.f64)));
	// b 0x82ba468c
	goto loc_82BA468C;
loc_82BA464C:
	// cmpwi cr6,r5,5
	cr6.compare<int32_t>(ctx.r5.s32, 5, xer);
	// bne cr6,0x82ba4688
	if (!cr6.eq) goto loc_82BA4688;
	// lfs f0,8(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f9
	cr6.compare(f0.f64, ctx.f9.f64);
	// bge cr6,0x82ba4664
	if (!cr6.lt) goto loc_82BA4664;
	// fmr f0,f9
	f0.f64 = ctx.f9.f64;
loc_82BA4664:
	// fcmpu cr6,f0,f11
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f11.f64);
	// ble cr6,0x82ba4670
	if (!cr6.gt) goto loc_82BA4670;
	// fmr f0,f11
	f0.f64 = ctx.f11.f64;
loc_82BA4670:
	// lfs f13,17816(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 17816);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8812(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8812);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f0,f13,f12
	ctx.f12.f64 = double(std::fma(float(f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// lfs f13,8216(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8216);
	ctx.f13.f64 = double(temp.f32);
loc_82BA4680:
	// fmadds f2,f12,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = double(std::fma(float(ctx.f12.f64), float(f0.f64), float(ctx.f13.f64)));
	// b 0x82ba468c
	goto loc_82BA468C;
loc_82BA4688:
	// lfs f2,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f2.f64 = double(temp.f32);
loc_82BA468C:
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// li r3,7
	ctx.r3.s64 = 7;
	// bne cr6,0x82ba469c
	if (!cr6.eq) goto loc_82BA469C;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
loc_82BA469C:
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// bl 0x82ba3e90
	sub_82BA3E90(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82ba37b8
	sub_82BA37B8(ctx, base);
	// addi r1,r1,784
	ctx.r1.s64 = ctx.r1.s64 + 784;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA4438) {
	__imp__sub_82BA4438(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA46E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -168, f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// stw r10,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, ctx.r10.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r26,r6
	r26.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r19,r8
	r19.u64 = ctx.r8.u64;
	// mr r14,r9
	r14.u64 = ctx.r9.u64;
	// bl 0x832b313c
	__imp__VdQueryVideoMode(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r29,120(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(120) );
	// cntlzw r11,r29
	r11.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// rlwinm r11,r11,27,31,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	r11.u64 = r11.u64 ^ 1;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// bne cr6,0x82ba4744
	if (!cr6.eq) goto loc_82BA4744;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82ba4744
	if (!cr6.eq) goto loc_82BA4744;
	// mr r30,r19
	r30.u64 = r19.u64;
loc_82BA4744:
	// lwz r20,420(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(420) );
	// rlwinm r24,r26,16,16,31
	r24.u64 = rotl64(r26.u32 | (r26.u64 << 32), 16) & 0xFFFF;
	// lwz r17,21552(r28)
	r17.u64 = PPC_LOAD_U32(r28.u32 + int32_t(21552) );
	// rlwinm r18,r31,16,16,31
	r18.u64 = rotl64(r31.u32 | (r31.u64 << 32), 16) & 0xFFFF;
	// lwz r15,21548(r28)
	r15.u64 = PPC_LOAD_U32(r28.u32 + int32_t(21548) );
	// clrlwi r16,r31,16
	r16.u64 = r31.u32 & 0xFFFF;
	// rlwinm r27,r30,16,16,31
	r27.u64 = rotl64(r30.u32 | (r30.u64 << 32), 16) & 0xFFFF;
	// clrlwi r23,r30,16
	r23.u64 = r30.u32 & 0xFFFF;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// clrlwi r26,r26,16
	r26.u64 = r26.u32 & 0xFFFF;
	// rlwinm r22,r25,16,16,31
	r22.u64 = rotl64(r25.u32 | (r25.u64 << 32), 16) & 0xFFFF;
	// clrlwi r21,r25,16
	r21.u64 = r25.u32 & 0xFFFF;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82ba47a0
	if (!cr6.eq) goto loc_82BA47A0;
	// cmplw cr6,r27,r22
	cr6.compare<uint32_t>(r27.u32, r22.u32, xer);
	// bne cr6,0x82ba478c
	if (!cr6.eq) goto loc_82BA478C;
	// li r11,7
	r11.s64 = 7;
	// b 0x82ba479c
	goto loc_82BA479C;
loc_82BA478C:
	// subfc r11,r22,r27
	xer.ca = r27.u32 >= r22.u32;
	r11.s64 = r27.s64 - r22.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
loc_82BA479C:
	// stw r11,0(r20)
	PPC_STORE_U32(r20.u32 + 0, r11.u32);
loc_82BA47A0:
	// lwz r11,0(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + int32_t(0) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82ba47e4
	if (!cr6.eq) goto loc_82BA47E4;
	// divwu r11,r21,r10
	r11.u32 = r21.u32 / ctx.r10.u32;
	// twllei r10,0
	// cmplw cr6,r23,r11
	cr6.compare<uint32_t>(r23.u32, r11.u32, xer);
	// bne cr6,0x82ba47d0
	if (!cr6.eq) goto loc_82BA47D0;
	// subfic r11,r29,0
	xer.ca = r29.u32 <= 0;
	r11.s64 = 0 - r29.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,7
	r11.s64 = r11.s64 + 7;
	// b 0x82ba47e0
	goto loc_82BA47E0;
loc_82BA47D0:
	// subfc r11,r11,r23
	xer.ca = r23.u32 >= r11.u32;
	r11.s64 = r23.s64 - r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// addi r11,r11,5
	r11.s64 = r11.s64 + 5;
loc_82BA47E0:
	// stw r11,0(r14)
	PPC_STORE_U32(r14.u32 + 0, r11.u32);
loc_82BA47E4:
	// clrldi r11,r22,32
	r11.u64 = r22.u64 & 0xFFFFFFFF;
	// clrldi r9,r27,32
	ctx.r9.u64 = r27.u64 & 0xFFFFFFFF;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f0
	ctx.f12.f64 = double(f0.s64);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// addi r30,r11,-27852
	r30.s64 = r11.s64 + -27852;
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f0,8600(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8600);
	f0.f64 = double(temp.f32);
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfd f13,1920(r11)
	ctx.f13.u64 = PPC_LOAD_U64(r11.u32 + 1920);
	// fdivs f30,f11,f12
	f30.f64 = double(float(ctx.f11.f64 / ctx.f12.f64));
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// blt cr6,0x82ba4838
	if (cr6.lt) goto loc_82BA4838;
	// fcmpu cr6,f30,f13
	cr6.compare(f30.f64, ctx.f13.f64);
	// bgt cr6,0x82ba4838
	if (cr6.gt) goto loc_82BA4838;
	// fmr f30,f0
	f30.f64 = f0.f64;
loc_82BA4838:
	// clrldi r11,r10,32
	r11.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// clrldi r10,r21,32
	ctx.r10.u64 = r21.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// clrldi r11,r23,32
	r11.u64 = r23.u64 & 0xFFFFFFFF;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f10,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// fdivs f31,f12,f10
	f31.f64 = double(float(ctx.f12.f64 / ctx.f10.f64));
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// blt cr6,0x82ba4894
	if (cr6.lt) goto loc_82BA4894;
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// bgt cr6,0x82ba4894
	if (cr6.gt) goto loc_82BA4894;
	// fmr f31,f0
	f31.f64 = f0.f64;
	// b 0x82ba48d0
	goto loc_82BA48D0;
loc_82BA4894:
	// lfs f13,9940(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 9940);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// blt cr6,0x82ba48b0
	if (cr6.lt) goto loc_82BA48B0;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f0,1912(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 1912);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// ble cr6,0x82ba48cc
	if (!cr6.gt) goto loc_82BA48CC;
loc_82BA48B0:
	// lfs f13,8620(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8620);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f31,f13
	cr6.compare(f31.f64, ctx.f13.f64);
	// blt cr6,0x82ba48d0
	if (cr6.lt) goto loc_82BA48D0;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f0,1768(r11)
	f0.u64 = PPC_LOAD_U64(r11.u32 + 1768);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// bgt cr6,0x82ba48d0
	if (cr6.gt) goto loc_82BA48D0;
loc_82BA48CC:
	// fmr f31,f13
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f13.f64;
loc_82BA48D0:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r9,140(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(140) );
	// twllei r9,0
	// lwz r11,2572(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2572) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// divwu r11,r10,r9
	r11.u32 = ctx.r10.u32 / ctx.r9.u32;
	// rlwinm r25,r11,1,0,30
	r25.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r25,10
	cr6.compare<uint32_t>(r25.u32, 10, xer);
	// ble cr6,0x82ba48f8
	if (!cr6.gt) goto loc_82BA48F8;
	// li r25,10
	r25.s64 = 10;
loc_82BA48F8:
	// li r11,7680
	r11.s64 = 7680;
	// twllei r27,0
	// divwu r11,r11,r27
	r11.u32 = r11.u32 / r27.u32;
	// addi r28,r11,-1
	r28.s64 = r11.s64 + -1;
	// li r11,6
	r11.s64 = 6;
	// cmplwi cr6,r28,6
	cr6.compare<uint32_t>(r28.u32, 6, xer);
	// bgt cr6,0x82ba4918
	if (cr6.gt) goto loc_82BA4918;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_82BA4918:
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f13,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f0,96(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// fmuls f13,f0,f30
	ctx.f13.f64 = double(float(f0.f64 * f30.f64));
	// frsp f0,f12
	f0.f64 = double(float(ctx.f12.f64));
	// fdivs f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 / ctx.f13.f64));
	// fctidz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f12,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f12.u64);
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ba496c
	if (!cr6.lt) goto loc_82BA496C;
	// cmplwi cr6,r28,6
	cr6.compare<uint32_t>(r28.u32, 6, xer);
	// ble cr6,0x82ba497c
	if (!cr6.gt) goto loc_82BA497C;
	// li r28,6
	r28.s64 = 6;
	// b 0x82ba497c
	goto loc_82BA497C;
loc_82BA496C:
	// fdivs f0,f0,f13
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f0.f64 / ctx.f13.f64));
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f0,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, f0.u64);
	// lwz r28,100(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
loc_82BA497C:
	// lwz r31,436(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(436) );
	// li r5,1408
	ctx.r5.s64 = 1408;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r5,0(r14)
	ctx.r5.u64 = PPC_LOAD_U32(r14.u32 + int32_t(0) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r6,412(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(412) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r28,r31,652
	r28.s64 = r31.s64 + 652;
	// bl 0x82ba4438
	sub_82BA4438(ctx, base);
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r8,r25
	ctx.r8.u64 = r25.u64;
	// lwz r5,0(r20)
	ctx.r5.u64 = PPC_LOAD_U32(r20.u32 + int32_t(0) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r6,428(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(428) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f30.f64;
	// bl 0x82ba4438
	sub_82BA4438(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// lfs f0,396(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 396);
	f0.f64 = double(temp.f32);
	// stw r11,1348(r31)
	PPC_STORE_U32(r31.u32 + 1348, r11.u32);
	// addi r9,r27,3
	ctx.r9.s64 = r27.s64 + 3;
	// stw r10,1360(r31)
	PPC_STORE_U32(r31.u32 + 1360, ctx.r10.u32);
	// clrlwi r8,r19,16
	ctx.r8.u64 = r19.u32 & 0xFFFF;
	// rlwinm r10,r9,30,2,31
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r5,r27,16,4,15
	ctx.r5.u64 = rotl64(r27.u32 | (r27.u64 << 32), 16) & 0xFFF0000;
	// clrlwi r4,r23,20
	ctx.r4.u64 = r23.u32 & 0xFFF;
	// clrlwi r28,r16,20
	r28.u64 = r16.u32 & 0xFFF;
	// rlwinm r3,r18,16,4,15
	ctx.r3.u64 = rotl64(r18.u32 | (r18.u64 << 32), 16) & 0xFFF0000;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// or r5,r5,r4
	ctx.r5.u64 = ctx.r5.u64 | ctx.r4.u64;
	// or r3,r3,r28
	ctx.r3.u64 = ctx.r3.u64 | r28.u64;
	// rlwinm r28,r8,0,0,30
	r28.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// clrlwi r4,r10,22
	ctx.r4.u64 = ctx.r10.u32 & 0x3FF;
	// rlwinm r9,r19,16,16,31
	ctx.r9.u64 = rotl64(r19.u32 | (r19.u64 << 32), 16) & 0xFFFF;
	// subf r7,r21,r15
	ctx.r7.s64 = r15.s64 - r21.s64;
	// subf r6,r22,r17
	ctx.r6.s64 = r17.s64 - r22.s64;
	// subf r7,r26,r7
	ctx.r7.s64 = ctx.r7.s64 - r26.s64;
	// addi r9,r9,31
	ctx.r9.s64 = ctx.r9.s64 + 31;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// subf r6,r24,r6
	ctx.r6.s64 = ctx.r6.s64 - r24.s64;
	// rlwinm r27,r9,0,0,26
	r27.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFE0;
	// lwz r9,648(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(648) );
	// addi r25,r10,-1
	r25.s64 = ctx.r10.s64 + -1;
	// lwz r8,652(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(652) );
	// clrlwi r7,r7,20
	ctx.r7.u64 = ctx.r7.u32 & 0xFFF;
	// stw r5,1356(r31)
	PPC_STORE_U32(r31.u32 + 1356, ctx.r5.u32);
	// clrlwi r26,r26,20
	r26.u64 = r26.u32 & 0xFFF;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stw r4,1404(r31)
	PPC_STORE_U32(r31.u32 + 1404, ctx.r4.u32);
	// clrlwi r5,r24,20
	ctx.r5.u64 = r24.u32 & 0xFFF;
	// stw r3,1352(r31)
	PPC_STORE_U32(r31.u32 + 1352, ctx.r3.u32);
	// rlwimi r25,r8,8,20,23
	r25.u64 = (rotl32(ctx.r8.u32, 8) & 0xF00) | (r25.u64 & 0xFFFFFFFFFFFFF0FF);
	// stw r11,1332(r31)
	PPC_STORE_U32(r31.u32 + 1332, r11.u32);
	// clrlwi r6,r6,20
	ctx.r6.u64 = ctx.r6.u32 & 0xFFF;
	// stw r11,1336(r31)
	PPC_STORE_U32(r31.u32 + 1336, r11.u32);
	// andi. r8,r25,3847
	ctx.r8.u64 = r25.u64 & 3847;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// stw r27,1340(r31)
	PPC_STORE_U32(r31.u32 + 1340, r27.u32);
	// stw r8,1364(r31)
	PPC_STORE_U32(r31.u32 + 1364, ctx.r8.u32);
	// cntlzw r4,r29
	ctx.r4.u64 = r29.u32 == 0 ? 32 : __builtin_clz(r29.u32);
	// stw r28,1344(r31)
	PPC_STORE_U32(r31.u32 + 1344, r28.u32);
	// lis r3,256
	ctx.r3.s64 = 16777216;
	// stw r27,1320(r31)
	PPC_STORE_U32(r31.u32 + 1320, r27.u32);
	// rlwinm r8,r4,27,31,31
	ctx.r8.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x1;
	// stw r26,1304(r31)
	PPC_STORE_U32(r31.u32 + 1304, r26.u32);
	// stw r7,1308(r31)
	PPC_STORE_U32(r31.u32 + 1308, ctx.r7.u32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stw r5,1312(r31)
	PPC_STORE_U32(r31.u32 + 1312, ctx.r5.u32);
	// stw r6,1316(r31)
	PPC_STORE_U32(r31.u32 + 1316, ctx.r6.u32);
	// slw r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// clrlwi r8,r8,6
	ctx.r8.u64 = ctx.r8.u32 & 0x3FFFFFF;
	// lwz r7,1300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(1300) );
	// stw r8,1392(r31)
	PPC_STORE_U32(r31.u32 + 1392, ctx.r8.u32);
	// rlwinm r8,r7,5,6,26
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0x3FFFFE0;
	// stw r8,1376(r31)
	PPC_STORE_U32(r31.u32 + 1376, ctx.r8.u32);
	// stw r11,1372(r31)
	PPC_STORE_U32(r31.u32 + 1372, r11.u32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// stw r3,1388(r31)
	PPC_STORE_U32(r31.u32 + 1388, ctx.r3.u32);
	// lfs f13,256(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 256);
	ctx.f13.f64 = double(temp.f32);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// lwz r11,1300(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(1300) );
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lwz r11,652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(652) );
	// lfd f12,96(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lfd f11,96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f12
	ctx.f9.f64 = double(ctx.f12.s64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// lfs f11,1904(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 1904);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmadds f10,f9,f11,f10
	ctx.f10.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f11.f64), float(ctx.f10.f64)));
	// fadds f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 + f0.f64));
	// fmadds f10,f10,f13,f12
	ctx.f10.f64 = double(std::fma(float(ctx.f10.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// fctidz f10,f10
	ctx.f10.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f10.u64);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// rlwinm r11,r11,9,12,22
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 9) & 0xFFE00;
	// stw r11,1380(r31)
	PPC_STORE_U32(r31.u32 + 1380, r11.u32);
	// beq cr6,0x82ba4ba4
	if (cr6.eq) goto loc_82BA4BA4;
	// clrldi r11,r10,32
	r11.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// clrldi r10,r9,32
	ctx.r10.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f8,88(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f8,f8
	ctx.f8.f64 = double(ctx.f8.s64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfd f9,96(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// frsp f8,f8
	ctx.f8.f64 = double(float(ctx.f8.f64));
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfs f11,1900(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 1900);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,2688(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2688);
	ctx.f10.f64 = double(temp.f32);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// fmadds f11,f8,f11,f9
	ctx.f11.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f11.f64), float(ctx.f9.f64)));
	// fmadds f10,f8,f10,f9
	ctx.f10.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f10.f64), float(ctx.f9.f64)));
	// fadds f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 + f0.f64));
	// fadds f0,f10,f0
	f0.f64 = double(float(ctx.f10.f64 + f0.f64));
	// fmadds f11,f11,f13,f12
	ctx.f11.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// fmadds f0,f0,f13,f12
	f0.f64 = double(std::fma(float(f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// fctidz f13,f11
	ctx.f13.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f11.f64);
	// stfd f13,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f13.u64);
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f0,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, f0.u64);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// rlwinm r11,r11,9,13,22
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 9) & 0x7FE00;
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// stw r11,1396(r31)
	PPC_STORE_U32(r31.u32 + 1396, r11.u32);
	// rlwinm r11,r10,9,13,22
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 9) & 0x7FE00;
	// stw r11,1400(r31)
	PPC_STORE_U32(r31.u32 + 1400, r11.u32);
	// b 0x82ba4bec
	goto loc_82BA4BEC;
loc_82BA4BA4:
	// clrldi r11,r9,32
	r11.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f9,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fmadds f11,f9,f11,f10
	ctx.f11.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f11.f64), float(ctx.f10.f64)));
	// fadds f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 + f0.f64));
	// fmadds f0,f0,f13,f12
	f0.f64 = double(std::fma(float(f0.f64), float(ctx.f13.f64), float(ctx.f12.f64)));
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f0,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, f0.u64);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// rlwinm r11,r11,9,13,22
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 9) & 0x7FE00;
	// stw r11,1396(r31)
	PPC_STORE_U32(r31.u32 + 1396, r11.u32);
loc_82BA4BEC:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// lfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BA46E0) {
	__imp__sub_82BA46E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA4C00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r7,12(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// stw r9,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r9.u32);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// stw r7,12(r10)
	PPC_STORE_U32(ctx.r10.u32 + 12, ctx.r7.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// bne cr6,0x82ba4c74
	if (!cr6.eq) goto loc_82BA4C74;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82ba4c74
	if (!cr6.eq) goto loc_82BA4C74;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x82ba4c74
	if (!cr6.eq) goto loc_82BA4C74;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82ba4c74
	if (!cr6.eq) goto loc_82BA4C74;
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
loc_82BA4C74:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r8,88(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// lwz r7,92(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r29,r9,r7
	r29.s64 = ctx.r7.s64 - ctx.r9.s64;
	// mr r28,r11
	r28.u64 = r11.u64;
	// bne cr6,0x82ba4c98
	if (!cr6.eq) goto loc_82BA4C98;
	// lwz r28,21544(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + int32_t(21544) );
loc_82BA4C98:
	// lwz r30,20(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82ba4ce0
	if (!cr6.eq) goto loc_82BA4CE0;
	// lwz r11,13588(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(13588) );
	// mullw r9,r28,r29
	ctx.r9.s64 = int64_t(r28.s32) * int64_t(r29.s32);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// divwu r30,r9,r10
	r30.u32 = ctx.r9.u32 / ctx.r10.u32;
	// twllei r10,0
	// bne 0x82ba4cdc
	if (!cr0.eq) goto loc_82BA4CDC;
	// bl 0x832b30fc
	__imp__VdQueryVideoFlags(ctx, base);
	// clrlwi. r11,r3,31
	r11.u64 = ctx.r3.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba4cdc
	if (!cr0.eq) goto loc_82BA4CDC;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// bgt cr6,0x82ba4cdc
	if (cr6.gt) goto loc_82BA4CDC;
	// lwz r11,21548(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(21548) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x82ba4ce0
	if (!cr6.gt) goto loc_82BA4CE0;
loc_82BA4CDC:
	// lwz r30,21548(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + int32_t(21548) );
loc_82BA4CE0:
	// li r5,56
	ctx.r5.s64 = 56;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stw r28,16(r27)
	PPC_STORE_U32(r27.u32 + 16, r28.u32);
	// stw r30,20(r27)
	PPC_STORE_U32(r27.u32 + 20, r30.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
	// stw r9,4(r27)
	PPC_STORE_U32(r27.u32 + 4, ctx.r9.u32);
	// stw r8,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r8.u32);
	// stw r11,12(r27)
	PPC_STORE_U32(r27.u32 + 12, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BA4C00) {
	__imp__sub_82BA4C00(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA4D28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-2464(r1)
	ea = -2464 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// bl 0x82ba4c00
	sub_82BA4C00(ctx, base);
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// lwz r11,21552(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21552) );
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// lwz r8,21544(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21544) );
	// lwz r6,136(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// lwz r5,140(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(140) );
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(144) );
	// subf r9,r9,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r9.s64;
	// subf r5,r4,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r4.s64;
	// beq cr6,0x82ba4dac
	if (cr6.eq) goto loc_82BA4DAC;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x82ba4d90
	if (cr6.lt) goto loc_82BA4D90;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bgt cr6,0x82ba4d90
	if (cr6.gt) goto loc_82BA4D90;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BA4D90:
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// blt cr6,0x82ba4dac
	if (cr6.lt) goto loc_82BA4DAC;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bgt cr6,0x82ba4dac
	if (cr6.gt) goto loc_82BA4DAC;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// rlwinm r7,r8,31,1,31
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
loc_82BA4DAC:
	// lwz r8,152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(152) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// lwz r6,21548(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21548) );
	// rlwimi r30,r29,16,0,15
	r30.u64 = (rotl32(r29.u32, 16) & 0xFFFF0000) | (r30.u64 & 0xFFFFFFFF0000FFFF);
	// rlwinm r11,r11,31,1,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r3,168(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(168) );
	// rlwinm r28,r10,16,0,15
	r28.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// rlwimi r5,r9,16,0,15
	ctx.r5.u64 = (rotl32(ctx.r9.u32, 16) & 0xFFFF0000) | (ctx.r5.u64 & 0xFFFFFFFF0000FFFF);
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// addi r10,r1,172
	ctx.r10.s64 = ctx.r1.s64 + 172;
	// lwz r8,148(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(148) );
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// mr r27,r30
	r27.u64 = r30.u64;
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// subf r6,r8,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r8.s64;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// clrlwi r29,r8,16
	r29.u64 = ctx.r8.u32 & 0xFFFF;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// rlwinm r8,r6,31,1,31
	ctx.r8.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwimi r4,r7,16,0,15
	ctx.r4.u64 = (rotl32(ctx.r7.u32, 16) & 0xFFFF0000) | (ctx.r4.u64 & 0xFFFFFFFF0000FFFF);
	// rlwimi r8,r11,16,0,15
	ctx.r8.u64 = (rotl32(r11.u32, 16) & 0xFFFF0000) | (ctx.r8.u64 & 0xFFFFFFFF0000FFFF);
	// addi r11,r1,992
	r11.s64 = ctx.r1.s64 + 992;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// addi r10,r1,156
	ctx.r10.s64 = ctx.r1.s64 + 156;
	// addi r9,r1,116
	ctx.r9.s64 = ctx.r1.s64 + 116;
	// or r7,r29,r28
	ctx.r7.u64 = r29.u64 | r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba46e0
	sub_82BA46E0(ctx, base);
	// lis r5,-32768
	ctx.r5.s64 = -2147483648;
	// li r4,800
	ctx.r4.s64 = 800;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x832b296c
	__imp__RtlFillMemoryUlong(ctx, base);
	// li r4,220
	ctx.r4.s64 = 220;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// lwz r9,132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// li r7,200
	ctx.r7.s64 = 200;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// mr r25,r9
	r25.u64 = ctx.r9.u64;
	// lwz r8,140(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(140) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// subf r6,r11,r10
	ctx.r6.s64 = ctx.r10.s64 - r11.s64;
	// stw r7,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r7.u32);
	// subf r4,r9,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r9.s64;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(112) );
	// addi r7,r1,992
	ctx.r7.s64 = ctx.r1.s64 + 992;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(116) );
	// rlwimi r4,r6,16,0,15
	ctx.r4.u64 = (rotl32(ctx.r6.u32, 16) & 0xFFFF0000) | (ctx.r4.u64 & 0xFFFFFFFF0000FFFF);
	// addi r6,r1,172
	ctx.r6.s64 = ctx.r1.s64 + 172;
	// stw r7,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r7.u32);
	// rlwimi r25,r11,16,0,15
	r25.u64 = (rotl32(r11.u32, 16) & 0xFFFF0000) | (r25.u64 & 0xFFFFFFFF0000FFFF);
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r9,r1,156
	ctx.r9.s64 = ctx.r1.s64 + 156;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// or r6,r29,r28
	ctx.r6.u64 = r29.u64 | r28.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x832b314c
	__imp__VdInitializeScalerCommandBuffer(ctx, base);
	// rlwinm r29,r3,2,0,29
	r29.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// add r11,r29,r30
	r11.u64 = r29.u64 + r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// addi r1,r1,2464
	ctx.r1.s64 = ctx.r1.s64 + 2464;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BA4D28) {
	__imp__sub_82BA4D28(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA4ED8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r8,4(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r7,8(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r6,12(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(12) );
	// lwz r5,16(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(16) );
	// lwz r3,20(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(20) );
	// lwz r4,24(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(24) );
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r7,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r7.u32);
	// stw r6,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r6.u32);
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// stw r3,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r3.u32);
	// bne cr6,0x82ba4f38
	if (!cr6.eq) goto loc_82BA4F38;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// b 0x82ba4f44
	goto loc_82BA4F44;
loc_82BA4F38:
	// li r11,3
	r11.s64 = 3;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
loc_82BA4F44:
	// addi r9,r1,108
	ctx.r9.s64 = ctx.r1.s64 + 108;
	// lwz r11,14828(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(14828) );
	// addi r8,r1,124
	ctx.r8.s64 = ctx.r1.s64 + 124;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
	// stw r10,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r10.u32);
	// stw r10,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r10.u32);
	// beq cr6,0x82ba4f94
	if (cr6.eq) goto loc_82BA4F94;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// lwz r9,36(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// rlwinm r11,r10,2,30,30
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x2;
	// clrlwi r10,r9,19
	ctx.r10.u64 = ctx.r9.u32 & 0x1FFF;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r9,r9,19,19,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 19) & 0x1FFF;
	// add r30,r10,r11
	r30.u64 = ctx.r10.u64 + r11.u64;
	// add r29,r9,r11
	r29.u64 = ctx.r9.u64 + r11.u64;
	// b 0x82ba4f9c
	goto loc_82BA4F9C;
loc_82BA4F94:
	// lwz r30,13544(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(13544) );
	// lwz r29,13548(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(13548) );
loc_82BA4F9C:
	// addi r28,r31,13724
	r28.s64 = r31.s64 + 13724;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// li r5,56
	ctx.r5.s64 = 56;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// li r8,0
	ctx.r8.s64 = 0;
	// ori r11,r11,16
	r11.u64 = r11.u64 | 16;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// stb r11,10942(r31)
	PPC_STORE_U8(r31.u32 + 10942, r11.u8);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba4c00
	sub_82BA4C00(ctx, base);
	// lwz r11,21544(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21544) );
	// lwz r10,21548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21548) );
	// addi r3,r1,152
	ctx.r3.s64 = ctx.r1.s64 + 152;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// sth r30,144(r1)
	PPC_STORE_U16(ctx.r1.u32 + 144, r30.u16);
	// li r5,56
	ctx.r5.s64 = 56;
	// sth r29,146(r1)
	PPC_STORE_U16(ctx.r1.u32 + 146, r29.u16);
	// sth r11,148(r1)
	PPC_STORE_U16(ctx.r1.u32 + 148, r11.u16);
	// sth r10,150(r1)
	PPC_STORE_U16(ctx.r1.u32 + 150, ctx.r10.u16);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x832b315c
	__imp__VdCallGraphicsNotificationRoutines(ctx, base);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BA4ED8) {
	__imp__sub_82BA4ED8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA5018) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// rlwinm r31,r4,0,0,29
	r31.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xFFFFFFFC;
	// add r11,r4,r29
	r11.u64 = ctx.r4.u64 + r29.u64;
	// subf r10,r31,r4
	ctx.r10.s64 = ctx.r4.s64 - r31.s64;
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,0,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// rlwinm r28,r10,2,0,29
	r28.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r31,r11
	r11.s64 = r11.s64 - r31.s64;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// rlwinm r30,r11,2,0,29
	r30.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// subf r11,r9,r30
	r11.s64 = r30.s64 - ctx.r9.s64;
	// addi r4,r30,1
	ctx.r4.s64 = r30.s64 + 1;
	// subf r27,r28,r11
	r27.s64 = r11.s64 - r28.s64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82ba50e8
	if (cr0.eq) goto loc_82BA50E8;
	// addi r11,r31,4096
	r11.s64 = r31.s64 + 4096;
	// addi r10,r30,-1
	ctx.r10.s64 = r30.s64 + -1;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,16,0,15
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xFFFF0000;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// or r11,r11,r10
	r11.u64 = r11.u64 | ctx.r10.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// beq cr6,0x82ba50a8
	if (cr6.eq) goto loc_82BA50A8;
	// rlwinm r30,r28,2,0,29
	r30.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
loc_82BA50A8:
	// rlwinm r30,r29,4,0,27
	r30.u64 = rotl64(r29.u32 | (r29.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ba50e0
	if (cr6.eq) goto loc_82BA50E0;
	// rlwinm r30,r27,2,0,29
	r30.u64 = rotl64(r27.u32 | (r27.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// add r31,r30,r31
	r31.u64 = r30.u64 + r31.u64;
loc_82BA50E0:
	// stw r31,48(r25)
	PPC_STORE_U32(r25.u32 + 48, r31.u32);
	// li r3,1
	ctx.r3.s64 = 1;
loc_82BA50E8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BA5018) {
	__imp__sub_82BA5018(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA50F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lwz r11,56(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(56) );
	// lwz r3,48(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(48) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba5120
	if (!cr6.gt) goto loc_82BA5120;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA5120:
	// lwz r11,12716(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12716) );
	// li r9,1
	ctx.r9.s64 = 1;
	// mulli r10,r29,6
	ctx.r10.s64 = r29.s64 * 6;
	// oris r11,r11,49158
	r11.u64 = r11.u64 | 3221618688;
	// rlwimi r10,r9,16,0,20
	ctx.r10.u64 = (rotl32(ctx.r9.u32, 16) & 0xFFFFF800) | (ctx.r10.u64 & 0xFFFFFFFF000007FF);
	// ori r11,r11,11520
	r11.u64 = r11.u64 | 11520;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// rlwinm r11,r9,12,20,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r9,0,3,19
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1FFFF000;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// clrlwi r9,r9,20
	ctx.r9.u64 = ctx.r9.u32 & 0xFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// rlwinm r11,r9,12,20,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFF;
	// rlwinm r10,r9,0,3,19
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x1FFFF000;
	// addi r11,r11,512
	r11.s64 = r11.s64 + 512;
	// clrlwi r9,r9,20
	ctx.r9.u64 = ctx.r9.u32 & 0xFFF;
	// rlwinm r11,r11,0,19,19
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// or r11,r11,r9
	r11.u64 = r11.u64 | ctx.r9.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r30)
	PPC_STORE_U32(r30.u32 + 48, ctx.r3.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BA50F0) {
	__imp__sub_82BA50F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA51B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,56
	ctx.r5.s64 = 56;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,21752
	ctx.r3.s64 = r31.s64 + 21752;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r30,r31,21808
	r30.s64 = r31.s64 + 21808;
	// li r5,1536
	ctx.r5.s64 = 1536;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,10280
	r11.s64 = 673710080;
	// ori r29,r11,262
	r29.u64 = r11.u64 | 262;
	// lwz r11,21680(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21680) );
	// cmpw cr6,r11,r29
	cr6.compare<int32_t>(r11.s32, r29.s32, xer);
	// beq cr6,0x82ba5238
	if (cr6.eq) goto loc_82BA5238;
	// li r5,1536
	ctx.r5.s64 = 1536;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,23344(r31)
	PPC_STORE_U32(r31.u32 + 23344, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9e048
	sub_82B9E048(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9c9d8
	sub_82B9C9D8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA5238:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BA51B8) {
	__imp__sub_82BA51B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA5248) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,21680(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21680) );
	// bl 0x82b9c9d8
	sub_82B9C9D8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r6,21696(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21696) );
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r5,21688(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21688) );
	// lwz r4,21684(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21684) );
	// bl 0x82ba4d28
	sub_82BA4D28(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,21700(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21700) );
	// lwz r4,21680(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21680) );
	// bl 0x82b9ca80
	sub_82B9CA80(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA5248) {
	__imp__sub_82BA5248(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA52A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8221eb58
	sub_8221EB58(ctx, base);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lwz r10,21676(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21676) );
	// sradi r11,r11,10
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0x3FF) != 0);
	r11.s64 = r11.s64 >> 10;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA52A8) {
	__imp__sub_82BA52A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA52F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba5318
	if (!cr6.gt) goto loc_82BA5318;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA5318:
	// lis r11,2
	r11.s64 = 131072;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r11,r11,8448
	r11.u64 = r11.u64 | 8448;
	// ori r30,r10,65535
	r30.u64 = ctx.r10.u64 | 65535;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,8851
	ctx.r8.s64 = 8851;
	// li r7,0
	ctx.r7.s64 = 0;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lis r6,2
	ctx.r6.s64 = 131072;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r6,r6,8708
	ctx.r6.u64 = ctx.r6.u64 | 8708;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// li r4,768
	ctx.r4.s64 = 768;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// li r29,8978
	r29.s64 = 8978;
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r11,8205
	r11.s64 = 8205;
	// li r27,0
	r27.s64 = 0;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// stwu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r3.u32 = ea;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stwu r27,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, r27.u32);
	r11.u32 = ea;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// ble cr6,0x82ba53b8
	if (!cr6.gt) goto loc_82BA53B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA53B8:
	// li r11,8452
	r11.s64 = 8452;
	// li r10,0
	ctx.r10.s64 = 0;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// li r11,8706
	r11.s64 = 8706;
	// li r9,8705
	ctx.r9.s64 = 8705;
	// lis r8,1
	ctx.r8.s64 = 65536;
	// li r7,8962
	ctx.r7.s64 = 8962;
	// ori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 | 1;
	// stwu r30,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r30.u32);
	ctx.r3.u32 = ea;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,8704
	ctx.r5.s64 = 8704;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r30,8712
	r30.s64 = 8712;
	// li r29,4
	r29.s64 = 4;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// li r28,8707
	r28.s64 = 8707;
	// li r11,0
	r11.s64 = 0;
	// li r27,8578
	r27.s64 = 8578;
	// li r26,-1
	r26.s64 = -1;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stwu r7,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r7.u32);
	ctx.r3.u32 = ea;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// stwu r5,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r5.u32);
	ctx.r3.u32 = ea;
	// stwu r4,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r4.u32);
	ctx.r3.u32 = ea;
	// stwu r30,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r30.u32);
	ctx.r3.u32 = ea;
	// stwu r29,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r29.u32);
	ctx.r3.u32 = ea;
	// stwu r28,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r28.u32);
	ctx.r3.u32 = ea;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r27,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r27.u32);
	ctx.r3.u32 = ea;
	// stwu r26,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r26.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BA52F0) {
	__imp__sub_82BA52F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA5448) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stfd f31,-128(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -128, f31.u64);
	// stwu r1,-864(r1)
	ea = -864 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,148
	r11.s64 = ctx.r1.s64 + 148;
	// li r29,0
	r29.s64 = 0;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// stw r29,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r29.u32);
	// addi r10,r25,7
	ctx.r10.s64 = r25.s64 + 7;
	// addi r9,r24,7
	ctx.r9.s64 = r24.s64 + 7;
	// stw r29,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r29.u32);
	// stw r29,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r29.u32);
	// rlwinm r20,r10,0,0,28
	r20.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF8;
	// stw r29,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r29.u32);
	// rlwinm r19,r9,0,0,28
	r19.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF8;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r22,r4
	r22.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r23,r6
	r23.u64 = ctx.r6.u64;
	// addi r10,r1,688
	ctx.r10.s64 = ctx.r1.s64 + 688;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// li r11,6
	r11.s64 = 6;
	// mtctr r11
	ctr.u64 = r11.u64;
loc_82BA54A8:
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x82ba54a8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA54A8;
	// lis r5,10280
	ctx.r5.s64 = 673710080;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// addi r9,r1,136
	ctx.r9.s64 = ctx.r1.s64 + 136;
	// addi r8,r1,688
	ctx.r8.s64 = ctx.r1.s64 + 688;
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// li r6,0
	ctx.r6.s64 = 0;
	// ori r5,r5,262
	ctx.r5.u64 = ctx.r5.u64 | 262;
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x821f4ef8
	sub_821F4EF8(ctx, base);
	// lwz r21,12456(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12456) );
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x82ba54f0
	if (cr6.eq) goto loc_82BA54F0;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x821fc048
	sub_821FC048(ctx, base);
loc_82BA54F0:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82264550
	sub_82264550(ctx, base);
	// mr r30,r29
	r30.u64 = r29.u64;
	// addi r28,r1,608
	r28.s64 = ctx.r1.s64 + 608;
	// addi r26,r31,12440
	r26.s64 = r31.s64 + 12440;
loc_82BA5508:
	// lwz r8,0(r26)
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82ba551c
	if (cr6.eq) goto loc_82BA551C;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// bl 0x821fc048
	sub_821FC048(ctx, base);
loc_82BA551C:
	// addi r5,r1,688
	ctx.r5.s64 = ctx.r1.s64 + 688;
	// stw r8,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r8.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82286508
	sub_82286508(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// blt cr6,0x82ba5508
	if (cr6.lt) goto loc_82BA5508;
	// lwz r11,10688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10688) );
	// li r10,1
	ctx.r10.s64 = 1;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// rldicr r26,r10,35,63
	r26.u64 = rotl64(ctx.r10.u64, 35) & 0xFFFFFFFFFFFFFFFF;
	// stw r11,10688(r31)
	PPC_STORE_U32(r31.u32 + 10688, r11.u32);
	// clrlwi r18,r9,31
	r18.u64 = ctx.r9.u32 & 0x1;
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// or r11,r11,r26
	r11.u64 = r11.u64 | r26.u64;
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
	// ld r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// cmpldi cr6,r4,0
	cr6.compare<uint64_t>(ctx.r4.u64, 0, xer);
	// beq cr6,0x82ba568c
	if (cr6.eq) goto loc_82BA568C;
	// ld r11,40(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 40);
	// and r11,r11,r4
	r11.u64 = r11.u64 & ctx.r4.u64;
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba5598
	if (cr6.eq) goto loc_82BA5598;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,10560(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10560) );
	// bl 0x822155e0
	sub_822155E0(ctx, base);
	// std r3,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r3.u64);
loc_82BA5598:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// clrldi r10,r11,52
	ctx.r10.u64 = r11.u64 & 0xFFF;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba55c8
	if (cr6.eq) goto loc_82BA55C8;
	// addi r6,r31,10548
	ctx.r6.s64 = r31.s64 + 10548;
	// li r5,8704
	ctx.r5.s64 = 8704;
	// rldicr r4,r11,52,11
	ctx.r4.u64 = rotl64(r11.u64, 52) & 0xFFF0000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// rldicr r11,r11,0,51
	r11.u64 = rotl64(r11.u64, 0) & 0xFFFFFFFFFFFFF000;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA55C8:
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// rlwinm r10,r11,0,15,19
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1F000;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba5600
	if (cr6.eq) goto loc_82BA5600;
	// addi r6,r31,10528
	ctx.r6.s64 = r31.s64 + 10528;
	// li r5,8576
	ctx.r5.s64 = 8576;
	// rldicr r4,r11,47,4
	ctx.r4.u64 = rotl64(r11.u64, 47) & 0xF800000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-2
	r12.s64 = -131072;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,4095
	r12.u64 = r12.u64 | 4095;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA5600:
	// lis r12,0
	r12.s64 = 0;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// rldicr r12,r12,42,21
	r12.u64 = rotl64(r12.u64, 42) & 0xFFFFFC0000000000;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba5648
	if (cr6.eq) goto loc_82BA5648;
	// addi r6,r31,10368
	ctx.r6.s64 = r31.s64 + 10368;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// rldicr r4,r11,6,15
	ctx.r4.u64 = rotl64(r11.u64, 6) & 0xFFFF000000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-1
	r12.s64 = -65536;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,42,63
	r12.u64 = rotl64(r12.u64, 42) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA5648:
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// clrldi r12,r12,22
	r12.u64 = r12.u64 & 0x3FFFFFFFFFF;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba568c
	if (cr6.eq) goto loc_82BA568C;
	// addi r6,r31,10444
	ctx.r6.s64 = r31.s64 + 10444;
	// li r5,8448
	ctx.r5.s64 = 8448;
	// rldicr r4,r11,22,20
	ctx.r4.u64 = rotl64(r11.u64, 22) & 0xFFFFF80000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,16(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 16);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,21,63
	r12.u64 = rotl64(r12.u64, 21) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(r31.u32 + 16, r11.u64);
loc_82BA568C:
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba56dc
	if (cr6.eq) goto loc_82BA56DC;
	// lis r12,31
	r12.s64 = 2031616;
	// ori r12,r12,65535
	r12.u64 = r12.u64 | 65535;
	// rldicr r12,r12,34,29
	r12.u64 = rotl64(r12.u64, 34) & 0xFFFFFFFC00000000;
	// and r10,r11,r12
	ctx.r10.u64 = r11.u64 & r12.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba56dc
	if (cr6.eq) goto loc_82BA56DC;
	// addi r6,r31,10596
	ctx.r6.s64 = r31.s64 + 10596;
	// li r5,8832
	ctx.r5.s64 = 8832;
	// rldicr r4,r11,9,20
	ctx.r4.u64 = rotl64(r11.u64, 9) & 0xFFFFF80000000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// lis r12,-32
	r12.s64 = -2097152;
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// ori r12,r12,0
	r12.u64 = r12.u64 | 0;
	// rldicr r12,r12,34,63
	r12.u64 = rotl64(r12.u64, 34) & 0xFFFFFFFFFFFFFFFF;
	// and r11,r11,r12
	r11.u64 = r11.u64 & r12.u64;
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
loc_82BA56DC:
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba5714
	if (cr6.eq) goto loc_82BA5714;
	// clrldi r10,r11,26
	ctx.r10.u64 = r11.u64 & 0x3FFFFFFFFF;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba5714
	if (cr6.eq) goto loc_82BA5714;
	// addi r6,r31,10680
	ctx.r6.s64 = r31.s64 + 10680;
	// li r5,8960
	ctx.r5.s64 = 8960;
	// rldicr r4,r11,26,37
	ctx.r4.u64 = rotl64(r11.u64, 26) & 0xFFFFFFFFFC000000;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221c908
	sub_8221C908(ctx, base);
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// rldicr r11,r11,0,25
	r11.u64 = rotl64(r11.u64, 0) & 0xFFFFFFC000000000;
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
loc_82BA5714:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x82ba5730
	if (!cr6.gt) goto loc_82BA5730;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82BA5730:
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// li r9,768
	ctx.r9.s64 = 768;
	// ori r10,r10,15104
	ctx.r10.u64 = ctx.r10.u64 | 15104;
	// li r4,27
	ctx.r4.s64 = 27;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// addi r28,r11,13616
	r28.s64 = r11.s64 + 13616;
	// bne 0x82ba576c
	if (!cr0.eq) goto loc_82BA576C;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x82ba57a8
	goto loc_82BA57A8;
loc_82BA576C:
	// lis r11,-16359
	r11.s64 = -1072103424;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// ori r11,r11,11008
	r11.u64 = r11.u64 | 11008;
	// li r9,24
	ctx.r9.s64 = 24;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r4,r28,-96
	ctx.r4.s64 = r28.s64 + -96;
	// li r5,96
	ctx.r5.s64 = 96;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stwu r9,4(r30)
	ea = 4 + r30.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r30.u32 = ea;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r10,r30,96
	ctx.r10.s64 = r30.s64 + 96;
	// li r11,1
	r11.s64 = 1;
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
loc_82BA57A8:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba57c4
	if (cr0.eq) goto loc_82BA57C4;
	// addi r5,r28,-160
	ctx.r5.s64 = r28.s64 + -160;
	// li r6,4
	ctx.r6.s64 = 4;
	// li r4,252
	ctx.r4.s64 = 252;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
loc_82BA57C4:
	// li r4,432
	ctx.r4.s64 = 432;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x82ba57e0
	if (!cr0.eq) goto loc_82BA57E0;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x82ba581c
	goto loc_82BA581C;
loc_82BA57E0:
	// lis r11,-15954
	r11.s64 = -1045561344;
	// li r10,1
	ctx.r10.s64 = 1;
	// ori r11,r11,11008
	r11.u64 = r11.u64 | 11008;
	// li r9,429
	ctx.r9.s64 = 429;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r4,r28,192
	ctx.r4.s64 = r28.s64 + 192;
	// li r5,1716
	ctx.r5.s64 = 1716;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stwu r9,4(r30)
	ea = 4 + r30.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r30.u32 = ea;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r10,r30,1716
	ctx.r10.s64 = r30.s64 + 1716;
	// li r11,1
	r11.s64 = 1;
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
loc_82BA581C:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba5838
	if (cr0.eq) goto loc_82BA5838;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r6,12
	ctx.r6.s64 = 12;
	// li r4,500
	ctx.r4.s64 = 500;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
loc_82BA5838:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x82ba5854
	if (!cr6.gt) goto loc_82BA5854;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82BA5854:
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lis r9,4096
	ctx.r9.s64 = 268435456;
	// ori r10,r10,8576
	ctx.r10.u64 = ctx.r10.u64 | 8576;
	// ori r9,r9,2048
	ctx.r9.u64 = ctx.r9.u64 | 2048;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// li r10,4
	ctx.r10.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x82ba52f0
	sub_82BA52F0(ctx, base);
	// rlwinm r11,r22,31,1,31
	r11.u64 = rotl64(r22.u32 | (r22.u64 << 32), 31) & 0x7FFFFFFF;
	// clrldi r10,r24,32
	ctx.r10.u64 = r24.u64 & 0xFFFFFFFF;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// clrldi r11,r25,32
	r11.u64 = r25.u64 & 0xFFFFFFFF;
	// std r10,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r10.u64);
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// addi r11,r10,-18104
	r11.s64 = ctx.r10.s64 + -18104;
	// fcfid f6,f0
	ctx.f6.f64 = double(f0.s64);
	// addi r9,r1,292
	ctx.r9.s64 = ctx.r1.s64 + 292;
	// lfs f31,-9364(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9364);
	f31.f64 = double(temp.f32);
	// fcfid f5,f13
	ctx.f5.f64 = double(ctx.f13.s64);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// stfs f31,288(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// lfs f0,-9352(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9352);
	f0.f64 = double(temp.f32);
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r29.u32);
	// lfs f8,-9748(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9748);
	ctx.f8.f64 = double(temp.f32);
	// stw r29,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r29.u32);
	// lfs f13,-1148(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -1148);
	ctx.f13.f64 = double(temp.f32);
	// stw r29,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, r29.u32);
	// lfs f9,-9296(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -9296);
	ctx.f9.f64 = double(temp.f32);
	// frsp f6,f6
	ctx.f6.f64 = double(float(ctx.f6.f64));
	// lfs f11,2664(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2664);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f10,2660(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2660);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,352(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 352, temp.u32);
	// frsp f5,f5
	ctx.f5.f64 = double(float(ctx.f5.f64));
	// frsp f7,f12
	ctx.f7.f64 = double(float(ctx.f12.f64));
	// lfs f12,-18104(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -18104);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,356(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 356, temp.u32);
	// lfs f13,3128(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 3128);
	ctx.f13.f64 = double(temp.f32);
	// stfs f31,160(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// stfs f31,164(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// stfs f31,168(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// stfs f0,172(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// stfs f31,184(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f0,188(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f31,192(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// fmsubs f4,f7,f8,f0
	ctx.f4.f64 = double(std::fma(float(ctx.f7.f64), float(ctx.f8.f64), -float(f0.f64)));
	// lfs f8,3216(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3216);
	ctx.f8.f64 = double(temp.f32);
	// stfs f31,196(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// stfs f31,200(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f0,204(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f31,216(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// stfs f0,220(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f31,304(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// stfs f11,312(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 312, temp.u32);
	// stfs f10,316(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 316, temp.u32);
	// stfs f31,320(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 320, temp.u32);
	// stfs f31,324(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// stfs f9,328(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 328, temp.u32);
	// stfs f8,332(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 332, temp.u32);
	// stfs f31,336(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// stfs f31,340(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfs f12,360(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f6,308(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// stfs f12,364(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f31,368(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 368, temp.u32);
	// stfs f31,372(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 372, temp.u32);
	// stfs f31,384(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// stfs f31,388(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// stfs f7,176(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// fmuls f3,f7,f13
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lfs f13,2656(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2656);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// stfs f5,180(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// stfs f7,208(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// stfs f5,212(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// lfs f12,2652(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2652);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f11,2648(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2648);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lfs f10,2644(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2644);
	ctx.f10.f64 = double(temp.f32);
	// li r5,160
	ctx.r5.s64 = 160;
	// lfs f9,2640(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2640);
	ctx.f9.f64 = double(temp.f32);
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f8,2636(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 2636);
	ctx.f8.f64 = double(temp.f32);
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// lfs f7,2632(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2632);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,2628(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2628);
	ctx.f6.f64 = double(temp.f32);
	// fdivs f2,f0,f3
	ctx.f2.f64 = double(float(f0.f64 / ctx.f3.f64));
	// lfs f5,2624(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2624);
	ctx.f5.f64 = double(temp.f32);
	// stfs f4,344(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 344, temp.u32);
	// stfs f3,376(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 376, temp.u32);
	// stfs f2,380(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 380, temp.u32);
	// stfs f4,348(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 348, temp.u32);
	// stfs f31,392(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f31,396(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 396, temp.u32);
	// stfs f0,400(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// stfs f13,404(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// stfs f12,408(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f11,412(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// stfs f0,416(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 416, temp.u32);
	// stfs f10,420(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 420, temp.u32);
	// stfs f9,424(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 424, temp.u32);
	// stfs f8,428(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 428, temp.u32);
	// stfs f0,432(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// stfs f7,436(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// stfs f6,440(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f5,444(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// mr r30,r29
	r30.u64 = r29.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x82ba5b0c
	if (cr6.eq) goto loc_82BA5B0C;
	// addi r28,r1,452
	r28.s64 = ctx.r1.s64 + 452;
loc_82BA5A54:
	// clrldi r11,r30,32
	r11.u64 = r30.u64 & 0xFFFFFFFF;
	// stfs f31,-4(r28)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r28.u32 + -4, temp.u32);
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// std r11,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, r11.u64);
	// lfd f0,144(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// lis r9,2048
	ctx.r9.s64 = 134217728;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// addi r11,r1,136
	r11.s64 = ctx.r1.s64 + 136;
	// stw r7,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r7.u32);
	// li r10,2
	ctx.r10.s64 = 2;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// ori r9,r9,74
	ctx.r9.u64 = ctx.r9.u64 | 74;
	// stfs f0,0(r28)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r28.u32 + 0, temp.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,2048
	ctx.r4.s64 = 2048;
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x821fb520
	sub_821FB520(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// lwz r8,256(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(256) );
	// li r10,73
	ctx.r10.s64 = 73;
	// lwz r7,264(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(264) );
	// addi r5,r1,252
	ctx.r5.s64 = ctx.r1.s64 + 252;
	// lwz r9,252(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(252) );
	// rlwimi r8,r11,0,0,19
	ctx.r8.u64 = (rotl32(r11.u32, 0) & 0xFFFFF000) | (ctx.r8.u64 & 0xFFFFFFFF00000FFF);
	// rlwimi r9,r10,11,13,21
	ctx.r9.u64 = (rotl32(ctx.r10.u32, 11) & 0x7FC00) | (ctx.r9.u64 & 0xFFFFFFFFFFF803FF);
	// stw r8,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r8.u32);
	// rlwinm r11,r7,0,13,8
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFF87FFFF;
	// stw r9,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r9.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba50f0
	sub_82BA50F0(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmplw cr6,r30,r23
	cr6.compare<uint32_t>(r30.u32, r23.u32, xer);
	// blt cr6,0x82ba5a54
	if (cr6.lt) goto loc_82BA5A54;
loc_82BA5B0C:
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba5b24
	if (!cr6.gt) goto loc_82BA5B24;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA5B24:
	// lis r11,2
	r11.s64 = 131072;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// ori r11,r11,20480
	r11.u64 = r11.u64 | 20480;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// mr r11,r29
	r11.u64 = r29.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r10,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r8.u32 = ea;
	// stwu r9,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r8.u32 = ea;
	// stwu r11,4(r8)
	ea = 4 + ctx.r8.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r8.u32 = ea;
	// stw r8,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r8.u32);
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
	// li r6,20
	ctx.r6.s64 = 20;
	// addi r5,r1,288
	ctx.r5.s64 = ctx.r1.s64 + 288;
	// li r4,256
	ctx.r4.s64 = 256;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba5b90
	if (!cr6.gt) goto loc_82BA5B90;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA5B90:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// lis r10,3
	ctx.r10.s64 = 196608;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// ori r11,r11,13824
	r11.u64 = r11.u64 | 13824;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// ori r8,r10,136
	ctx.r8.u64 = ctx.r10.u64 | 136;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r11,r1,136
	r11.s64 = ctx.r1.s64 + 136;
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// lis r9,10280
	ctx.r9.s64 = 673710080;
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// addi r11,r1,624
	r11.s64 = ctx.r1.s64 + 624;
	// li r10,2
	ctx.r10.s64 = 2;
	// ori r9,r9,262
	ctx.r9.u64 = ctx.r9.u64 | 262;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// stwu r8,4(r30)
	ea = 4 + r30.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	r30.u32 = ea;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r5,21688(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21688) );
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r4,21684(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21684) );
	// li r3,3
	ctx.r3.s64 = 3;
	// stw r30,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r30.u32);
	// bl 0x821fb520
	sub_821FB520(ctx, base);
	// lwz r11,21692(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21692) );
	// lis r10,8192
	ctx.r10.s64 = 536870912;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82ba5c14
	if (!cr6.lt) goto loc_82BA5C14;
	// addis r11,r11,-16384
	r11.s64 = r11.s64 + -1073741824;
	// b 0x82ba5c18
	goto loc_82BA5C18;
loc_82BA5C14:
	// addis r11,r11,-16640
	r11.s64 = r11.s64 + -1090519040;
loc_82BA5C18:
	// lwz r8,656(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(656) );
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r19,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r19.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwimi r11,r8,0,20,31
	r11.u64 = (rotl32(ctx.r8.u32, 0) & 0xFFF) | (r11.u64 & 0xFFFFFFFFFFFFF000);
	// stw r29,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r29.u32);
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r11,656(r1)
	PPC_STORE_U32(ctx.r1.u32 + 656, r11.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// addi r6,r1,624
	ctx.r6.s64 = ctx.r1.s64 + 624;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// stw r20,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r20.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r29,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x822069c0
	sub_822069C0(ctx, base);
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82264550
	sub_82264550(ctx, base);
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x82ba5c80
	if (cr6.eq) goto loc_82BA5C80;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x821fc1f0
	sub_821FC1F0(ctx, base);
loc_82BA5C80:
	// addi r28,r1,608
	r28.s64 = ctx.r1.s64 + 608;
loc_82BA5C84:
	// lwz r30,0(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82286508
	sub_82286508(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ba5ca8
	if (cr6.eq) goto loc_82BA5CA8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821fc1f0
	sub_821FC1F0(ctx, base);
loc_82BA5CA8:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// cmplwi cr6,r29,4
	cr6.compare<uint32_t>(r29.u32, 4, xer);
	// blt cr6,0x82ba5c84
	if (cr6.lt) goto loc_82BA5C84;
	// lwz r11,10688(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10688) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rlwimi r18,r11,0,0,30
	r18.u64 = (rotl32(r11.u32, 0) & 0xFFFFFFFE) | (r18.u64 & 0xFFFFFFFF00000001);
	// stw r18,10688(r31)
	PPC_STORE_U32(r31.u32 + 10688, r18.u32);
	// ld r11,32(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 32);
	// or r11,r11,r26
	r11.u64 = r11.u64 | r26.u64;
	// std r11,32(r31)
	PPC_STORE_U64(r31.u32 + 32, r11.u64);
	// bl 0x82b9b970
	sub_82B9B970(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,864
	ctx.r1.s64 = ctx.r1.s64 + 864;
	// lfd f31,-128(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82BA5448) {
	__imp__sub_82BA5448(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA5CE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bcc
	// addi r12,r1,-96
	r12.s64 = ctx.r1.s64 + -96;
	// bl 0x82ca7500
	// stwu r1,-544(r1)
	ea = -544 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// addi r7,r1,336
	ctx.r7.s64 = ctx.r1.s64 + 336;
	// lis r9,2048
	ctx.r9.s64 = 134217728;
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// li r29,0
	r29.s64 = 0;
	// stw r7,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r7.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
	// addi r11,r1,136
	r11.s64 = ctx.r1.s64 + 136;
	// li r10,2
	ctx.r10.s64 = 2;
	// ori r9,r9,74
	ctx.r9.u64 = ctx.r9.u64 | 74;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,2048
	ctx.r4.s64 = 2048;
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x821fb520
	sub_821FB520(ctx, base);
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// lwz r10,368(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(368) );
	// li r6,16
	ctx.r6.s64 = 16;
	// lwz r9,384(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(384) );
	// li r5,112
	ctx.r5.s64 = 112;
	// rlwimi r10,r11,0,0,19
	ctx.r10.u64 = (rotl32(r11.u32, 0) & 0xFFFFF000) | (ctx.r10.u64 & 0xFFFFFFFF00000FFF);
	// clrlwi r11,r9,20
	r11.u64 = ctx.r9.u32 & 0xFFF;
	// stw r10,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, ctx.r10.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ce10
	sub_8219CE10(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,336
	ctx.r5.s64 = ctx.r1.s64 + 336;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821968b8
	sub_821968B8(ctx, base);
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x82ba5dcc
	if (!cr6.gt) goto loc_82BA5DCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82BA5DCC:
	// lis r10,-16384
	ctx.r10.s64 = -1073741824;
	// li r9,768
	ctx.r9.s64 = 768;
	// ori r10,r10,15104
	ctx.r10.u64 = ctx.r10.u64 | 15104;
	// li r4,312
	ctx.r4.s64 = 312;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// addi r30,r11,-27852
	r30.s64 = r11.s64 + -27852;
	// bne 0x82ba5e08
	if (!cr0.eq) goto loc_82BA5E08;
	// mr r11,r29
	r11.u64 = r29.u64;
	// b 0x82ba5e44
	goto loc_82BA5E44;
loc_82BA5E08:
	// lis r11,-16074
	r11.s64 = -1053425664;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// ori r11,r11,11008
	r11.u64 = r11.u64 | 11008;
	// li r9,309
	ctx.r9.s64 = 309;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r4,r30,-21908
	ctx.r4.s64 = r30.s64 + -21908;
	// li r5,1236
	ctx.r5.s64 = 1236;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// stwu r9,4(r28)
	ea = 4 + r28.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r28.u32 = ea;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r10,r28,1236
	ctx.r10.s64 = r28.s64 + 1236;
	// li r11,1
	r11.s64 = 1;
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
loc_82BA5E44:
	// clrlwi. r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba5e60
	if (cr0.eq) goto loc_82BA5E60;
	// addi r5,r30,-22100
	ctx.r5.s64 = r30.s64 + -22100;
	// li r6,12
	ctx.r6.s64 = 12;
	// li r4,244
	ctx.r4.s64 = 244;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
loc_82BA5E60:
	// li r4,12
	ctx.r4.s64 = 12;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82ba5eac
	if (cr0.eq) goto loc_82BA5EAC;
	// lis r11,-16374
	r11.s64 = -1073086464;
	// li r10,1
	ctx.r10.s64 = 1;
	// ori r11,r11,11008
	r11.u64 = r11.u64 | 11008;
	// li r9,9
	ctx.r9.s64 = 9;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// addi r4,r30,-24696
	ctx.r4.s64 = r30.s64 + -24696;
	// li r5,36
	ctx.r5.s64 = 36;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// stwu r9,4(r28)
	ea = 4 + r28.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r28.u32 = ea;
	// addi r3,r28,4
	ctx.r3.s64 = r28.s64 + 4;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r11,r28,36
	r11.s64 = r28.s64 + 36;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
loc_82BA5EAC:
	// lwz r11,48(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x82ba5ec8
	if (!cr6.gt) goto loc_82BA5EC8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
loc_82BA5EC8:
	// lis r10,1
	ctx.r10.s64 = 65536;
	// lis r9,5888
	ctx.r9.s64 = 385875968;
	// ori r10,r10,8576
	ctx.r10.u64 = ctx.r10.u64 | 8576;
	// ori r9,r9,16
	ctx.r9.u64 = ctx.r9.u64 | 16;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwu r9,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	r11.u32 = ea;
	// stwu r10,4(r11)
	ea = 4 + r11.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	r11.u32 = ea;
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x82ba52f0
	sub_82BA52F0(ctx, base);
	// addi r5,r24,28
	ctx.r5.s64 = r24.s64 + 28;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba50f0
	sub_82BA50F0(ctx, base);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba5f1c
	if (!cr6.gt) goto loc_82BA5F1C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA5F1C:
	// lis r10,2
	ctx.r10.s64 = 131072;
	// lwz r24,636(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(636) );
	// lfs f12,8600(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8600);
	ctx.f12.f64 = double(temp.f32);
	// lwz r11,628(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(628) );
	// ori r10,r10,20480
	ctx.r10.u64 = ctx.r10.u64 | 20480;
	// lfs f10,4(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// lfs f11,0(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// std r5,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r5.u64);
	// lfd f0,128(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f9,f0
	ctx.f9.f64 = double(f0.s64);
	// lfs f0,384(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 384);
	f0.f64 = double(temp.f32);
	// lfs f6,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f6.f64 = double(temp.f32);
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// fmuls f5,f6,f12
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// mr r8,r29
	ctx.r8.u64 = r29.u64;
	// lfs f13,396(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 396);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lfs f4,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lis r5,-32256
	ctx.r5.s64 = -2113929216;
	// frsp f31,f9
	f31.f64 = double(float(ctx.f9.f64));
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// fdivs f1,f10,f4
	ctx.f1.f64 = double(float(ctx.f10.f64 / ctx.f4.f64));
	// lfs f10,9748(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 9748);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,452(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 452);
	ctx.f9.f64 = double(temp.f32);
	// fdivs f3,f11,f6
	ctx.f3.f64 = double(float(ctx.f11.f64 / ctx.f6.f64));
	// lfs f8,3216(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3216);
	ctx.f8.f64 = double(temp.f32);
	// addi r11,r1,148
	r11.s64 = ctx.r1.s64 + 148;
	// stfs f0,144(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// stwu r6,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r6.u32);
	ctx.r3.u32 = ea;
	// lfs f7,2948(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2948);
	ctx.f7.f64 = double(temp.f32);
	// fdivs f2,f11,f5
	ctx.f2.f64 = double(float(ctx.f11.f64 / ctx.f5.f64));
	// lfs f11,2460(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2460);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,0(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	f30.f64 = double(temp.f32);
	// fmuls f28,f4,f12
	f28.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stw r29,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r29.u32);
	// lfs f29,4(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 4);
	f29.f64 = double(temp.f32);
	// stw r29,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r29.u32);
	// fdivs f26,f13,f31
	f26.f64 = double(float(ctx.f13.f64 / f31.f64));
	// stw r29,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r29.u32);
	// stfs f5,224(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// fsubs f3,f3,f13
	ctx.f3.f64 = static_cast<float>(ctx.f3.f64 - ctx.f13.f64);
	// fsubs f1,f1,f13
	ctx.f1.f64 = static_cast<float>(ctx.f1.f64 - ctx.f13.f64);
	// stfs f30,176(r1)
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f10,216(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// fdivs f27,f13,f2
	f27.f64 = double(float(ctx.f13.f64 / ctx.f2.f64));
	// stfs f10,220(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// lfs f10,1076(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 1076);
	ctx.f10.f64 = double(temp.f32);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// stfs f9,184(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lfs f9,256(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 256);
	ctx.f9.f64 = double(temp.f32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// stfs f29,180(r1)
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fmuls f5,f31,f10
	ctx.f5.f64 = double(float(f31.f64 * ctx.f10.f64));
	// stfs f0,164(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// stfs f0,168(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// stfs f0,172(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// stfs f8,188(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f6,192(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// stfs f4,196(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// stfs f12,208(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// stfs f12,212(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// stfs f3,200(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f1,204(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f28,228(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// stfs f2,232(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 232, temp.u32);
	// stfs f27,236(r1)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 236, temp.u32);
	// stfs f7,256(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// stfs f11,260(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
	// stfs f31,240(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 240, temp.u32);
	// fadds f2,f1,f13
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f13.f64));
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// fadds f13,f3,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 + ctx.f13.f64));
	// stfs f0,268(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 268, temp.u32);
	// fmuls f4,f26,f9
	ctx.f4.f64 = double(float(f26.f64 * ctx.f9.f64));
	// lfs f12,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,2456(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2456);
	ctx.f10.f64 = double(temp.f32);
	// li r6,8
	ctx.r6.s64 = 8;
	// lfs f9,2452(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2452);
	ctx.f9.f64 = double(temp.f32);
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// lfs f6,2440(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 2440);
	ctx.f6.f64 = double(temp.f32);
	// li r4,4
	ctx.r4.s64 = 4;
	// lfs f8,2448(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2448);
	ctx.f8.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfs f7,2444(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2444);
	ctx.f7.f64 = double(temp.f32);
	// mr r27,r29
	r27.u64 = r29.u64;
	// stfs f5,252(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f10,264(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// stfs f11,272(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// fctidz f0,f2
	f0.s64 = (ctx.f2.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f2.f64);
	// stfs f9,276(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// fctidz f13,f13
	ctx.f13.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f13.f64);
	// stfs f8,280(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// stfs f12,284(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 284, temp.u32);
	// stfs f7,288(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// stfs f26,244(r1)
	temp.f32 = float(f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
	// stfs f4,248(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 248, temp.u32);
	// stfs f6,292(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
	// stfs f11,296(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
	// stfs f12,300(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 300, temp.u32);
	// std r29,0(r11)
	PPC_STORE_U64(r11.u32 + 0, r29.u64);
	// stfd f0,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, f0.u64);
	// std r29,8(r11)
	PPC_STORE_U64(r11.u32 + 8, r29.u64);
	// stfd f13,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.f13.u64);
	// std r29,16(r11)
	PPC_STORE_U64(r11.u32 + 16, r29.u64);
	// std r29,24(r11)
	PPC_STORE_U64(r11.u32 + 24, r29.u64);
	// lwz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(140) );
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// mullw r28,r10,r11
	r28.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
	// mr r11,r29
	r11.u64 = r29.u64;
	// cmplwi cr6,r22,0
	cr6.compare<uint32_t>(r22.u32, 0, xer);
	// beq cr6,0x82ba6214
	if (cr6.eq) goto loc_82BA6214;
loc_82BA6108:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// addi r26,r11,1
	r26.s64 = r11.s64 + 1;
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// add r11,r10,r23
	r11.u64 = ctx.r10.u64 + r23.u64;
	// cmplw cr6,r26,r22
	cr6.compare<uint32_t>(r26.u32, r22.u32, xer);
	// beq cr6,0x82ba6124
	if (cr6.eq) goto loc_82BA6124;
	// li r9,16384
	ctx.r9.s64 = 16384;
loc_82BA6124:
	// subf r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - r11.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r9,r10,255
	ctx.r9.s64 = ctx.r10.s64 + 255;
	// rlwinm r9,r9,24,8,31
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 24) & 0xFFFFFF;
	// mulli r30,r9,42
	r30.s64 = ctx.r9.s64 * 42;
	// cmplw cr6,r28,r30
	cr6.compare<uint32_t>(r28.u32, r30.u32, xer);
	// bge cr6,0x82ba6144
	if (!cr6.lt) goto loc_82BA6144;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_82BA6144:
	// clrldi r7,r27,32
	ctx.r7.u64 = r27.u64 & 0xFFFFFFFF;
	// lwz r8,152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(152) );
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// rlwinm r11,r11,0,3,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1FFFFFFC;
	// addi r6,r9,512
	ctx.r6.s64 = ctx.r9.s64 + 512;
	// rlwinm r9,r10,31,1,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r6,0,19,19
	ctx.r10.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x1000;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// andi. r10,r8,49400
	ctx.r10.u64 = ctx.r8.u64 & 49400;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// rlwinm r11,r11,30,2,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// clrlwi r9,r9,9
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFF;
	// oris r11,r11,16384
	r11.u64 = r11.u64 | 1073741824;
	// oris r10,r10,19200
	ctx.r10.u64 = ctx.r10.u64 | 1258291200;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// lis r8,19200
	ctx.r8.s64 = 1258291200;
	// ori r10,r10,2561
	ctx.r10.u64 = ctx.r10.u64 | 2561;
	// oris r11,r9,19200
	r11.u64 = ctx.r9.u64 | 1258291200;
	// stw r8,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r8.u32);
	// stw r10,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r10.u32);
	// li r6,4
	ctx.r6.s64 = 4;
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,160(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// bl 0x82ba5018
	sub_82BA5018(ctx, base);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba61d8
	if (!cr6.gt) goto loc_82BA61D8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA61D8:
	// lis r11,-16384
	r11.s64 = -1073741824;
	// mullw r10,r30,r24
	ctx.r10.s64 = int64_t(r30.s32) * int64_t(r24.s32);
	// ori r11,r11,13824
	r11.u64 = r11.u64 | 13824;
	// li r9,129
	ctx.r9.s64 = 129;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// subf r28,r30,r28
	r28.s64 = r28.s64 - r30.s64;
	// rlwimi r9,r10,16,0,15
	ctx.r9.u64 = (rotl32(ctx.r10.u32, 16) & 0xFFFF0000) | (ctx.r9.u64 & 0xFFFFFFFF0000FFFF);
	// add r27,r30,r27
	r27.u64 = r30.u64 + r27.u64;
	// mr r23,r29
	r23.u64 = r29.u64;
	// mr r11,r26
	r11.u64 = r26.u64;
	// stwu r9,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r9.u32);
	ctx.r3.u32 = ea;
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// cmplw cr6,r26,r22
	cr6.compare<uint32_t>(r26.u32, r22.u32, xer);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
	// blt cr6,0x82ba6108
	if (cr6.lt) goto loc_82BA6108;
loc_82BA6214:
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,336
	ctx.r5.s64 = ctx.r1.s64 + 336;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821969e0
	sub_821969E0(ctx, base);
	// li r6,64
	ctx.r6.s64 = 64;
	// li r5,64
	ctx.r5.s64 = 64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8219ce10
	sub_8219CE10(ctx, base);
	// addi r1,r1,544
	ctx.r1.s64 = ctx.r1.s64 + 544;
	// addi r12,r1,-96
	r12.s64 = ctx.r1.s64 + -96;
	// bl 0x82ca754c
	// b 0x82ca2c1c
	return;
}

PPC_WEAK_FUNC(sub_82BA5CE8) {
	__imp__sub_82BA5CE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA6250) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb4
	// stwu r1,-3408(r1)
	ea = -3408 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r20,r6
	r20.u64 = ctx.r6.u64;
	// mr r18,r8
	r18.u64 = ctx.r8.u64;
	// mr r19,r9
	r19.u64 = ctx.r9.u64;
	// clrlwi. r11,r4,31
	r11.u64 = ctx.r4.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// li r21,0
	r21.s64 = 0;
	// beq 0x82ba62b4
	if (cr0.eq) goto loc_82BA62B4;
	// addi r30,r31,21808
	r30.s64 = r31.s64 + 21808;
	// li r5,1536
	ctx.r5.s64 = 1536;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,23344(r31)
	PPC_STORE_U32(r31.u32 + 23344, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9e048
	sub_82B9E048(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ba664c
	goto loc_82BA664C;
loc_82BA62B4:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r10,r11,12,20,31
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r11,r11,3
	r11.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r10,r10,512
	ctx.r10.s64 = ctx.r10.s64 + 512;
	// rlwinm r10,r10,0,19,19
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x1000;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addis r30,r11,-16384
	r30.s64 = r11.s64 + -1073741824;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba6634
	if (!cr6.eq) goto loc_82BA6634;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bgt cr6,0x82ba6634
	if (cr6.gt) goto loc_82BA6634;
	// lwz r26,21684(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21684) );
	// lwz r28,16(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// cmplw cr6,r28,r26
	cr6.compare<uint32_t>(r28.u32, r26.u32, xer);
	// bgt cr6,0x82ba662c
	if (cr6.gt) goto loc_82BA662C;
	// lwz r27,21688(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21688) );
	// lwz r29,20(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// cmplw cr6,r29,r27
	cr6.compare<uint32_t>(r29.u32, r27.u32, xer);
	// bgt cr6,0x82ba662c
	if (cr6.gt) goto loc_82BA662C;
	// addi r22,r7,4096
	r22.s64 = ctx.r7.s64 + 4096;
	// cmplwi cr6,r22,16384
	cr6.compare<uint32_t>(r22.u32, 16384, xer);
	// blt cr6,0x82ba6324
	if (cr6.lt) goto loc_82BA6324;
	// mr r22,r21
	r22.u64 = r21.u64;
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// addi r20,r20,-1
	r20.s64 = r20.s64 + -1;
loc_82BA6324:
	// li r5,52
	ctx.r5.s64 = 52;
	// stw r21,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r21.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r11,21708(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21708) );
	// lwz r10,21704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21704) );
	// lfs f0,24(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 24);
	f0.f64 = double(temp.f32);
	// lfs f13,28(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// addi r3,r31,21752
	ctx.r3.s64 = r31.s64 + 21752;
	// stw r28,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r28.u32);
	// stw r29,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r29.u32);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// addi r11,r1,96
	r11.s64 = ctx.r1.s64 + 96;
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r9,r11,56
	ctx.r9.s64 = r11.s64 + 56;
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// fmuls f0,f11,f0
	f0.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, f0.u64);
	// fmuls f0,f12,f13
	f0.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// fctidz f0,f0
	f0.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f0,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, f0.u64);
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// rlwinm r7,r7,31,1,31
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r8,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r8.u32);
loc_82BA63B0:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x82ba63d0
	if (!cr0.eq) goto loc_82BA63D0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bne cr6,0x82ba63b0
	if (!cr6.eq) goto loc_82BA63B0;
loc_82BA63D0:
	// cmpwi r8,0
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq 0x82ba63fc
	if (cr0.eq) goto loc_82BA63FC;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// li r5,56
	ctx.r5.s64 = 56;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// li r7,1
	ctx.r7.s64 = 1;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba4d28
	sub_82BA4D28(ctx, base);
loc_82BA63FC:
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// clrlwi. r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82ba6414
	if (cr0.eq) goto loc_82BA6414;
	// li r24,1
	r24.s64 = 1;
	// addi r25,r30,36
	r25.s64 = r30.s64 + 36;
	// b 0x82ba64ac
	goto loc_82BA64AC;
loc_82BA6414:
	// rlwinm. r11,r11,0,30,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba64a4
	if (cr0.eq) goto loc_82BA64A4;
	// addi r10,r30,36
	ctx.r10.s64 = r30.s64 + 36;
	// addi r9,r1,1728
	ctx.r9.s64 = ctx.r1.s64 + 1728;
	// addi r8,r1,1730
	ctx.r8.s64 = ctx.r1.s64 + 1730;
	// addi r7,r1,2240
	ctx.r7.s64 = ctx.r1.s64 + 2240;
	// addi r6,r1,2242
	ctx.r6.s64 = ctx.r1.s64 + 2242;
	// subf r29,r10,r9
	r29.s64 = ctx.r9.s64 - ctx.r10.s64;
	// addi r11,r10,512
	r11.s64 = ctx.r10.s64 + 512;
	// subf r28,r10,r8
	r28.s64 = ctx.r8.s64 - ctx.r10.s64;
	// subf r27,r10,r7
	r27.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r26,r10,r6
	r26.s64 = ctx.r6.s64 - ctx.r10.s64;
	// li r24,1
	r24.s64 = 1;
	// addi r25,r1,1728
	r25.s64 = ctx.r1.s64 + 1728;
	// addi r9,r1,1730
	ctx.r9.s64 = ctx.r1.s64 + 1730;
	// li r10,128
	ctx.r10.s64 = 128;
loc_82BA6454:
	// lhz r17,-512(r11)
	r17.u64 = PPC_LOAD_U16(r11.u32 + -512);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lhz r6,-510(r11)
	ctx.r6.u64 = PPC_LOAD_U16(r11.u32 + -510);
	// lhz r16,0(r11)
	r16.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r15,512(r11)
	r15.u64 = PPC_LOAD_U16(r11.u32 + 512);
	// add r7,r6,r17
	ctx.r7.u64 = ctx.r6.u64 + r17.u64;
	// lhz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U16(r11.u32 + 2);
	// lhz r8,514(r11)
	ctx.r8.u64 = PPC_LOAD_U16(r11.u32 + 514);
	// add r6,r4,r16
	ctx.r6.u64 = ctx.r4.u64 + r16.u64;
	// sth r17,-2(r9)
	PPC_STORE_U16(ctx.r9.u32 + -2, r17.u16);
	// add r8,r8,r15
	ctx.r8.u64 = ctx.r8.u64 + r15.u64;
	// sthx r16,r29,r11
	PPC_STORE_U16(r29.u32 + r11.u32, r16.u16);
	// sthx r15,r27,r11
	PPC_STORE_U16(r27.u32 + r11.u32, r15.u16);
	// sth r7,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r7.u16);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// sthx r6,r28,r11
	PPC_STORE_U16(r28.u32 + r11.u32, ctx.r6.u16);
	// sthx r8,r26,r11
	PPC_STORE_U16(r26.u32 + r11.u32, ctx.r8.u16);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x82ba6454
	if (!cr0.eq) goto loc_82BA6454;
	// b 0x82ba64ac
	goto loc_82BA64AC;
loc_82BA64A4:
	// mr r24,r21
	r24.u64 = r21.u64;
	// mr r25,r21
	r25.u64 = r21.u64;
loc_82BA64AC:
	// lwz r11,23344(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(23344) );
	// cmpw cr6,r24,r11
	cr6.compare<int32_t>(r24.s32, r11.s32, xer);
	// bne cr6,0x82ba64f4
	if (!cr6.eq) goto loc_82BA64F4;
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x82ba6578
	if (cr6.eq) goto loc_82BA6578;
	// addi r10,r31,21808
	ctx.r10.s64 = r31.s64 + 21808;
	// mr r11,r25
	r11.u64 = r25.u64;
	// addi r8,r25,1536
	ctx.r8.s64 = r25.s64 + 1536;
loc_82BA64CC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x82ba64ec
	if (!cr0.eq) goto loc_82BA64EC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x82ba64cc
	if (!cr6.eq) goto loc_82BA64CC;
loc_82BA64EC:
	// cmpwi r9,0
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x82ba6578
	if (cr0.eq) goto loc_82BA6578;
loc_82BA64F4:
	// stw r24,23344(r31)
	PPC_STORE_U32(r31.u32 + 23344, r24.u32);
	// cmpwi cr6,r24,0
	cr6.compare<int32_t>(r24.s32, 0, xer);
	// beq cr6,0x82ba6518
	if (cr6.eq) goto loc_82BA6518;
	// addi r3,r31,21808
	ctx.r3.s64 = r31.s64 + 21808;
	// li r5,1536
	ctx.r5.s64 = 1536;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x82ba651c
	goto loc_82BA651C;
loc_82BA6518:
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
loc_82BA651C:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82ba6570
	if (!cr6.eq) goto loc_82BA6570;
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// addi r11,r1,1184
	r11.s64 = ctx.r1.s64 + 1184;
loc_82BA652C:
	// li r9,255
	ctx.r9.s64 = 255;
	// lis r8,3
	ctx.r8.s64 = 196608;
	// divwu r9,r10,r9
	ctx.r9.u32 = ctx.r10.u32 / ctx.r9.u32;
	// ori r8,r8,65280
	ctx.r8.u64 = ctx.r8.u64 | 65280;
	// rlwinm r9,r9,6,16,25
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 6) & 0xFFC0;
	// addi r10,r10,1023
	ctx.r10.s64 = ctx.r10.s64 + 1023;
	// sth r9,-1024(r11)
	PPC_STORE_U16(r11.u32 + -1024, ctx.r9.u16);
	// sth r9,-512(r11)
	PPC_STORE_U16(r11.u32 + -512, ctx.r9.u16);
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// blt cr6,0x82ba652c
	if (cr6.lt) goto loc_82BA652C;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x82b9de70
	sub_82B9DE70(ctx, base);
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
loc_82BA6570:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9e048
	sub_82B9E048(ctx, base);
loc_82BA6578:
	// lwz r11,21536(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21536) );
	// mr r6,r20
	ctx.r6.u64 = r20.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// clrlwi r10,r11,1
	ctx.r10.u64 = r11.u32 & 0x7FFFFFFF;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// stw r10,21536(r31)
	PPC_STORE_U32(r31.u32 + 21536, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,20(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// srawi r29,r11,31
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7FFFFFFF) != 0);
	r29.s64 = r11.s32 >> 31;
	// lwz r7,16(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// bl 0x82ba5448
	sub_82BA5448(ctx, base);
	// lwz r10,21536(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21536) );
	// li r11,1
	r11.s64 = 1;
	// lwz r9,21692(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21692) );
	// rlwimi r10,r29,31,0,0
	ctx.r10.u64 = (rotl32(r29.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r21,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r21.u32);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// rlwinm r9,r9,12,0,19
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 12) & 0xFFFFF000;
	// stw r10,21536(r31)
	PPC_STORE_U32(r31.u32 + 21536, ctx.r10.u32);
	// lfs f0,32(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 32);
	f0.f64 = double(temp.f32);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// stfs f0,180(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(20) );
	// clrlwi. r8,r11,31
	ctx.r8.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// lwz r7,16(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// stw r9,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r9.u32);
	// stw r7,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r7.u32);
	// stw r10,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r10.u32);
	// beq 0x82ba65f4
	if (cr0.eq) goto loc_82BA65F4;
	// addi r4,r30,36
	ctx.r4.s64 = r30.s64 + 36;
	// b 0x82ba6600
	goto loc_82BA6600;
loc_82BA65F4:
	// rlwinm. r11,r11,0,30,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba660c
	if (cr0.eq) goto loc_82BA660C;
	// addi r4,r1,1728
	ctx.r4.s64 = ctx.r1.s64 + 1728;
loc_82BA6600:
	// li r5,1536
	ctx.r5.s64 = 1536;
	// addi r3,r1,184
	ctx.r3.s64 = ctx.r1.s64 + 184;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_82BA660C:
	// cmplwi cr6,r19,1560
	cr6.compare<uint32_t>(r19.u32, 1560, xer);
	// li r5,1560
	ctx.r5.s64 = 1560;
	// bgt cr6,0x82ba661c
	if (cr6.gt) goto loc_82BA661C;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
loc_82BA661C:
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// b 0x82ba6638
	goto loc_82BA6638;
loc_82BA662C:
	// li r21,7
	r21.s64 = 7;
	// b 0x82ba6638
	goto loc_82BA6638;
loc_82BA6634:
	// li r21,6
	r21.s64 = 6;
loc_82BA6638:
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r30,1572
	ctx.r4.s64 = r30.s64 + 1572;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821d11c8
	sub_821D11C8(ctx, base);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
loc_82BA664C:
	// addi r1,r1,3408
	ctx.r1.s64 = ctx.r1.s64 + 3408;
	// b 0x82ca2c04
	return;
}

PPC_WEAK_FUNC(sub_82BA6250) {
	__imp__sub_82BA6250(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA6658) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lis r11,10280
	r11.s64 = 673710080;
	// lwz r10,21680(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21680) );
	// lwz r8,21684(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21684) );
	// lis r9,21
	ctx.r9.s64 = 1376256;
	// ori r11,r11,310
	r11.u64 = r11.u64 | 310;
	// lwz r7,21688(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21688) );
	// ori r6,r9,6144
	ctx.r6.u64 = ctx.r9.u64 | 6144;
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// mullw r8,r7,r8
	ctx.r8.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// cmplw cr6,r8,r6
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, xer);
	// rlwinm r10,r11,27,31,31
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// ble cr6,0x82ba6698
	if (!cr6.gt) goto loc_82BA6698;
	// li r11,2
	r11.s64 = 2;
	// b 0x82ba66ac
	goto loc_82BA66AC;
loc_82BA6698:
	// lis r11,7
	r11.s64 = 458752;
	// ori r11,r11,59648
	r11.u64 = r11.u64 | 59648;
	// subfc r11,r8,r11
	xer.ca = r11.u32 >= ctx.r8.u32;
	r11.s64 = r11.s64 - ctx.r8.s64;
	// subfe r11,r11,r11
	temp.u8 = (~r11.u32 + r11.u32 < ~r11.u32) | (~r11.u32 + r11.u32 + xer.ca < xer.ca);
	r11.u64 = ~r11.u64 + r11.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
loc_82BA66AC:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ba66ec
	if (!cr6.eq) goto loc_82BA66EC;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r9,r9,-17296
	ctx.r9.s64 = ctx.r9.s64 + -17296;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// mulli r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 * 3;
	// lfs f13,416(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 416);
	ctx.f13.f64 = double(temp.f32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,412(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 412);
	f0.f64 = double(temp.f32);
	// lfsx f12,r11,r9
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	ctx.f12.f64 = double(temp.f32);
	// b 0x82ba672c
	goto loc_82BA672C;
loc_82BA66EC:
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// rlwinm r7,r10,27,31,31
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r10,r9,-17296
	ctx.r10.s64 = ctx.r9.s64 + -17296;
	// xori r9,r7,1
	ctx.r9.u64 = ctx.r7.u64 ^ 1;
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// mulli r9,r9,3
	ctx.r9.s64 = ctx.r9.s64 * 3;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lfs f13,416(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 416);
	ctx.f13.f64 = double(temp.f32);
	// lfsx f12,r11,r10
	temp.u32 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,408(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 408);
	f0.f64 = double(temp.f32);
loc_82BA672C:
	// clrldi r11,r8,32
	r11.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// fmuls f13,f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r11.u64);
	// lfd f12,-16(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// frsp f10,f12
	ctx.f10.f64 = double(float(ctx.f12.f64));
	// lfs f11,404(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 404);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r10,-19236
	r11.s64 = ctx.r10.s64 + -19236;
	// lfs f12,-19236(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19236);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(f0.f64 * ctx.f12.f64));
	// li r10,12
	ctx.r10.s64 = 12;
	// lfs f0,-8232(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8232);
	f0.f64 = double(temp.f32);
	// stfs f0,8(r4)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 8, temp.u32);
	// fmuls f0,f10,f11
	f0.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// fctidz f12,f12
	ctx.f12.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfiwx f12,r4,r10
	PPC_STORE_U32(ctx.r4.u32 + ctx.r10.u32, ctx.f12.u32);
	// fmuls f0,f0,f13
	f0.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f0,4(r4)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r4.u32 + 4, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA6658) {
	__imp__sub_82BA6658(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA6780) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x832b225c
	__imp__KeGetCurrentProcessType(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// bne cr6,0x82ba67b0
	if (!cr6.eq) goto loc_82BA67B0;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2472) );
	// b 0x82ba67b8
	goto loc_82BA67B8;
loc_82BA67B0:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2476(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2476) );
loc_82BA67B8:
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r30.u32);
	// bl 0x8221eb58
	sub_8221EB58(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// sradi r10,r10,10
	xer.ca = (ctx.r10.s64 < 0) & ((ctx.r10.u64 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r10.s64 >> 10;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r10,21676(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21676) );
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba6804
	if (cr6.eq) goto loc_82BA6804;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// li r3,93
	ctx.r3.s64 = 93;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA6804:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA6780) {
	__imp__sub_82BA6780(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA6820) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stfd f29,-176(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -176, f29.u64);
	// stfd f30,-168(r1)
	PPC_STORE_U64(ctx.r1.u32 + -168, f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-1312(r1)
	ea = -1312 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r15,r5
	r15.u64 = ctx.r5.u64;
	// li r17,0
	r17.s64 = 0;
	// li r5,24
	ctx.r5.s64 = 24;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r17,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r17.u32);
	// addi r3,r1,164
	ctx.r3.s64 = ctx.r1.s64 + 164;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// mr r16,r9
	r16.u64 = ctx.r9.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r9,r1,136
	ctx.r9.s64 = ctx.r1.s64 + 136;
	// stw r17,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r17.u32);
	// addi r8,r1,208
	ctx.r8.s64 = ctx.r1.s64 + 208;
	// stw r17,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r17.u32);
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// stw r9,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r9.u32);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// li r10,2
	ctx.r10.s64 = 2;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r17,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r17.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// li r3,3
	ctx.r3.s64 = 3;
	// bl 0x821fb520
	sub_821FB520(ctx, base);
	// lis r9,8192
	ctx.r9.s64 = 536870912;
	// lwz r8,248(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(248) );
	// li r11,9
	r11.s64 = 9;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwimi r8,r10,21,9,10
	ctx.r8.u64 = (rotl32(ctx.r10.u32, 21) & 0x600000) | (ctx.r8.u64 & 0xFFFFFFFFFF9FFFFF);
	// lwz r9,236(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(236) );
	// stw r8,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r8.u32);
	// rlwimi r9,r11,11,16,21
	ctx.r9.u64 = (rotl32(r11.u32, 11) & 0xFC00) | (ctx.r9.u64 & 0xFFFFFFFFFFFF03FF);
	// addis r11,r31,-16384
	r11.s64 = r31.s64 + -1073741824;
	// stw r9,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r9.u32);
	// blt cr6,0x82ba68e8
	if (cr6.lt) goto loc_82BA68E8;
	// addis r11,r31,-16640
	r11.s64 = r31.s64 + -1090519040;
loc_82BA68E8:
	// lwz r10,240(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(240) );
	// rlwimi r11,r10,0,20,31
	r11.u64 = (rotl32(ctx.r10.u32, 0) & 0xFFF) | (r11.u64 & 0xFFFFFFFFFFFFF000);
	// mr r14,r11
	r14.u64 = r11.u64;
	// clrlwi r11,r14,26
	r11.u64 = r14.u32 & 0x3F;
	// stw r14,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r14.u32);
	// cmplwi cr6,r11,7
	cr6.compare<uint32_t>(r11.u32, 7, xer);
	// bne cr6,0x82ba6910
	if (!cr6.eq) goto loc_82BA6910;
	// li r11,27
	r11.s64 = 27;
	// rlwimi r14,r11,1,26,31
	r14.u64 = (rotl32(r11.u32, 1) & 0x3F) | (r14.u64 & 0xFFFFFFFFFFFFFFC0);
	// stw r14,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, r14.u32);
loc_82BA6910:
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// li r5,56
	ctx.r5.s64 = 56;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r19,288(r1)
	r19.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// bne cr6,0x82ba6930
	if (!cr6.eq) goto loc_82BA6930;
	// lwz r19,21704(r24)
	r19.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21704) );
loc_82BA6930:
	// lwz r20,292(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(292) );
	// cmplwi cr6,r20,0
	cr6.compare<uint32_t>(r20.u32, 0, xer);
	// bne cr6,0x82ba6940
	if (!cr6.eq) goto loc_82BA6940;
	// lwz r20,21708(r24)
	r20.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21708) );
loc_82BA6940:
	// lwz r23,280(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(280) );
	// lwz r25,272(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(272) );
	// cmpw cr6,r23,r25
	cr6.compare<int32_t>(r23.s32, r25.s32, xer);
	// bne cr6,0x82ba6958
	if (!cr6.eq) goto loc_82BA6958;
	// mr r25,r17
	r25.u64 = r17.u64;
	// mr r23,r30
	r23.u64 = r30.u64;
loc_82BA6958:
	// lwz r26,284(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(284) );
	// lwz r27,276(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(276) );
	// cmpw cr6,r26,r27
	cr6.compare<int32_t>(r26.s32, r27.s32, xer);
	// bne cr6,0x82ba6970
	if (!cr6.eq) goto loc_82BA6970;
	// mr r27,r17
	r27.u64 = r17.u64;
	// mr r26,r28
	r26.u64 = r28.u64;
loc_82BA6970:
	// lwz r11,21704(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21704) );
	// clrldi r10,r20,32
	ctx.r10.u64 = r20.u64 & 0xFFFFFFFF;
	// lwz r9,21708(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21708) );
	// clrldi r8,r19,32
	ctx.r8.u64 = r19.u64 & 0xFFFFFFFF;
	// std r10,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r10.u64);
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// fcfid f9,f0
	ctx.f9.f64 = double(f0.s64);
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r10,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r10.u64);
	// lfd f11,128(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r8,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r8.u64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lis r11,-31950
	r11.s64 = -2093875200;
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcfid f10,f12
	ctx.f10.f64 = double(ctx.f12.s64);
	// lfs f12,2248(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2248);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// frsp f9,f9
	ctx.f9.f64 = double(float(ctx.f9.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// addi r18,r11,-17912
	r18.s64 = r11.s64 + -17912;
	// lfs f0,-9544(r18)
	temp.u32 = PPC_LOAD_U32(r18.u32 + -9544);
	f0.f64 = double(temp.f32);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f10,f10
	ctx.f10.f64 = double(float(ctx.f10.f64));
	// fdivs f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 / ctx.f11.f64));
	// fdivs f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 / ctx.f13.f64));
	// fsubs f13,f12,f13
	ctx.f13.f64 = static_cast<float>(ctx.f12.f64 - ctx.f13.f64);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x82ba69f4
	if (!cr6.lt) goto loc_82BA69F4;
	// fmr f10,f13
	ctx.f10.f64 = ctx.f13.f64;
	// b 0x82ba69f8
	goto loc_82BA69F8;
loc_82BA69F4:
	// fmr f10,f0
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = f0.f64;
loc_82BA69F8:
	// fsubs f13,f12,f11
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = static_cast<float>(ctx.f12.f64 - ctx.f11.f64);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x82ba6a0c
	if (!cr6.lt) goto loc_82BA6A0C;
	// fmr f31,f13
	f31.f64 = ctx.f13.f64;
	// b 0x82ba6a10
	goto loc_82BA6A10;
loc_82BA6A0C:
	// fmr f31,f0
	ctx.fpscr.disableFlushMode();
	f31.f64 = f0.f64;
loc_82BA6A10:
	// subf r31,r25,r23
	r31.s64 = r23.s64 - r25.s64;
	// subf r28,r27,r26
	r28.s64 = r26.s64 - r27.s64;
	// extsw r11,r31
	r11.s64 = r31.s32;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// fmuls f1,f0,f10
	ctx.f1.f64 = double(float(f0.f64 * ctx.f10.f64));
	// bl 0x822955c0
	sub_822955C0(ctx, base);
	// extsw r11,r28
	r11.s64 = r28.s32;
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// fctiwz f0,f0
	f0.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// stfd f0,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, f0.u64);
	// lwz r30,132(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// frsp f0,f13
	f0.f64 = double(float(ctx.f13.f64));
	// fmuls f1,f0,f31
	ctx.f1.f64 = double(float(f0.f64 * f31.f64));
	// bl 0x822955c0
	sub_822955C0(ctx, base);
	// lwz r11,21540(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21540) );
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// subfic r10,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r10.s64 = 0 - r11.s64;
	// subfe r10,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r10.u64 + ctx.r10.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfic r9,r11,0
	xer.ca = r11.u32 <= 0;
	ctx.r9.s64 = 0 - r11.s64;
	// clrlwi r11,r10,28
	r11.u64 = ctx.r10.u32 & 0xF;
	// subfe r10,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r10.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// addi r11,r11,35
	r11.s64 = r11.s64 + 35;
	// andi. r9,r10,9
	ctx.r9.u64 = ctx.r10.u64 & 9;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// fctiwz f0,f0
	f0.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// addi r8,r9,19
	ctx.r8.s64 = ctx.r9.s64 + 19;
	// stfd f0,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, f0.u64);
	// addi r9,r10,-1
	ctx.r9.s64 = ctx.r10.s64 + -1;
	// lwz r7,132(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// twllei r11,0
	// divw r10,r9,r11
	ctx.r10.s32 = ctx.r9.s32 / r11.s32;
	// rotlwi r9,r9,1
	ctx.r9.u64 = rotl32(ctx.r9.u32, 1);
	// addi r10,r10,3
	ctx.r10.s64 = ctx.r10.s64 + 3;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rlwinm r10,r10,0,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// andc r11,r11,r9
	r11.u64 = r11.u64 & ~ctx.r9.u64;
	// cmpwi cr6,r10,24
	cr6.compare<int32_t>(ctx.r10.s32, 24, xer);
	// twlgei r11,-1
	// bgt cr6,0x82ba6ac8
	if (cr6.gt) goto loc_82BA6AC8;
	// li r10,24
	ctx.r10.s64 = 24;
loc_82BA6AC8:
	// add r11,r8,r7
	r11.u64 = ctx.r8.u64 + ctx.r7.u64;
	// twllei r8,0
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// divw r11,r9,r8
	r11.s32 = ctx.r9.s32 / ctx.r8.s32;
	// rotlwi r9,r9,1
	ctx.r9.u64 = rotl32(ctx.r9.u32, 1);
	// addi r11,r11,3
	r11.s64 = r11.s64 + 3;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// rlwinm r11,r11,0,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFC;
	// andc r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 & ~ctx.r9.u64;
	// cmpwi cr6,r11,24
	cr6.compare<int32_t>(r11.s32, 24, xer);
	// twlgei r9,-1
	// bgt cr6,0x82ba6afc
	if (cr6.gt) goto loc_82BA6AFC;
	// li r11,24
	r11.s64 = 24;
loc_82BA6AFC:
	// add r8,r11,r7
	ctx.r8.u64 = r11.u64 + ctx.r7.u64;
	// add r9,r10,r30
	ctx.r9.u64 = ctx.r10.u64 + r30.u64;
	// addi r6,r8,-1
	ctx.r6.s64 = ctx.r8.s64 + -1;
	// addi r7,r9,-1
	ctx.r7.s64 = ctx.r9.s64 + -1;
	// rotlwi r8,r6,1
	ctx.r8.u64 = rotl32(ctx.r6.u32, 1);
	// rotlwi r9,r7,1
	ctx.r9.u64 = rotl32(ctx.r7.u32, 1);
	// srawi r5,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 2;
	// divw r7,r7,r10
	ctx.r7.s32 = ctx.r7.s32 / ctx.r10.s32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// addze r22,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	r22.s64 = temp.s64;
	// mullw r30,r7,r10
	r30.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r5.s64 = r11.s32 >> 2;
	// andc r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// divw r7,r6,r11
	ctx.r7.s32 = ctx.r6.s32 / r11.s32;
	// andc r8,r11,r8
	ctx.r8.u64 = r11.u64 & ~ctx.r8.u64;
	// twllei r10,0
	// twllei r11,0
	// addze r21,r5
	temp.s64 = ctx.r5.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r5.u32;
	r21.s64 = temp.s64;
	// twlgei r9,-1
	// twlgei r8,-1
	// mullw r29,r7,r11
	r29.s64 = int64_t(ctx.r7.s32) * int64_t(r11.s32);
	// cmpw cr6,r30,r31
	cr6.compare<int32_t>(r30.s32, r31.s32, xer);
	// ble cr6,0x82ba6b78
	if (!cr6.gt) goto loc_82BA6B78;
	// rotlwi r9,r31,1
	ctx.r9.u64 = rotl32(r31.u32, 1);
	// divw r8,r31,r10
	ctx.r8.s32 = r31.s32 / ctx.r10.s32;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// twllei r10,0
	// mullw r30,r8,r10
	r30.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// andc r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 & ~ctx.r9.u64;
	// twlgei r10,-1
loc_82BA6B78:
	// cmpw cr6,r29,r28
	cr6.compare<int32_t>(r29.s32, r28.s32, xer);
	// ble cr6,0x82ba6b9c
	if (!cr6.gt) goto loc_82BA6B9C;
	// rotlwi r10,r28,1
	ctx.r10.u64 = rotl32(r28.u32, 1);
	// divw r9,r28,r11
	ctx.r9.s32 = r28.s32 / r11.s32;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// twllei r11,0
	// mullw r29,r9,r11
	r29.s64 = int64_t(ctx.r9.s32) * int64_t(r11.s32);
	// andc r11,r11,r10
	r11.u64 = r11.u64 & ~ctx.r10.u64;
	// twlgei r11,-1
loc_82BA6B9C:
	// lwz r10,21704(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21704) );
	// mullw r11,r29,r20
	r11.s64 = int64_t(r29.s32) * int64_t(r20.s32);
	// lwz r9,21708(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21708) );
	// lwz r8,21716(r24)
	ctx.r8.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21716) );
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// divwu r11,r11,r28
	r11.u32 = r11.u32 / r28.u32;
	// add r10,r25,r23
	ctx.r10.u64 = r25.u64 + r23.u64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// srawi r10,r10,1
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// mullw r11,r30,r19
	r11.s64 = int64_t(r30.s32) * int64_t(r19.s32);
	// lfd f0,128(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r9,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r9.u64);
	// divwu r11,r11,r31
	r11.u32 = r11.u32 / r31.u32;
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// lfd f12,136(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// add r9,r27,r26
	ctx.r9.u64 = r27.u64 + r26.u64;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f11,128(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	r11.s64 = temp.s64;
	// srawi r10,r30,1
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x1) != 0);
	ctx.r10.s64 = r30.s32 >> 1;
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// twllei r28,0
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// srawi r9,r9,1
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// subf r20,r10,r11
	r20.s64 = r11.s64 - ctx.r10.s64;
	// addze r11,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	r11.s64 = temp.s64;
	// srawi r10,r29,1
	xer.ca = (r29.s32 < 0) & ((r29.u32 & 0x1) != 0);
	ctx.r10.s64 = r29.s32 >> 1;
	// twllei r31,0
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// subf r19,r10,r11
	r19.s64 = r11.s64 - ctx.r10.s64;
	// fdivs f31,f0,f13
	f31.f64 = double(float(f0.f64 / ctx.f13.f64));
	// fdivs f30,f11,f12
	f30.f64 = double(float(ctx.f11.f64 / ctx.f12.f64));
	// beq cr6,0x82ba6c54
	if (cr6.eq) goto loc_82BA6C54;
	// lfs f0,-9488(r18)
	temp.u32 = PPC_LOAD_U32(r18.u32 + -9488);
	f0.f64 = double(temp.f32);
	// lfs f13,-9960(r18)
	temp.u32 = PPC_LOAD_U32(r18.u32 + -9960);
	ctx.f13.f64 = double(temp.f32);
	// b 0x82ba6c5c
	goto loc_82BA6C5C;
loc_82BA6C54:
	// lfs f0,-1340(r18)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r18.u32 + -1340);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r18)
	temp.u32 = PPC_LOAD_U32(r18.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
loc_82BA6C5C:
	// divw r8,r30,r22
	ctx.r8.s32 = r30.s32 / r22.s32;
	// fmuls f13,f31,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f31.f64 * ctx.f13.f64));
	// divw r7,r29,r21
	ctx.r7.s32 = r29.s32 / r21.s32;
	// fmuls f0,f30,f0
	f0.f64 = double(float(f30.f64 * f0.f64));
	// li r11,42
	r11.s64 = 42;
	// mullw r6,r7,r8
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r8.s32);
	// fdivs f29,f0,f13
	f29.f64 = double(float(f0.f64 / ctx.f13.f64));
	// divwu r9,r6,r11
	ctx.r9.u32 = ctx.r6.u32 / r11.u32;
	// rotlwi r10,r30,1
	ctx.r10.u64 = rotl32(r30.u32, 1);
	// rotlwi r11,r29,1
	r11.u64 = rotl32(r29.u32, 1);
	// addi r9,r9,1024
	ctx.r9.s64 = ctx.r9.s64 + 1024;
	// addi r5,r10,-1
	ctx.r5.s64 = ctx.r10.s64 + -1;
	// addi r4,r11,-1
	ctx.r4.s64 = r11.s64 + -1;
	// rlwinm r11,r9,2,0,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mulli r10,r6,6
	ctx.r10.s64 = ctx.r6.s64 * 6;
	// andc r9,r22,r5
	ctx.r9.u64 = r22.u64 & ~ctx.r5.u64;
	// andc r6,r21,r4
	ctx.r6.u64 = r21.u64 & ~ctx.r4.u64;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// twllei r22,0
	// twllei r21,0
	// twlgei r9,-1
	// twlgei r6,-1
	// rlwinm r26,r8,1,0,30
	r26.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r7,1,0,30
	r25.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r31,16384
	cr6.compare<uint32_t>(r31.u32, 16384, xer);
	// bgt cr6,0x82ba6cc8
	if (cr6.gt) goto loc_82BA6CC8;
	// li r31,16384
	r31.s64 = 16384;
loc_82BA6CC8:
	// addi r8,r1,336
	ctx.r8.s64 = ctx.r1.s64 + 336;
	// stw r17,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r17.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r31,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r31.u32);
	// stw r8,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r8.u32);
	// li r10,4096
	ctx.r10.s64 = 4096;
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba6d10
	if (cr6.eq) goto loc_82BA6D10;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// li r3,92
	ctx.r3.s64 = 92;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,176(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// b 0x82ba6d14
	goto loc_82BA6D14;
loc_82BA6D10:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
loc_82BA6D14:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba6eec
	if (cr6.eq) goto loc_82BA6EEC;
	// lwz r11,172(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(172) );
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// lwz r28,180(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(180) );
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mr r31,r8
	r31.u64 = ctx.r8.u64;
	// clrlwi r9,r10,18
	ctx.r9.u64 = ctx.r10.u32 & 0x3FFF;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// addi r23,r9,1
	r23.s64 = ctx.r9.s64 + 1;
	// beq cr6,0x82ba6d60
	if (cr6.eq) goto loc_82BA6D60;
	// lis r10,10280
	ctx.r10.s64 = 673710080;
	// ori r10,r10,310
	ctx.r10.u64 = ctx.r10.u64 | 310;
	// subf r10,r15,r10
	ctx.r10.s64 = ctx.r10.s64 - r15.s64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_82BA6D60:
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// lis r8,20585
	ctx.r8.s64 = 1349058560;
	// cmplwi cr6,r16,0
	cr6.compare<uint32_t>(r16.u32, 0, xer);
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// ori r9,r8,30806
	ctx.r9.u64 = ctx.r8.u64 | 30806;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// stw r17,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r17.u32);
	// lwz r9,21540(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21540) );
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// stw r26,16(r11)
	PPC_STORE_U32(r11.u32 + 16, r26.u32);
	// stw r25,20(r11)
	PPC_STORE_U32(r11.u32 + 20, r25.u32);
	// stfs f30,24(r11)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// stfs f31,28(r11)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r11.u32 + 28, temp.u32);
	// stfs f29,32(r11)
	temp.f32 = float(f29.f64);
	PPC_STORE_U32(r11.u32 + 32, temp.u32);
	// beq cr6,0x82ba6db0
	if (cr6.eq) goto loc_82BA6DB0;
	// addi r3,r11,36
	ctx.r3.s64 = r11.s64 + 36;
	// li r5,1536
	ctx.r5.s64 = 1536;
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_82BA6DB0:
	// addi r4,r27,4096
	ctx.r4.s64 = r27.s64 + 4096;
	// cmplwi cr6,r4,16384
	cr6.compare<uint32_t>(ctx.r4.u32, 16384, xer);
	// blt cr6,0x82ba6dc8
	if (cr6.lt) goto loc_82BA6DC8;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r28,r28,-1
	r28.s64 = r28.s64 + -1;
loc_82BA6DC8:
	// extsw r11,r30
	r11.s64 = r30.s32;
	// extsw r10,r19
	ctx.r10.s64 = r19.s32;
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// extsw r11,r21
	r11.s64 = r21.s32;
	// extsw r9,r22
	ctx.r9.s64 = r22.s32;
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, r11.u64);
	// lfd f11,136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// extsw r11,r20
	r11.s64 = r20.s32;
	// std r9,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, ctx.r9.u64);
	// lfd f9,192(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 192);
	// fcfid f9,f9
	ctx.f9.f64 = double(ctx.f9.s64);
	// lfd f0,128(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r10,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r10.u64);
	// extsw r10,r29
	ctx.r10.s64 = r29.s32;
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// std r10,144(r1)
	PPC_STORE_U64(ctx.r1.u32 + 144, ctx.r10.u64);
	// lis r10,21
	ctx.r10.s64 = 1376256;
	// ori r10,r10,6144
	ctx.r10.u64 = ctx.r10.u64 | 6144;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// lfd f13,128(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f10,144(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fcfid f10,f10
	ctx.f10.f64 = double(ctx.f10.s64);
	// stfs f0,128(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// mullw r11,r29,r30
	r11.s64 = int64_t(r29.s32) * int64_t(r30.s32);
	// frsp f12,f12
	ctx.f12.f64 = double(float(ctx.f12.f64));
	// stfs f12,144(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// stfs f13,148(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// frsp f0,f10
	f0.f64 = double(float(ctx.f10.f64));
	// stfs f0,132(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// frsp f0,f9
	f0.f64 = double(float(ctx.f9.f64));
	// stfs f0,136(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// frsp f0,f11
	f0.f64 = double(float(ctx.f11.f64));
	// stfs f0,140(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x82ba6e74
	if (!cr6.gt) goto loc_82BA6E74;
	// li r9,2
	ctx.r9.s64 = 2;
	// b 0x82ba6e8c
	goto loc_82BA6E8C;
loc_82BA6E74:
	// lis r10,7
	ctx.r10.s64 = 458752;
	// li r9,1
	ctx.r9.s64 = 1;
	// ori r10,r10,59648
	ctx.r10.u64 = ctx.r10.u64 | 59648;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bgt cr6,0x82ba6e8c
	if (cr6.gt) goto loc_82BA6E8C;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
loc_82BA6E8C:
	// clrlwi r11,r14,26
	r11.u64 = r14.u32 & 0x3F;
	// lwz r10,21540(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + int32_t(21540) );
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// addi r11,r11,-54
	r11.s64 = r11.s64 + -54;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// addi r3,r8,-17248
	ctx.r3.s64 = ctx.r8.s64 + -17248;
	// rlwinm r11,r11,27,31,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// addi r8,r1,136
	ctx.r8.s64 = ctx.r1.s64 + 136;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// mulli r11,r11,3
	r11.s64 = r11.s64 * 3;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,208
	ctx.r8.s64 = ctx.r1.s64 + 208;
	// mr r7,r23
	ctx.r7.u64 = r23.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwzx r11,r11,r3
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r3.u32);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// bl 0x82ba5ce8
	sub_82BA5CE8(ctx, base);
loc_82BA6EEC:
	// lwz r3,184(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(184) );
	// addi r1,r1,1312
	ctx.r1.s64 = ctx.r1.s64 + 1312;
	// lfd f29,-176(r1)
	ctx.fpscr.disableFlushMode();
	f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -176);
	// lfd f30,-168(r1)
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BA6820) {
	__imp__sub_82BA6820(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA6F08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r25,276(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(276) );
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// mr r29,r7
	r29.u64 = ctx.r7.u64;
	// mr r28,r8
	r28.u64 = ctx.r8.u64;
	// mr r27,r9
	r27.u64 = ctx.r9.u64;
	// mr r26,r10
	r26.u64 = ctx.r10.u64;
	// stw r29,21680(r31)
	PPC_STORE_U32(r31.u32 + 21680, r29.u32);
	// mr r24,r5
	r24.u64 = ctx.r5.u64;
	// stw r28,21684(r31)
	PPC_STORE_U32(r31.u32 + 21684, r28.u32);
	// rlwinm. r11,r4,0,28,28
	r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0x8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r27,21688(r31)
	PPC_STORE_U32(r31.u32 + 21688, r27.u32);
	// stw r30,21692(r31)
	PPC_STORE_U32(r31.u32 + 21692, r30.u32);
	// stw r26,21696(r31)
	PPC_STORE_U32(r31.u32 + 21696, r26.u32);
	// stw r25,21700(r31)
	PPC_STORE_U32(r31.u32 + 21700, r25.u32);
	// beq 0x82ba6f5c
	if (cr0.eq) goto loc_82BA6F5C;
	// addi r3,r31,21704
	ctx.r3.s64 = r31.s64 + 21704;
	// bl 0x832b313c
	__imp__VdQueryVideoMode(ctx, base);
loc_82BA6F5C:
	// lwz r11,21536(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21536) );
	// rlwinm. r11,r11,0,1,1
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba702c
	if (cr0.eq) goto loc_82BA702C;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// bne cr6,0x82ba6fa4
	if (!cr6.eq) goto loc_82BA6FA4;
	// lbz r11,10941(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10941);
	// rlwinm. r11,r11,0,30,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba6fa4
	if (!cr0.eq) goto loc_82BA6FA4;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// mr r8,r26
	ctx.r8.u64 = r26.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba6820
	sub_82BA6820(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x82ba6ff4
	goto loc_82BA6FF4;
loc_82BA6FA4:
	// li r5,24
	ctx.r5.s64 = 24;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// cntlzw r11,r24
	r11.u64 = r24.u32 == 0 ? 32 : __builtin_clz(r24.u32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// rlwinm r11,r11,27,31,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// xori r10,r11,1
	ctx.r10.u64 = r11.u64 ^ 1;
	// lwz r11,2316(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(2316) );
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba6ff0
	if (cr6.eq) goto loc_82BA6FF0;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,92
	ctx.r3.s64 = 92;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA6FF0:
	// lwz r30,104(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(104) );
loc_82BA6FF4:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// lwz r4,48(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// ble cr6,0x82ba7010
	if (!cr6.gt) goto loc_82BA7010;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
loc_82BA7010:
	// lis r11,-32070
	r11.s64 = -2101739520;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// addi r6,r11,26496
	ctx.r6.s64 = r11.s64 + 26496;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821f5f20
	sub_821F5F20(ctx, base);
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
loc_82BA702C:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c28
	return;
}

PPC_WEAK_FUNC(sub_82BA6F08) {
	__imp__sub_82BA6F08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7038) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r11,10943(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 10943);
	// ori r11,r11,4
	r11.u64 = r11.u64 | 4;
	// stb r11,10943(r3)
	PPC_STORE_U8(ctx.r3.u32 + 10943, r11.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA7038) {
	__imp__sub_82BA7038(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7048) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,21660(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21660) );
	// lwz r10,16560(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16560) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ba7070
	if (!cr6.eq) goto loc_82BA7070;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82ba709c
	goto loc_82BA709C;
loc_82BA7070:
	// bl 0x832b225c
	__imp__KeGetCurrentProcessType(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// beq cr6,0x82ba7098
	if (cr6.eq) goto loc_82BA7098;
	// lwz r11,21648(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21648) );
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// beq cr6,0x82ba70a4
	if (cr6.eq) goto loc_82BA70A4;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba7098
	if (!cr6.eq) goto loc_82BA7098;
	// li r11,1
	r11.s64 = 1;
	// stw r11,21648(r30)
	PPC_STORE_U32(r30.u32 + 21648, r11.u32);
loc_82BA7098:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82BA709C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82BA70A4:
	// lwz r10,21656(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21656) );
	// lwz r11,21652(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21652) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ba7098
	if (cr6.eq) goto loc_82BA7098;
	// rlwinm r10,r11,4,0,27
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,21640(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21640) );
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r3,12(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba70d8
	if (cr6.eq) goto loc_82BA70D8;
	// bl 0x821f6140
	sub_821F6140(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x82ba7098
	if (!cr0.eq) goto loc_82BA7098;
loc_82BA70D8:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r5,480
	ctx.r5.s64 = 480;
	// addi r31,r11,25304
	r31.s64 = r11.s64 + 25304;
	// addi r3,r31,40
	ctx.r3.s64 = r31.s64 + 40;
	// addi r4,r31,-552
	ctx.r4.s64 = r31.s64 + -552;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,21640(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21640) );
	// stw r11,-556(r31)
	PPC_STORE_U32(r31.u32 + -556, r11.u32);
	// lwz r11,21652(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21652) );
	// rlwinm r11,r11,4,0,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r29,r11,r10
	r29.u64 = r11.u64 + ctx.r10.u64;
	// lwz r28,12(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + int32_t(12) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ba7150
	if (cr6.eq) goto loc_82BA7150;
	// bl 0x832b225c
	__imp__KeGetCurrentProcessType(ctx, base);
	// cmpwi cr6,r3,2
	cr6.compare<int32_t>(ctx.r3.s32, 2, xer);
	// bne cr6,0x82ba712c
	if (!cr6.eq) goto loc_82BA712C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2472) );
	// b 0x82ba7134
	goto loc_82BA7134;
loc_82BA712C:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2476(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2476) );
loc_82BA7134:
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ba7150
	if (cr6.eq) goto loc_82BA7150;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,6
	ctx.r5.s64 = 6;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82242628
	sub_82242628(ctx, base);
loc_82BA7150:
	// lwz r29,4(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// addi r3,r31,-552
	ctx.r3.s64 = r31.s64 + -552;
	// li r5,480
	ctx.r5.s64 = 480;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r4,r29,480
	ctx.r4.s64 = r29.s64 + 480;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x821d11c8
	sub_821D11C8(ctx, base);
	// lwz r11,21652(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21652) );
	// lwz r10,21644(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21644) );
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bne cr6,0x82ba71a4
	if (!cr6.eq) goto loc_82BA71A4;
	// li r11,0
	r11.s64 = 0;
	// b 0x82ba71a8
	goto loc_82BA71A8;
loc_82BA71A4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82BA71A8:
	// lwz r10,16560(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16560) );
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,21652(r30)
	PPC_STORE_U32(r30.u32 + 21652, r11.u32);
	// stw r10,21660(r30)
	PPC_STORE_U32(r30.u32 + 21660, ctx.r10.u32);
	// b 0x82ba709c
	goto loc_82BA709C;
}

PPC_WEAK_FUNC(sub_82BA7048) {
	__imp__sub_82BA7048(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA71C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,32767
	r11.s64 = 2147418112;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x82ba71f4
	if (!cr6.eq) goto loc_82BA71F4;
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// b 0x82ba7214
	goto loc_82BA7214;
loc_82BA71F4:
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lfd f0,3544(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 3544);
	// fmul f31,f1,f0
	f31.f64 = ctx.f1.f64 * f0.f64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// fdiv f0,f31,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = f31.f64 / ctx.f1.f64;
	// frsp f1,f0
	ctx.f1.f64 = double(float(f0.f64));
loc_82BA7214:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA71C0) {
	__imp__sub_82BA71C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7230) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpdi cr6,r3,0
	cr6.compare<int64_t>(ctx.r3.s64, 0, xer);
	// bge cr6,0x82ba7240
	if (!cr6.lt) goto loc_82BA7240;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82BA7240:
	// cmpdi cr6,r4,0
	cr6.compare<int64_t>(ctx.r4.s64, 0, xer);
	// bne cr6,0x82ba7254
	if (!cr6.eq) goto loc_82BA7254;
	// lis r3,0
	ctx.r3.s64 = 0;
	// ori r3,r3,65535
	ctx.r3.u64 = ctx.r3.u64 | 65535;
	// blr 
	return;
loc_82BA7254:
	// cmpd cr6,r3,r4
	cr6.compare<int64_t>(ctx.r3.s64, ctx.r4.s64, xer);
	// blt cr6,0x82ba7268
	if (cr6.lt) goto loc_82BA7268;
	// lis r3,0
	ctx.r3.s64 = 0;
	// ori r3,r3,65534
	ctx.r3.u64 = ctx.r3.u64 | 65534;
	// blr 
	return;
loc_82BA7268:
	// lis r11,0
	r11.s64 = 0;
	// tdllei r4,0
	// ori r11,r11,65534
	r11.u64 = r11.u64 | 65534;
	// mulld r10,r3,r11
	ctx.r10.s64 = ctx.r3.s64 * r11.s64;
	// rotldi r11,r10,1
	r11.u64 = rotl64(ctx.r10.u64, 1);
	// divd r10,r10,r4
	ctx.r10.s64 = ctx.r10.s64 / ctx.r4.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r3,r10,0
	ctx.r3.u64 = rotl32(ctx.r10.u32, 0);
	// andc r11,r4,r11
	r11.u64 = ctx.r4.u64 & ~r11.u64;
	// tdlgei r11,-1
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA7230) {
	__imp__sub_82BA7230(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7298) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r8,8
	ctx.r8.s64 = 8;
	// lwz r11,21584(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(21584) );
	// lwz r6,21580(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(21580) );
	// rldicr r11,r11,32,63
	r11.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// or r11,r11,r6
	r11.u64 = r11.u64 | ctx.r6.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_82BA72C8:
	// std r7,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r7.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x82ba72c8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA72C8;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// sradi r7,r11,32
	xer.ca = (r11.s64 < 0) & ((r11.u64 & 0xFFFFFFFF) != 0);
	ctx.r7.s64 = r11.s64 >> 32;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// lwz r11,21624(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(21624) );
	// ori r6,r10,65535
	ctx.r6.u64 = ctx.r10.u64 | 65535;
	// lwz r4,21576(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(21576) );
	// clrlwi r10,r11,29
	ctx.r10.u64 = r11.u32 & 0x7;
	// lwz r3,16560(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(16560) );
	// lwz r31,21616(r9)
	r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(21616) );
	// lfs f0,21592(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 21592);
	f0.f64 = double(temp.f32);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// lwz r30,21620(r9)
	r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(21620) );
	// lwz r29,23348(r9)
	r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(23348) );
	// lfs f13,21588(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 21588);
	ctx.f13.f64 = double(temp.f32);
	// rlwinm r27,r10,2,0,29
	r27.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r28,10896(r9)
	r28.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(10896) );
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// stfs f0,112(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stw r6,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r6.u32);
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// stw r3,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r3.u32);
	// stw r31,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r31.u32);
	// clrlwi r8,r8,29
	ctx.r8.u64 = ctx.r8.u32 & 0x7;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r30,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r30.u32);
	// stw r5,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r5.u32);
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// stw r29,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r29.u32);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwbrx r7,r27,r28
	ctx.r7.u64 = __builtin_bswap32(PPC_LOAD_U32(r27.u32 + r28.u32));
	// lwbrx r10,r10,r28
	ctx.r10.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r10.u32 + r28.u32));
	// lwbrx r5,r8,r28
	ctx.r5.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r8.u32 + r28.u32));
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x82ba73c4
	if (cr6.eq) goto loc_82BA73C4;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpldi cr6,r8,0
	cr6.compare<uint64_t>(ctx.r8.u64, 0, xer);
	// stw r11,21624(r9)
	PPC_STORE_U32(ctx.r9.u32 + 21624, r11.u32);
	// beq cr6,0x82ba73c4
	if (cr6.eq) goto loc_82BA73C4;
	// cmpldi cr6,r7,0
	cr6.compare<uint64_t>(ctx.r7.u64, 0, xer);
	// beq cr6,0x82ba73c4
	if (cr6.eq) goto loc_82BA73C4;
	// li r11,1
	r11.s64 = 1;
	// cmpld cr6,r8,r7
	cr6.compare<uint64_t>(ctx.r8.u64, ctx.r7.u64, xer);
	// rldicr r11,r11,32,63
	r11.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bgt cr6,0x82ba73a4
	if (cr6.gt) goto loc_82BA73A4;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + r11.u64;
loc_82BA73A4:
	// cmpld cr6,r10,r8
	cr6.compare<uint64_t>(ctx.r10.u64, ctx.r8.u64, xer);
	// bgt cr6,0x82ba73b0
	if (cr6.gt) goto loc_82BA73B0;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
loc_82BA73B0:
	// subf r4,r7,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r7.s64;
	// subf r3,r8,r10
	ctx.r3.s64 = ctx.r10.s64 - ctx.r8.s64;
	// bl 0x82ba7230
	sub_82BA7230(ctx, base);
	// rlwimi r3,r6,0,0,15
	ctx.r3.u64 = (rotl32(ctx.r6.u32, 0) & 0xFFFF0000) | (ctx.r3.u64 & 0xFFFFFFFF0000FFFF);
	// stw r3,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r3.u32);
loc_82BA73C4:
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba76a4
	if (cr0.eq) goto loc_82BA76A4;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r9,r11,25304
	ctx.r9.s64 = r11.s64 + 25304;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,25304(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(25304) );
	// ld r10,40(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 40);
	// ld r8,-552(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + -552);
	// ld r7,48(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 48);
	// ld r6,-544(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + -544);
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// subf r7,r7,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r7.s64;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// subf r3,r11,r7
	ctx.r3.s64 = ctx.r7.s64 - r11.s64;
	// bl 0x82ba7230
	sub_82BA7230(ctx, base);
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// rlwimi r11,r3,16,0,15
	r11.u64 = (rotl32(ctx.r3.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r11.u32);
	// bl 0x82ba7230
	sub_82BA7230(ctx, base);
	// ld r11,160(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 160);
	// ld r10,-432(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + -432);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// ld r7,168(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 168);
	// ld r6,-424(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + -424);
	// subf r29,r11,r10
	r29.s64 = ctx.r10.s64 - r11.s64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// subf r3,r7,r6
	ctx.r3.s64 = ctx.r6.s64 - ctx.r7.s64;
	// bl 0x82ba7230
	sub_82BA7230(ctx, base);
	// ld r5,-416(r9)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r9.u32 + -416);
	// ld r31,176(r9)
	r31.u64 = PPC_LOAD_U64(ctx.r9.u32 + 176);
	// rlwimi r30,r3,16,0,15
	r30.u64 = (rotl32(ctx.r3.u32, 16) & 0xFFFF0000) | (r30.u64 & 0xFFFFFFFF0000FFFF);
	// subf r11,r5,r29
	r11.s64 = r29.s64 - ctx.r5.s64;
	// stw r30,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r30.u32);
	// subf r11,r6,r11
	r11.s64 = r11.s64 - ctx.r6.s64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r3,r11,r7
	ctx.r3.u64 = r11.u64 + ctx.r7.u64;
	// bl 0x82ba7230
	sub_82BA7230(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// subf r3,r31,r5
	ctx.r3.s64 = ctx.r5.s64 - r31.s64;
	// bl 0x82ba7230
	sub_82BA7230(ctx, base);
	// ld r11,480(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 480);
	// ld r10,-112(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + -112);
	// rlwimi r6,r3,16,0,15
	ctx.r6.u64 = (rotl32(ctx.r3.u32, 16) & 0xFFFF0000) | (ctx.r6.u64 & 0xFFFFFFFF0000FFFF);
	// ld r7,472(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 472);
	// lis r5,0
	ctx.r5.s64 = 0;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// ld r11,-120(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + -120);
	// lis r3,0
	ctx.r3.s64 = 0;
	// stw r6,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r6.u32);
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// ori r4,r5,65535
	ctx.r4.u64 = ctx.r5.u64 | 65535;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// ori r5,r3,43689
	ctx.r5.u64 = ctx.r3.u64 | 43689;
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// bge cr6,0x82ba74c0
	if (!cr6.lt) goto loc_82BA74C0;
	// li r11,0
	r11.s64 = 0;
	// b 0x82ba74f0
	goto loc_82BA74F0;
loc_82BA74C0:
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bne cr6,0x82ba74d0
	if (!cr6.eq) goto loc_82BA74D0;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x82ba74f0
	goto loc_82BA74F0;
loc_82BA74D0:
	// mulld r10,r11,r5
	ctx.r10.s64 = r11.s64 * ctx.r5.s64;
	// rotldi r11,r10,1
	r11.u64 = rotl64(ctx.r10.u64, 1);
	// divd r10,r10,r8
	ctx.r10.s64 = ctx.r10.s64 / ctx.r8.s64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// rotlwi r11,r10,0
	r11.u64 = rotl32(ctx.r10.u32, 0);
	// andc r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// tdllei r8,0
	// tdlgei r10,-1
loc_82BA74F0:
	// ld r10,488(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 488);
	// ld r7,-104(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + -104);
	// lwz r6,148(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(148) );
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwimi r11,r6,0,0,15
	r11.u64 = (rotl32(ctx.r6.u32, 0) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// bge cr6,0x82ba7514
	if (!cr6.lt) goto loc_82BA7514;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82ba7544
	goto loc_82BA7544;
loc_82BA7514:
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bne cr6,0x82ba7524
	if (!cr6.eq) goto loc_82BA7524;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// b 0x82ba7544
	goto loc_82BA7544;
loc_82BA7524:
	// mulld r7,r10,r5
	ctx.r7.s64 = ctx.r10.s64 * ctx.r5.s64;
	// rotldi r10,r7,1
	ctx.r10.u64 = rotl64(ctx.r7.u64, 1);
	// divd r7,r7,r8
	ctx.r7.s64 = ctx.r7.s64 / ctx.r8.s64;
	// addi r6,r10,-1
	ctx.r6.s64 = ctx.r10.s64 + -1;
	// rotlwi r10,r7,0
	ctx.r10.u64 = rotl32(ctx.r7.u32, 0);
	// andc r7,r8,r6
	ctx.r7.u64 = ctx.r8.u64 & ~ctx.r6.u64;
	// tdllei r8,0
	// tdlgei r7,-1
loc_82BA7544:
	// ld r7,272(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 272);
	// rlwimi r11,r10,16,0,15
	r11.u64 = (rotl32(ctx.r10.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// ld r6,-320(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + -320);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// subf r10,r7,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r7.s64;
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// bge cr6,0x82ba7568
	if (!cr6.lt) goto loc_82BA7568;
	// li r11,0
	r11.s64 = 0;
	// b 0x82ba7598
	goto loc_82BA7598;
loc_82BA7568:
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bne cr6,0x82ba7578
	if (!cr6.eq) goto loc_82BA7578;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x82ba7598
	goto loc_82BA7598;
loc_82BA7578:
	// mulld r10,r10,r5
	ctx.r10.s64 = ctx.r10.s64 * ctx.r5.s64;
	// rotldi r11,r10,1
	r11.u64 = rotl64(ctx.r10.u64, 1);
	// divd r10,r10,r8
	ctx.r10.s64 = ctx.r10.s64 / ctx.r8.s64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// rotlwi r11,r10,0
	r11.u64 = rotl32(ctx.r10.u32, 0);
	// andc r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// tdllei r8,0
	// tdlgei r10,-1
loc_82BA7598:
	// ld r10,128(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 128);
	// ld r7,-464(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + -464);
	// lwz r6,152(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(152) );
	// subf r10,r10,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwimi r11,r6,0,0,15
	r11.u64 = (rotl32(ctx.r6.u32, 0) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// bge cr6,0x82ba75bc
	if (!cr6.lt) goto loc_82BA75BC;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82ba75ec
	goto loc_82BA75EC;
loc_82BA75BC:
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bne cr6,0x82ba75cc
	if (!cr6.eq) goto loc_82BA75CC;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// b 0x82ba75ec
	goto loc_82BA75EC;
loc_82BA75CC:
	// mulld r7,r10,r5
	ctx.r7.s64 = ctx.r10.s64 * ctx.r5.s64;
	// rotldi r10,r7,1
	ctx.r10.u64 = rotl64(ctx.r7.u64, 1);
	// divd r7,r7,r8
	ctx.r7.s64 = ctx.r7.s64 / ctx.r8.s64;
	// addi r6,r10,-1
	ctx.r6.s64 = ctx.r10.s64 + -1;
	// rotlwi r10,r7,0
	ctx.r10.u64 = rotl32(ctx.r7.u32, 0);
	// andc r7,r8,r6
	ctx.r7.u64 = ctx.r8.u64 & ~ctx.r6.u64;
	// tdllei r8,0
	// tdlgei r7,-1
loc_82BA75EC:
	// ld r7,496(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 496);
	// rlwimi r11,r10,16,0,15
	r11.u64 = (rotl32(ctx.r10.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// ld r6,-96(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + -96);
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
	// subf r10,r7,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r7.s64;
	// rldicr r11,r10,1,62
	r11.u64 = rotl64(ctx.r10.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// cmpdi cr6,r11,0
	cr6.compare<int64_t>(r11.s64, 0, xer);
	// bge cr6,0x82ba7614
	if (!cr6.lt) goto loc_82BA7614;
	// li r11,0
	r11.s64 = 0;
	// b 0x82ba7644
	goto loc_82BA7644;
loc_82BA7614:
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bne cr6,0x82ba7624
	if (!cr6.eq) goto loc_82BA7624;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// b 0x82ba7644
	goto loc_82BA7644;
loc_82BA7624:
	// mulld r10,r11,r5
	ctx.r10.s64 = r11.s64 * ctx.r5.s64;
	// rotldi r11,r10,1
	r11.u64 = rotl64(ctx.r10.u64, 1);
	// divd r10,r10,r8
	ctx.r10.s64 = ctx.r10.s64 / ctx.r8.s64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// rotlwi r11,r10,0
	r11.u64 = rotl32(ctx.r10.u32, 0);
	// andc r10,r8,r7
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// tdllei r8,0
	// tdlgei r10,-1
loc_82BA7644:
	// ld r10,504(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 504);
	// ld r9,-88(r9)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r9.u32 + -88);
	// lwz r7,156(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(156) );
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// rlwimi r11,r7,0,0,15
	r11.u64 = (rotl32(ctx.r7.u32, 0) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// rldicr r10,r10,1,62
	ctx.r10.u64 = rotl64(ctx.r10.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// bge cr6,0x82ba766c
	if (!cr6.lt) goto loc_82BA766C;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x82ba769c
	goto loc_82BA769C;
loc_82BA766C:
	// cmpdi cr6,r8,0
	cr6.compare<int64_t>(ctx.r8.s64, 0, xer);
	// bne cr6,0x82ba767c
	if (!cr6.eq) goto loc_82BA767C;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// b 0x82ba769c
	goto loc_82BA769C;
loc_82BA767C:
	// mulld r9,r10,r5
	ctx.r9.s64 = ctx.r10.s64 * ctx.r5.s64;
	// rotldi r10,r9,1
	ctx.r10.u64 = rotl64(ctx.r9.u64, 1);
	// divd r9,r9,r8
	ctx.r9.s64 = ctx.r9.s64 / ctx.r8.s64;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// rotlwi r10,r9,0
	ctx.r10.u64 = rotl32(ctx.r9.u32, 0);
	// andc r9,r8,r7
	ctx.r9.u64 = ctx.r8.u64 & ~ctx.r7.u64;
	// tdllei r8,0
	// tdlgei r9,-1
loc_82BA769C:
	// rlwimi r11,r10,16,0,15
	r11.u64 = (rotl32(ctx.r10.u32, 16) & 0xFFFF0000) | (r11.u64 & 0xFFFFFFFF0000FFFF);
	// b 0x82ba76c4
	goto loc_82BA76C4;
loc_82BA76A4:
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// li r11,-1
	r11.s64 = -1;
	// oris r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 4294901760;
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r11.u32);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r11.u32);
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r11.u32);
loc_82BA76C4:
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r11.u32);
	// lis r3,17459
	ctx.r3.s64 = 1144193024;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// ori r3,r3,25703
	ctx.r3.u64 = ctx.r3.u64 | 25703;
	// bl 0x83004a90
	sub_83004A90(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA7298) {
	__imp__sub_82BA7298(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA76E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,32767
	r11.s64 = 2147418112;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// ori r11,r11,65535
	r11.u64 = r11.u64 | 65535;
	// cmplwi cr6,r31,18
	cr6.compare<uint32_t>(r31.u32, 18, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bgt cr6,0x82ba7bb0
	if (cr6.gt) goto loc_82BA7BB0;
	// lis r12,-32246
	r12.s64 = -2113273856;
	// addi r12,r12,1016
	r12.s64 = r12.s64 + 1016;
	// rlwinm r0,r31,1,0,30
	r0.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r0,r12,r0
	r0.u64 = PPC_LOAD_U16(r12.u32 + r0.u32);
	// lis r12,-32070
	r12.s64 = -2101739520;
	// addi r12,r12,30532
	r12.s64 = r12.s64 + 30532;
	// add r12,r12,r0
	r12.u64 = r12.u64 + r0.u64;
	// mtctr r12
	ctr.u64 = r12.u64;
	// nop 
	// bctr 
	switch (r31.u64) {
	case 0:
		goto loc_82BA7744;
	case 1:
		goto loc_82BA7764;
	case 2:
		goto loc_82BA7790;
	case 3:
		goto loc_82BA77A8;
	case 4:
		goto loc_82BA780C;
	case 5:
		goto loc_82BA7830;
	case 6:
		goto loc_82BA7880;
	case 7:
		goto loc_82BA7920;
	case 8:
		goto loc_82BA79A8;
	case 9:
		goto loc_82BA7A08;
	case 10:
		goto loc_82BA7A68;
	case 11:
		goto loc_82BA7A9C;
	case 12:
		goto loc_82BA7AE0;
	case 13:
		goto loc_82BA7B0C;
	case 14:
		goto loc_82BA7B34;
	case 15:
		goto loc_82BA7B5C;
	case 16:
		goto loc_82BA7B90;
	case 17:
		goto loc_82BA7910;
	case 18:
		goto loc_82BA7920;
	default:
		__builtin_unreachable();
	}
loc_82BA7744:
	// lwz r11,21576(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21576) );
	// lfs f0,21592(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 21592);
	f0.f64 = double(temp.f32);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// b 0x82ba7bb4
	goto loc_82BA7BB4;
loc_82BA7764:
	// lwz r11,21576(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21576) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// lfs f0,21588(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 21588);
	f0.f64 = double(temp.f32);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fdivs f1,f0,f13
	ctx.f1.f64 = double(float(f0.f64 / ctx.f13.f64));
	// b 0x82ba7bb4
	goto loc_82BA7BB4;
loc_82BA7790:
	// lwz r11,16560(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16560) );
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
loc_82BA779C:
	// fcfid f0,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(f0.s64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(f0.f64));
	// b 0x82ba7bb4
	goto loc_82BA7BB4;
loc_82BA77A8:
	// lwz r11,21616(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21616) );
	// lwz r9,21576(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21576) );
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// fcfid f11,f0
	ctx.f11.f64 = double(f0.s64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
loc_82BA77C8:
	// fcfid f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(ctx.f13.s64);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r11,r10,-27456
	r11.s64 = ctx.r10.s64 + -27456;
	// lfs f12,-12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,10656(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 10656);
	f0.f64 = double(temp.f32);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
loc_82BA77E0:
	// fdivs f13,f11,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f11.f64 / ctx.f13.f64));
loc_82BA77E4:
	// fmuls f13,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f13.f64 * f0.f64));
	// fcmpu cr6,f13,f12
	cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// bge cr6,0x82ba77f8
	if (!cr6.lt) goto loc_82BA77F8;
	// fmr f0,f12
	f0.f64 = ctx.f12.f64;
	// b 0x82ba7804
	goto loc_82BA7804;
loc_82BA77F8:
	// fcmpu cr6,f13,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x82ba7804
	if (cr6.gt) goto loc_82BA7804;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_82BA7804:
	// fmr f1,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f0.f64;
	// b 0x82ba7bb4
	goto loc_82BA7BB4;
loc_82BA780C:
	// lwz r9,21620(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21620) );
	// lwz r11,21576(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21576) );
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// frsp f11,f0
	ctx.f11.f64 = double(float(f0.f64));
	// b 0x82ba77c8
	goto loc_82BA77C8;
loc_82BA7830:
	// lwz r11,21576(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21576) );
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lwz r9,21620(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21620) );
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lwz r8,21616(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21616) );
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// subf r11,r8,r11
	r11.s64 = r11.s64 - ctx.r8.s64;
	// clrldi r11,r11,32
	r11.u64 = r11.u64 & 0xFFFFFFFF;
	// frsp f13,f0
	ctx.f13.f64 = double(float(f0.f64));
	// addi r10,r10,-27456
	ctx.r10.s64 = ctx.r10.s64 + -27456;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f11,f0
	ctx.f11.f64 = double(float(f0.f64));
	// lfs f0,10656(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 10656);
	f0.f64 = double(temp.f32);
	// lfs f12,-12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// b 0x82ba77e0
	goto loc_82BA77E0;
loc_82BA7880:
	// lwz r11,21624(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(21624) );
	// lwz r8,10896(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(10896) );
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// clrlwi r9,r11,29
	ctx.r9.u64 = r11.u32 & 0x7;
	// addi r6,r10,16
	ctx.r6.s64 = ctx.r10.s64 + 16;
	// clrlwi r10,r7,29
	ctx.r10.u64 = ctx.r7.u32 & 0x7;
	// rlwinm r7,r6,2,0,29
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwbrx r31,r7,r8
	r31.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32));
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// lwbrx r30,r10,r8
	r30.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r10.u32 + ctx.r8.u32));
	// lwbrx r10,r9,r8
	ctx.r10.u64 = __builtin_bswap32(PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32));
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// stw r11,21624(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21624, r11.u32);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// cmpldi cr6,r30,0
	cr6.compare<uint64_t>(r30.u64, 0, xer);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// li r11,1
	r11.s64 = 1;
	// cmpld cr6,r10,r30
	cr6.compare<uint64_t>(ctx.r10.u64, r30.u64, xer);
	// rldicr r11,r11,32,63
	r11.u64 = rotl64(r11.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// bgt cr6,0x82ba78f4
	if (cr6.gt) goto loc_82BA78F4;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
loc_82BA78F4:
	// cmpld cr6,r31,r10
	cr6.compare<uint64_t>(r31.u64, ctx.r10.u64, xer);
	// bgt cr6,0x82ba7900
	if (cr6.gt) goto loc_82BA7900;
	// add r31,r31,r11
	r31.u64 = r31.u64 + r11.u64;
loc_82BA7900:
	// subf r3,r10,r31
	ctx.r3.s64 = r31.s64 - ctx.r10.s64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// subf r3,r30,r31
	ctx.r3.s64 = r31.s64 - r30.s64;
	// b 0x82ba79e4
	goto loc_82BA79E4;
loc_82BA7910:
	// lwz r11,23348(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(23348) );
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x82ba779c
	goto loc_82BA779C;
loc_82BA7920:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r7,r11,25304
	ctx.r7.s64 = r11.s64 + 25304;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// cmpwi cr6,r31,7
	cr6.compare<int32_t>(r31.s32, 7, xer);
	// lwz r9,25304(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(25304) );
	// ld r11,40(r7)
	r11.u64 = PPC_LOAD_U64(ctx.r7.u32 + 40);
	// ld r10,-552(r7)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r7.u32 + -552);
	// ld r8,48(r7)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r7.u32 + 48);
	// ld r7,-544(r7)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r7.u32 + -544);
	// subf r31,r11,r10
	r31.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// subf r3,r8,r7
	ctx.r3.s64 = ctx.r7.s64 - ctx.r8.s64;
	// bne cr6,0x82ba796c
	if (!cr6.eq) goto loc_82BA796C;
	// ld r11,88(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// subf r3,r11,r3
	ctx.r3.s64 = ctx.r3.s64 - r11.s64;
loc_82BA796C:
	// cmpldi cr6,r3,0
	cr6.compare<uint64_t>(ctx.r3.u64, 0, xer);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// fdiv f11,f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = f31.f64 / ctx.f1.f64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r11,-27456
	ctx.r10.s64 = r11.s64 + -27456;
	// lfs f13,-27456(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27456);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,10656(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 10656);
	f0.f64 = double(temp.f32);
	// lfs f12,-12(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fsubs f13,f13,f11
	ctx.f13.f64 = static_cast<float>(ctx.f13.f64 - ctx.f11.f64);
	// b 0x82ba77e4
	goto loc_82BA77E4;
loc_82BA79A8:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r11,r11,25304
	r11.s64 = r11.s64 + 25304;
	// ld r10,160(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 160);
	// ld r9,-432(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + -432);
	// subf r31,r10,r9
	r31.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// ld r10,168(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 168);
	// ld r11,-424(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + -424);
loc_82BA79D8:
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BA79E4:
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// fdiv f13,f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = f31.f64 / ctx.f1.f64;
loc_82BA79F0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// frsp f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// addi r11,r11,-27456
	r11.s64 = r11.s64 + -27456;
	// lfs f12,-12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,10656(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 10656);
	f0.f64 = double(temp.f32);
	// b 0x82ba77e4
	goto loc_82BA77E4;
loc_82BA7A08:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r31,r11,25304
	r31.s64 = r11.s64 + 25304;
	// ld r11,160(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 160);
	// ld r10,-432(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + -432);
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// cmpldi cr6,r3,0
	cr6.compare<uint64_t>(ctx.r3.u64, 0, xer);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// ld r11,168(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 168);
	// ld r10,-424(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + -424);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// ld r11,176(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 176);
	// ld r10,-416(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + -416);
	// fsub f30,f31,f1
	ctx.fpscr.disableFlushMode();
	f30.f64 = f31.f64 - ctx.f1.f64;
	// subf r3,r11,r10
	ctx.r3.s64 = ctx.r10.s64 - r11.s64;
	// bl 0x82fffb40
	sub_82FFFB40(ctx, base);
	// fsub f13,f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = f30.f64 - ctx.f1.f64;
	// fdiv f13,f13,f31
	ctx.f13.f64 = ctx.f13.f64 / f31.f64;
	// b 0x82ba79f0
	goto loc_82BA79F0;
loc_82BA7A68:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r11,r11,25304
	r11.s64 = r11.s64 + 25304;
	// ld r10,160(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 160);
	// ld r9,-432(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + -432);
	// subf r31,r10,r9
	r31.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// beq cr6,0x82ba7bb0
	if (cr6.eq) goto loc_82BA7BB0;
	// ld r10,176(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 176);
	// ld r11,-416(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + -416);
	// b 0x82ba79d8
	goto loc_82BA79D8;
loc_82BA7A9C:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,25304
	ctx.r8.s64 = r11.s64 + 25304;
	// ld r11,480(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 480);
	// ld r10,-112(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -112);
	// ld r9,472(r8)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 472);
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - r11.s64;
	// ld r11,40(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 40);
	// ld r10,-552(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -552);
	// ld r8,-120(r8)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r8.u32 + -120);
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// add r3,r9,r8
	ctx.r3.u64 = ctx.r9.u64 + ctx.r8.u64;
loc_82BA7AD4:
	// subf r4,r11,r10
	ctx.r4.s64 = ctx.r10.s64 - r11.s64;
loc_82BA7AD8:
	// bl 0x82ba71c0
	sub_82BA71C0(ctx, base);
	// b 0x82ba7bb4
	goto loc_82BA7BB4;
loc_82BA7AE0:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,25304
	ctx.r8.s64 = r11.s64 + 25304;
	// ld r11,40(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 40);
	// ld r10,-552(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -552);
	// ld r9,488(r8)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 488);
	// ld r8,-104(r8)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r8.u32 + -104);
loc_82BA7B04:
	// subf r3,r9,r8
	ctx.r3.s64 = ctx.r8.s64 - ctx.r9.s64;
	// b 0x82ba7ad4
	goto loc_82BA7AD4;
loc_82BA7B0C:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,25304
	ctx.r8.s64 = r11.s64 + 25304;
	// ld r11,40(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 40);
	// ld r10,-552(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -552);
	// ld r9,272(r8)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 272);
	// ld r8,-320(r8)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r8.u32 + -320);
	// b 0x82ba7b04
	goto loc_82BA7B04;
loc_82BA7B34:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,25304
	ctx.r8.s64 = r11.s64 + 25304;
	// ld r11,40(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 40);
	// ld r10,-552(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -552);
	// ld r9,128(r8)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 128);
	// ld r8,-464(r8)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r8.u32 + -464);
	// b 0x82ba7b04
	goto loc_82BA7B04;
loc_82BA7B5C:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,25304
	ctx.r8.s64 = r11.s64 + 25304;
	// ld r11,496(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 496);
	// ld r10,-96(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -96);
loc_82BA7B78:
	// ld r9,40(r8)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r8.u32 + 40);
	// subf r11,r11,r10
	r11.s64 = ctx.r10.s64 - r11.s64;
	// ld r8,-552(r8)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r8.u32 + -552);
	// rldicr r3,r11,1,62
	ctx.r3.u64 = rotl64(r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// subf r4,r9,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r9.s64;
	// b 0x82ba7ad8
	goto loc_82BA7AD8;
loc_82BA7B90:
	// bl 0x82ba7048
	sub_82BA7048(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba7bb0
	if (cr0.eq) goto loc_82BA7BB0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,25304
	ctx.r8.s64 = r11.s64 + 25304;
	// ld r11,504(r8)
	r11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 504);
	// ld r10,-88(r8)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r8.u32 + -88);
	// b 0x82ba7b78
	goto loc_82BA7B78;
loc_82BA7BB0:
	// lfs f1,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
loc_82BA7BB4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// lfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA76E8) {
	__imp__sub_82BA76E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7BD8) {
	PPC_FUNC_PROLOGUE();
	// b 0x82b9b970
	sub_82B9B970(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82BA7BD8) {
	__imp__sub_82BA7BD8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7BE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x83004bb0
	sub_83004BB0(ctx, base);
	// rlwinm. r11,r3,0,26,26
	r11.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 0) & 0x20;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba7c0c
	if (cr0.eq) goto loc_82BA7C0C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba7298
	sub_82BA7298(ctx, base);
loc_82BA7C0C:
	// li r5,60
	ctx.r5.s64 = 60;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r8,16560(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16560) );
	// lis r9,-32070
	ctx.r9.s64 = -2101739520;
	// lbz r11,10943(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// lis r30,-32256
	r30.s64 = -2113929216;
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// addi r9,r9,31704
	ctx.r9.s64 = ctx.r9.s64 + 31704;
	// lis r7,-32070
	ctx.r7.s64 = -2101739520;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// li r3,7645
	ctx.r3.s64 = 7645;
	// stw r9,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r9.u32);
	// addi r9,r7,21160
	ctx.r9.s64 = ctx.r7.s64 + 21160;
	// li r8,4
	ctx.r8.s64 = 4;
	// lwz r10,2316(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(2316) );
	// lis r6,-32070
	ctx.r6.s64 = -2101739520;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// lis r5,-32070
	ctx.r5.s64 = -2101739520;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// lis r4,-32070
	ctx.r4.s64 = -2101739520;
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r8,r6,20920
	ctx.r8.s64 = ctx.r6.s64 + 20920;
	// stb r11,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, r11.u8);
	// addi r7,r5,21064
	ctx.r7.s64 = ctx.r5.s64 + 21064;
	// stw r3,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r3.u32);
	// addi r9,r4,25168
	ctx.r9.s64 = ctx.r4.s64 + 25168;
	// stw r8,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r8.u32);
	// stw r7,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r7.u32);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7cb8
	if (cr6.eq) goto loc_82BA7CB8;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,82
	ctx.r3.s64 = 82;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,2316(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(2316) );
	// b 0x82ba7cbc
	goto loc_82BA7CBC;
loc_82BA7CB8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82BA7CBC:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82ba7d08
	if (!cr6.eq) goto loc_82BA7D08;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2504(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2504) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7ce8
	if (cr6.eq) goto loc_82BA7CE8;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7d08
	if (cr6.eq) goto loc_82BA7D08;
	// b 0x82ba7cf8
	goto loc_82BA7CF8;
loc_82BA7CE8:
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7d08
	if (cr6.eq) goto loc_82BA7D08;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
loc_82BA7CF8:
	// lwz r4,16560(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16560) );
	// li r3,46
	ctx.r3.s64 = 46;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA7D08:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(120) );
	// lbz r10,10943(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10943);
	// lwz r9,21536(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21536) );
	// rlwinm r11,r11,30,2,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r8,124(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(124) );
	// andi. r10,r10,253
	ctx.r10.u64 = ctx.r10.u64 & 253;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// rlwimi r9,r11,30,1,1
	ctx.r9.u64 = (rotl32(r11.u32, 30) & 0x40000000) | (ctx.r9.u64 & 0xFFFFFFFFBFFFFFFF);
	// stb r10,10943(r31)
	PPC_STORE_U8(r31.u32 + 10943, ctx.r10.u8);
	// stw r8,21540(r31)
	PPC_STORE_U32(r31.u32 + 21540, ctx.r8.u32);
	// stw r9,21536(r31)
	PPC_STORE_U32(r31.u32 + 21536, ctx.r9.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA7BE0) {
	__imp__sub_82BA7BE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7D48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82ba7d6c
	if (!cr6.eq) goto loc_82BA7D6C;
	// lis r3,-32038
	ctx.r3.s64 = -2099642368;
	// ori r3,r3,23
	ctx.r3.u64 = ctx.r3.u64 | 23;
	// b 0x82ba7ddc
	goto loc_82BA7DDC;
loc_82BA7D6C:
	// lbz r11,4(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 4);
	// extsb r11,r11
	r11.s64 = r11.s8;
	// cmpwi cr6,r11,107
	cr6.compare<int32_t>(r11.s32, 107, xer);
	// bgt cr6,0x82ba7dbc
	if (cr6.gt) goto loc_82BA7DBC;
	// beq cr6,0x82ba7da8
	if (cr6.eq) goto loc_82BA7DA8;
	// cmpwi cr6,r11,97
	cr6.compare<int32_t>(r11.s32, 97, xer);
	// beq cr6,0x82ba7da8
	if (cr6.eq) goto loc_82BA7DA8;
	// cmpwi cr6,r11,98
	cr6.compare<int32_t>(r11.s32, 98, xer);
	// ble cr6,0x82ba7dd8
	if (!cr6.gt) goto loc_82BA7DD8;
	// cmpwi cr6,r11,100
	cr6.compare<int32_t>(r11.s32, 100, xer);
	// ble cr6,0x82ba7da8
	if (!cr6.gt) goto loc_82BA7DA8;
	// cmpwi cr6,r11,101
	cr6.compare<int32_t>(r11.s32, 101, xer);
	// ble cr6,0x82ba7dd8
	if (!cr6.gt) goto loc_82BA7DD8;
	// cmpwi cr6,r11,103
	cr6.compare<int32_t>(r11.s32, 103, xer);
	// bgt cr6,0x82ba7dd8
	if (cr6.gt) goto loc_82BA7DD8;
loc_82BA7DA8:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r4,r11,-17460
	ctx.r4.s64 = r11.s64 + -17460;
	// bl 0x832b301c
	__imp__sprintf(ctx, base);
	// b 0x82ba7dd8
	goto loc_82BA7DD8;
loc_82BA7DBC:
	// cmpwi cr6,r11,109
	cr6.compare<int32_t>(r11.s32, 109, xer);
	// beq cr6,0x82ba7dd0
	if (cr6.eq) goto loc_82BA7DD0;
	// cmpwi cr6,r11,116
	cr6.compare<int32_t>(r11.s32, 116, xer);
	// bne cr6,0x82ba7dd8
	if (!cr6.eq) goto loc_82BA7DD8;
	// b 0x82ba7da8
	goto loc_82BA7DA8;
loc_82BA7DD0:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x82ba10c0
	sub_82BA10C0(ctx, base);
loc_82BA7DD8:
	// lis r3,730
	ctx.r3.s64 = 47841280;
loc_82BA7DDC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA7D48) {
	__imp__sub_82BA7D48(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7DF0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2476(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2476) );
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82ba7fbc
	if (cr6.eq) goto loc_82BA7FBC;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(60) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7fbc
	if (cr6.eq) goto loc_82BA7FBC;
	// ld r11,10880(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 10880);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// beq cr6,0x82ba7fbc
	if (cr6.eq) goto loc_82BA7FBC;
	// cmplwi cr6,r3,224
	cr6.compare<uint32_t>(ctx.r3.u32, 224, xer);
	// bgt cr6,0x82ba7ed4
	if (cr6.gt) goto loc_82BA7ED4;
	// beq cr6,0x82ba7f84
	if (cr6.eq) goto loc_82BA7F84;
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// blt cr6,0x82ba7ec0
	if (cr6.lt) goto loc_82BA7EC0;
	// beq cr6,0x82ba7ec0
	if (cr6.eq) goto loc_82BA7EC0;
	// cmplwi cr6,r3,16
	cr6.compare<uint32_t>(ctx.r3.u32, 16, xer);
	// beq cr6,0x82ba7e94
	if (cr6.eq) goto loc_82BA7E94;
	// cmplwi cr6,r3,17
	cr6.compare<uint32_t>(ctx.r3.u32, 17, xer);
	// beq cr6,0x82ba7e60
	if (cr6.eq) goto loc_82BA7E60;
	// cmplwi cr6,r3,34
	cr6.compare<uint32_t>(ctx.r3.u32, 34, xer);
	// bne cr6,0x82ba7fbc
	if (!cr6.eq) goto loc_82BA7FBC;
	// stw r4,23356(r31)
	PPC_STORE_U32(r31.u32 + 23356, ctx.r4.u32);
	// b 0x82ba7fbc
	goto loc_82BA7FBC;
loc_82BA7E60:
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r4,6
	cr6.compare<uint32_t>(ctx.r4.u32, 6, xer);
	// slw r8,r10,r4
	ctx.r8.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r4.u8 & 0x3F));
	// lwz r11,28544(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(28544) );
	// or r11,r8,r11
	r11.u64 = ctx.r8.u64 | r11.u64;
	// stw r11,28544(r9)
	PPC_STORE_U32(ctx.r9.u32 + 28544, r11.u32);
	// bne cr6,0x82ba7fbc
	if (!cr6.eq) goto loc_82BA7FBC;
	// li r11,0
	r11.s64 = 0;
	// stw r10,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, ctx.r10.u32);
	// stw r11,21624(r31)
	PPC_STORE_U32(r31.u32 + 21624, r11.u32);
	// stw r11,21628(r31)
	PPC_STORE_U32(r31.u32 + 21628, r11.u32);
	// b 0x82ba7fbc
	goto loc_82BA7FBC;
loc_82BA7E94:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r4,6
	cr6.compare<uint32_t>(ctx.r4.u32, 6, xer);
	// slw r9,r11,r4
	ctx.r9.u64 = ctx.r4.u8 & 0x20 ? 0 : (r11.u32 << (ctx.r4.u8 & 0x3F));
	// lwz r11,28544(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(28544) );
	// andc r11,r11,r9
	r11.u64 = r11.u64 & ~ctx.r9.u64;
	// stw r11,28544(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28544, r11.u32);
	// bne cr6,0x82ba7fbc
	if (!cr6.eq) goto loc_82BA7FBC;
	// li r11,0
	r11.s64 = 0;
	// stw r11,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, r11.u32);
	// b 0x82ba7fbc
	goto loc_82BA7FBC;
loc_82BA7EC0:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,0
	r11.s64 = 0;
	// stw r11,21632(r31)
	PPC_STORE_U32(r31.u32 + 21632, r11.u32);
	// stw r11,28544(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28544, r11.u32);
	// b 0x82ba7fbc
	goto loc_82BA7FBC;
loc_82BA7ED4:
	// cmplwi cr6,r3,225
	cr6.compare<uint32_t>(ctx.r3.u32, 225, xer);
	// beq cr6,0x82ba7f84
	if (cr6.eq) goto loc_82BA7F84;
	// cmplwi cr6,r3,226
	cr6.compare<uint32_t>(ctx.r3.u32, 226, xer);
	// beq cr6,0x82ba7f84
	if (cr6.eq) goto loc_82BA7F84;
	// cmplwi cr6,r3,227
	cr6.compare<uint32_t>(ctx.r3.u32, 227, xer);
	// beq cr6,0x82ba7f78
	if (cr6.eq) goto loc_82BA7F78;
	// cmplwi cr6,r3,255
	cr6.compare<uint32_t>(ctx.r3.u32, 255, xer);
	// bne cr6,0x82ba7fbc
	if (!cr6.eq) goto loc_82BA7FBC;
	// lis r28,-31927
	r28.s64 = -2092367872;
	// lwz r11,28544(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28544) );
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba7f34
	if (cr0.eq) goto loc_82BA7F34;
	// lwz r11,21576(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21576) );
	// lfs f13,21592(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 21592);
	ctx.f13.f64 = double(temp.f32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r3,2
	ctx.r3.s64 = 131072;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f0,80(r1)
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	f0.f64 = double(f0.s64);
	// frsp f12,f0
	ctx.f12.f64 = double(float(f0.f64));
	// lfs f0,-17748(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -17748);
	f0.f64 = double(temp.f32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x83004908
	sub_83004908(ctx, base);
loc_82BA7F34:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// li r29,17
	r29.s64 = 17;
	// addi r11,r11,23056
	r11.s64 = r11.s64 + 23056;
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
loc_82BA7F44:
	// lwz r11,-8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-8) );
	// lwz r10,28544(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28544) );
	// and. r11,r11,r10
	r11.u64 = r11.u64 & ctx.r10.u64;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba7f68
	if (cr0.eq) goto loc_82BA7F68;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// bl 0x82ba76e8
	sub_82BA76E8(ctx, base);
	// lwz r3,-4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-4) );
	// bl 0x83004908
	sub_83004908(ctx, base);
loc_82BA7F68:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,12
	r30.s64 = r30.s64 + 12;
	// bne 0x82ba7f44
	if (!cr0.eq) goto loc_82BA7F44;
	// b 0x82ba7fbc
	goto loc_82BA7FBC;
loc_82BA7F78:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba6658
	sub_82BA6658(ctx, base);
	// b 0x82ba7fbc
	goto loc_82BA7FBC;
loc_82BA7F84:
	// lwz r11,21532(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21532) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba7fbc
	if (cr6.eq) goto loc_82BA7FBC;
	// bl 0x8221ee38
	sub_8221EE38(ctx, base);
	// lwz r11,10888(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10888) );
	// cmplw cr6,r11,r3
	cr6.compare<uint32_t>(r11.u32, ctx.r3.u32, xer);
	// bne cr6,0x82ba7fbc
	if (!cr6.eq) goto loc_82BA7FBC;
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba7fb8
	if (!cr6.gt) goto loc_82BA7FB8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA7FB8:
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
loc_82BA7FBC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BA7DF0) {
	__imp__sub_82BA7DF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA7FC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r31,-32256
	r31.s64 = -2113929216;
	// lwz r11,2504(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2504) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8008
	if (cr6.eq) goto loc_82BA8008;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8030
	if (cr6.eq) goto loc_82BA8030;
	// b 0x82ba801c
	goto loc_82BA801C;
loc_82BA8008:
	// lwz r11,2316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8030
	if (cr6.eq) goto loc_82BA8030;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
loc_82BA801C:
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// li r3,28
	ctx.r3.s64 = 28;
	// addi r4,r10,32072
	ctx.r4.s64 = ctx.r10.s64 + 32072;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA8030:
	// lis r11,-32070
	r11.s64 = -2101739520;
	// lwz r10,2316(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(2316) );
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// addi r11,r11,28728
	r11.s64 = r11.s64 + 28728;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8064
	if (cr6.eq) goto loc_82BA8064;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,66
	ctx.r3.s64 = 66;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA8064:
	// lwz r11,2316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba808c
	if (cr6.eq) goto loc_82BA808C;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// li r3,89
	ctx.r3.s64 = 89;
	// addi r4,r10,32240
	ctx.r4.s64 = ctx.r10.s64 + 32240;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA808C:
	// li r11,0
	r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,21624(r30)
	PPC_STORE_U32(r30.u32 + 21624, r11.u32);
	// stw r11,21628(r30)
	PPC_STORE_U32(r30.u32 + 21628, r11.u32);
	// stw r10,21632(r30)
	PPC_STORE_U32(r30.u32 + 21632, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA7FC8) {
	__imp__sub_82BA7FC8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA80B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r31,-32256
	r31.s64 = -2113929216;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,2316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(2316) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ba80fc
	if (cr6.eq) goto loc_82BA80FC;
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(24) );
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,28
	ctx.r3.s64 = 28;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,2316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(2316) );
loc_82BA80FC:
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// addi r10,r10,28728
	ctx.r10.s64 = ctx.r10.s64 + 28728;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ba8130
	if (cr6.eq) goto loc_82BA8130;
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(24) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,65
	ctx.r3.s64 = 65;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,2316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(2316) );
loc_82BA8130:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8154
	if (cr6.eq) goto loc_82BA8154;
	// lis r10,-32070
	ctx.r10.s64 = -2101739520;
	// lwz r11,24(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// li r3,90
	ctx.r3.s64 = 90;
	// addi r4,r10,32240
	ctx.r4.s64 = ctx.r10.s64 + 32240;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA8154:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA80B8) {
	__imp__sub_82BA80B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8170) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,21648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21648) );
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ba8230
	if (!cr6.eq) goto loc_82BA8230;
	// lwz r11,21656(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21656) );
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x82ba81a8
	if (!cr6.eq) goto loc_82BA81A8;
	// li r11,0
	r11.s64 = 0;
	// b 0x82ba81ac
	goto loc_82BA81AC;
loc_82BA81A8:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_82BA81AC:
	// lwz r10,21652(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21652) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ba8230
	if (cr6.eq) goto loc_82BA8230;
	// rlwinm r30,r11,1,0,30
	r30.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne cr6,0x82ba81c8
	if (!cr6.eq) goto loc_82BA81C8;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
loc_82BA81C8:
	// lwz r11,56(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x82ba81e0
	if (!cr6.gt) goto loc_82BA81E0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e8d20
	sub_821E8D20(ctx, base);
loc_82BA81E0:
	// lis r11,-16382
	r11.s64 = -1073610752;
	// lis r10,-32768
	ctx.r10.s64 = -2147483648;
	// ori r11,r11,22528
	r11.u64 = r11.u64 | 22528;
	// ori r10,r10,3
	ctx.r10.u64 = ctx.r10.u64 | 3;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// rlwinm r11,r30,2,0,29
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-8531
	ctx.r9.s64 = -559087616;
	// ori r8,r9,48879
	ctx.r8.u64 = ctx.r9.u64 | 48879;
	// stwu r10,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r10.u32);
	ctx.r3.u32 = ea;
	// lwz r10,21644(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21644) );
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r9,r11,12,20,31
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 12) & 0xFFF;
	// clrlwi r10,r11,3
	ctx.r10.u64 = r11.u32 & 0x1FFFFFFF;
	// addi r11,r9,512
	r11.s64 = ctx.r9.s64 + 512;
	// rlwinm r11,r11,0,19,19
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, r11.u32);
	ctx.r3.u32 = ea;
	// stwu r8,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r8.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r3.u32);
loc_82BA8230:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8170) {
	__imp__sub_82BA8170(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8248) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,21636(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21636) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba8284
	if (!cr6.eq) goto loc_82BA8284;
	// lis r4,-23680
	ctx.r4.s64 = -1551892480;
	// li r3,1952
	ctx.r3.s64 = 1952;
	// bl 0x8222ca28
	sub_8222CA28(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne 0x82ba8280
	if (!cr0.eq) goto loc_82BA8280;
loc_82BA8278:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x82ba831c
	goto loc_82BA831C;
loc_82BA8280:
	// stw r3,21636(r30)
	PPC_STORE_U32(r30.u32 + 21636, ctx.r3.u32);
loc_82BA8284:
	// lwz r11,21640(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21640) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba82a8
	if (!cr6.eq) goto loc_82BA82A8;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// li r3,64
	ctx.r3.s64 = 64;
	// bl 0x8222ca28
	sub_8222CA28(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq 0x82ba8278
	if (cr0.eq) goto loc_82BA8278;
	// stw r3,21640(r30)
	PPC_STORE_U32(r30.u32 + 21640, ctx.r3.u32);
loc_82BA82A8:
	// li r27,0
	r27.s64 = 0;
	// li r28,0
	r28.s64 = 0;
loc_82BA82B0:
	// lwz r11,21640(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21640) );
	// lwz r10,21636(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(21636) );
	// add. r31,r11,r28
	r31.u64 = r11.u64 + r28.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// add r29,r27,r10
	r29.u64 = r27.u64 + ctx.r10.u64;
	// bne 0x82ba82d8
	if (!cr0.eq) goto loc_82BA82D8;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8222ca28
	sub_8222CA28(ctx, base);
	// mr. r31,r3
	r31.u64 = ctx.r3.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// beq 0x82ba8308
	if (cr0.eq) goto loc_82BA8308;
loc_82BA82D8:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba8324
	if (cr6.eq) goto loc_82BA8324;
	// stw r29,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r29.u32);
loc_82BA82E4:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r3,480
	ctx.r4.s64 = ctx.r3.s64 + 480;
	// bl 0x821d11c8
	sub_821D11C8(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
loc_82BA8308:
	// addi r28,r28,16
	r28.s64 = r28.s64 + 16;
	// addi r27,r27,480
	r27.s64 = r27.s64 + 480;
	// cmpwi cr6,r28,64
	cr6.compare<int32_t>(r28.s32, 64, xer);
	// blt cr6,0x82ba82b0
	if (cr6.lt) goto loc_82BA82B0;
	// li r3,1
	ctx.r3.s64 = 1;
loc_82BA831C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BA8324:
	// lis r4,-23936
	ctx.r4.s64 = -1568669696;
	// li r3,480
	ctx.r3.s64 = 480;
	// bl 0x8222ca28
	sub_8222CA28(ctx, base);
	// cmplwi r3,0
	cr0.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// bne 0x82ba82e4
	if (!cr0.eq) goto loc_82BA82E4;
	// lis r4,9344
	ctx.r4.s64 = 612368384;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822023f0
	sub_822023F0(ctx, base);
	// b 0x82ba8308
	goto loc_82BA8308;
}

PPC_WEAK_FUNC(sub_82BA8248) {
	__imp__sub_82BA8248(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8350) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-368(r1)
	ea = -368 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// lwz r10,16720(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16720) );
	// rlwinm. r11,r10,0,21,21
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x400;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba8500
	if (cr0.eq) goto loc_82BA8500;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8394
	if (cr6.eq) goto loc_82BA8394;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// b 0x82ba8398
	goto loc_82BA8398;
loc_82BA8394:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82BA8398:
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,16724(r31)
	PPC_STORE_U32(r31.u32 + 16724, ctx.r3.u32);
loc_82BA83A8:
	// stw r30,16720(r31)
	PPC_STORE_U32(r31.u32 + 16720, r30.u32);
loc_82BA83AC:
	// lwz r11,21584(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21584) );
	// lwz r10,21580(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21580) );
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mftb r11
	r11.u64 = read_timestamp_counter();
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// cmpdi cr6,r10,0
	cr6.compare<int64_t>(ctx.r10.s64, 0, xer);
	// beq cr6,0x82ba83e0
	if (cr6.eq) goto loc_82BA83E0;
	// rotlwi r11,r11,0
	r11.u64 = rotl32(r11.u32, 0);
	// rotlwi r10,r10,0
	ctx.r10.u64 = rotl32(ctx.r10.u32, 0);
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// stw r11,21576(r31)
	PPC_STORE_U32(r31.u32 + 21576, r11.u32);
loc_82BA83E0:
	// ld r8,21600(r31)
	ctx.r8.u64 = PPC_LOAD_U64(r31.u32 + 21600);
	// ld r10,21608(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 21608);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// lwz r11,21648(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21648) );
	// stw r8,21616(r31)
	PPC_STORE_U32(r31.u32 + 21616, ctx.r8.u32);
	// std r30,21600(r31)
	PPC_STORE_U64(r31.u32 + 21600, r30.u64);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// stw r9,21584(r31)
	PPC_STORE_U32(r31.u32 + 21584, ctx.r9.u32);
	// stw r7,21580(r31)
	PPC_STORE_U32(r31.u32 + 21580, ctx.r7.u32);
	// stw r10,21620(r31)
	PPC_STORE_U32(r31.u32 + 21620, ctx.r10.u32);
	// std r30,21608(r31)
	PPC_STORE_U64(r31.u32 + 21608, r30.u64);
	// bne cr6,0x82ba856c
	if (!cr6.eq) goto loc_82BA856C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba8248
	sub_82BA8248(ctx, base);
	// clrlwi. r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba8564
	if (cr0.eq) goto loc_82BA8564;
	// li r5,240
	ctx.r5.s64 = 240;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r9,8
	ctx.r9.s64 = 8;
	// li r8,13
	ctx.r8.s64 = 13;
	// lbz r10,10942(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// stw r9,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r9.u32);
	// li r9,37
	ctx.r9.s64 = 37;
	// stw r8,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r8.u32);
	// li r11,19
	r11.s64 = 19;
	// stw r9,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r9.u32);
	// li r8,6
	ctx.r8.s64 = 6;
	// li r9,25
	ctx.r9.s64 = 25;
	// stw r11,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r11.u32);
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r8,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, ctx.r8.u32);
	// li r11,26
	r11.s64 = 26;
	// stw r9,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, ctx.r9.u32);
	// li r8,200
	ctx.r8.s64 = 200;
	// li r9,30
	ctx.r9.s64 = 30;
	// stw r30,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r30.u32);
	// clrlwi. r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// stw r11,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r11.u32);
	// stw r8,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r8.u32);
	// stw r9,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r9.u32);
	// bne 0x82ba84c4
	if (!cr0.eq) goto loc_82BA84C4;
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x82ba84b8
	if (cr6.eq) goto loc_82BA84B8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9b970
	sub_82B9B970(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x832b300c
	__imp__VdEnableDisableClockGating(ctx, base);
loc_82BA84B8:
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stb r11,10942(r31)
	PPC_STORE_U8(r31.u32 + 10942, r11.u8);
loc_82BA84C4:
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b99578
	sub_82B99578(ctx, base);
	// lwz r11,21644(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21644) );
	// stw r30,21652(r31)
	PPC_STORE_U32(r31.u32 + 21652, r30.u32);
	// stw r30,21656(r31)
	PPC_STORE_U32(r31.u32 + 21656, r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba84f4
	if (!cr6.eq) goto loc_82BA84F4;
	// lwz r11,21636(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21636) );
	// addi r11,r11,1920
	r11.s64 = r11.s64 + 1920;
	// stw r11,21644(r31)
	PPC_STORE_U32(r31.u32 + 21644, r11.u32);
loc_82BA84F4:
	// li r11,2
	r11.s64 = 2;
	// stw r11,21648(r31)
	PPC_STORE_U32(r31.u32 + 21648, r11.u32);
	// b 0x82ba85e4
	goto loc_82BA85E4;
loc_82BA8500:
	// rlwinm. r11,r10,0,23,23
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x100;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82ba83ac
	if (cr0.eq) goto loc_82BA83AC;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2316(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2316) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8524
	if (cr6.eq) goto loc_82BA8524;
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// b 0x82ba8528
	goto loc_82BA8528;
loc_82BA8524:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_82BA8528:
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// rlwinm r4,r10,20,4,11
	ctx.r4.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0xFF00000;
	// addi r3,r9,-17200
	ctx.r3.s64 = ctx.r9.s64 + -17200;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r3,16724(r31)
	PPC_STORE_U32(r31.u32 + 16724, ctx.r3.u32);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt 0x82ba83a8
	if (cr0.lt) goto loc_82BA83A8;
	// lwz r11,16720(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16720) );
	// li r10,1
	ctx.r10.s64 = 1;
	// rlwimi r11,r10,10,23,23
	r11.u64 = (rotl32(ctx.r10.u32, 10) & 0x100) | (r11.u64 & 0xFFFFFFFFFFFFFEFF);
	// rlwimi r11,r10,10,21,21
	r11.u64 = (rotl32(ctx.r10.u32, 10) & 0x400) | (r11.u64 & 0xFFFFFFFFFFFFFBFF);
	// stw r11,16720(r31)
	PPC_STORE_U32(r31.u32 + 16720, r11.u32);
	// b 0x82ba83ac
	goto loc_82BA83AC;
loc_82BA8564:
	// stw r30,21648(r31)
	PPC_STORE_U32(r31.u32 + 21648, r30.u32);
	// b 0x82ba85e4
	goto loc_82BA85E4;
loc_82BA856C:
	// cmplwi cr6,r11,2
	cr6.compare<uint32_t>(r11.u32, 2, xer);
	// bne cr6,0x82ba85e4
	if (!cr6.eq) goto loc_82BA85E4;
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// clrlwi. r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bne 0x82ba85ac
	if (!cr0.eq) goto loc_82BA85AC;
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// clrlwi r11,r11,31
	r11.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// beq cr6,0x82ba85a0
	if (cr6.eq) goto loc_82BA85A0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9b970
	sub_82B9B970(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x832b300c
	__imp__VdEnableDisableClockGating(ctx, base);
loc_82BA85A0:
	// lbz r11,10942(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 10942);
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stb r11,10942(r31)
	PPC_STORE_U8(r31.u32 + 10942, r11.u8);
loc_82BA85AC:
	// lwz r11,21656(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21656) );
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x82ba85bc
	if (cr6.eq) goto loc_82BA85BC;
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_82BA85BC:
	// lwz r11,21652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21652) );
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x82ba85e4
	if (cr6.eq) goto loc_82BA85E4;
	// lwz r10,21640(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(21640) );
	// rlwinm r11,r30,4,0,27
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 4) & 0xFFFFFFF0;
	// li r5,0
	ctx.r5.s64 = 0;
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b99c10
	sub_82B99C10(ctx, base);
	// stw r30,21656(r31)
	PPC_STORE_U32(r31.u32 + 21656, r30.u32);
loc_82BA85E4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba7be0
	sub_82BA7BE0(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r11,2504(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(2504) );
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8618
	if (cr6.eq) goto loc_82BA8618;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8618
	if (cr6.eq) goto loc_82BA8618;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA8618:
	// addi r1,r1,368
	ctx.r1.s64 = ctx.r1.s64 + 368;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8350) {
	__imp__sub_82BA8350(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8630) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-2192(r1)
	ea = -2192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x832b285c
	__imp__KeEnterCriticalRegion(ctx, base);
	// lis r28,-32256
	r28.s64 = -2113929216;
	// lwz r3,2580(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(2580) );
	// bl 0x832b227c
	__imp__RtlEnterCriticalSection(ctx, base);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// beq cr6,0x82ba8668
	if (cr6.eq) goto loc_82BA8668;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x832b316c
	__imp__VdRetrainEDRAMWorker(ctx, base);
loc_82BA8668:
	// li r11,0
	r11.s64 = 0;
	// li r8,2048
	ctx.r8.s64 = 2048;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b317c
	__imp__VdRetrainEDRAM(ctx, base);
	// cmpwi r3,0
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq 0x82ba86fc
	if (cr0.eq) goto loc_82BA86FC;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x82ba86a8
	if (!cr6.eq) goto loc_82BA86A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b9b970
	sub_82B9B970(ctx, base);
loc_82BA86A8:
	// li r4,4096
	ctx.r4.s64 = 4096;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82213c10
	sub_82213C10(ctx, base);
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// li r8,2048
	ctx.r8.s64 = 2048;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x832b317c
	__imp__VdRetrainEDRAM(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r11.u32);
	// bl 0x82b9b970
	sub_82B9B970(ctx, base);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x82ba86a8
	if (!cr6.eq) goto loc_82BA86A8;
loc_82BA86FC:
	// lwz r3,2580(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(2580) );
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// bl 0x832b283c
	__imp__KeLeaveCriticalRegion(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,2192
	ctx.r1.s64 = ctx.r1.s64 + 2192;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BA8630) {
	__imp__sub_82BA8630(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8718) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r11,8088(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8088) );
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82ba8778
	if (!cr6.eq) goto loc_82BA8778;
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// lis r9,-31924
	ctx.r9.s64 = -2092171264;
	// stw r11,8088(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8088, r11.u32);
	// addi r31,r9,-5536
	r31.s64 = ctx.r9.s64 + -5536;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba8bb0
	sub_82BA8BB0(ctx, base);
	// lis r8,-31957
	ctx.r8.s64 = -2094333952;
	// addi r3,r8,-1568
	ctx.r3.s64 = ctx.r8.s64 + -1568;
	// bl 0x82ca3700
	sub_82CA3700(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82BA8778:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// addi r3,r11,-5536
	ctx.r3.s64 = r11.s64 + -5536;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8718) {
	__imp__sub_82BA8718(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8798) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// sth r11,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r11.u16);
	// lhz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r4.u32 + 4);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82ba87e4
	if (cr6.eq) goto loc_82BA87E4;
	// lwz r4,0(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82ba87dc
	if (!cr6.eq) goto loc_82BA87DC;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r4,r11,3224
	ctx.r4.s64 = r11.s64 + 3224;
loc_82BA87DC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
loc_82BA87E4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8798) {
	__imp__sub_82BA8798(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8800) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ba8844
	if (cr6.eq) goto loc_82BA8844;
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// sth r11,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r11.u16);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_82BA8844:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BA8800) {
	__imp__sub_82BA8800(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8850) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// bne cr6,0x82ba8890
	if (!cr6.eq) goto loc_82BA8890;
	// mr r11,r27
	r11.u64 = r27.u64;
loc_82BA8874:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ba8874
	if (!cr6.eq) goto loc_82BA8874;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r30,r11,0
	r30.u64 = rotl32(r11.u32, 0);
loc_82BA8890:
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// li r29,0
	r29.s64 = 0;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bge cr6,0x82ba8908
	if (!cr6.lt) goto loc_82BA8908;
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ba88cc
	if (cr6.eq) goto loc_82BA88CC;
	// addi r26,r11,1
	r26.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r29,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r29.u16);
	// sth r29,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r29.u16);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
loc_82BA88CC:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r28,r11,-17176
	r28.s64 = r11.s64 + -17176;
	// addi r26,r10,-17072
	r26.s64 = ctx.r10.s64 + -17072;
	// addi r25,r30,1
	r25.s64 = r30.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r7,110
	ctx.r7.s64 = 110;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// clrlwi r9,r30,16
	ctx.r9.u64 = r30.u32 & 0xFFFF;
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// sth r9,6(r31)
	PPC_STORE_U16(r31.u32 + 6, ctx.r9.u16);
	// sth r9,4(r31)
	PPC_STORE_U16(r31.u32 + 4, ctx.r9.u16);
loc_82BA8908:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba892c
	if (cr6.eq) goto loc_82BA892C;
	// clrlwi r5,r30,16
	ctx.r5.u64 = r30.u32 & 0xFFFF;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// sth r5,4(r31)
	PPC_STORE_U16(r31.u32 + 4, ctx.r5.u16);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// stbx r29,r11,r30
	PPC_STORE_U8(r11.u32 + r30.u32, r29.u8);
loc_82BA892C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BA8850) {
	__imp__sub_82BA8850(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8938) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lhz r11,4(r30)
	r11.u64 = PPC_LOAD_U16(r30.u32 + 4);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// ble cr6,0x82ba8964
	if (!cr6.gt) goto loc_82BA8964;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
loc_82BA8964:
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x82ba8970
	if (!cr6.eq) goto loc_82BA8970;
	// subf r31,r4,r11
	r31.s64 = r11.s64 - ctx.r4.s64;
loc_82BA8970:
	// sth r31,4(r30)
	PPC_STORE_U16(r30.u32 + 4, r31.u16);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// add r4,r3,r4
	ctx.r4.u64 = ctx.r3.u64 + ctx.r4.u64;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stbx r10,r9,r31
	PPC_STORE_U8(ctx.r9.u32 + r31.u32, ctx.r10.u8);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8938) {
	__imp__sub_82BA8938(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA89B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// cmpwi cr6,r25,-1
	cr6.compare<int32_t>(r25.s32, -1, xer);
	// bne cr6,0x82ba89f0
	if (!cr6.eq) goto loc_82BA89F0;
	// mr r11,r24
	r11.u64 = r24.u64;
loc_82BA89D4:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ba89d4
	if (!cr6.eq) goto loc_82BA89D4;
	// subf r11,r24,r11
	r11.s64 = r11.s64 - r24.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r25,r11,0
	r25.u64 = rotl32(r11.u32, 0);
loc_82BA89F0:
	// cmpwi cr6,r25,0
	cr6.compare<int32_t>(r25.s32, 0, xer);
	// ble cr6,0x82ba8ab8
	if (!cr6.gt) goto loc_82BA8AB8;
	// lhz r11,4(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// li r29,0
	r29.s64 = 0;
	// lhz r10,6(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// add r30,r11,r25
	r30.u64 = r11.u64 + r25.u64;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// ble cr6,0x82ba8a94
	if (!cr6.gt) goto loc_82BA8A94;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r28,r11,-17176
	r28.s64 = r11.s64 + -17176;
	// addi r27,r10,-17072
	r27.s64 = ctx.r10.s64 + -17072;
	// addi r26,r30,1
	r26.s64 = r30.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r7,141
	ctx.r7.s64 = 141;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// lhz r26,4(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lhz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// stbx r29,r9,r28
	PPC_STORE_U8(ctx.r9.u32 + r28.u32, r29.u8);
	// lwz r27,0(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82ba8a88
	if (cr6.eq) goto loc_82BA8A88;
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r23,r11,1
	r23.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r29,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r29.u16);
	// sth r29,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r29.u16);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
loc_82BA8A88:
	// sth r26,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r26.u16);
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// sth r30,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r30.u16);
loc_82BA8A94:
	// lhz r11,4(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// sth r30,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r30.u16);
	// stbx r29,r10,r30
	PPC_STORE_U8(ctx.r10.u32 + r30.u32, r29.u8);
loc_82BA8AB8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c24
	return;
}

PPC_WEAK_FUNC(sub_82BA89B0) {
	__imp__sub_82BA89B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8AC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82ba8b40
	sub_82BA8B40(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba8b08
	if (cr6.eq) goto loc_82BA8B08;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
loc_82BA8B08:
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// lhz r5,4(r7)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r7.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba8b20
	if (!cr6.eq) goto loc_82BA8B20;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r11,r11,3224
	r11.s64 = r11.s64 + 3224;
loc_82BA8B20:
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8AC8) {
	__imp__sub_82BA8AC8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8B40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lhz r11,4(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 4);
	// lhz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82ba8b58
	if (cr6.eq) goto loc_82BA8B58;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82BA8B58:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba8b68
	if (!cr6.eq) goto loc_82BA8B68;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_82BA8B68:
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82ba8b7c
	if (!cr6.eq) goto loc_82BA8B7C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r10,r11,3224
	ctx.r10.s64 = r11.s64 + 3224;
loc_82BA8B7C:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
loc_82BA8B80:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x82ba8ba4
	if (cr6.eq) goto loc_82BA8BA4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x82ba8b80
	if (cr6.eq) goto loc_82BA8B80;
loc_82BA8BA4:
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8B40) {
	__imp__sub_82BA8B40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8BB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// li r29,11
	r29.s64 = 11;
	// addi r27,r11,-5536
	r27.s64 = r11.s64 + -5536;
	// li r30,0
	r30.s64 = 0;
	// addi r31,r27,36
	r31.s64 = r27.s64 + 36;
	// lis r28,-31927
	r28.s64 = -2092367872;
loc_82BA8BD4:
	// stw r30,-4(r31)
	PPC_STORE_U32(r31.u32 + -4, r30.u32);
	// lis r11,-32069
	r11.s64 = -2101673984;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// li r5,2
	ctx.r5.s64 = 2;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// addi r6,r11,-28032
	ctx.r6.s64 = r11.s64 + -28032;
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// li r4,16
	ctx.r4.s64 = 16;
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// addi r3,r31,20
	ctx.r3.s64 = r31.s64 + 20;
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// bl 0x8221fc28
	sub_8221FC28(ctx, base);
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r6,r10,-28032
	ctx.r6.s64 = ctx.r10.s64 + -28032;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,52
	ctx.r3.s64 = r31.s64 + 52;
	// bl 0x8221fc28
	sub_8221FC28(ctx, base);
	// lwz r11,28560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28560) );
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// addi r31,r31,120
	r31.s64 = r31.s64 + 120;
	// stw r11,28560(r28)
	PPC_STORE_U32(r28.u32 + 28560, r11.u32);
	// bge 0x82ba8bd4
	if (!cr0.lt) goto loc_82BA8BD4;
	// addi r3,r27,1472
	ctx.r3.s64 = r27.s64 + 1472;
	// bl 0x82ba8ce8
	sub_82BA8CE8(ctx, base);
	// li r31,11
	r31.s64 = 11;
	// addi r29,r27,1600
	r29.s64 = r27.s64 + 1600;
loc_82BA8C44:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ba8ce8
	sub_82BA8CE8(ctx, base);
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r29,r29,128
	r29.s64 = r29.s64 + 128;
	// bge 0x82ba8c44
	if (!cr0.lt) goto loc_82BA8C44;
	// li r10,511
	ctx.r10.s64 = 511;
	// addi r11,r27,3144
	r11.s64 = r27.s64 + 3144;
loc_82BA8C60:
	// stw r30,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, r30.u32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// stw r30,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, r30.u32);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bge 0x82ba8c60
	if (!cr0.lt) goto loc_82BA8C60;
	// lis r8,-31927
	ctx.r8.s64 = -2092367872;
	// lwz r11,28548(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(28548) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba8cb8
	if (!cr6.eq) goto loc_82BA8CB8;
	// lis r11,-32054
	r11.s64 = -2100690944;
	// lis r10,-32053
	ctx.r10.s64 = -2100625408;
	// lis r9,-32054
	ctx.r9.s64 = -2100690944;
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// lis r6,-31927
	ctx.r6.s64 = -2092367872;
	// addi r11,r11,15464
	r11.s64 = r11.s64 + 15464;
	// addi r10,r10,-31376
	ctx.r10.s64 = ctx.r10.s64 + -31376;
	// addi r9,r9,24000
	ctx.r9.s64 = ctx.r9.s64 + 24000;
	// stw r11,28548(r8)
	PPC_STORE_U32(ctx.r8.u32 + 28548, r11.u32);
	// stw r10,28552(r7)
	PPC_STORE_U32(ctx.r7.u32 + 28552, ctx.r10.u32);
	// stw r9,28556(r6)
	PPC_STORE_U32(ctx.r6.u32 + 28556, ctx.r9.u32);
loc_82BA8CB8:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82baa948
	sub_82BAA948(ctx, base);
	// li r5,8192
	ctx.r5.s64 = 8192;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r27,3136
	ctx.r3.s64 = r27.s64 + 3136;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r11,28560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28560) );
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r11,r11,8192
	r11.s64 = r11.s64 + 8192;
	// stw r11,28560(r28)
	PPC_STORE_U32(r28.u32 + 28560, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA8BB0) {
	__imp__sub_82BA8BB0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8CE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// li r11,0
	r11.s64 = 0;
	// addi r10,r3,64
	ctx.r10.s64 = ctx.r3.s64 + 64;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// stw r11,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, r11.u32);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, r11.u32);
	// stw r11,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, r11.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// stw r11,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r11.u32);
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, r11.u32);
	// stw r11,40(r3)
	PPC_STORE_U32(ctx.r3.u32 + 40, r11.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, r11.u32);
	// stw r11,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, r11.u32);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, r11.u32);
	// stw r11,56(r3)
	PPC_STORE_U32(ctx.r3.u32 + 56, r11.u32);
	// stw r11,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, r11.u32);
	// lwz r10,28560(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(28560) );
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// stw r10,28560(r9)
	PPC_STORE_U32(ctx.r9.u32 + 28560, ctx.r10.u32);
	// stw r11,64(r3)
	PPC_STORE_U32(ctx.r3.u32 + 64, r11.u32);
	// stw r11,68(r3)
	PPC_STORE_U32(ctx.r3.u32 + 68, r11.u32);
	// stw r11,72(r3)
	PPC_STORE_U32(ctx.r3.u32 + 72, r11.u32);
	// stw r11,76(r3)
	PPC_STORE_U32(ctx.r3.u32 + 76, r11.u32);
	// stw r11,80(r3)
	PPC_STORE_U32(ctx.r3.u32 + 80, r11.u32);
	// stw r11,84(r3)
	PPC_STORE_U32(ctx.r3.u32 + 84, r11.u32);
	// stw r11,88(r3)
	PPC_STORE_U32(ctx.r3.u32 + 88, r11.u32);
	// stw r11,92(r3)
	PPC_STORE_U32(ctx.r3.u32 + 92, r11.u32);
	// stw r11,96(r3)
	PPC_STORE_U32(ctx.r3.u32 + 96, r11.u32);
	// stw r11,100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 100, r11.u32);
	// stw r11,104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 104, r11.u32);
	// stw r11,108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 108, r11.u32);
	// stw r11,112(r3)
	PPC_STORE_U32(ctx.r3.u32 + 112, r11.u32);
	// stw r11,116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 116, r11.u32);
	// stw r11,120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 120, r11.u32);
	// stw r11,124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 124, r11.u32);
	// lwz r11,28560(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(28560) );
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stw r11,28560(r9)
	PPC_STORE_U32(ctx.r9.u32 + 28560, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA8CE8) {
	__imp__sub_82BA8CE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8D90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// addi r10,r10,28564
	ctx.r10.s64 = ctx.r10.s64 + 28564;
	// addi r25,r11,-5536
	r25.s64 = r11.s64 + -5536;
	// li r30,0
	r30.s64 = 0;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r25,r11
	cr6.compare<uint32_t>(r25.u32, r11.u32, xer);
	// beq cr6,0x82ba8de4
	if (cr6.eq) goto loc_82BA8DE4;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BA8DC4:
	// addi r8,r10,20
	ctx.r8.s64 = ctx.r10.s64 + 20;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x82ba8de4
	if (!cr6.lt) goto loc_82BA8DE4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r25,r8
	cr6.compare<uint32_t>(r25.u32, ctx.r8.u32, xer);
	// bne cr6,0x82ba8dc4
	if (!cr6.eq) goto loc_82BA8DC4;
loc_82BA8DE4:
	// rlwinm r11,r9,2,0,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmplw cr6,r25,r9
	cr6.compare<uint32_t>(r25.u32, ctx.r9.u32, xer);
	// bne cr6,0x82ba8df8
	if (!cr6.eq) goto loc_82BA8DF8;
	// stwx r30,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, r30.u32);
loc_82BA8DF8:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// addi r31,r25,36
	r31.s64 = r25.s64 + 36;
	// lis r23,-31927
	r23.s64 = -2092367872;
	// addi r24,r11,-7176
	r24.s64 = r11.s64 + -7176;
loc_82BA8E08:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r28,r31,-4
	r28.s64 = r31.s64 + -4;
	// lwz r10,-4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-4) );
	// addi r27,r31,8
	r27.s64 = r31.s64 + 8;
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mullw r26,r11,r10
	r26.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba8e4c
	if (cr6.eq) goto loc_82BA8E4C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82baa5d8
	sub_82BAA5D8(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// lwz r11,28556(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(28556) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA8E4C:
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// addi r11,r25,1476
	r11.s64 = r25.s64 + 1476;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// addi r31,r31,120
	r31.s64 = r31.s64 + 120;
	// stw r30,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r30.u32);
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x82ba8e08
	if (cr6.lt) goto loc_82BA8E08;
	// lis r27,-31927
	r27.s64 = -2092367872;
	// li r10,11
	ctx.r10.s64 = 11;
	// lwz r11,28560(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(28560) );
loc_82BA8E80:
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// bge 0x82ba8e80
	if (!cr0.lt) goto loc_82BA8E80;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// li r29,11
	r29.s64 = 11;
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// addi r31,r25,1476
	r31.s64 = r25.s64 + 1476;
	// stw r11,28560(r27)
	PPC_STORE_U32(r27.u32 + 28560, r11.u32);
loc_82BA8EA4:
	// addi r31,r31,-120
	r31.s64 = r31.s64 + -120;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,-4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-4) );
	// lwz r28,8(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mullw r26,r11,r10
	r26.s64 = int64_t(r11.s32) * int64_t(ctx.r10.s32);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ba8ee4
	if (cr6.eq) goto loc_82BA8EE4;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82baa5d8
	sub_82BAA5D8(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// lwz r11,28556(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(28556) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA8EE4:
	// stw r30,-4(r31)
	PPC_STORE_U32(r31.u32 + -4, r30.u32);
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// lwz r11,28560(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(28560) );
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// stw r11,28560(r27)
	PPC_STORE_U32(r27.u32 + 28560, r11.u32);
	// bge 0x82ba8ea4
	if (!cr0.lt) goto loc_82BA8EA4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c24
	return;
}

PPC_WEAK_FUNC(sub_82BA8D90) {
	__imp__sub_82BA8D90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA8F18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// cmpwi cr6,r28,511
	cr6.compare<int32_t>(r28.s32, 511, xer);
	// mr r11,r28
	r11.u64 = r28.u64;
	// blt cr6,0x82ba8f44
	if (cr6.lt) goto loc_82BA8F44;
	// li r11,511
	r11.s64 = 511;
loc_82BA8F44:
	// addi r11,r11,196
	r11.s64 = r11.s64 + 196;
	// rlwinm r11,r11,4,0,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stw r10,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r10.u32);
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// ble cr6,0x82ba8f78
	if (!cr6.gt) goto loc_82BA8F78;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
loc_82BA8F78:
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// li r31,0
	r31.s64 = 0;
	// addi r11,r30,32
	r11.s64 = r30.s64 + 32;
loc_82BA8F84:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmpw cr6,r28,r10
	cr6.compare<int32_t>(r28.s32, ctx.r10.s32, xer);
	// ble cr6,0x82ba8fe4
	if (!cr6.gt) goto loc_82BA8FE4;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r11,r11,120
	r11.s64 = r11.s64 + 120;
	// cmpwi cr6,r31,12
	cr6.compare<int32_t>(r31.s32, 12, xer);
	// blt cr6,0x82ba8f84
	if (cr6.lt) goto loc_82BA8F84;
loc_82BA8FA0:
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82baa4a8
	sub_82BAA4A8(ctx, base);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r30,1536
	ctx.r3.s64 = r30.s64 + 1536;
	// bl 0x82baa6f0
	sub_82BAA6F0(ctx, base);
	// cmpwi cr6,r31,12
	cr6.compare<int32_t>(r31.s32, 12, xer);
	// bge cr6,0x82ba8fd8
	if (!cr6.lt) goto loc_82BA8FD8;
	// addi r11,r31,13
	r11.s64 = r31.s64 + 13;
	// rlwinm r11,r11,7,0,24
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// bl 0x82baa6f0
	sub_82BAA6F0(ctx, base);
loc_82BA8FD8:
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BA8FE4:
	// mulli r11,r31,120
	r11.s64 = r31.s64 * 120;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// lwz r29,20(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba900c
	if (cr6.eq) goto loc_82BA900C;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r10,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r10.u32);
	// b 0x82ba9030
	goto loc_82BA9030;
loc_82BA900C:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// lwz r29,16(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// blt cr6,0x82ba9024
	if (cr6.lt) goto loc_82BA9024;
	// li r29,0
	r29.s64 = 0;
	// b 0x82ba9038
	goto loc_82BA9038;
loc_82BA9024:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// add r10,r29,r4
	ctx.r10.u64 = r29.u64 + ctx.r4.u64;
	// stw r10,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r10.u32);
loc_82BA9030:
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// bl 0x82baa6f0
	sub_82BAA6F0(ctx, base);
loc_82BA9038:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba8fa0
	if (cr6.eq) goto loc_82BA8FA0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r30,1472
	ctx.r3.s64 = r30.s64 + 1472;
	// bl 0x82baa6f0
	sub_82BAA6F0(ctx, base);
	// rlwinm r11,r31,7,0,24
	r11.u64 = rotl64(r31.u32 | (r31.u64 << 32), 7) & 0xFFFFFF80;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r3,r11,1600
	ctx.r3.s64 = r11.s64 + 1600;
	// bl 0x82baa6f0
	sub_82BAA6F0(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA8F18) {
	__imp__sub_82BA8F18(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9068) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82ba91b4
	if (cr6.eq) goto loc_82BA91B4;
	// cmpwi cr6,r27,511
	cr6.compare<int32_t>(r27.s32, 511, xer);
	// mr r11,r27
	r11.u64 = r27.u64;
	// blt cr6,0x82ba9098
	if (cr6.lt) goto loc_82BA9098;
	// li r11,511
	r11.s64 = 511;
loc_82BA9098:
	// addi r11,r11,196
	r11.s64 = r11.s64 + 196;
	// li r29,0
	r29.s64 = 0;
	// rlwinm r11,r11,4,0,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r8,r30,44
	ctx.r8.s64 = r30.s64 + 44;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r10,-1
	ctx.r7.s64 = ctx.r10.s64 + -1;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// stw r7,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r7.u32);
loc_82BA90C4:
	// lwz r11,-4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(-4) );
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bge cr6,0x82ba90e0
	if (!cr6.lt) goto loc_82BA90E0;
	// lwz r11,0(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// li r11,1
	r11.s64 = 1;
	// bge cr6,0x82ba90e4
	if (!cr6.lt) goto loc_82BA90E4;
loc_82BA90E0:
	// li r11,0
	r11.s64 = 0;
loc_82BA90E4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82ba9150
	if (!cr6.eq) goto loc_82BA9150;
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r8,r8,120
	ctx.r8.s64 = ctx.r8.s64 + 120;
	// cmpwi cr6,r29,12
	cr6.compare<int32_t>(r29.s32, 12, xer);
	// blt cr6,0x82ba90c4
	if (cr6.lt) goto loc_82BA90C4;
loc_82BA9100:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82baa570
	sub_82BAA570(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r30,1536
	ctx.r3.s64 = r30.s64 + 1536;
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// lwz r11,1352(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(1352) );
	// cmpw cr6,r27,r11
	cr6.compare<int32_t>(r27.s32, r11.s32, xer);
	// bgt cr6,0x82ba91b4
	if (cr6.gt) goto loc_82BA91B4;
	// li r11,0
	r11.s64 = 0;
	// addi r10,r30,32
	ctx.r10.s64 = r30.s64 + 32;
loc_82BA912C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmpw cr6,r27,r9
	cr6.compare<int32_t>(r27.s32, ctx.r9.s32, xer);
	// ble cr6,0x82ba91a0
	if (!cr6.gt) goto loc_82BA91A0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,120
	ctx.r10.s64 = ctx.r10.s64 + 120;
	// cmpwi cr6,r11,12
	cr6.compare<int32_t>(r11.s32, 12, xer);
	// blt cr6,0x82ba912c
	if (cr6.lt) goto loc_82BA912C;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BA9150:
	// cmpwi cr6,r29,12
	cr6.compare<int32_t>(r29.s32, 12, xer);
	// bge cr6,0x82ba9100
	if (!cr6.lt) goto loc_82BA9100;
	// mulli r11,r29,120
	r11.s64 = r29.s64 * 120;
	// add r31,r11,r30
	r31.u64 = r11.u64 + r30.u64;
	// addi r11,r31,32
	r11.s64 = r31.s64 + 32;
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// lwz r4,32(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(52) );
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r30,1472
	ctx.r3.s64 = r30.s64 + 1472;
	// stw r11,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r11.u32);
	// stw r28,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r28.u32);
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// rlwinm r11,r29,7,0,24
	r11.u64 = rotl64(r29.u32 | (r29.u64 << 32), 7) & 0xFFFFFF80;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// addi r3,r11,1600
	ctx.r3.s64 = r11.s64 + 1600;
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BA91A0:
	// addi r11,r11,13
	r11.s64 = r11.s64 + 13;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r11,r11,7,0,24
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 7) & 0xFFFFFF80;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
loc_82BA91B4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA9068) {
	__imp__sub_82BA9068(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA91C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82ba9208
	if (!cr6.eq) goto loc_82BA9208;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// addi r6,r11,-10856
	ctx.r6.s64 = r11.s64 + -10856;
	// addi r5,r10,-6812
	ctx.r5.s64 = ctx.r10.s64 + -6812;
	// li r7,848
	ctx.r7.s64 = 848;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BA9208:
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// bne cr6,0x82ba9228
	if (!cr6.eq) goto loc_82BA9228;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BA9228:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// addi r6,r11,-10856
	ctx.r6.s64 = r11.s64 + -10856;
	// addi r5,r10,-6812
	ctx.r5.s64 = ctx.r10.s64 + -6812;
	// li r7,848
	ctx.r7.s64 = 848;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmpw cr6,r30,r31
	cr6.compare<int32_t>(r30.s32, r31.s32, xer);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// blt cr6,0x82ba9258
	if (cr6.lt) goto loc_82BA9258;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
loc_82BA9258:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA91C0) {
	__imp__sub_82BA91C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9280) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9280) {
	__imp__sub_82BA9280(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9298) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-15008
	ctx.r9.s64 = r11.s64 + -15008;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82ba92cc
	if (cr6.eq) goto loc_82BA92CC;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BA92CC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9298) {
	__imp__sub_82BA9298(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA92E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r30,0
	r30.s64 = 0;
	// addi r10,r11,28008
	ctx.r10.s64 = r11.s64 + 28008;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// sth r30,8(r31)
	PPC_STORE_U16(r31.u32 + 8, r30.u16);
	// sth r30,10(r31)
	PPC_STORE_U16(r31.u32 + 10, r30.u16);
	// beq cr6,0x82ba9328
	if (cr6.eq) goto loc_82BA9328;
	// li r5,-1
	ctx.r5.s64 = -1;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
loc_82BA9328:
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// sth r30,16(r31)
	PPC_STORE_U16(r31.u32 + 16, r30.u16);
	// sth r30,18(r31)
	PPC_STORE_U16(r31.u32 + 18, r30.u16);
	// stw r29,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r29.u32);
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// stw r30,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// bl 0x82bacd98
	sub_82BACD98(ctx, base);
	// stw r30,356(r31)
	PPC_STORE_U32(r31.u32 + 356, r30.u32);
	// stw r30,360(r31)
	PPC_STORE_U32(r31.u32 + 360, r30.u32);
	// lis r11,-32245
	r11.s64 = -2113208320;
	// stw r30,364(r31)
	PPC_STORE_U32(r31.u32 + 364, r30.u32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// stw r30,368(r31)
	PPC_STORE_U32(r31.u32 + 368, r30.u32);
	// addi r27,r11,-6332
	r27.s64 = r11.s64 + -6332;
	// stw r30,372(r31)
	PPC_STORE_U32(r31.u32 + 372, r30.u32);
	// addi r29,r31,324
	r29.s64 = r31.s64 + 324;
	// stb r30,376(r31)
	PPC_STORE_U8(r31.u32 + 376, r30.u8);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r6,r10,-15396
	ctx.r6.s64 = ctx.r10.s64 + -15396;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// addi r28,r9,-15120
	r28.s64 = ctx.r9.s64 + -15120;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,10240
	ctx.r4.s64 = 10240;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r7,111
	ctx.r7.s64 = 111;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// lis r28,-32246
	r28.s64 = -2113273856;
	// li r8,512
	ctx.r8.s64 = 512;
	// stw r3,356(r31)
	PPC_STORE_U32(r31.u32 + 356, ctx.r3.u32);
	// addi r7,r28,-27468
	ctx.r7.s64 = r28.s64 + -27468;
	// stw r8,360(r31)
	PPC_STORE_U32(r31.u32 + 360, ctx.r8.u32);
	// addi r29,r31,380
	r29.s64 = r31.s64 + 380;
	// stw r30,412(r31)
	PPC_STORE_U32(r31.u32 + 412, r30.u32);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// stw r30,416(r31)
	PPC_STORE_U32(r31.u32 + 416, r30.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r30,420(r31)
	PPC_STORE_U32(r31.u32 + 420, r30.u32);
	// addi r6,r6,-13164
	ctx.r6.s64 = ctx.r6.s64 + -13164;
	// lfs f31,10872(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 10872);
	f31.f64 = double(temp.f32);
	// li r4,32
	ctx.r4.s64 = 32;
	// stfs f31,424(r31)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 424, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f1,-27468(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r28.u32 + -27468);
	ctx.f1.f64 = double(temp.f32);
	// addi r6,r5,-13144
	ctx.r6.s64 = ctx.r5.s64 + -13144;
	// addi r3,r29,48
	ctx.r3.s64 = r29.s64 + 48;
	// bl 0x82bac658
	sub_82BAC658(ctx, base);
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// stw r30,476(r31)
	PPC_STORE_U32(r31.u32 + 476, r30.u32);
	// addi r29,r31,492
	r29.s64 = r31.s64 + 492;
	// addi r3,r4,-12848
	ctx.r3.s64 = ctx.r4.s64 + -12848;
	// stw r31,496(r31)
	PPC_STORE_U32(r31.u32 + 496, r31.u32);
	// stw r30,500(r31)
	PPC_STORE_U32(r31.u32 + 500, r30.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// stw r3,492(r31)
	PPC_STORE_U32(r31.u32 + 492, ctx.r3.u32);
	// addi r28,r29,24
	r28.s64 = r29.s64 + 24;
	// stw r30,504(r31)
	PPC_STORE_U32(r31.u32 + 504, r30.u32);
	// addi r6,r11,-11896
	ctx.r6.s64 = r11.s64 + -11896;
	// stw r30,508(r31)
	PPC_STORE_U32(r31.u32 + 508, r30.u32);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r30,512(r31)
	PPC_STORE_U32(r31.u32 + 512, r30.u32);
	// stfs f31,560(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 560, temp.u32);
	// stw r30,548(r31)
	PPC_STORE_U32(r31.u32 + 548, r30.u32);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// stw r30,552(r31)
	PPC_STORE_U32(r31.u32 + 552, r30.u32);
	// li r4,32
	ctx.r4.s64 = 32;
	// stw r30,556(r31)
	PPC_STORE_U32(r31.u32 + 556, r30.u32);
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// stw r30,564(r31)
	PPC_STORE_U32(r31.u32 + 564, r30.u32);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// addi r6,r10,-10616
	ctx.r6.s64 = ctx.r10.s64 + -10616;
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// addi r4,r9,26080
	ctx.r4.s64 = ctx.r9.s64 + 26080;
	// stw r6,568(r31)
	PPC_STORE_U32(r31.u32 + 568, ctx.r6.u32);
	// addi r3,r8,12488
	ctx.r3.s64 = ctx.r8.s64 + 12488;
	// stw r30,572(r31)
	PPC_STORE_U32(r31.u32 + 572, r30.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// stw r4,492(r31)
	PPC_STORE_U32(r31.u32 + 492, ctx.r4.u32);
	// addi r10,r7,-12344
	ctx.r10.s64 = ctx.r7.s64 + -12344;
	// stw r3,568(r31)
	PPC_STORE_U32(r31.u32 + 568, ctx.r3.u32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// stw r30,576(r31)
	PPC_STORE_U32(r31.u32 + 576, r30.u32);
	// stw r30,580(r31)
	PPC_STORE_U32(r31.u32 + 580, r30.u32);
	// addi r8,r5,-12184
	ctx.r8.s64 = ctx.r5.s64 + -12184;
	// stw r30,584(r31)
	PPC_STORE_U32(r31.u32 + 584, r30.u32);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// stw r30,588(r31)
	PPC_STORE_U32(r31.u32 + 588, r30.u32);
	// addi r6,r11,-12040
	ctx.r6.s64 = r11.s64 + -12040;
	// stw r10,592(r31)
	PPC_STORE_U32(r31.u32 + 592, ctx.r10.u32);
	// addi r5,r9,6416
	ctx.r5.s64 = ctx.r9.s64 + 6416;
	// stw r30,596(r31)
	PPC_STORE_U32(r31.u32 + 596, r30.u32);
	// addi r4,r7,-13332
	ctx.r4.s64 = ctx.r7.s64 + -13332;
	// stw r30,600(r31)
	PPC_STORE_U32(r31.u32 + 600, r30.u32);
	// addi r11,r31,768
	r11.s64 = r31.s64 + 768;
	// stw r30,604(r31)
	PPC_STORE_U32(r31.u32 + 604, r30.u32);
	// stw r30,608(r31)
	PPC_STORE_U32(r31.u32 + 608, r30.u32);
	// stw r30,612(r31)
	PPC_STORE_U32(r31.u32 + 612, r30.u32);
	// stw r30,616(r31)
	PPC_STORE_U32(r31.u32 + 616, r30.u32);
	// stw r29,652(r31)
	PPC_STORE_U32(r31.u32 + 652, r29.u32);
	// addi r29,r11,16
	r29.s64 = r11.s64 + 16;
	// stw r31,624(r31)
	PPC_STORE_U32(r31.u32 + 624, r31.u32);
	// stw r30,628(r31)
	PPC_STORE_U32(r31.u32 + 628, r30.u32);
	// stw r30,632(r31)
	PPC_STORE_U32(r31.u32 + 632, r30.u32);
	// stw r30,636(r31)
	PPC_STORE_U32(r31.u32 + 636, r30.u32);
	// stw r30,640(r31)
	PPC_STORE_U32(r31.u32 + 640, r30.u32);
	// stw r30,644(r31)
	PPC_STORE_U32(r31.u32 + 644, r30.u32);
	// stw r30,648(r31)
	PPC_STORE_U32(r31.u32 + 648, r30.u32);
	// stw r8,620(r31)
	PPC_STORE_U32(r31.u32 + 620, ctx.r8.u32);
	// stw r30,656(r31)
	PPC_STORE_U32(r31.u32 + 656, r30.u32);
	// stw r30,660(r31)
	PPC_STORE_U32(r31.u32 + 660, r30.u32);
	// stw r30,664(r31)
	PPC_STORE_U32(r31.u32 + 664, r30.u32);
	// stw r30,668(r31)
	PPC_STORE_U32(r31.u32 + 668, r30.u32);
	// stw r30,672(r31)
	PPC_STORE_U32(r31.u32 + 672, r30.u32);
	// stw r30,676(r31)
	PPC_STORE_U32(r31.u32 + 676, r30.u32);
	// stw r30,680(r31)
	PPC_STORE_U32(r31.u32 + 680, r30.u32);
	// stw r31,688(r31)
	PPC_STORE_U32(r31.u32 + 688, r31.u32);
	// stw r6,684(r31)
	PPC_STORE_U32(r31.u32 + 684, ctx.r6.u32);
	// stw r30,692(r31)
	PPC_STORE_U32(r31.u32 + 692, r30.u32);
	// stw r30,696(r31)
	PPC_STORE_U32(r31.u32 + 696, r30.u32);
	// stw r30,700(r31)
	PPC_STORE_U32(r31.u32 + 700, r30.u32);
	// stw r30,704(r31)
	PPC_STORE_U32(r31.u32 + 704, r30.u32);
	// stw r30,708(r31)
	PPC_STORE_U32(r31.u32 + 708, r30.u32);
	// stw r30,712(r31)
	PPC_STORE_U32(r31.u32 + 712, r30.u32);
	// stw r30,716(r31)
	PPC_STORE_U32(r31.u32 + 716, r30.u32);
	// stw r30,720(r31)
	PPC_STORE_U32(r31.u32 + 720, r30.u32);
	// stw r30,724(r31)
	PPC_STORE_U32(r31.u32 + 724, r30.u32);
	// stw r30,728(r31)
	PPC_STORE_U32(r31.u32 + 728, r30.u32);
	// stw r31,736(r31)
	PPC_STORE_U32(r31.u32 + 736, r31.u32);
	// stw r5,732(r31)
	PPC_STORE_U32(r31.u32 + 732, ctx.r5.u32);
	// stw r30,740(r31)
	PPC_STORE_U32(r31.u32 + 740, r30.u32);
	// stw r30,744(r31)
	PPC_STORE_U32(r31.u32 + 744, r30.u32);
	// stw r30,748(r31)
	PPC_STORE_U32(r31.u32 + 748, r30.u32);
	// stw r30,752(r31)
	PPC_STORE_U32(r31.u32 + 752, r30.u32);
	// stw r30,756(r31)
	PPC_STORE_U32(r31.u32 + 756, r30.u32);
	// stw r30,760(r31)
	PPC_STORE_U32(r31.u32 + 760, r30.u32);
	// stw r30,764(r31)
	PPC_STORE_U32(r31.u32 + 764, r30.u32);
	// stw r4,768(r31)
	PPC_STORE_U32(r31.u32 + 768, ctx.r4.u32);
	// stw r30,772(r31)
	PPC_STORE_U32(r31.u32 + 772, r30.u32);
	// stw r30,776(r31)
	PPC_STORE_U32(r31.u32 + 776, r30.u32);
	// stw r30,780(r31)
	PPC_STORE_U32(r31.u32 + 780, r30.u32);
	// stfs f31,828(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 828, temp.u32);
	// stw r30,816(r31)
	PPC_STORE_U32(r31.u32 + 816, r30.u32);
	// stw r30,820(r31)
	PPC_STORE_U32(r31.u32 + 820, r30.u32);
	// stw r30,824(r31)
	PPC_STORE_U32(r31.u32 + 824, r30.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// addi r6,r11,-13616
	ctx.r6.s64 = r11.s64 + -13616;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r9,3
	ctx.r9.s64 = 3;
	// stw r30,836(r31)
	PPC_STORE_U32(r31.u32 + 836, r30.u32);
	// addi r8,r10,-13172
	ctx.r8.s64 = ctx.r10.s64 + -13172;
	// stw r30,840(r31)
	PPC_STORE_U32(r31.u32 + 840, r30.u32);
	// stw r30,844(r31)
	PPC_STORE_U32(r31.u32 + 844, r30.u32);
	// li r7,53
	ctx.r7.s64 = 53;
	// stw r8,832(r31)
	PPC_STORE_U32(r31.u32 + 832, ctx.r8.u32);
	// li r6,-1
	ctx.r6.s64 = -1;
	// stw r30,848(r31)
	PPC_STORE_U32(r31.u32 + 848, r30.u32);
	// addi r11,r31,852
	r11.s64 = r31.s64 + 852;
	// stb r30,872(r31)
	PPC_STORE_U8(r31.u32 + 872, r30.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r30,873(r31)
	PPC_STORE_U8(r31.u32 + 873, r30.u8);
	// stw r30,480(r31)
	PPC_STORE_U32(r31.u32 + 480, r30.u32);
	// stw r30,484(r31)
	PPC_STORE_U32(r31.u32 + 484, r30.u32);
	// stw r9,488(r31)
	PPC_STORE_U32(r31.u32 + 488, ctx.r9.u32);
	// lbz r5,868(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 868);
	// rlwimi r5,r7,1,0,30
	ctx.r5.u64 = (rotl32(ctx.r7.u32, 1) & 0xFFFFFFFE) | (ctx.r5.u64 & 0xFFFFFFFF00000001);
	// stw r30,852(r31)
	PPC_STORE_U32(r31.u32 + 852, r30.u32);
	// stw r30,856(r31)
	PPC_STORE_U32(r31.u32 + 856, r30.u32);
	// stw r30,860(r31)
	PPC_STORE_U32(r31.u32 + 860, r30.u32);
	// stw r6,864(r31)
	PPC_STORE_U32(r31.u32 + 864, ctx.r6.u32);
	// stb r5,868(r31)
	PPC_STORE_U8(r31.u32 + 868, ctx.r5.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BA92E0) {
	__imp__sub_82BA92E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9648) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82ba9708
	sub_82BA9708(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82ba9680
	if (cr6.eq) goto loc_82BA9680;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BA9680:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9648) {
	__imp__sub_82BA9648(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9698) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,240
	ctx.r3.s64 = r31.s64 + 240;
	// bl 0x823f9a78
	sub_823F9A78(ctx, base);
	// addi r3,r31,192
	ctx.r3.s64 = r31.s64 + 192;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x823f9a78
	sub_823F9A78(ctx, base);
	// addi r3,r31,144
	ctx.r3.s64 = r31.s64 + 144;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x823f9a78
	sub_823F9A78(ctx, base);
	// addi r3,r31,96
	ctx.r3.s64 = r31.s64 + 96;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x823f9a78
	sub_823F9A78(ctx, base);
	// addi r3,r31,48
	ctx.r3.s64 = r31.s64 + 48;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x823f9a78
	sub_823F9A78(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x823f9a78
	sub_823F9A78(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9698) {
	__imp__sub_82BA9698(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9708) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r10,r11,28008
	ctx.r10.s64 = r11.s64 + 28008;
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba974c
	if (cr6.eq) goto loc_82BA974C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// bl 0x82baa458
	sub_82BAA458(ctx, base);
loc_82BA974C:
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba9774
	if (cr6.eq) goto loc_82BA9774;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82baa458
	sub_82BAA458(ctx, base);
loc_82BA9774:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,844(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(844) );
	// lwz r30,836(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(836) );
	// addi r9,r11,-13172
	ctx.r9.s64 = r11.s64 + -13172;
	// rlwinm r29,r10,3,0,28
	r29.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,832(r31)
	PPC_STORE_U32(r31.u32 + 832, ctx.r9.u32);
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// addi r3,r31,768
	ctx.r3.s64 = r31.s64 + 768;
	// addi r7,r8,-13180
	ctx.r7.s64 = ctx.r8.s64 + -13180;
	// stw r7,832(r31)
	PPC_STORE_U32(r31.u32 + 832, ctx.r7.u32);
	// bl 0x82babaf0
	sub_82BABAF0(ctx, base);
	// addi r3,r31,732
	ctx.r3.s64 = r31.s64 + 732;
	// bl 0x82baec90
	sub_82BAEC90(ctx, base);
	// addi r3,r31,684
	ctx.r3.s64 = r31.s64 + 684;
	// bl 0x82bae460
	sub_82BAE460(ctx, base);
	// addi r3,r31,620
	ctx.r3.s64 = r31.s64 + 620;
	// bl 0x82bad7c8
	sub_82BAD7C8(ctx, base);
	// addi r3,r31,592
	ctx.r3.s64 = r31.s64 + 592;
	// bl 0x82bad6d8
	sub_82BAD6D8(ctx, base);
	// addi r3,r31,492
	ctx.r3.s64 = r31.s64 + 492;
	// bl 0x82bacf08
	sub_82BACF08(ctx, base);
	// addi r3,r31,380
	ctx.r3.s64 = r31.s64 + 380;
	// bl 0x82babe18
	sub_82BABE18(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,324
	ctx.r3.s64 = r31.s64 + 324;
	// bl 0x82baa170
	sub_82BAA170(ctx, base);
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x82ba9698
	sub_82BA9698(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// lwz r29,12(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba9828
	if (cr6.eq) goto loc_82BA9828;
	// lhz r11,18(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 18);
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r30,18(r31)
	PPC_STORE_U16(r31.u32 + 18, r30.u16);
	// sth r30,16(r31)
	PPC_STORE_U16(r31.u32 + 16, r30.u16);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
loc_82BA9828:
	// lwz r29,4(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82ba9858
	if (cr6.eq) goto loc_82BA9858;
	// lhz r11,10(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 10);
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r30,10(r31)
	PPC_STORE_U16(r31.u32 + 10, r30.u16);
	// sth r30,8(r31)
	PPC_STORE_U16(r31.u32 + 8, r30.u16);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
loc_82BA9858:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r10,r11,-15008
	ctx.r10.s64 = r11.s64 + -15008;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BA9708) {
	__imp__sub_82BA9708(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9870) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r3,r31,380
	ctx.r3.s64 = r31.s64 + 380;
	// bl 0x82babee8
	sub_82BABEE8(ctx, base);
	// lwz r11,492(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(492) );
	// addi r3,r31,492
	ctx.r3.s64 = r31.s64 + 492;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,128
	ctx.r5.s64 = 128;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9870) {
	__imp__sub_82BA9870(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA98D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r3,r31,380
	ctx.r3.s64 = r31.s64 + 380;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82bac1c8
	sub_82BAC1C8(ctx, base);
	// lbz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82ba9928
	if (cr6.eq) goto loc_82BA9928;
	// lwz r11,492(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(492) );
	// addi r3,r31,492
	ctx.r3.s64 = r31.s64 + 492;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,128
	ctx.r5.s64 = 128;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA9928:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BA98D0) {
	__imp__sub_82BA98D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9938) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ba997c
	if (cr6.eq) goto loc_82BA997C;
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// sth r11,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r11.u16);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_82BA997C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BA9938) {
	__imp__sub_82BA9938(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9988) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,-7268(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-7268) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ba9a48
	if (!cr6.eq) goto loc_82BA9A48;
	// lwz r30,12(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ba99fc
	if (cr6.eq) goto loc_82BA99FC;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r8,96(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(96) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ba99fc
	if (cr6.eq) goto loc_82BA99FC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// b 0x82ba9a48
	goto loc_82BA9A48;
loc_82BA99FC:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mr r11,r31
	r11.u64 = r31.u64;
	// li r9,5
	ctx.r9.s64 = 5;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BA9A0C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ba9a0c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA9A0C;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lbz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r9,r9,0,27,24
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFF9F;
	// lwz r11,-7260(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-7260) );
	// stb r9,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, ctx.r9.u8);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82ba9e18
	sub_82BA9E18(ctx, base);
loc_82BA9A48:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,-7272(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-7272) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x82ba9af8
	if (!cr6.eq) goto loc_82BA9AF8;
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82ba9aac
	if (cr6.eq) goto loc_82BA9AAC;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r8,96(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(96) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ba9aac
	if (cr6.eq) goto loc_82BA9AAC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x82ba9af8
	goto loc_82BA9AF8;
loc_82BA9AAC:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mr r11,r31
	r11.u64 = r31.u64;
	// li r9,5
	ctx.r9.s64 = 5;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BA9ABC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ba9abc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA9ABC;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lbz r10,98(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// clrlwi r9,r10,24
	ctx.r9.u64 = ctx.r10.u32 & 0xFF;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r9,r9,0,27,24
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFFF9F;
	// lwz r11,-7264(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-7264) );
	// stb r9,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, ctx.r9.u8);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// bl 0x82ba9e18
	sub_82BA9E18(ctx, base);
loc_82BA9AF8:
	// lbz r11,18(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 18);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82ba9b38
	if (cr6.eq) goto loc_82BA9B38;
	// lwz r11,492(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(492) );
	// addi r3,r29,492
	ctx.r3.s64 = r29.s64 + 492;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba9b38
	if (cr6.eq) goto loc_82BA9B38;
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ba9e18
	sub_82BA9E18(ctx, base);
loc_82BA9B38:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BA9988) {
	__imp__sub_82BA9988(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9B40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r10,-7332(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7332) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9ba0
	if (!cr6.eq) goto loc_82BA9BA0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r6,12(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// clrlwi r5,r30,5
	ctx.r5.u64 = r30.u32 & 0x7FFFFFF;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r8,72(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(72) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9BA0:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7328(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7328) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9bdc
	if (!cr6.eq) goto loc_82BA9BDC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9BDC:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7320(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7320) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9c18
	if (!cr6.eq) goto loc_82BA9C18;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r8,60(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(60) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9C18:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7316(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7316) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9c54
	if (!cr6.eq) goto loc_82BA9C54;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r8,60(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(60) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9C54:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7312(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7312) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9c90
	if (!cr6.eq) goto loc_82BA9C90;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r8,44(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(44) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9C90:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7304(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7304) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9cd0
	if (!cr6.eq) goto loc_82BA9CD0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9CD0:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7300(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7300) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9d0c
	if (!cr6.eq) goto loc_82BA9D0C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(12) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9D0C:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7296(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7296) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9d48
	if (!cr6.eq) goto loc_82BA9D48;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(20) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9D48:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7292(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7292) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9d84
	if (!cr6.eq) goto loc_82BA9D84;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r8,24(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(24) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82ba9dfc
	goto loc_82BA9DFC;
loc_82BA9D84:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7288(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7288) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9db8
	if (!cr6.eq) goto loc_82BA9DB8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(16) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// b 0x82ba9df8
	goto loc_82BA9DF8;
loc_82BA9DB8:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r10,-7284(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7284) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82ba9de0
	if (!cr6.eq) goto loc_82BA9DE0;
	// lwz r3,28(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// b 0x82ba9df8
	goto loc_82BA9DF8;
loc_82BA9DE0:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r4,r10,-15276
	ctx.r4.s64 = ctx.r10.s64 + -15276;
	// lwz r9,19068(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BA9DF8:
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BA9DFC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9B40) {
	__imp__sub_82BA9B40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9E18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r3,324
	ctx.r3.s64 = ctx.r3.s64 + 324;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82baa1d0
	sub_82BAA1D0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// li r9,5
	ctx.r9.s64 = 5;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BA9E44:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ba9e44
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA9E44;
	// lbz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 16);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// cmplwi cr6,r9,5
	cr6.compare<uint32_t>(ctx.r9.u32, 5, xer);
	// addi r30,r10,-15140
	r30.s64 = ctx.r10.s64 + -15140;
	// addi r29,r11,-15248
	r29.s64 = r11.s64 + -15248;
	// bne cr6,0x82ba9ea8
	if (!cr6.eq) goto loc_82BA9EA8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r7,343
	ctx.r7.s64 = 343;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba9ea0
	if (cr6.eq) goto loc_82BA9EA0;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// bl 0x82ba8798
	sub_82BA8798(ctx, base);
	// b 0x82ba9ea4
	goto loc_82BA9EA4;
loc_82BA9EA0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82BA9EA4:
	// stw r3,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r3.u32);
loc_82BA9EA8:
	// lbz r11,17(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 17);
	// cmplwi cr6,r11,5
	cr6.compare<uint32_t>(r11.u32, 5, xer);
	// bne cr6,0x82ba9ee8
	if (!cr6.eq) goto loc_82BA9EE8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r7,346
	ctx.r7.s64 = 346;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82ba9ee0
	if (cr6.eq) goto loc_82BA9EE0;
	// lwz r4,12(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// bl 0x82ba8798
	sub_82BA8798(ctx, base);
	// b 0x82ba9ee4
	goto loc_82BA9EE4;
loc_82BA9EE0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82BA9EE4:
	// stw r3,12(r28)
	PPC_STORE_U32(r28.u32 + 12, ctx.r3.u32);
loc_82BA9EE8:
	// lbz r11,18(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 18);
	// ori r10,r11,128
	ctx.r10.u64 = r11.u64 | 128;
	// stb r10,18(r28)
	PPC_STORE_U8(r28.u32 + 18, ctx.r10.u8);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BA9E18) {
	__imp__sub_82BA9E18(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9F00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// stw r5,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r5.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// stb r8,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r8.u8);
	// stb r9,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r9.u8);
	// lbz r11,98(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 98);
	// ori r10,r11,192
	ctx.r10.u64 = r11.u64 | 192;
	// stb r10,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, ctx.r10.u8);
	// beq cr6,0x82ba9f48
	if (cr6.eq) goto loc_82BA9F48;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_82BA9F48:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ba9f58
	if (cr6.eq) goto loc_82BA9F58;
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
loc_82BA9F58:
	// addi r3,r3,324
	ctx.r3.s64 = ctx.r3.s64 + 324;
	// bl 0x82baa1d0
	sub_82BAA1D0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r9,5
	ctx.r9.s64 = 5;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BA9F70:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ba9f70
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA9F70;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9F00) {
	__imp__sub_82BA9F00(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BA9F98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// stw r5,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r5.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// std r11,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, r11.u64);
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// stb r8,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r8.u8);
	// stb r9,97(r1)
	PPC_STORE_U8(ctx.r1.u32 + 97, ctx.r9.u8);
	// beq cr6,0x82ba9fd4
	if (cr6.eq) goto loc_82BA9FD4;
	// lwz r11,0(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
loc_82BA9FD4:
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82ba9fe4
	if (cr6.eq) goto loc_82BA9FE4;
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(0) );
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
loc_82BA9FE4:
	// addi r3,r3,324
	ctx.r3.s64 = ctx.r3.s64 + 324;
	// bl 0x82baa1d0
	sub_82BAA1D0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r9,5
	ctx.r9.s64 = 5;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BA9FFC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82ba9ffc
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BA9FFC;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BA9F98) {
	__imp__sub_82BA9F98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA020) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,32(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(32) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// rotlwi r3,r11,0
	ctx.r3.u64 = rotl32(r11.u32, 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

PPC_WEAK_FUNC(sub_82BAA020) {
	__imp__sub_82BAA020(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA040) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA040) {
	__imp__sub_82BAA040(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA048) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r31,0
	r31.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// sth r31,86(r1)
	PPC_STORE_U16(ctx.r1.u32 + 86, r31.u16);
	// sth r31,84(r1)
	PPC_STORE_U16(ctx.r1.u32 + 84, r31.u16);
	// beq cr6,0x82baa088
	if (cr6.eq) goto loc_82BAA088;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82BAA088:
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// cmplw cr6,r11,r3
	cr6.compare<uint32_t>(r11.u32, ctx.r3.u32, xer);
	// beq cr6,0x82baa0a0
	if (cr6.eq) goto loc_82BAA0A0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba8ac8
	sub_82BA8AC8(ctx, base);
loc_82BAA0A0:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baa0c0
	if (cr6.eq) goto loc_82BAA0C0;
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAA0C0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA048) {
	__imp__sub_82BAA048(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA0D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r4,r4,12
	ctx.r4.s64 = ctx.r4.s64 + 12;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82ba8798
	sub_82BA8798(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA0D8) {
	__imp__sub_82BAA0D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA110) {
	PPC_FUNC_PROLOGUE();
	// stb r4,872(r3)
	PPC_STORE_U8(ctx.r3.u32 + 872, ctx.r4.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA110) {
	__imp__sub_82BAA110(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA118) {
	PPC_FUNC_PROLOGUE();
	// stb r4,873(r3)
	PPC_STORE_U8(ctx.r3.u32 + 873, ctx.r4.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA118) {
	__imp__sub_82BAA118(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA120) {
	PPC_FUNC_PROLOGUE();
	// stw r4,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r4.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA120) {
	__imp__sub_82BAA120(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA128) {
	PPC_FUNC_PROLOGUE();
	// stw r4,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r4.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA128) {
	__imp__sub_82BAA128(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA130) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,28(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA130) {
	__imp__sub_82BAA130(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA138) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,492
	ctx.r3.s64 = ctx.r3.s64 + 492;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA138) {
	__imp__sub_82BAA138(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA140) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,620
	ctx.r3.s64 = ctx.r3.s64 + 620;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA140) {
	__imp__sub_82BAA140(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA148) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,732
	ctx.r3.s64 = ctx.r3.s64 + 732;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA148) {
	__imp__sub_82BAA148(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA150) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,684
	ctx.r3.s64 = ctx.r3.s64 + 684;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA150) {
	__imp__sub_82BAA150(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA158) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,832
	ctx.r3.s64 = ctx.r3.s64 + 832;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA158) {
	__imp__sub_82BAA158(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA160) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,592
	ctx.r3.s64 = ctx.r3.s64 + 592;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA160) {
	__imp__sub_82BAA160(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA168) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,768
	ctx.r3.s64 = ctx.r3.s64 + 768;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA168) {
	__imp__sub_82BAA168(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA170) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// stw r30,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r30.u32);
	// stw r30,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r30.u32);
	// stb r30,52(r31)
	PPC_STORE_U8(r31.u32 + 52, r30.u8);
	// beq cr6,0x82baa1c8
	if (cr6.eq) goto loc_82BAA1C8;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r29,32(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
loc_82BAA1C8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BAA170) {
	__imp__sub_82BAA170(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA1D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(48) );
	// lwz r8,32(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
	// lwz r7,36(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(36) );
	// cmpw cr6,r10,r7
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, xer);
	// add r3,r8,r9
	ctx.r3.u64 = ctx.r8.u64 + ctx.r9.u64;
	// blt cr6,0x82baa20c
	if (cr6.lt) goto loc_82BAA20C;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,48(r11)
	PPC_STORE_U32(r11.u32 + 48, ctx.r10.u32);
loc_82BAA20C:
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(48) );
	// lwz r8,44(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(44) );
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// cntlzw r5,r7
	ctx.r5.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// stw r6,40(r11)
	PPC_STORE_U32(r11.u32 + 40, ctx.r6.u32);
	// rlwinm r4,r5,27,31,31
	ctx.r4.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x1;
	// stb r4,52(r11)
	PPC_STORE_U8(r11.u32 + 52, ctx.r4.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA1D0) {
	__imp__sub_82BAA1D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA238) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lhz r11,4(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 4);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82baa288
	if (cr6.lt) goto loc_82BAA288;
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
loc_82BAA264:
	// lbzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + r11.u32);
	// cmplwi cr6,r8,92
	cr6.compare<uint32_t>(ctx.r8.u32, 92, xer);
	// beq cr6,0x82baa27c
	if (cr6.eq) goto loc_82BAA27C;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bge 0x82baa264
	if (!cr0.lt) goto loc_82BAA264;
	// b 0x82baa288
	goto loc_82BAA288;
loc_82BAA27C:
	// mr r31,r11
	r31.u64 = r11.u64;
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x82baa2b0
	if (!cr6.eq) goto loc_82BAA2B0;
loc_82BAA288:
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blt cr6,0x82baa2ac
	if (cr6.lt) goto loc_82BAA2AC;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
loc_82BAA298:
	// lbzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// cmplwi cr6,r9,47
	cr6.compare<uint32_t>(ctx.r9.u32, 47, xer);
	// beq cr6,0x82baa304
	if (cr6.eq) goto loc_82BAA304;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bge 0x82baa298
	if (!cr0.lt) goto loc_82BAA298;
loc_82BAA2AC:
	// li r31,-1
	r31.s64 = -1;
loc_82BAA2B0:
	// li r11,0
	r11.s64 = 0;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// li r5,-1
	ctx.r5.s64 = -1;
	// sth r11,4(r30)
	PPC_STORE_U16(r30.u32 + 4, r11.u16);
	// addi r4,r10,3224
	ctx.r4.s64 = ctx.r10.s64 + 3224;
	// sth r11,6(r30)
	PPC_STORE_U16(r30.u32 + 6, r11.u16);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// beq cr6,0x82baa2fc
	if (cr6.eq) goto loc_82BAA2FC;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82ba8ac8
	sub_82BA8AC8(ctx, base);
	// addi r5,r31,1
	ctx.r5.s64 = r31.s64 + 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ba8938
	sub_82BA8938(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
loc_82BAA2FC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82BAA304:
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x82baa2b0
	goto loc_82BAA2B0;
}

PPC_WEAK_FUNC(sub_82BAA238) {
	__imp__sub_82BAA238(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA310) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lhz r11,4(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 4);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x82baa348
	if (cr0.lt) goto loc_82BAA348;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
loc_82BAA334:
	// lbzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// cmplwi cr6,r9,46
	cr6.compare<uint32_t>(ctx.r9.u32, 46, xer);
	// beq cr6,0x82baa3c4
	if (cr6.eq) goto loc_82BAA3C4;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bge 0x82baa334
	if (!cr0.lt) goto loc_82BAA334;
loc_82BAA348:
	// li r30,-1
	r30.s64 = -1;
loc_82BAA34C:
	// li r29,0
	r29.s64 = 0;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// li r5,-1
	ctx.r5.s64 = -1;
	// sth r29,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r29.u16);
	// addi r4,r11,3224
	ctx.r4.s64 = r11.s64 + 3224;
	// sth r29,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r29.u16);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// cmpwi cr6,r30,-1
	cr6.compare<int32_t>(r30.s32, -1, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x82baa3bc
	if (cr6.eq) goto loc_82BAA3BC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82ba8ac8
	sub_82BA8AC8(ctx, base);
	// lhz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// ble cr6,0x82baa398
	if (!cr6.gt) goto loc_82BAA398;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAA398:
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// sth r30,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r30.u16);
	// add r4,r3,r11
	ctx.r4.u64 = ctx.r3.u64 + r11.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stbx r29,r9,r30
	PPC_STORE_U8(ctx.r9.u32 + r30.u32, r29.u8);
loc_82BAA3BC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82BAA3C4:
	// mr r30,r11
	r30.u64 = r11.u64;
	// b 0x82baa34c
	goto loc_82BAA34C;
}

PPC_WEAK_FUNC(sub_82BAA310) {
	__imp__sub_82BAA310(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA3D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lhz r11,4(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// blt 0x82baa43c
	if (cr0.lt) goto loc_82BAA43C;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
loc_82BAA3FC:
	// lbzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// cmplwi cr6,r9,46
	cr6.compare<uint32_t>(ctx.r9.u32, 46, xer);
	// beq cr6,0x82baa414
	if (cr6.eq) goto loc_82BAA414;
	// addic. r11,r11,-1
	xer.ca = r11.u32 > 0;
	r11.s64 = r11.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// bge 0x82baa3fc
	if (!cr0.lt) goto loc_82BAA3FC;
	// b 0x82baa43c
	goto loc_82BAA43C;
loc_82BAA414:
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x82baa440
	if (cr6.eq) goto loc_82BAA440;
	// addi r5,r11,1
	ctx.r5.s64 = r11.s64 + 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82ba8938
	sub_82BA8938(ctx, base);
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ba89b0
	sub_82BA89B0(ctx, base);
loc_82BAA43C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAA440:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA3D0) {
	__imp__sub_82BAA3D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA458) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82baa48c
	if (cr6.eq) goto loc_82BAA48C;
	// lwz r30,-12(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(-12) );
	// addi r31,r3,-16
	r31.s64 = ctx.r3.s64 + -16;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAA48C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA458) {
	__imp__sub_82BAA458(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA4A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r11,-7176
	ctx.r3.s64 = r11.s64 + -7176;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// bl 0x82baa6f0
	sub_82BAA6F0(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,28548(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(28548) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-31950
	r11.s64 = -2093875200;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r11,-20320
	ctx.r3.s64 = r11.s64 + -20320;
	// lwz r11,10244(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(10244) );
	// lwz r10,10240(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(10240) );
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82baa530
	if (!cr6.lt) goto loc_82BAA530;
	// rldicr r6,r27,32,31
	ctx.r6.u64 = rotl64(r27.u64, 32) & 0xFFFFFFFF00000000;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r29,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r29.u32);
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x82baa688
	sub_82BAA688(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
loc_82BAA530:
	// lis r31,-31924
	r31.s64 = -2092171264;
	// lbz r11,-5548(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + -5548);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82baa564
	if (!cr6.eq) goto loc_82BAA564;
	// lis r11,-31951
	r11.s64 = -2093940736;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r4,r10,-14888
	ctx.r4.s64 = ctx.r10.s64 + -14888;
	// lwz r9,19068(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r11,1
	r11.s64 = 1;
	// stb r11,-5548(r31)
	PPC_STORE_U8(r31.u32 + -5548, r11.u8);
loc_82BAA564:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BAA4A8) {
	__imp__sub_82BAA4A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA570) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baa5bc
	if (cr6.eq) goto loc_82BAA5BC;
	// bl 0x82baa5d8
	sub_82BAA5D8(ctx, base);
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r11,-7176
	ctx.r3.s64 = r11.s64 + -7176;
	// bl 0x82baa7b8
	sub_82BAA7B8(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,28556(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(28556) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAA5BC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA570) {
	__imp__sub_82BAA570(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA5D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// li r11,0
	r11.s64 = 0;
	// addi r31,r10,-20320
	r31.s64 = ctx.r10.s64 + -20320;
	// lwz r10,10244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10244) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82baa670
	if (!cr6.gt) goto loc_82BAA670;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
loc_82BAA604:
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// cmplw cr6,r8,r3
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r3.u32, xer);
	// beq cr6,0x82baa634
	if (cr6.eq) goto loc_82BAA634;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82baa604
	if (cr6.lt) goto loc_82BAA604;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82BAA634:
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r31,20
	ctx.r10.s64 = r31.s64 + 20;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// add r3,r11,r31
	ctx.r3.u64 = r11.u64 + r31.u64;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r11,10244(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10244) );
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,10244(r31)
	PPC_STORE_U32(r31.u32 + 10244, r11.u32);
loc_82BAA670:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA5D8) {
	__imp__sub_82BAA5D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA688) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r11,-31950
	r11.s64 = -2093875200;
	// std r4,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.r4.u64);
	// std r5,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.r5.u64);
	// addi r10,r11,-20320
	ctx.r10.s64 = r11.s64 + -20320;
	// std r6,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.r6.u64);
	// lwz r11,10244(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(10244) );
	// lwz r9,10240(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(10240) );
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,32
	ctx.r9.s64 = ctx.r1.s64 + 32;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// li r8,5
	ctx.r8.s64 = 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_82BAA6C8:
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82baa6c8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BAA6C8;
	// lwz r11,10244(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(10244) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,10244(r10)
	PPC_STORE_U32(ctx.r10.u32 + 10244, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA688) {
	__imp__sub_82BAA688(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA6F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// addi r11,r10,1
	r11.s64 = ctx.r10.s64 + 1;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x82baa720
	if (!cr6.gt) goto loc_82BAA720;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAA720:
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// addi r9,r3,32
	ctx.r9.s64 = ctx.r3.s64 + 32;
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(44) );
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(32) );
	// add r8,r10,r4
	ctx.r8.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// lwz r10,40(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(40) );
	// stw r8,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, ctx.r8.u32);
	// stw r11,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, r11.u32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// ble cr6,0x82baa750
	if (!cr6.gt) goto loc_82BAA750;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAA750:
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, r11.u32);
	// addi r10,r3,16
	ctx.r10.s64 = ctx.r3.s64 + 16;
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(24) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r8,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r8.u32);
	// stw r11,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r11.u32);
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// ble cr6,0x82baa780
	if (!cr6.gt) goto loc_82BAA780;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_82BAA780:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// addi r8,r3,48
	ctx.r8.s64 = ctx.r3.s64 + 48;
	// lwz r11,60(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(60) );
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// lwz r9,48(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(48) );
	// lwz r10,56(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(56) );
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// stw r11,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, r11.u32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// stw r9,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r9.u32);
	// ble cr6,0x82baa7b0
	if (!cr6.gt) goto loc_82BAA7B0;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAA7B0:
	// stw r11,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r11.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA6F0) {
	__imp__sub_82BAA6F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA7B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// addi r11,r3,32
	r11.s64 = ctx.r3.s64 + 32;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// addi r11,r3,16
	r11.s64 = ctx.r3.s64 + 16;
	// addi r11,r10,-1
	r11.s64 = ctx.r10.s64 + -1;
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r11.u32);
	// addi r11,r3,48
	r11.s64 = ctx.r3.s64 + 48;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r11,36(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(36) );
	// lwz r9,44(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(44) );
	// subf r7,r4,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r4.s64;
	// add r8,r11,r4
	ctx.r8.u64 = r11.u64 + ctx.r4.u64;
	// stw r7,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r7.u32);
	// stw r8,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r8.u32);
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(28) );
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(20) );
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// addi r6,r11,-1
	ctx.r6.s64 = r11.s64 + -1;
	// stw r5,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r5.u32);
	// stw r6,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r6.u32);
	// lwz r11,52(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(52) );
	// lwz r10,60(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(60) );
	// subf r8,r4,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r4.s64;
	// add r9,r4,r11
	ctx.r9.u64 = ctx.r4.u64 + r11.u64;
	// stw r8,60(r3)
	PPC_STORE_U32(ctx.r3.u32 + 60, ctx.r8.u32);
	// stw r9,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r9.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAA7B8) {
	__imp__sub_82BAA7B8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA830) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	// mflr r12
	// bl 0x82ca2be4
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(16) );
	// lwz r10,16(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(16) );
	// lwz r7,20(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + int32_t(20) );
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r8,24(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(24) );
	// lwz r9,28(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28) );
	// stw r11,16(r28)
	PPC_STORE_U32(r28.u32 + 16, r11.u32);
	// lwz r11,20(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(20) );
	// add r10,r11,r7
	ctx.r10.u64 = r11.u64 + ctx.r7.u64;
	// stw r10,20(r28)
	PPC_STORE_U32(r28.u32 + 20, ctx.r10.u32);
	// lwz r11,24(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(24) );
	// add r8,r11,r8
	ctx.r8.u64 = r11.u64 + ctx.r8.u64;
	// stw r8,24(r28)
	PPC_STORE_U32(r28.u32 + 24, ctx.r8.u32);
	// lwz r11,28(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(28) );
	// add r7,r11,r9
	ctx.r7.u64 = r11.u64 + ctx.r9.u64;
	// lwz r10,48(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(48) );
	// stw r7,28(r28)
	PPC_STORE_U32(r28.u32 + 28, ctx.r7.u32);
	// lwz r11,48(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(48) );
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// lwz r29,52(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + int32_t(52) );
	// stw r6,48(r28)
	PPC_STORE_U32(r28.u32 + 48, ctx.r6.u32);
	// lwz r11,52(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(52) );
	// add r5,r11,r29
	ctx.r5.u64 = r11.u64 + r29.u64;
	// lwz r30,56(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + int32_t(56) );
	// stw r5,52(r28)
	PPC_STORE_U32(r28.u32 + 52, ctx.r5.u32);
	// lwz r11,56(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(56) );
	// add r4,r11,r30
	ctx.r4.u64 = r11.u64 + r30.u64;
	// lwz r31,60(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + int32_t(60) );
	// stw r4,56(r28)
	PPC_STORE_U32(r28.u32 + 56, ctx.r4.u32);
	// lwz r11,60(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(60) );
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// stw r11,60(r28)
	PPC_STORE_U32(r28.u32 + 60, r11.u32);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// add r10,r11,r3
	ctx.r10.u64 = r11.u64 + ctx.r3.u64;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + int32_t(4) );
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// add r9,r11,r4
	ctx.r9.u64 = r11.u64 + ctx.r4.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + int32_t(8) );
	// stw r9,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r9.u32);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// add r8,r11,r5
	ctx.r8.u64 = r11.u64 + ctx.r5.u64;
	// lwz r6,12(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + int32_t(12) );
	// stw r8,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r8.u32);
	// lwz r11,12(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(12) );
	// add r6,r11,r6
	ctx.r6.u64 = r11.u64 + ctx.r6.u64;
	// lwz r7,32(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + int32_t(32) );
	// stw r6,12(r28)
	PPC_STORE_U32(r28.u32 + 12, ctx.r6.u32);
	// lwz r11,32(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(32) );
	// add r5,r11,r7
	ctx.r5.u64 = r11.u64 + ctx.r7.u64;
	// lwz r8,36(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(36) );
	// stw r5,32(r28)
	PPC_STORE_U32(r28.u32 + 32, ctx.r5.u32);
	// lwz r11,36(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(36) );
	// add r4,r11,r8
	ctx.r4.u64 = r11.u64 + ctx.r8.u64;
	// lwz r9,40(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(40) );
	// stw r4,36(r28)
	PPC_STORE_U32(r28.u32 + 36, ctx.r4.u32);
	// lwz r11,40(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(40) );
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// lwz r10,44(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(44) );
	// stw r3,40(r28)
	PPC_STORE_U32(r28.u32 + 40, ctx.r3.u32);
	// lwz r11,44(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(44) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,44(r28)
	PPC_STORE_U32(r28.u32 + 44, r11.u32);
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BAA830) {
	__imp__sub_82BAA830(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA948) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r9,0
	ctx.r9.s64 = 0;
	// addi r10,r11,28564
	ctx.r10.s64 = r11.s64 + 28564;
	// lwz r11,28564(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28564) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82baa984
	if (cr6.eq) goto loc_82BAA984;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAA964:
	// addi r8,r10,20
	ctx.r8.s64 = ctx.r10.s64 + 20;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bge cr6,0x82baa984
	if (!cr6.lt) goto loc_82BAA984;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82baa964
	if (!cr6.eq) goto loc_82BAA964;
loc_82BAA984:
	// rlwinm r11,r9,2,0,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82baa9a4
	if (!cr6.eq) goto loc_82BAA9A4;
	// lis r9,-31924
	ctx.r9.s64 = -2092171264;
	// addi r8,r9,-5536
	ctx.r8.s64 = ctx.r9.s64 + -5536;
	// stwx r8,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r8.u32);
	// blr 
	return;
loc_82BAA9A4:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r4,r10,-14752
	ctx.r4.s64 = ctx.r10.s64 + -14752;
	// lwz r9,19068(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

PPC_WEAK_FUNC(sub_82BAA948) {
	__imp__sub_82BAA948(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAA9C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r5,3884
	ctx.r5.s64 = 3884;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// cntlzw r9,r11
	ctx.r9.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// cntlzw r8,r10
	ctx.r8.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r7,r9,27,31,31
	ctx.r7.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwinm r11,r8,27,31,31
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r20,r7,1
	r20.u64 = ctx.r7.u64 ^ 1;
	// addi r21,r11,2
	r21.s64 = r11.s64 + 2;
	// rlwinm r11,r20,2,0,29
	r11.u64 = rotl64(r20.u32 | (r20.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r6,-31924
	ctx.r6.s64 = -2092171264;
	// add r14,r11,r21
	r14.u64 = r11.u64 + r21.u64;
	// addi r11,r6,-7176
	r11.s64 = ctx.r6.s64 + -7176;
	// rlwinm r9,r14,2,0,29
	ctx.r9.u64 = rotl64(r14.u32 | (r14.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r11,32
	ctx.r8.s64 = r11.s64 + 32;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r5,r7,-14712
	ctx.r5.s64 = ctx.r7.s64 + -14712;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r3,r29,52
	ctx.r3.s64 = r29.s64 + 52;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwzx r6,r9,r8
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// stw r6,32(r29)
	PPC_STORE_U32(r29.u32 + 32, ctx.r6.u32);
	// lwzx r11,r9,r11
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r11.u32);
	// stw r11,36(r29)
	PPC_STORE_U32(r29.u32 + 36, r11.u32);
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r10,r29,364
	ctx.r10.s64 = r29.s64 + 364;
	// addi r15,r11,28564
	r15.s64 = r11.s64 + 28564;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// stw r15,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r15.u32);
	// addi r11,r11,-14688
	r11.s64 = r11.s64 + -14688;
	// addi r10,r10,-14696
	ctx.r10.s64 = ctx.r10.s64 + -14696;
	// addi r16,r29,796
	r16.s64 = r29.s64 + 796;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// addi r30,r29,104
	r30.s64 = r29.s64 + 104;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
loc_82BAAA80:
	// lwz r18,0(r15)
	r18.u64 = PPC_LOAD_U32(r15.u32 + int32_t(0) );
	// li r23,0
	r23.s64 = 0;
	// li r22,0
	r22.s64 = 0;
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// beq cr6,0x82baaca0
	if (cr6.eq) goto loc_82BAACA0;
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// lwz r5,92(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// addi r11,r20,2
	r11.s64 = r20.s64 + 2;
	// addi r10,r14,5
	ctx.r10.s64 = r14.s64 + 5;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r26,0
	r26.s64 = 0;
	// add r9,r11,r21
	ctx.r9.u64 = r11.u64 + r21.u64;
	// addi r25,r18,1600
	r25.s64 = r18.s64 + 1600;
	// rlwinm r17,r9,2,0,29
	r17.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r28,r18,36
	r28.s64 = r18.s64 + 36;
	// mr r31,r16
	r31.u64 = r16.u64;
	// rlwinm r19,r10,2,0,29
	r19.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_82BAAAD0:
	// lwz r8,0(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lwzx r24,r25,r17
	r24.u64 = PPC_LOAD_U32(r25.u32 + r17.u32);
	// lwzx r27,r19,r28
	r27.u64 = PPC_LOAD_U32(r19.u32 + r28.u32);
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x82baac1c
	if (cr6.eq) goto loc_82BAAC1C;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// li r4,32
	ctx.r4.s64 = 32;
	// lwz r7,-4(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-4) );
	// addi r3,r31,-32
	ctx.r3.s64 = r31.s64 + -32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lwz r8,-4(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-4) );
	// mulli r11,r27,100
	r11.s64 = r27.s64 * 100;
	// mullw r7,r9,r8
	ctx.r7.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// stw r7,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r7.u32);
	// lwzx r6,r19,r28
	ctx.r6.u64 = PPC_LOAD_U32(r19.u32 + r28.u32);
	// stw r6,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r6.u32);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// li r10,100
	ctx.r10.s64 = 100;
	// divw r4,r11,r5
	ctx.r4.s32 = r11.s32 / ctx.r5.s32;
	// cmpwi cr6,r27,0
	cr6.compare<int32_t>(r27.s32, 0, xer);
	// stw r4,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r4.u32);
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// ble cr6,0x82baab48
	if (!cr6.gt) goto loc_82BAAB48;
	// lwz r11,-4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-4) );
	// mulli r10,r24,100
	ctx.r10.s64 = r24.s64 * 100;
	// mullw r9,r27,r11
	ctx.r9.s64 = int64_t(r27.s32) * int64_t(r11.s32);
	// divw r8,r10,r9
	ctx.r8.s32 = ctx.r10.s32 / ctx.r9.s32;
	// stw r8,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r8.u32);
loc_82BAAB48:
	// addi r10,r20,4
	ctx.r10.s64 = r20.s64 + 4;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r26,12
	cr6.compare<int32_t>(r26.s32, 12, xer);
	// add r9,r10,r21
	ctx.r9.u64 = ctx.r10.u64 + r21.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r8,r25
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + r25.u32);
	// addi r25,r25,128
	r25.s64 = r25.s64 + 128;
	// stw r7,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r7.u32);
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// lwz r9,36(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(36) );
	// lwz r11,44(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(44) );
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// add r5,r27,r10
	ctx.r5.u64 = r27.u64 + ctx.r10.u64;
	// stw r6,32(r30)
	PPC_STORE_U32(r30.u32 + 32, ctx.r6.u32);
	// add r4,r24,r11
	ctx.r4.u64 = r24.u64 + r11.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r5,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r5.u32);
	// stw r4,44(r30)
	PPC_STORE_U32(r30.u32 + 44, ctx.r4.u32);
	// stw r3,36(r30)
	PPC_STORE_U32(r30.u32 + 36, ctx.r3.u32);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r10,48(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(48) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,48(r30)
	PPC_STORE_U32(r30.u32 + 48, r11.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(84) );
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,96(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(96) );
	// lwz r9,88(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(88) );
	// lwz r10,92(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(92) );
	// add r7,r10,r27
	ctx.r7.u64 = ctx.r10.u64 + r27.u64;
	// stw r8,84(r29)
	PPC_STORE_U32(r29.u32 + 84, ctx.r8.u32);
	// add r6,r11,r24
	ctx.r6.u64 = r11.u64 + r24.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// add r5,r10,r9
	ctx.r5.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r7,92(r29)
	PPC_STORE_U32(r29.u32 + 92, ctx.r7.u32);
	// stw r6,96(r29)
	PPC_STORE_U32(r29.u32 + 96, ctx.r6.u32);
	// stw r5,88(r29)
	PPC_STORE_U32(r29.u32 + 88, ctx.r5.u32);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// addi r31,r31,52
	r31.s64 = r31.s64 + 52;
	// lwz r11,100(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(100) );
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// stw r4,100(r29)
	PPC_STORE_U32(r29.u32 + 100, ctx.r4.u32);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lwz r3,-4(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-4) );
	// mullw r11,r27,r3
	r11.s64 = int64_t(r27.s32) * int64_t(ctx.r3.s32);
	// addi r28,r28,120
	r28.s64 = r28.s64 + 120;
	// add r22,r11,r22
	r22.u64 = r11.u64 + r22.u64;
	// add r23,r23,r10
	r23.u64 = r23.u64 + ctx.r10.u64;
	// blt cr6,0x82baaad0
	if (cr6.lt) goto loc_82BAAAD0;
loc_82BAAC1C:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// li r11,0
	r11.s64 = 0;
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// cmpwi cr6,r23,0
	cr6.compare<int32_t>(r23.s32, 0, xer);
	// add r8,r23,r10
	ctx.r8.u64 = r23.u64 + ctx.r10.u64;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// add r7,r22,r9
	ctx.r7.u64 = r22.u64 + ctx.r9.u64;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// ble cr6,0x82baac4c
	if (!cr6.gt) goto loc_82BAAC4C;
	// divw r11,r11,r23
	r11.s32 = r11.s32 / r23.s32;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
loc_82BAAC4C:
	// li r11,100
	r11.s64 = 100;
	// cmpwi cr6,r22,0
	cr6.compare<int32_t>(r22.s32, 0, xer);
	// stw r11,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r11.u32);
	// ble cr6,0x82baac68
	if (!cr6.gt) goto loc_82BAAC68;
	// li r11,10000
	r11.s64 = 10000;
	// divw r10,r11,r22
	ctx.r10.s32 = r11.s32 / r22.s32;
	// stw r10,44(r30)
	PPC_STORE_U32(r30.u32 + 44, ctx.r10.u32);
loc_82BAAC68:
	// lwz r31,88(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// addi r3,r18,3136
	ctx.r3.s64 = r18.s64 + 3136;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x82baace8
	sub_82BAACE8(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// addi r15,r15,4
	r15.s64 = r15.s64 + 4;
	// addi r10,r11,20
	ctx.r10.s64 = r11.s64 + 20;
	// addi r9,r31,80
	ctx.r9.s64 = r31.s64 + 80;
	// addi r30,r30,52
	r30.s64 = r30.s64 + 52;
	// addi r16,r16,624
	r16.s64 = r16.s64 + 624;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// cmpw cr6,r15,r10
	cr6.compare<int32_t>(r15.s32, ctx.r10.s32, xer);
	// blt cr6,0x82baaa80
	if (cr6.lt) goto loc_82BAAA80;
loc_82BAACA0:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,92(r29)
	PPC_STORE_U32(r29.u32 + 92, ctx.r10.u32);
	// ble cr6,0x82baacbc
	if (!cr6.gt) goto loc_82BAACBC;
	// divw r11,r10,r11
	r11.s32 = ctx.r10.s32 / r11.s32;
	// stw r11,92(r29)
	PPC_STORE_U32(r29.u32 + 92, r11.u32);
loc_82BAACBC:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// li r10,100
	ctx.r10.s64 = 100;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r10,96(r29)
	PPC_STORE_U32(r29.u32 + 96, ctx.r10.u32);
	// ble cr6,0x82baacdc
	if (!cr6.gt) goto loc_82BAACDC;
	// li r10,10000
	ctx.r10.s64 = 10000;
	// divw r9,r10,r11
	ctx.r9.s32 = ctx.r10.s32 / r11.s32;
	// stw r9,96(r29)
	PPC_STORE_U32(r29.u32 + 96, ctx.r9.u32);
loc_82BAACDC:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BAA9C0) {
	__imp__sub_82BAA9C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAACE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// stw r5,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r5.u32);
	// beq cr6,0x82baadac
	if (cr6.eq) goto loc_82BAADAC;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
loc_82BAAD18:
	// rlwinm r9,r6,2,0,29
	ctx.r9.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r10,3,0,28
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r11,r11,r7
	r11.u64 = r11.u64 + ctx.r7.u64;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// lwzx r9,r8,r3
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r3.u32);
	// cmpw cr6,r9,r31
	cr6.compare<int32_t>(ctx.r9.s32, r31.s32, xer);
	// ble cr6,0x82baad80
	if (!cr6.gt) goto loc_82BAAD80;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// stw r6,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r6.u32);
	// li r11,1
	r11.s64 = 1;
loc_82BAAD4C:
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// lwz r8,4(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(4) );
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// bge cr6,0x82baad70
	if (!cr6.lt) goto loc_82BAAD70;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAAD70:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// cmplwi cr6,r11,10
	cr6.compare<uint32_t>(r11.u32, 10, xer);
	// blt cr6,0x82baad4c
	if (cr6.lt) goto loc_82BAAD4C;
loc_82BAAD80:
	// addi r11,r6,1
	r11.s64 = ctx.r6.s64 + 1;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// cmplwi cr6,r11,512
	cr6.compare<uint32_t>(r11.u32, 512, xer);
	// blt cr6,0x82baad18
	if (cr6.lt) goto loc_82BAAD18;
	// lis r11,-32069
	r11.s64 = -2101673984;
	// li r5,8
	ctx.r5.s64 = 8;
	// addi r6,r11,-21056
	ctx.r6.s64 = r11.s64 + -21056;
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x82ca5868
	sub_82CA5868(ctx, base);
loc_82BAADAC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAACE8) {
	__imp__sub_82BAACE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAADC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAADC0) {
	__imp__sub_82BAADC0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAADD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAADD0) {
	__imp__sub_82BAADD0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAADE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-496(r1)
	ea = -496 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r28,-31927
	r28.s64 = -2092367872;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r27,-31951
	r27.s64 = -2093940736;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r11,28560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28560) );
	// addi r11,r11,64
	r11.s64 = r11.s64 + 64;
	// stw r11,28560(r28)
	PPC_STORE_U32(r28.u32 + 28560, r11.u32);
	// beq cr6,0x82baae2c
	if (cr6.eq) goto loc_82BAAE2C;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,40
	ctx.r5.s64 = 40;
	// addi r4,r10,-14664
	ctx.r4.s64 = ctx.r10.s64 + -14664;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82baae44
	goto loc_82BAAE44;
loc_82BAAE2C:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14664
	ctx.r4.s64 = r11.s64 + -14664;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAAE44:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// li r5,64
	ctx.r5.s64 = 64;
	// lwz r30,28564(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28564) );
	// addi r4,r30,1472
	ctx.r4.s64 = r30.s64 + 1472;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r4,r30,1536
	ctx.r4.s64 = r30.s64 + 1536;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82baa830
	sub_82BAA830(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baae94
	if (cr6.eq) goto loc_82BAAE94;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,67
	ctx.r5.s64 = 67;
	// addi r4,r10,-14616
	ctx.r4.s64 = ctx.r10.s64 + -14616;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82baaeac
	goto loc_82BAAEAC;
loc_82BAAE94:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14616
	ctx.r4.s64 = r11.s64 + -14616;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAAEAC:
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(164) );
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r9,160(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(160) );
	// li r4,256
	ctx.r4.s64 = 256;
	// srawi r8,r10,10
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 10;
	// lwz r6,168(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(168) );
	// lwz r30,172(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(172) );
	// addi r29,r11,-14548
	r29.s64 = r11.s64 + -14548;
	// addze r3,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r3.s64 = temp.s64;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// srawi r10,r9,10
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 10;
	// lwz r26,128(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// srawi r8,r6,10
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 10;
	// lwz r7,140(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(140) );
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r26.u32);
	// srawi r6,r30,10
	xer.ca = (r30.s32 < 0) & ((r30.u32 & 0x3FF) != 0);
	ctx.r6.s64 = r30.s32 >> 10;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baaf58
	if (cr6.eq) goto loc_82BAAF58;
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAAF20:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82baaf20
	if (!cr6.eq) goto loc_82BAAF20;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82baaf6c
	goto loc_82BAAF6C;
loc_82BAAF58:
	// lwz r11,19068(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAAF6C:
	// lwz r10,148(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(148) );
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r7,144(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(144) );
	// li r4,256
	ctx.r4.s64 = 256;
	// srawi r8,r10,10
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 10;
	// lwz r6,152(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(152) );
	// addi r30,r11,-14492
	r30.s64 = r11.s64 + -14492;
	// lwz r11,156(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(156) );
	// addze r5,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r5.s64 = temp.s64;
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(112) );
	// srawi r10,r7,10
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 10;
	// lwz r26,116(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(116) );
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// lwz r7,124(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(124) );
	// srawi r8,r6,10
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r6.s32 >> 10;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(120) );
	// srawi r6,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r6.s64 = r11.s32 >> 10;
	// stw r26,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r26.u32);
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bab018
	if (cr6.eq) goto loc_82BAB018;
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAAFE0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82baafe0
	if (!cr6.eq) goto loc_82BAAFE0;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab02c
	goto loc_82BAB02C;
loc_82BAB018:
	// lwz r11,19068(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB02C:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,-7176
	ctx.r4.s64 = r11.s64 + -7176;
	// li r5,64
	ctx.r5.s64 = 64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bab06c
	if (cr6.eq) goto loc_82BAB06C;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,6
	ctx.r5.s64 = 6;
	// addi r4,r10,-14436
	ctx.r4.s64 = ctx.r10.s64 + -14436;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab084
	goto loc_82BAB084;
loc_82BAB06C:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14436
	ctx.r4.s64 = r11.s64 + -14436;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB084:
	// lwz r11,164(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(164) );
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// li r4,256
	ctx.r4.s64 = 256;
	// srawi r9,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r9.s64 = r11.s32 >> 10;
	// lwz r8,160(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(160) );
	// lwz r7,168(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(168) );
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// addze r6,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r6.s64 = temp.s64;
	// lwz r11,172(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(172) );
	// srawi r9,r8,10
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FF) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 10;
	// lwz r29,128(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// addze r10,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r8,r7,10
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 10;
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// lwz r7,140(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(140) );
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// srawi r6,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r6.s64 = r11.s32 >> 10;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bab128
	if (cr6.eq) goto loc_82BAB128;
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB0F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab0f0
	if (!cr6.eq) goto loc_82BAB0F0;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab13c
	goto loc_82BAB13C;
loc_82BAB128:
	// lwz r11,19068(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB13C:
	// lwz r11,148(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(148) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r10,144(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(144) );
	// li r4,256
	ctx.r4.s64 = 256;
	// srawi r9,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r9.s64 = r11.s32 >> 10;
	// lwz r8,152(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(152) );
	// lwz r6,156(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(156) );
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// addze r11,r9
	temp.s64 = ctx.r9.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r9.u32;
	r11.s64 = temp.s64;
	// lwz r30,112(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(112) );
	// srawi r10,r10,10
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 10;
	// lwz r29,116(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(116) );
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(120) );
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// lwz r7,124(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(124) );
	// srawi r8,r8,10
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 10;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r29.u32);
	// srawi r6,r6,10
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x3FF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 10;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bab1e0
	if (cr6.eq) goto loc_82BAB1E0;
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB1A8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab1a8
	if (!cr6.eq) goto loc_82BAB1A8;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab1f4
	goto loc_82BAB1F4;
loc_82BAB1E0:
	// lwz r11,19068(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB1F4:
	// lwz r11,28560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28560) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r4,256
	ctx.r4.s64 = 256;
	// addi r9,r11,10248
	ctx.r9.s64 = r11.s64 + 10248;
	// addi r5,r10,-14428
	ctx.r5.s64 = ctx.r10.s64 + -14428;
	// rlwinm r6,r9,22,10,31
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 22) & 0x3FFFFF;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bab26c
	if (cr6.eq) goto loc_82BAB26C;
	// addi r11,r1,176
	r11.s64 = ctx.r1.s64 + 176;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB224:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab224
	if (!cr6.eq) goto loc_82BAB224;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,28560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28560) );
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// stw r11,28560(r28)
	PPC_STORE_U32(r28.u32 + 28560, r11.u32);
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x82ca2c30
	return;
loc_82BAB26C:
	// lwz r11,19068(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,28560(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28560) );
	// addi r11,r11,-64
	r11.s64 = r11.s64 + -64;
	// stw r11,28560(r28)
	PPC_STORE_U32(r28.u32 + 28560, r11.u32);
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAADE0) {
	__imp__sub_82BAADE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAB298) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-400(r1)
	ea = -400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r26,-31951
	r26.s64 = -2093940736;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82bab2d4
	if (cr6.eq) goto loc_82BAB2D4;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,38
	ctx.r5.s64 = 38;
	// addi r4,r10,-14400
	ctx.r4.s64 = ctx.r10.s64 + -14400;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab2ec
	goto loc_82BAB2EC;
loc_82BAB2D4:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14400
	ctx.r4.s64 = r11.s64 + -14400;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB2EC:
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// addi r31,r11,-20320
	r31.s64 = r11.s64 + -20320;
	// addi r6,r10,-21040
	ctx.r6.s64 = ctx.r10.s64 + -21040;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r5,20
	ctx.r5.s64 = 20;
	// lwz r28,10244(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(10244) );
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82ca5868
	sub_82CA5868(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r28,0
	cr6.compare<int32_t>(r28.s32, 0, xer);
	// ble cr6,0x82bab3b8
	if (!cr6.gt) goto loc_82BAB3B8;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r31,r31,12
	r31.s64 = r31.s64 + 12;
	// addi r27,r11,-14360
	r27.s64 = r11.s64 + -14360;
loc_82BAB328:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r4,256
	ctx.r4.s64 = 256;
	// lwz r8,-4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-4) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r7,-8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-8) );
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82bab394
	if (cr6.eq) goto loc_82BAB394;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB35C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab35c
	if (!cr6.eq) goto loc_82BAB35C;
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab3a8
	goto loc_82BAB3A8;
loc_82BAB394:
	// lwz r11,19068(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(19068) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB3A8:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// blt cr6,0x82bab328
	if (cr6.lt) goto loc_82BAB328;
loc_82BAB3B8:
	// addi r1,r1,400
	ctx.r1.s64 = ctx.r1.s64 + 400;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAB298) {
	__imp__sub_82BAB298(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAB3C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bbc
	// ld r12,-4096(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// stwu r1,-4400(r1)
	ea = -4400 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r11,0
	r11.s64 = 0;
	// li r5,255
	ctx.r5.s64 = 255;
	// li r4,0
	ctx.r4.s64 = 0;
	// stb r11,128(r1)
	PPC_STORE_U8(ctx.r1.u32 + 128, r11.u8);
	// addi r3,r1,129
	ctx.r3.s64 = ctx.r1.s64 + 129;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x82baa9c0
	sub_82BAA9C0(ctx, base);
	// lis r20,-31951
	r20.s64 = -2093940736;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab434
	if (cr6.eq) goto loc_82BAB434;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,38
	ctx.r5.s64 = 38;
	// addi r4,r10,-14324
	ctx.r4.s64 = ctx.r10.s64 + -14324;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab44c
	goto loc_82BAB44C;
loc_82BAB434:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14324
	ctx.r4.s64 = r11.s64 + -14324;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB44C:
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bab464
	if (cr6.eq) goto loc_82BAB464;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r7,r10,-14284
	ctx.r7.s64 = ctx.r10.s64 + -14284;
	// b 0x82bab46c
	goto loc_82BAB46C;
loc_82BAB464:
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r7,r10,-14276
	ctx.r7.s64 = ctx.r10.s64 + -14276;
loc_82BAB46C:
	// clrlwi r19,r29,24
	r19.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x82bab484
	if (cr6.eq) goto loc_82BAB484;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r6,r10,-14268
	ctx.r6.s64 = ctx.r10.s64 + -14268;
	// b 0x82bab48c
	goto loc_82BAB48C;
loc_82BAB484:
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r6,r10,-14260
	ctx.r6.s64 = ctx.r10.s64 + -14260;
loc_82BAB48C:
	// cntlzw r11,r11
	r11.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// lwz r8,416(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(416) );
	// lis r4,-31927
	ctx.r4.s64 = -2092367872;
	// lwz r9,420(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(420) );
	// rlwinm r11,r11,27,31,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// lis r3,-32240
	ctx.r3.s64 = -2112880640;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r5,r3,-14248
	ctx.r5.s64 = ctx.r3.s64 + -14248;
	// lwz r10,28564(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(28564) );
	// addi r4,r11,24
	ctx.r4.s64 = r11.s64 + 24;
	// addi r3,r11,8
	ctx.r3.s64 = r11.s64 + 8;
	// addi r11,r10,1472
	r11.s64 = ctx.r10.s64 + 1472;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r31,r3,2,0,29
	r31.u64 = rotl64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,256
	ctx.r4.s64 = 256;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// lwzx r11,r31,r11
	r11.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi r10,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r10.s64 = r11.s32 >> 10;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r8,r8,10
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3FF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 10;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab574
	if (cr6.eq) goto loc_82BAB574;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB4FC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab4fc
	if (!cr6.eq) goto loc_82BAB4FC;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,60
	ctx.r5.s64 = 60;
	// addi r4,r10,-14180
	ctx.r4.s64 = ctx.r10.s64 + -14180;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,66
	ctx.r5.s64 = 66;
	// addi r4,r10,-14112
	ctx.r4.s64 = ctx.r10.s64 + -14112;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab5b8
	goto loc_82BAB5B8;
loc_82BAB574:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14180
	ctx.r4.s64 = r11.s64 + -14180;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-14112
	ctx.r4.s64 = r11.s64 + -14112;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB5B8:
	// lwz r11,468(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(468) );
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// li r4,256
	ctx.r4.s64 = 256;
	// lwz r10,484(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(484) );
	// srawi r7,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r7.s64 = r11.s32 >> 10;
	// lwz r9,480(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(480) );
	// addi r5,r8,-14044
	ctx.r5.s64 = ctx.r8.s64 + -14044;
	// lwz r8,476(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(476) );
	// addze r6,r7
	temp.s64 = ctx.r7.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r6.s64 = temp.s64;
	// lwz r7,472(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(472) );
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab630
	if (cr6.eq) goto loc_82BAB630;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB5F8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab5f8
	if (!cr6.eq) goto loc_82BAB5F8;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab644
	goto loc_82BAB644;
loc_82BAB630:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB644:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// addi r29,r1,780
	r29.s64 = ctx.r1.s64 + 780;
	// addi r25,r1,1192
	r25.s64 = ctx.r1.s64 + 1192;
	// addi r27,r1,532
	r27.s64 = ctx.r1.s64 + 532;
	// li r24,5
	r24.s64 = 5;
	// addi r22,r11,-13808
	r22.s64 = r11.s64 + -13808;
	// addi r21,r10,-13896
	r21.s64 = ctx.r10.s64 + -13896;
	// addi r23,r9,-13948
	r23.s64 = ctx.r9.s64 + -13948;
	// addi r26,r8,-13988
	r26.s64 = ctx.r8.s64 + -13988;
loc_82BAB674:
	// lwz r11,-12(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(-12) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82bab93c
	if (!cr6.gt) goto loc_82BAB93C;
	// lwz r31,4(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// srawi r11,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	r11.s64 = r11.s32 >> 10;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// addze r7,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r7.s64 = temp.s64;
	// lwz r9,-4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(-4) );
	// addi r6,r27,-44
	ctx.r6.s64 = r27.s64 + -44;
	// lwz r8,-8(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + int32_t(-8) );
	// li r4,256
	ctx.r4.s64 = 256;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab6f8
	if (cr6.eq) goto loc_82BAB6F8;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB6C0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab6c0
	if (!cr6.eq) goto loc_82BAB6C0;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab70c
	goto loc_82BAB70C;
loc_82BAB6F8:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB70C:
	// mr r31,r25
	r31.u64 = r25.u64;
	// li r28,12
	r28.s64 = 12;
loc_82BAB714:
	// lwz r11,-12(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(-12) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82bab7b0
	if (!cr6.gt) goto loc_82BAB7B0;
	// lwz r11,-12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-12) );
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// lwz r18,4(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r6,r31,-44
	ctx.r6.s64 = r31.s64 + -44;
	// srawi r8,r11,10
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3FF) != 0);
	ctx.r8.s64 = r11.s32 >> 10;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r4,256
	ctx.r4.s64 = 256;
	// lwz r9,-4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-4) );
	// addze r7,r8
	temp.s64 = ctx.r8.s64 + xer.ca;
	xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r7.s64 = temp.s64;
	// lwz r8,-8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-8) );
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab79c
	if (cr6.eq) goto loc_82BAB79C;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB764:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab764
	if (!cr6.eq) goto loc_82BAB764;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab7b0
	goto loc_82BAB7B0;
loc_82BAB79C:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB7B0:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r31,r31,52
	r31.s64 = r31.s64 + 52;
	// bne 0x82bab714
	if (!cr0.eq) goto loc_82BAB714;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x82bab7ec
	if (cr6.eq) goto loc_82BAB7EC;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// beq cr6,0x82bab92c
	if (cr6.eq) goto loc_82BAB92C;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,44
	ctx.r5.s64 = 44;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab93c
	goto loc_82BAB93C;
loc_82BAB7EC:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// mr r5,r21
	ctx.r5.u64 = r21.u64;
	// lwz r31,0(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// li r4,256
	ctx.r4.s64 = 256;
	// lwz r28,-4(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-4) );
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r18,-8(r29)
	r18.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-8) );
	// lwz r17,-12(r29)
	r17.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-12) );
	// lwz r10,-16(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-16) );
	// lwz r9,-20(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-20) );
	// lwz r8,-24(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-24) );
	// lwz r7,-28(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-28) );
	// lwz r6,-32(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-32) );
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// stw r18,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r18.u32);
	// stw r17,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r17.u32);
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab880
	if (cr6.eq) goto loc_82BAB880;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB848:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab848
	if (!cr6.eq) goto loc_82BAB848;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab894
	goto loc_82BAB894;
loc_82BAB880:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB894:
	// lwz r10,32(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// lwz r9,28(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(28) );
	// li r4,256
	ctx.r4.s64 = 256;
	// lwz r11,44(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(44) );
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r31,40(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// lwz r28,36(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + int32_t(36) );
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// lwz r10,24(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(24) );
	// lwz r9,20(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(20) );
	// lwz r8,16(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// lwz r7,12(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + int32_t(12) );
	// lwz r6,8(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// stw r31,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r31.u32);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bab928
	if (cr6.eq) goto loc_82BAB928;
	// addi r11,r1,128
	r11.s64 = ctx.r1.s64 + 128;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_82BAB8F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bab8f0
	if (!cr6.eq) goto loc_82BAB8F0;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// rotlwi r5,r8,0
	ctx.r5.u64 = rotl32(ctx.r8.u32, 0);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab93c
	goto loc_82BAB93C;
loc_82BAB928:
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
loc_82BAB92C:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB93C:
	// addic. r24,r24,-1
	xer.ca = r24.u32 > 0;
	r24.s64 = r24.s64 + -1;
	cr0.compare<int32_t>(r24.s32, 0, xer);
	// addi r27,r27,52
	r27.s64 = r27.s64 + 52;
	// addi r25,r25,624
	r25.s64 = r25.s64 + 624;
	// addi r29,r29,80
	r29.s64 = r29.s64 + 80;
	// bne 0x82bab674
	if (!cr0.eq) goto loc_82BAB674;
	// addi r1,r1,4400
	ctx.r1.s64 = ctx.r1.s64 + 4400;
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82BAB3C0) {
	__imp__sub_82BAB3C0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAB958) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82baade0
	sub_82BAADE0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bab298
	sub_82BAB298(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bab9c8
	if (cr6.eq) goto loc_82BAB9C8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,37
	ctx.r5.s64 = 37;
	// addi r4,r10,-13720
	ctx.r4.s64 = ctx.r10.s64 + -13720;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r5,63
	ctx.r5.s64 = 63;
	// addi r4,r10,-13680
	ctx.r4.s64 = ctx.r10.s64 + -13680;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x82bab9fc
	goto loc_82BAB9FC;
loc_82BAB9C8:
	// lis r30,-31951
	r30.s64 = -2093940736;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-13720
	ctx.r4.s64 = r11.s64 + -13720;
	// lwz r10,19068(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,19068(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(19068) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r4,r11,-13680
	ctx.r4.s64 = r11.s64 + -13680;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAB9FC:
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x82bab3c0
	sub_82BAB3C0(ctx, base);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82bab3c0
	sub_82BAB3C0(ctx, base);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x82bab3c0
	sub_82BAB3C0(ctx, base);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82bab3c0
	sub_82BAB3C0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAB958) {
	__imp__sub_82BAB958(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABA58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-13352
	ctx.r9.s64 = r11.s64 + -13352;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82baba8c
	if (cr6.eq) goto loc_82BABA8C;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BABA8C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BABA58) {
	__imp__sub_82BABA58(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABAA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82babaf0
	sub_82BABAF0(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82babad8
	if (cr6.eq) goto loc_82BABAD8;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BABAD8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BABAA0) {
	__imp__sub_82BABAA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABAF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r10,r11,-13332
	ctx.r10.s64 = r11.s64 + -13332;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x82babb50
	sub_82BABB50(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// addi r11,r31,16
	r11.s64 = r31.s64 + 16;
	// stw r30,56(r31)
	PPC_STORE_U32(r31.u32 + 56, r30.u32);
	// lwz r29,52(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(52) );
	// lwz r28,48(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r5,r29,2,0,29
	ctx.r5.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// stw r30,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r30.u32);
	// addi r8,r9,-13352
	ctx.r8.s64 = ctx.r9.s64 + -13352;
	// stw r30,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r30.u32);
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BABAF0) {
	__imp__sub_82BABAF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABB50) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// mr r26,r28
	r26.u64 = r28.u64;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82babbc4
	if (!cr6.gt) goto loc_82BABBC4;
	// mr r27,r28
	r27.u64 = r28.u64;
loc_82BABB78:
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// add r31,r27,r11
	r31.u64 = r27.u64 + r11.u64;
	// lwzx r29,r27,r11
	r29.u64 = PPC_LOAD_U32(r27.u32 + r11.u32);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82babbb0
	if (cr6.eq) goto loc_82BABBB0;
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r25,r11,1
	r25.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r28,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r28.u16);
	// sth r28,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r28.u16);
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
loc_82BABBB0:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,8
	r27.s64 = r27.s64 + 8;
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// blt cr6,0x82babb78
	if (cr6.lt) goto loc_82BABB78;
loc_82BABBC4:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r29,12(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r5,r31,3,0,28
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r11,56(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(56) );
	// stw r28,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r28.u32);
	// mr r26,r28
	r26.u64 = r28.u64;
	// stw r28,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r28.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// stw r28,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r28.u32);
	// ble cr6,0x82babc58
	if (!cr6.gt) goto loc_82BABC58;
	// mr r27,r28
	r27.u64 = r28.u64;
loc_82BABBFC:
	// lwz r11,48(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(48) );
	// lwzx r31,r11,r27
	r31.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82babc34
	if (cr6.eq) goto loc_82BABC34;
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r25,r11,1
	r25.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r28,6(r31)
	PPC_STORE_U16(r31.u32 + 6, r28.u16);
	// sth r28,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r28.u16);
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
loc_82BABC34:
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// li r5,8
	ctx.r5.s64 = 8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r11,56(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(56) );
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// cmpw cr6,r26,r11
	cr6.compare<int32_t>(r26.s32, r11.s32, xer);
	// blt cr6,0x82babbfc
	if (cr6.lt) goto loc_82BABBFC;
loc_82BABC58:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BABB50) {
	__imp__sub_82BABB50(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABC60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 + r11.u64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BABC60) {
	__imp__sub_82BABC60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABC70) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister r11{};
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// subf r10,r11,r4
	ctx.r10.s64 = ctx.r4.s64 - r11.s64;
	// srawi r3,r10,3
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 3;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BABC70) {
	__imp__sub_82BABC70(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABC80) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r30,r11,-13592
	r30.s64 = r11.s64 + -13592;
	// addi r28,r10,-13468
	r28.s64 = ctx.r10.s64 + -13468;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r7,149
	ctx.r7.s64 = 149;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// sth r11,6(r30)
	PPC_STORE_U16(r30.u32 + 6, r11.u16);
	// sth r11,4(r30)
	PPC_STORE_U16(r30.u32 + 4, r11.u16);
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82babce4
	if (!cr6.eq) goto loc_82BABCE4;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r11,r11,3224
	r11.s64 = r11.s64 + 3224;
loc_82BABCE4:
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82babd40
	if (!cr6.eq) goto loc_82BABD40;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
loc_82BABD40:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r30,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r30.u32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// stw r8,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r8.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BABC80) {
	__imp__sub_82BABC80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABD68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-13180
	ctx.r9.s64 = r11.s64 + -13180;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82babd9c
	if (cr6.eq) goto loc_82BABD9C;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BABD9C:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BABD68) {
	__imp__sub_82BABD68(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABDB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r9,r11,-13172
	ctx.r9.s64 = r11.s64 + -13172;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r29,4(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// rlwinm r28,r10,3,0,28
	r28.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// clrlwi r7,r30,31
	ctx.r7.u64 = r30.u32 & 0x1;
	// addi r6,r8,-13180
	ctx.r6.s64 = ctx.r8.s64 + -13180;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stw r6,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r6.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x82babe10
	if (cr6.eq) goto loc_82BABE10;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BABE10:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BABDB0) {
	__imp__sub_82BABDB0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABE18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r30,32(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82babe84
	if (cr6.eq) goto loc_82BABE84;
loc_82BABE40:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82babe68
	if (cr6.eq) goto loc_82BABE68;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82bac420
	sub_82BAC420(ctx, base);
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r5,52
	ctx.r5.s64 = 52;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BABE68:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x82babe40
	if (!cr6.eq) goto loc_82BABE40;
loc_82BABE84:
	// li r30,0
	r30.s64 = 0;
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r28,32(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r5,r29,2,0,29
	ctx.r5.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r31,48
	ctx.r3.s64 = r31.s64 + 48;
	// bl 0x82bac6e8
	sub_82BAC6E8(ctx, base);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r28,32(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r5,r29,2,0,29
	ctx.r5.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BABE18) {
	__imp__sub_82BABE18(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BABEE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bcc
	// stfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r10,-32245
	ctx.r10.s64 = -2113208320;
	// mr r21,r7
	r21.u64 = ctx.r7.u64;
	// addi r26,r10,-6332
	r26.s64 = ctx.r10.s64 + -6332;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// mr r22,r6
	r22.u64 = ctx.r6.u64;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32245
	ctx.r9.s64 = -2113208320;
	// add r6,r10,r11
	ctx.r6.u64 = ctx.r10.u64 + r11.u64;
	// lfs f31,-16596(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -16596);
	f31.f64 = double(temp.f32);
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// li r27,0
	r27.s64 = 0;
	// addi r25,r9,-6344
	r25.s64 = ctx.r9.s64 + -6344;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// addi r24,r8,-13128
	r24.s64 = ctx.r8.s64 + -13128;
	// beq cr6,0x82babf84
	if (cr6.eq) goto loc_82BABF84;
loc_82BABF48:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r28,r9
	cr6.compare<uint32_t>(r28.u32, ctx.r9.u32, xer);
	// beq cr6,0x82babf78
	if (cr6.eq) goto loc_82BABF78;
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82babf48
	if (!cr6.eq) goto loc_82BABF48;
	// b 0x82babf84
	goto loc_82BABF84;
loc_82BABF78:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addic. r30,r11,4
	xer.ca = r11.u32 > 4294967291;
	r30.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x82bac06c
	if (!cr0.eq) goto loc_82BAC06C;
loc_82BABF84:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r30,r11,-13016
	r30.s64 = r11.s64 + -13016;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,52
	ctx.r4.s64 = 52;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// li r7,82
	ctx.r7.s64 = 82;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82babfec
	if (cr6.eq) goto loc_82BABFEC;
	// stfs f31,48(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r30.u32 + 48, temp.u32);
	// stw r27,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r27.u32);
	// stw r27,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r27.u32);
	// addi r29,r30,4
	r29.s64 = r30.s64 + 4;
	// stw r27,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r27.u32);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// b 0x82babff0
	goto loc_82BABFF0;
loc_82BABFEC:
	// mr r30,r27
	r30.u64 = r27.u64;
loc_82BABFF0:
	// stw r28,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r28.u32);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82bac03c
	if (!cr6.eq) goto loc_82BAC03C;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
loc_82BAC03C:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r30,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r30.u32);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r11,-4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(-4) );
	// addi r30,r11,4
	r30.s64 = r11.s64 + 4;
loc_82BAC06C:
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac0c0
	if (cr6.eq) goto loc_82BAC0C0;
loc_82BAC084:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r23,r9
	cr6.compare<uint32_t>(r23.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac0b4
	if (cr6.eq) goto loc_82BAC0B4;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bac084
	if (!cr6.eq) goto loc_82BAC084;
	// b 0x82bac0c0
	goto loc_82BAC0C0;
loc_82BAC0B4:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addic. r3,r11,4
	xer.ca = r11.u32 > 4294967291;
	ctx.r3.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne 0x82bac1a8
	if (!cr0.eq) goto loc_82BAC1A8;
loc_82BAC0C0:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r31,r11,-12988
	r31.s64 = r11.s64 + -12988;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,52
	ctx.r4.s64 = 52;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
	// li r7,103
	ctx.r7.s64 = 103;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bac128
	if (cr6.eq) goto loc_82BAC128;
	// stfs f31,48(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(r31.u32 + 48, temp.u32);
	// stw r27,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r27.u32);
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// addi r29,r31,4
	r29.s64 = r31.s64 + 4;
	// stw r27,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r27.u32);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82baf4b0
	sub_82BAF4B0(ctx, base);
	// b 0x82bac12c
	goto loc_82BAC12C;
loc_82BAC128:
	// mr r31,r27
	r31.u64 = r27.u64;
loc_82BAC12C:
	// stw r23,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r23.u32);
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(36) );
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82bac178
	if (!cr6.eq) goto loc_82BAC178;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
loc_82BAC178:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r31,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r31.u32);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r11,-4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(-4) );
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
loc_82BAC1A8:
	// stw r22,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r22.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r21,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r21.u32);
	// bl 0x82bac750
	sub_82BAC750(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f31,-104(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x82ca2c1c
	return;
}

PPC_WEAK_FUNC(sub_82BABEE8) {
	__imp__sub_82BABEE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC1C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bcc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r24,r8
	r24.u64 = ctx.r8.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r26,r25
	r26.u64 = r25.u64;
	// lwz r10,40(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r25,0(r24)
	PPC_STORE_U8(r24.u32 + 0, r25.u8);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac230
	if (cr6.eq) goto loc_82BAC230;
loc_82BAC200:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac23c
	if (cr6.eq) goto loc_82BAC23C;
	// lwz r10,40(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,32(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bac200
	if (!cr6.eq) goto loc_82BAC200;
loc_82BAC230:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c1c
	return;
loc_82BAC23C:
	// lwz r23,0(r11)
	r23.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addic. r30,r23,4
	xer.ca = r23.u32 > 4294967291;
	r30.s64 = r23.s64 + 4;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// beq 0x82bac230
	if (cr0.eq) goto loc_82BAC230;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// mr r28,r25
	r28.u64 = r25.u64;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac230
	if (cr6.eq) goto loc_82BAC230;
loc_82BAC264:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r5,r9
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac2a0
	if (cr6.eq) goto loc_82BAC2A0;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bac264
	if (!cr6.eq) goto loc_82BAC264;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c1c
	return;
loc_82BAC2A0:
	// lwz r27,0(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r31,r27,4
	r31.s64 = r27.s64 + 4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bac230
	if (cr6.eq) goto loc_82BAC230;
	// lwz r4,40(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// add r9,r9,r5
	ctx.r9.u64 = ctx.r9.u64 + ctx.r5.u64;
	// cmplw cr6,r5,r9
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac230
	if (cr6.eq) goto loc_82BAC230;
loc_82BAC2D0:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r6,r9
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bac2e8
	if (!cr6.eq) goto loc_82BAC2E8;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmplw cr6,r7,r9
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bac314
	if (cr6.eq) goto loc_82BAC314;
loc_82BAC2E8:
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82bac2d0
	if (!cr6.eq) goto loc_82BAC2D0;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c1c
	return;
loc_82BAC314:
	// subf r11,r10,r4
	r11.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r3,r9,r5
	ctx.r3.u64 = ctx.r9.u64 + ctx.r5.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r4,r3,8
	ctx.r4.s64 = ctx.r3.s64 + 8;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bne 0x82bac3ac
	if (!cr0.eq) goto loc_82BAC3AC;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82bac37c
	if (cr6.eq) goto loc_82BAC37C;
	// stw r25,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r25.u32);
	// lwz r22,36(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r21,32(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// rlwinm r5,r22,3,0,28
	ctx.r5.u64 = rotl64(r22.u32 | (r22.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r25,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r25.u32);
	// stw r25,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r25.u32);
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// li r5,52
	ctx.r5.s64 = 52;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAC37C:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// subf r11,r28,r11
	r11.s64 = r11.s64 - r28.s64;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// addi r4,r3,4
	ctx.r4.s64 = ctx.r3.s64 + 4;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// stw r9,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r9.u32);
loc_82BAC3AC:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x82bac414
	if (!cr6.eq) goto loc_82BAC414;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x82bac3dc
	if (cr6.eq) goto loc_82BAC3DC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82bac420
	sub_82BAC420(ctx, base);
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// li r5,52
	ctx.r5.s64 = 52;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAC3DC:
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// rlwinm r10,r26,2,0,29
	ctx.r10.u64 = rotl64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,32(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// subf r11,r26,r11
	r11.s64 = r11.s64 - r26.s64;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// addi r4,r3,4
	ctx.r4.s64 = ctx.r3.s64 + 4;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r8,r11,-1
	ctx.r8.s64 = r11.s64 + -1;
	// stb r9,0(r24)
	PPC_STORE_U8(r24.u32 + 0, ctx.r9.u8);
	// stw r8,40(r29)
	PPC_STORE_U32(r29.u32 + 40, ctx.r8.u32);
loc_82BAC414:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c1c
	return;
}

PPC_WEAK_FUNC(sub_82BAC1C8) {
	__imp__sub_82BAC1C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC420) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// addi r29,r27,4
	r29.s64 = r27.s64 + 4;
	// lwz r11,44(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(44) );
	// lwz r10,36(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(36) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82bac4b8
	if (cr6.eq) goto loc_82BAC4B8;
loc_82BAC454:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bac49c
	if (cr6.eq) goto loc_82BAC49C;
	// stw r28,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r28.u32);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lwz r26,40(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r25,36(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// rlwinm r5,r26,3,0,28
	ctx.r5.u64 = rotl64(r26.u32 | (r26.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r28,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r28.u32);
	// stw r28,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r28.u32);
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// li r5,52
	ctx.r5.s64 = 52;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAC49C:
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// lwz r10,32(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bac454
	if (!cr6.eq) goto loc_82BAC454;
loc_82BAC4B8:
	// stw r28,40(r29)
	PPC_STORE_U32(r29.u32 + 40, r28.u32);
	// lwz r31,36(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(36) );
	// lwz r30,32(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r28,36(r29)
	PPC_STORE_U32(r29.u32 + 36, r28.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r28,32(r29)
	PPC_STORE_U32(r29.u32 + 32, r28.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BAC420) {
	__imp__sub_82BAC420(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC4E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lwz r10,40(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(40) );
	// lwz r11,32(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82bac64c
	if (cr6.eq) goto loc_82BAC64C;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
loc_82BAC518:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// beq cr6,0x82bac53c
	if (cr6.eq) goto loc_82BAC53C;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bac518
	if (!cr6.eq) goto loc_82BAC518;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BAC53C:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addic. r11,r11,4
	xer.ca = r11.u32 > 4294967291;
	r11.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82bac64c
	if (cr0.eq) goto loc_82BAC64C;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// lwz r11,32(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82bac64c
	if (cr6.eq) goto loc_82BAC64C;
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
loc_82BAC564:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// beq cr6,0x82bac588
	if (cr6.eq) goto loc_82BAC588;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bac564
	if (!cr6.eq) goto loc_82BAC564;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BAC588:
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + int32_t(84) );
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r10,3,0,28
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r3,80(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(80) );
	// addi r31,r28,48
	r31.s64 = r28.s64 + 48;
	// stw r9,88(r28)
	PPC_STORE_U32(r28.u32 + 88, ctx.r9.u32);
	// addi r30,r11,4
	r30.s64 = r11.s64 + 4;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,40(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82baf4b0
	sub_82BAF4B0(ctx, base);
	// lwz r8,40(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// rlwinm r11,r8,3,0,28
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r29,32(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// add r7,r11,r29
	ctx.r7.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r29,r7
	cr6.compare<uint32_t>(r29.u32, ctx.r7.u32, xer);
	// beq cr6,0x82bac600
	if (cr6.eq) goto loc_82BAC600;
loc_82BAC5D8:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bac750
	sub_82BAC750(ctx, base);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bac5d8
	if (!cr6.eq) goto loc_82BAC5D8;
loc_82BAC600:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r30,80(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + int32_t(80) );
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82bac64c
	if (cr6.eq) goto loc_82BAC64C;
loc_82BAC61C:
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r30,r30,8
	r30.s64 = r30.s64 + 8;
	// rlwinm r11,r9,3,0,28
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r8
	cr6.compare<uint32_t>(r30.u32, ctx.r8.u32, xer);
	// bne cr6,0x82bac61c
	if (!cr6.eq) goto loc_82BAC61C;
loc_82BAC64C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BAC4E8) {
	__imp__sub_82BAC4E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC658) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r10,r10,-16596
	ctx.r10.s64 = ctx.r10.s64 + -16596;
	// li r11,0
	r11.s64 = 0;
	// stfs f1,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// lfs f0,-10860(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -10860);
	f0.f64 = double(temp.f32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bge cr6,0x82bac69c
	if (!cr6.lt) goto loc_82BAC69C;
	// lfs f0,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	f0.f64 = double(temp.f32);
	// stfs f0,44(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
loc_82BAC69C:
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// bne cr6,0x82bac6ac
	if (!cr6.eq) goto loc_82BAC6AC;
	// lis r11,-32245
	r11.s64 = -2113208320;
	// addi r6,r11,-6344
	ctx.r6.s64 = r11.s64 + -6344;
loc_82BAC6AC:
	// lis r11,-32245
	r11.s64 = -2113208320;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r5,r11,-6332
	ctx.r5.s64 = r11.s64 + -6332;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82baf4b0
	sub_82BAF4B0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC658) {
	__imp__sub_82BAC658(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC6E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// beq cr6,0x82bac734
	if (cr6.eq) goto loc_82BAC734;
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r28,32(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r5,r29,3,0,28
	ctx.r5.u64 = rotl64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82BAC734:
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,32(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BAC6E8) {
	__imp__sub_82BAC6E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC750) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82bac7b0
	if (!cr6.eq) goto loc_82BAC7B0;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82baf4b0
	sub_82BAF4B0(ctx, base);
loc_82BAC7B0:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// add r8,r11,r10
	ctx.r8.u64 = r11.u64 + ctx.r10.u64;
	// stwx r9,r11,r10
	PPC_STORE_U32(r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// stw r7,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r7.u32);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r6,r11,1
	ctx.r6.s64 = r11.s64 + 1;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r6,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r6.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC750) {
	__imp__sub_82BAC750(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC800) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-12960
	ctx.r9.s64 = r11.s64 + -12960;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82bac834
	if (cr6.eq) goto loc_82BAC834;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAC834:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC800) {
	__imp__sub_82BAC800(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC848) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x82baf038
	sub_82BAF038(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r30,31
	ctx.r10.u64 = r30.u32 & 0x1;
	// addi r9,r11,-12960
	ctx.r9.s64 = r11.s64 + -12960;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// beq cr6,0x82bac890
	if (cr6.eq) goto loc_82BAC890;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAC890:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC848) {
	__imp__sub_82BAC848(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC8A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// rlwinm r4,r11,27,5,31
	ctx.r4.u64 = rotl64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// bl 0x82284448
	sub_82284448(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bac8f8
	if (cr6.eq) goto loc_82BAC8F8;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// b 0x82bac8fc
	goto loc_82BAC8FC;
loc_82BAC8F8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82BAC8FC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC8A8) {
	__imp__sub_82BAC8A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC918) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// bl 0x82284448
	sub_82284448(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82bac960
	if (cr6.eq) goto loc_82BAC960;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82BAC960:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC918) {
	__imp__sub_82BAC918(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC978) {
	PPC_FUNC_PROLOGUE();
	// lbz r3,31(r4)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r4.u32 + 31);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC978) {
	__imp__sub_82BAC978(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC980) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lhz r11,28(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 28);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bac9a0
	if (cr6.eq) goto loc_82BAC9A0;
	// rlwinm r11,r5,3,0,28
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
	// addi r11,r11,68
	r11.s64 = r11.s64 + 68;
	// b 0x82bac9ac
	goto loc_82BAC9AC;
loc_82BAC9A0:
	// addi r11,r5,4
	r11.s64 = ctx.r5.s64 + 4;
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r4
	r11.u64 = r11.u64 + ctx.r4.u64;
loc_82BAC9AC:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// rlwinm r9,r10,27,5,31
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// stw r9,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r8,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r8.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC980) {
	__imp__sub_82BAC980(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC9C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r3,r11,3224
	ctx.r3.s64 = r11.s64 + 3224;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC9C8) {
	__imp__sub_82BAC9C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC9E0) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,4(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC9E0) {
	__imp__sub_82BAC9E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC9E8) {
	PPC_FUNC_PROLOGUE();
	// stw r5,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r5.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAC9E8) {
	__imp__sub_82BAC9E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC9F0) {
	PPC_FUNC_PROLOGUE();
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// b 0x82bac9f8
	sub_82BAC9F8(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82BAC9F0) {
	__imp__sub_82BAC9F0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAC9F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(4) );
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpw cr6,r11,r30
	cr6.compare<int32_t>(r11.s32, r30.s32, xer);
	// bne cr6,0x82baca24
	if (!cr6.eq) goto loc_82BACA24;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82BACA24:
	// lwz r31,20(r5)
	r31.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(20) );
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baca58
	if (cr6.eq) goto loc_82BACA58;
loc_82BACA34:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82baca58
	if (!cr6.eq) goto loc_82BACA58;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82bac9f8
	sub_82BAC9F8(ctx, base);
	// lwz r31,16(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82baca34
	if (!cr6.eq) goto loc_82BACA34;
loc_82BACA58:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BAC9F8) {
	__imp__sub_82BAC9F8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACA60) {
	PPC_FUNC_PROLOGUE();
	// stw r5,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r5.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACA60) {
	__imp__sub_82BACA60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACA68) {
	PPC_FUNC_PROLOGUE();
	// lbz r3,30(r4)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r4.u32 + 30);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACA68) {
	__imp__sub_82BACA68(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACA70) {
	PPC_FUNC_PROLOGUE();
	// stb r5,30(r4)
	PPC_STORE_U8(ctx.r4.u32 + 30, ctx.r5.u8);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACA70) {
	__imp__sub_82BACA70(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACA78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// add. r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// blelr 
	if (!cr0.gt) return;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
loc_82BACA90:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82bacab4
	if (cr6.eq) goto loc_82BACAB4;
	// lwz r9,32(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(32) );
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r6,r7,r9
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// stw r6,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r6.u32);
	// lwz r5,32(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(32) );
	// stwx r11,r7,r5
	PPC_STORE_U32(ctx.r7.u32 + ctx.r5.u32, r11.u32);
loc_82BACAB4:
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82bacacc
	if (cr6.eq) goto loc_82BACACC;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// b 0x82bacb0c
	goto loc_82BACB0C;
loc_82BACACC:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82bacae0
	if (cr6.eq) goto loc_82BACAE0;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// b 0x82bacb0c
	goto loc_82BACB0C;
loc_82BACAE0:
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bacb0c
	if (cr6.eq) goto loc_82BACB0C;
loc_82BACAEC:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82bacb08
	if (!cr6.eq) goto loc_82BACB08;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bacaec
	if (!cr6.eq) goto loc_82BACAEC;
	// b 0x82bacb0c
	goto loc_82BACB0C;
loc_82BACB08:
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
loc_82BACB0C:
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// bne 0x82baca90
	if (!cr0.eq) goto loc_82BACA90;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACA78) {
	__imp__sub_82BACA78(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACB20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// bl 0x82baf110
	sub_82BAF110(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,64
	ctx.r5.s64 = 64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACB20) {
	__imp__sub_82BACB20(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACB80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82baf318
	sub_82BAF318(ctx, base);
	// lbz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bacbd8
	if (cr6.eq) goto loc_82BACBD8;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,64
	ctx.r5.s64 = 64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BACBD8:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BACB80) {
	__imp__sub_82BACB80(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACBE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r31,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r31.u32);
	// li r5,4096
	ctx.r5.s64 = 4096;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,116(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(116) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r4,r1,140
	ctx.r4.s64 = ctx.r1.s64 + 140;
	// addi r3,r3,144
	ctx.r3.s64 = ctx.r3.s64 + 144;
	// bl 0x82284810
	sub_82284810(ctx, base);
	// lwz r31,20(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bacc74
	if (cr6.eq) goto loc_82BACC74;
loc_82BACC4C:
	// lhz r11,28(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 28);
	// rlwinm r10,r11,0,19,19
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bacc68
	if (cr6.eq) goto loc_82BACC68;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bacbe8
	sub_82BACBE8(ctx, base);
loc_82BACC68:
	// lwz r31,16(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82bacc4c
	if (!cr6.eq) goto loc_82BACC4C;
loc_82BACC74:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACBE8) {
	__imp__sub_82BACBE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACC90) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x82baccb0
	if (!cr6.eq) goto loc_82BACCB0;
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lwz r11,0(r5)
	r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(0) );
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// bne cr6,0x82baccb0
	if (!cr6.eq) goto loc_82BACCB0;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// blr 
	return;
loc_82BACCB0:
	// lwz r3,20(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(20) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82baccd4
	if (cr6.eq) goto loc_82BACCD4;
loc_82BACCBC:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// cmplw cr6,r11,r4
	cr6.compare<uint32_t>(r11.u32, ctx.r4.u32, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r3,16(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x82baccbc
	if (!cr6.eq) goto loc_82BACCBC;
loc_82BACCD4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACC90) {
	__imp__sub_82BACC90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACCE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,20(r4)
	r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(20) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// beq cr6,0x82bacd54
	if (cr6.eq) goto loc_82BACD54;
loc_82BACD08:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bacd38
	if (!cr6.eq) goto loc_82BACD38;
	// clrlwi r11,r28,24
	r11.u64 = r28.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bacd44
	if (cr6.eq) goto loc_82BACD44;
loc_82BACD38:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82284810
	sub_82284810(ctx, base);
loc_82BACD44:
	// lwz r31,16(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// bne cr6,0x82bacd08
	if (!cr6.eq) goto loc_82BACD08;
loc_82BACD54:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BACCE0) {
	__imp__sub_82BACCE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACD60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x82bacd70
	if (!cr6.eq) goto loc_82BACD70;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82BACD70:
	// lwz r5,12(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(12) );
	// cmplw cr6,r5,r4
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r4.u32, xer);
	// bne cr6,0x82bacd84
	if (!cr6.eq) goto loc_82BACD84;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_82BACD84:
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,96(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(96) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
}

PPC_WEAK_FUNC(sub_82BACD60) {
	__imp__sub_82BACD60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACD98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r6,r10,-12740
	ctx.r6.s64 = ctx.r10.s64 + -12740;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lfs f31,-27468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27468);
	f31.f64 = double(temp.f32);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x823f99e8
	sub_823F99E8(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r3,r31,48
	ctx.r3.s64 = r31.s64 + 48;
	// addi r6,r9,-12724
	ctx.r6.s64 = ctx.r9.s64 + -12724;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823f99e8
	sub_823F99E8(ctx, base);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// addi r3,r31,96
	ctx.r3.s64 = r31.s64 + 96;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r6,r8,-12704
	ctx.r6.s64 = ctx.r8.s64 + -12704;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823f99e8
	sub_823F99E8(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// addi r3,r31,144
	ctx.r3.s64 = r31.s64 + 144;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r6,r7,-12680
	ctx.r6.s64 = ctx.r7.s64 + -12680;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823f99e8
	sub_823F99E8(ctx, base);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// addi r3,r31,192
	ctx.r3.s64 = r31.s64 + 192;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r6,r6,-12656
	ctx.r6.s64 = ctx.r6.s64 + -12656;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823f99e8
	sub_823F99E8(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// addi r3,r31,240
	ctx.r3.s64 = r31.s64 + 240;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r6,r5,-12636
	ctx.r6.s64 = ctx.r5.s64 + -12636;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x823f99e8
	sub_823F99E8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACD98) {
	__imp__sub_82BACD98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACE60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82bacf08
	sub_82BACF08(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bace98
	if (cr6.eq) goto loc_82BACE98;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BACE98:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACE60) {
	__imp__sub_82BACE60(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACEB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// addi r11,r31,76
	r11.s64 = r31.s64 + 76;
	// bne cr6,0x82baced4
	if (!cr6.eq) goto loc_82BACED4;
	// li r11,0
	r11.s64 = 0;
loc_82BACED4:
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r3,r31,24
	ctx.r3.s64 = r31.s64 + 24;
	// addi r9,r10,-10616
	ctx.r9.s64 = ctx.r10.s64 + -10616;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// bl 0x82baf038
	sub_82BAF038(ctx, base);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// addi r7,r8,-12960
	ctx.r7.s64 = ctx.r8.s64 + -12960;
	// stw r7,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACEB0) {
	__imp__sub_82BACEB0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACF08) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r7,r10,26080
	ctx.r7.s64 = ctx.r10.s64 + 26080;
	// addi r5,r8,12488
	ctx.r5.s64 = ctx.r8.s64 + 12488;
	// stw r7,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r7.u32);
	// stw r5,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r5.u32);
	// lwz r9,88(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(88) );
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(96) );
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,92(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(92) );
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + r11.u64;
	// mulli r9,r6,68
	ctx.r9.s64 = ctx.r6.s64 * 68;
	// rlwinm r11,r4,3,0,28
	r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r30,r11,r9
	r30.u64 = r11.u64 + ctx.r9.u64;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(72) );
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82bacf9c
	if (cr6.eq) goto loc_82BACF9C;
loc_82BACF70:
	// lwz r30,72(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(72) );
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r28,4(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// rotlwi r11,r29,0
	r11.u64 = rotl32(r29.u32, 0);
	// stw r29,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r29.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bacf70
	if (!cr6.eq) goto loc_82BACF70;
loc_82BACF9C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82baceb0
	sub_82BACEB0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BACF08) {
	__imp__sub_82BACF08(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BACFB0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// lwz r31,84(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(84) );
	// addi r9,r31,32
	ctx.r9.s64 = r31.s64 + 32;
	// stw r9,84(r30)
	PPC_STORE_U32(r30.u32 + 84, ctx.r9.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// sth r11,28(r31)
	PPC_STORE_U16(r31.u32 + 28, r11.u16);
	// stb r11,31(r31)
	PPC_STORE_U8(r31.u32 + 31, r11.u8);
	// stw r5,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r5.u32);
	// stw r6,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r6.u32);
	// stb r8,30(r31)
	PPC_STORE_U8(r31.u32 + 30, ctx.r8.u8);
	// beq cr6,0x82bad034
	if (cr6.eq) goto loc_82BAD034;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(20) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad020
	if (!cr6.eq) goto loc_82BAD020;
	// stw r31,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, r31.u32);
	// b 0x82bad034
	goto loc_82BAD034;
loc_82BAD020:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad020
	if (!cr6.eq) goto loc_82BAD020;
	// stw r31,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r31.u32);
loc_82BAD034:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r4,80(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(80) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82bad07c
	if (cr6.eq) goto loc_82BAD07C;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,8
	ctx.r5.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAD07C:
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r31,80(r30)
	PPC_STORE_U32(r30.u32 + 80, r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BACFB0) {
	__imp__sub_82BACFB0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD0A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mr r29,r8
	r29.u64 = ctx.r8.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// lwz r31,84(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(84) );
	// addi r9,r31,68
	ctx.r9.s64 = r31.s64 + 68;
	// stw r9,84(r30)
	PPC_STORE_U32(r30.u32 + 84, ctx.r9.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// sth r11,28(r31)
	PPC_STORE_U16(r31.u32 + 28, r11.u16);
	// stb r11,31(r31)
	PPC_STORE_U8(r31.u32 + 31, r11.u8);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// stb r11,42(r31)
	PPC_STORE_U8(r31.u32 + 42, r11.u8);
	// sth r11,40(r31)
	PPC_STORE_U16(r31.u32 + 40, r11.u16);
	// stb r11,43(r31)
	PPC_STORE_U8(r31.u32 + 43, r11.u8);
	// stb r11,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r11.u8);
	// stb r11,45(r31)
	PPC_STORE_U8(r31.u32 + 45, r11.u8);
	// stb r11,46(r31)
	PPC_STORE_U8(r31.u32 + 46, r11.u8);
	// stb r11,47(r31)
	PPC_STORE_U8(r31.u32 + 47, r11.u8);
	// stw r5,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r5.u32);
	// stw r6,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r6.u32);
	// stb r10,30(r31)
	PPC_STORE_U8(r31.u32 + 30, ctx.r10.u8);
	// beq cr6,0x82bad150
	if (cr6.eq) goto loc_82BAD150;
	// stw r4,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r4.u32);
	// lwz r11,20(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(20) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad13c
	if (!cr6.eq) goto loc_82BAD13C;
	// stw r31,20(r4)
	PPC_STORE_U32(ctx.r4.u32 + 20, r31.u32);
	// b 0x82bad150
	goto loc_82BAD150;
loc_82BAD13C:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad13c
	if (!cr6.eq) goto loc_82BAD13C;
	// stw r31,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, r31.u32);
loc_82BAD150:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// li r5,8192
	ctx.r5.s64 = 8192;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r8,80(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(80) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r29,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r29.u32);
	// lwz r4,80(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(80) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82bad1bc
	if (cr6.eq) goto loc_82BAD1BC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,8
	ctx.r5.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAD1BC:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r31,80(r30)
	PPC_STORE_U32(r30.u32 + 80, r31.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,16(r30)
	PPC_STORE_U32(r30.u32 + 16, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BAD0A8) {
	__imp__sub_82BAD0A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD1D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r23,r3
	r23.u64 = ctx.r3.u64;
	// mr r18,r6
	r18.u64 = ctx.r6.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r20,40(r24)
	r20.u64 = PPC_LOAD_U32(r24.u32 + int32_t(40) );
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f1,-16596(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -16596);
	ctx.f1.f64 = double(temp.f32);
	// mr r22,r5
	r22.u64 = ctx.r5.u64;
	// mr r4,r20
	ctx.r4.u64 = r20.u64;
	// mr r19,r7
	r19.u64 = ctx.r7.u64;
	// bl 0x82bae2e8
	sub_82BAE2E8(ctx, base);
	// lwz r10,40(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + int32_t(40) );
	// rlwinm r11,r10,2,0,29
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r31,32(r24)
	r31.u64 = PPC_LOAD_U32(r24.u32 + int32_t(32) );
	// add r9,r11,r31
	ctx.r9.u64 = r11.u64 + r31.u64;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bad29c
	if (cr6.eq) goto loc_82BAD29C;
loc_82BAD22C:
	// lwz r11,0(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// subfic r7,r9,0
	xer.ca = ctx.r9.u32 <= 0;
	ctx.r7.s64 = 0 - ctx.r9.s64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// subfe r6,r7,r7
	temp.u8 = (~ctx.r7.u32 + ctx.r7.u32 < ~ctx.r7.u32) | (~ctx.r7.u32 + ctx.r7.u32 + xer.ca < xer.ca);
	ctx.r6.u64 = ~ctx.r7.u64 + ctx.r7.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r6,0,26,29
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0x3C;
	// lbz r5,31(r8)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r8.u32 + 31);
	// rlwinm r11,r11,0,29,26
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFE7;
	// rotlwi r10,r5,3
	ctx.r10.u64 = rotl32(ctx.r5.u32, 3);
	// addi r11,r11,32
	r11.s64 = r11.s64 + 32;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
	// lwz r10,40(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + int32_t(40) );
	// lwz r11,32(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(32) );
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bad22c
	if (!cr6.eq) goto loc_82BAD22C;
loc_82BAD29C:
	// li r21,0
	r21.s64 = 0;
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// mr r30,r21
	r30.u64 = r21.u64;
	// mr r25,r21
	r25.u64 = r21.u64;
	// ble cr6,0x82bad39c
	if (!cr6.gt) goto loc_82BAD39C;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// mr r31,r21
	r31.u64 = r21.u64;
	// mr r28,r20
	r28.u64 = r20.u64;
	// addi r27,r11,-12480
	r27.s64 = r11.s64 + -12480;
	// addi r26,r10,-12608
	r26.s64 = ctx.r10.s64 + -12608;
loc_82BAD2C8:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwzx r11,r31,r11
	r11.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// beq cr6,0x82bad300
	if (cr6.eq) goto loc_82BAD300;
	// addi r29,r11,8
	r29.s64 = r11.s64 + 8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// li r7,279
	ctx.r7.s64 = 279;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x82bad324
	goto loc_82BAD324;
loc_82BAD300:
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// li r7,284
	ctx.r7.s64 = 284;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r25,r30
	r25.u64 = r30.u64;
loc_82BAD324:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// addi r29,r30,8
	r29.s64 = r30.s64 + 8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// lwzx r11,r31,r11
	r11.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// stw r21,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r21.u32);
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r10,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r10.u32);
	// lwz r9,32(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(32) );
	// lwz r8,128(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// lwzx r5,r31,r8
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + ctx.r8.u32);
	// lwzx r4,r9,r31
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r7,0(r23)
	ctx.r7.u64 = PPC_LOAD_U32(r23.u32 + int32_t(0) );
	// lwz r11,80(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(80) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,4
	ctx.r5.s64 = 4;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82bad5b0
	sub_82BAD5B0(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x82284810
	sub_82284810(ctx, base);
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// bne 0x82bad2c8
	if (!cr0.eq) goto loc_82BAD2C8;
loc_82BAD39C:
	// lwz r11,72(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(72) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bad3cc
	if (cr6.eq) goto loc_82BAD3CC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bad3c4
	if (cr6.eq) goto loc_82BAD3C4;
loc_82BAD3B4:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bad3b4
	if (!cr6.eq) goto loc_82BAD3B4;
loc_82BAD3C4:
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// b 0x82bad3d0
	goto loc_82BAD3D0;
loc_82BAD3CC:
	// stw r25,72(r23)
	PPC_STORE_U32(r23.u32 + 72, r25.u32);
loc_82BAD3D0:
	// lwz r6,40(r24)
	ctx.r6.u64 = PPC_LOAD_U32(r24.u32 + int32_t(40) );
	// cmpwi cr6,r6,0
	cr6.compare<int32_t>(ctx.r6.s32, 0, xer);
	// ble cr6,0x82bad464
	if (!cr6.gt) goto loc_82BAD464;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
loc_82BAD3E4:
	// lwz r11,32(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + int32_t(32) );
	// mr r10,r21
	ctx.r10.u64 = r21.u64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// lwzx r11,r7,r11
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + r11.u32);
loc_82BAD3F4:
	// lwz r9,32(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(32) );
	// lwz r4,12(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwzx r9,r9,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bad418
	if (!cr6.eq) goto loc_82BAD418;
	// lwz r9,32(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + int32_t(32) );
	// lwzx r4,r10,r9
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// stw r4,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r4.u32);
	// b 0x82bad44c
	goto loc_82BAD44C;
loc_82BAD418:
	// lwz r4,20(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bad434
	if (!cr6.eq) goto loc_82BAD434;
	// lwz r9,32(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + int32_t(32) );
	// lwzx r4,r10,r9
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// stw r4,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r4.u32);
	// b 0x82bad44c
	goto loc_82BAD44C;
loc_82BAD434:
	// lwz r4,16(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bad44c
	if (!cr6.eq) goto loc_82BAD44C;
	// lwz r9,32(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + int32_t(32) );
	// lwzx r4,r10,r9
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// stw r4,16(r11)
	PPC_STORE_U32(r11.u32 + 16, ctx.r4.u32);
loc_82BAD44C:
	// addic. r8,r8,-1
	xer.ca = ctx.r8.u32 > 0;
	ctx.r8.s64 = ctx.r8.s64 + -1;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x82bad3f4
	if (!cr0.eq) goto loc_82BAD3F4;
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// bne 0x82bad3e4
	if (!cr0.eq) goto loc_82BAD3E4;
loc_82BAD464:
	// lwz r11,32(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + int32_t(32) );
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// beq cr6,0x82bad47c
	if (cr6.eq) goto loc_82BAD47C;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// b 0x82bad488
	goto loc_82BAD488;
loc_82BAD47C:
	// lwz r11,32(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + int32_t(32) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(12) );
loc_82BAD488:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bad4bc
	if (cr6.eq) goto loc_82BAD4BC;
	// stw r10,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r10.u32);
	// lwz r11,20(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(20) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad4a8
	if (!cr6.eq) goto loc_82BAD4A8;
	// stw r9,20(r10)
	PPC_STORE_U32(ctx.r10.u32 + 20, ctx.r9.u32);
	// b 0x82bad4bc
	goto loc_82BAD4BC;
loc_82BAD4A8:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad4a8
	if (!cr6.eq) goto loc_82BAD4A8;
	// stw r9,16(r10)
	PPC_STORE_U32(ctx.r10.u32 + 16, ctx.r9.u32);
loc_82BAD4BC:
	// cmplwi cr6,r18,0
	cr6.compare<uint32_t>(r18.u32, 0, xer);
	// stw r21,16(r9)
	PPC_STORE_U32(ctx.r9.u32 + 16, r21.u32);
	// beq cr6,0x82bad4cc
	if (cr6.eq) goto loc_82BAD4CC;
	// stw r18,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r18.u32);
loc_82BAD4CC:
	// cmpwi cr6,r20,0
	cr6.compare<int32_t>(r20.s32, 0, xer);
	// ble cr6,0x82bad58c
	if (!cr6.gt) goto loc_82BAD58C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// mr r29,r21
	r29.u64 = r21.u64;
	// mr r28,r20
	r28.u64 = r20.u64;
	// lis r27,-31924
	r27.s64 = -2092171264;
	// addi r26,r11,3224
	r26.s64 = r11.s64 + 3224;
loc_82BAD4E8:
	// lwz r11,32(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + int32_t(32) );
	// lwzx r30,r29,r11
	r30.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// lbz r10,30(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 30);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// bne cr6,0x82bad580
	if (!cr6.eq) goto loc_82BAD580;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,-7424(r27)
	ctx.r4.u64 = PPC_LOAD_U32(r27.u32 + int32_t(-7424) );
	// bl 0x82284448
	sub_82284448(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bad580
	if (cr6.eq) goto loc_82BAD580;
	// lwz r3,4(r23)
	ctx.r3.u64 = PPC_LOAD_U32(r23.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad53c
	if (!cr6.eq) goto loc_82BAD53C;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_82BAD53C:
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,4(r23)
	ctx.r3.u64 = PPC_LOAD_U32(r23.u32 + int32_t(4) );
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r7,20(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(20) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// lwz r11,16(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(16) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAD580:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// bne 0x82bad4e8
	if (!cr0.eq) goto loc_82BAD4E8;
loc_82BAD58C:
	// stw r21,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r21.u32);
	// lwz r31,132(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c10
	return;
}

PPC_WEAK_FUNC(sub_82BAD1D8) {
	__imp__sub_82BAD1D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD5B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// lbz r11,31(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 31);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bad674
	if (cr6.eq) goto loc_82BAD674;
	// addi r29,r31,32
	r29.s64 = r31.s64 + 32;
loc_82BAD5D8:
	// lhz r11,28(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 28);
	// addi r30,r29,36
	r30.s64 = r29.s64 + 36;
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bad5f0
	if (!cr6.eq) goto loc_82BAD5F0;
	// mr r30,r29
	r30.u64 = r29.u64;
loc_82BAD5F0:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// rlwinm r10,r11,0,27,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x1C;
	// cmplwi cr6,r10,20
	cr6.compare<uint32_t>(ctx.r10.u32, 20, xer);
	// bne cr6,0x82bad660
	if (!cr6.eq) goto loc_82BAD660;
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(60) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(16) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r26,28(r31)
	r26.u64 = PPC_LOAD_U16(r31.u32 + 28);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// clrlwi r7,r26,16
	ctx.r7.u64 = r26.u32 & 0xFFFF;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// rlwinm r7,r7,0,26,24
	ctx.r7.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFFFFBF;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// sth r7,28(r31)
	PPC_STORE_U16(r31.u32 + 28, ctx.r7.u16);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// rlwinm r5,r10,27,5,31
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(72) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// sth r26,28(r31)
	PPC_STORE_U16(r31.u32 + 28, r26.u16);
loc_82BAD660:
	// lbz r11,31(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 31);
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// addi r29,r29,8
	r29.s64 = r29.s64 + 8;
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// blt cr6,0x82bad5d8
	if (cr6.lt) goto loc_82BAD5D8;
loc_82BAD674:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAD5B0) {
	__imp__sub_82BAD5B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD680) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,-76
	ctx.r3.s64 = ctx.r3.s64 + -76;
	// b 0x82bace60
	sub_82BACE60(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82BAD680) {
	__imp__sub_82BAD680(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD688) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82bad6d8
	sub_82BAD6D8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bad6c0
	if (cr6.eq) goto loc_82BAD6C0;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAD6C0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAD688) {
	__imp__sub_82BAD688(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD6D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r8,r10,-12344
	ctx.r8.s64 = ctx.r10.s64 + -12344;
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r30,4(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r29,r7,2,0,29
	r29.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lis r6,-32245
	ctx.r6.s64 = -2113208320;
	// addi r5,r6,2988
	ctx.r5.s64 = ctx.r6.s64 + 2988;
	// stw r5,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r5.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BAD6D8) {
	__imp__sub_82BAD6D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD730) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-12208
	ctx.r9.s64 = r11.s64 + -12208;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82bad764
	if (cr6.eq) goto loc_82BAD764;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAD764:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAD730) {
	__imp__sub_82BAD730(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD778) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82bad7c8
	sub_82BAD7C8(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bad7b0
	if (cr6.eq) goto loc_82BAD7B0;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAD7B0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAD778) {
	__imp__sub_82BAD778(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD7C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r7,r10,-12184
	ctx.r7.s64 = ctx.r10.s64 + -12184;
	// lwz r8,44(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(44) );
	// stw r7,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r7.u32);
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(52) );
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(48) );
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r8,60(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(60) );
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(56) );
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// rlwinm r11,r6,2,0,29
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r29,r5,1,0,30
	r29.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r4,28(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82bad894
	if (cr6.eq) goto loc_82BAD894;
loc_82BAD834:
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// lwz r30,20(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bad86c
	if (cr6.eq) goto loc_82BAD86C;
loc_82BAD848:
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r27,4(r30)
	r27.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// mr r30,r29
	r30.u64 = r29.u64;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82bad848
	if (!cr6.eq) goto loc_82BAD848;
loc_82BAD86C:
	// lwz r30,28(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(28) );
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// rotlwi r11,r28,0
	r11.u64 = rotl32(r28.u32, 0);
	// stw r28,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r28.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad834
	if (!cr6.eq) goto loc_82BAD834;
loc_82BAD894:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r10,r11,-12208
	ctx.r10.s64 = r11.s64 + -12208;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BAD7C8) {
	__imp__sub_82BAD7C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD8A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r3,r11,76
	ctx.r3.s64 = r11.s64 + 76;
	// lwz r11,76(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r11,42(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 42);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bad8f4
	if (!cr6.eq) goto loc_82BAD8F4;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// sth r10,40(r3)
	PPC_STORE_U16(ctx.r3.u32 + 40, ctx.r10.u16);
loc_82BAD8F4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stb r11,42(r3)
	PPC_STORE_U8(ctx.r3.u32 + 42, r11.u8);
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// rlwinm r8,r11,5,0,26
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// add r30,r8,r9
	r30.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r7,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r7.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bad948
	if (cr6.eq) goto loc_82BAD948;
	// lhz r11,6(r10)
	r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 6);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bad948
	if (cr6.eq) goto loc_82BAD948;
	// lwz r11,60(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(60) );
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// stw r11,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r11.u32);
	// stw r10,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r10.u32);
loc_82BAD948:
	// li r11,0
	r11.s64 = 0;
	// li r5,16
	ctx.r5.s64 = 16;
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// rlwinm r10,r29,0,29,29
	ctx.r10.u64 = rotl64(r29.u32 | (r29.u64 << 32), 0) & 0x4;
	// rlwinm r9,r29,0,28,28
	ctx.r9.u64 = rotl64(r29.u32 | (r29.u64 << 32), 0) & 0x8;
	// stw r27,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r27.u32);
	// subfic r8,r10,0
	xer.ca = ctx.r10.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r10.s64;
	// stw r26,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r26.u32);
	// rlwinm r7,r29,0,27,27
	ctx.r7.u64 = rotl64(r29.u32 | (r29.u64 << 32), 0) & 0x10;
	// subfe r6,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r6.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
	// subfic r5,r9,0
	xer.ca = ctx.r9.u32 <= 0;
	ctx.r5.s64 = 0 - ctx.r9.s64;
	// clrlwi r4,r6,31
	ctx.r4.u64 = ctx.r6.u32 & 0x1;
	// subfe r3,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + xer.ca < xer.ca);
	ctx.r3.u64 = ~ctx.r5.u64 + ctx.r5.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwimi r4,r29,1,23,30
	ctx.r4.u64 = (rotl32(r29.u32, 1) & 0x1FE) | (ctx.r4.u64 & 0xFFFFFFFFFFFFFE01);
	// clrlwi r11,r3,31
	r11.u64 = ctx.r3.u32 & 0x1;
	// clrlwi r10,r4,23
	ctx.r10.u64 = ctx.r4.u32 & 0x1FF;
	// subfic r9,r7,0
	xer.ca = ctx.r7.u32 <= 0;
	ctx.r9.s64 = 0 - ctx.r7.s64;
	// subfe r8,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r8.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r7,r8,31
	ctx.r7.u64 = ctx.r8.u32 & 0x1;
	// lwz r6,16(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// rlwimi r6,r10,29,0,2
	ctx.r6.u64 = (rotl32(ctx.r10.u32, 29) & 0xE0000000) | (ctx.r6.u64 & 0xFFFFFFFF1FFFFFFF);
	// mr r5,r6
	ctx.r5.u64 = ctx.r6.u64;
	// stw r6,16(r30)
	PPC_STORE_U32(r30.u32 + 16, ctx.r6.u32);
	// rlwimi r5,r11,28,3,3
	ctx.r5.u64 = (rotl32(r11.u32, 28) & 0x10000000) | (ctx.r5.u64 & 0xFFFFFFFFEFFFFFFF);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// stw r5,16(r30)
	PPC_STORE_U32(r30.u32 + 16, ctx.r5.u32);
	// rlwimi r4,r7,27,4,4
	ctx.r4.u64 = (rotl32(ctx.r7.u32, 27) & 0x8000000) | (ctx.r4.u64 & 0xFFFFFFFFF7FFFFFF);
	// rlwinm r3,r4,0,0,4
	ctx.r3.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 0) & 0xF8000000;
	// stw r3,16(r30)
	PPC_STORE_U32(r30.u32 + 16, ctx.r3.u32);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// stw r11,28(r30)
	PPC_STORE_U32(r30.u32 + 28, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAD8A8) {
	__imp__sub_82BAD8A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAD9D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-512(r1)
	ea = -512 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r16,r4
	r16.u64 = ctx.r4.u64;
	// mr r18,r3
	r18.u64 = ctx.r3.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lfs f31,-16596(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -16596);
	f31.f64 = double(temp.f32);
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// lwz r15,40(r16)
	r15.u64 = PPC_LOAD_U32(r16.u32 + int32_t(40) );
	// mr r14,r5
	r14.u64 = ctx.r5.u64;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82bae2e8
	sub_82BAE2E8(ctx, base);
	// li r17,0
	r17.s64 = 0;
	// stfs f31,188(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// lis r11,-32245
	r11.s64 = -2113208320;
	// stw r17,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r17.u32);
	// lis r10,-32245
	ctx.r10.s64 = -2113208320;
	// stw r17,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r17.u32);
	// addi r31,r11,-6344
	r31.s64 = r11.s64 + -6344;
	// stw r17,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r17.u32);
	// addi r30,r10,-6332
	r30.s64 = ctx.r10.s64 + -6332;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(180) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bge cr6,0x82badab0
	if (!cr6.lt) goto loc_82BADAB0;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r29,r1,144
	r29.s64 = ctx.r1.s64 + 144;
	// addi r28,r11,-13456
	r28.s64 = r11.s64 + -13456;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r7,141
	ctx.r7.s64 = 141;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(184) );
	// lwz r4,176(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r28,180(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(180) );
	// lwz r27,176(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r29,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r29.u32);
	// stw r17,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r17.u32);
loc_82BADAB0:
	// stfs f31,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stw r17,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r17.u32);
	// stw r17,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r17.u32);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// stw r17,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r17.u32);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// bl 0x82bae2e8
	sub_82BAE2E8(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// stw r17,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r17.u32);
	// mr r27,r17
	r27.u64 = r17.u64;
	// bl 0x82bae2e8
	sub_82BAE2E8(ctx, base);
	// mr r31,r17
	r31.u64 = r17.u64;
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// ble cr6,0x82badb70
	if (!cr6.gt) goto loc_82BADB70;
loc_82BADB24:
	// lwz r3,32(r18)
	ctx.r3.u64 = PPC_LOAD_U32(r18.u32 + int32_t(32) );
	// rlwinm r11,r31,2,0,29
	r11.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,32(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + int32_t(32) );
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwzx r4,r11,r10
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwz r8,76(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(76) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82badb60
	if (cr6.eq) goto loc_82BADB60;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
loc_82BADB60:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// cmpw cr6,r31,r15
	cr6.compare<int32_t>(r31.s32, r15.s32, xer);
	// blt cr6,0x82badb24
	if (cr6.lt) goto loc_82BADB24;
loc_82BADB70:
	// lwz r11,12(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(12) );
	// mr r25,r17
	r25.u64 = r17.u64;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82badd64
	if (!cr6.gt) goto loc_82BADD64;
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
loc_82BADB88:
	// lwz r10,232(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(232) );
	// mr r11,r17
	r11.u64 = r17.u64;
	// mr r30,r17
	r30.u64 = r17.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82badc18
	if (!cr6.gt) goto loc_82BADC18;
	// mr r31,r17
	r31.u64 = r17.u64;
loc_82BADBA0:
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82badc14
	if (!cr6.eq) goto loc_82BADC14;
	// lwz r11,32(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(32) );
	// lwz r9,224(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(224) );
	// addi r3,r11,76
	ctx.r3.s64 = r11.s64 + 76;
	// lwz r10,32(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + int32_t(32) );
	// lwzx r8,r9,r31
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + r31.u32);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rlwinm r11,r8,2,0,29
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(4) );
	// lwzx r4,r11,r10
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r11,40(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 40);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// blt cr6,0x82badbf8
	if (cr6.lt) goto loc_82BADBF8;
	// lbz r10,42(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 42);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// li r11,1
	r11.s64 = 1;
	// blt cr6,0x82badbfc
	if (cr6.lt) goto loc_82BADBFC;
loc_82BADBF8:
	// mr r11,r17
	r11.u64 = r17.u64;
loc_82BADBFC:
	// lwz r10,232(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(232) );
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmpw cr6,r30,r10
	cr6.compare<int32_t>(r30.s32, ctx.r10.s32, xer);
	// blt cr6,0x82badba0
	if (cr6.lt) goto loc_82BADBA0;
loc_82BADC14:
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
loc_82BADC18:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82badd50
	if (!cr6.eq) goto loc_82BADD50;
	// lwz r11,8(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(8) );
	// rlwinm r10,r25,5,0,26
	ctx.r10.u64 = rotl64(r25.u32 | (r25.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// add r26,r10,r11
	r26.u64 = ctx.r10.u64 + r11.u64;
	// mr r29,r17
	r29.u64 = r17.u64;
	// lwz r11,16(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(16) );
	// lwz r31,28(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + int32_t(28) );
	// clrlwi r10,r11,5
	ctx.r10.u64 = r11.u32 & 0x7FFFFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82badd20
	if (cr6.eq) goto loc_82BADD20;
loc_82BADC4C:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// ble cr6,0x82badce0
	if (!cr6.gt) goto loc_82BADCE0;
	// mr r30,r17
	r30.u64 = r17.u64;
	// mr r28,r15
	r28.u64 = r15.u64;
loc_82BADC5C:
	// lwz r11,32(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + int32_t(32) );
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwzx r7,r11,r30
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// cmplw cr6,r10,r7
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, xer);
	// bne cr6,0x82badcd4
	if (!cr6.eq) goto loc_82BADCD4;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// cmpw cr6,r9,r11
	cr6.compare<int32_t>(ctx.r9.s32, r11.s32, xer);
	// bne cr6,0x82badcb8
	if (!cr6.eq) goto loc_82BADCB8;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,240(r1)
	PPC_STORE_U64(ctx.r1.u32 + 240, r11.u64);
	// lfd f13,240(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 240);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,248(r1)
	PPC_STORE_U64(ctx.r1.u32 + 248, ctx.f9.u64);
	// lwz r11,252(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(252) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
loc_82BADCB8:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,1
	ctx.r8.s64 = 1;
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r9.u32);
loc_82BADCD4:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// bne 0x82badc5c
	if (!cr0.eq) goto loc_82BADC5C;
loc_82BADCE0:
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// lhz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r6,16(r26)
	ctx.r6.u64 = PPC_LOAD_U32(r26.u32 + int32_t(16) );
	// rlwinm r11,r7,31,1,31
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// rlwinm r11,r5,0,0,30
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// rlwinm r10,r4,2,0,29
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r3,r6,5
	ctx.r3.u64 = ctx.r6.u32 & 0x7FFFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r29,r3
	cr6.compare<uint32_t>(r29.u32, ctx.r3.u32, xer);
	// rlwinm r11,r11,1,0,30
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// blt cr6,0x82badc4c
	if (cr6.lt) goto loc_82BADC4C;
loc_82BADD20:
	// clrlwi r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82badd50
	if (cr6.eq) goto loc_82BADD50;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
loc_82BADD50:
	// lwz r11,12(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(12) );
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// cmpw cr6,r25,r11
	cr6.compare<int32_t>(r25.s32, r11.s32, xer);
	// blt cr6,0x82badb88
	if (cr6.lt) goto loc_82BADB88;
loc_82BADD64:
	// lwz r9,344(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(344) );
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r29,r17
	r29.u64 = r17.u64;
	// mr r26,r17
	r26.u64 = r17.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r20,r10,-12480
	r20.s64 = ctx.r10.s64 + -12480;
	// addi r19,r11,-12328
	r19.s64 = r11.s64 + -12328;
	// ble cr6,0x82bade84
	if (!cr6.gt) goto loc_82BADE84;
	// mr r30,r17
	r30.u64 = r17.u64;
	// li r25,24
	r25.s64 = 24;
loc_82BADD90:
	// lwz r11,336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(336) );
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r10,8(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + int32_t(8) );
	// lwzx r9,r11,r30
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// rlwinm r11,r9,5,0,26
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 5) & 0xFFFFFFE0;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x82baddd0
	if (cr6.eq) goto loc_82BADDD0;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// li r7,408
	ctx.r7.s64 = 408;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// stw r3,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r3.u32);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// b 0x82baddf0
	goto loc_82BADDF0;
loc_82BADDD0:
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// li r7,413
	ctx.r7.s64 = 413;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
loc_82BADDF0:
	// stw r31,16(r27)
	PPC_STORE_U32(r27.u32 + 16, r31.u32);
	// mr r31,r17
	r31.u64 = r17.u64;
	// stw r17,12(r27)
	PPC_STORE_U32(r27.u32 + 12, r17.u32);
	// stw r25,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r25.u32);
	// lwz r11,336(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(336) );
	// lwzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// stw r10,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r10.u32);
	// stw r17,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r17.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// lwzx r9,r11,r30
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// cmpw cr6,r29,r9
	cr6.compare<int32_t>(r29.s32, ctx.r9.s32, xer);
	// bge cr6,0x82bade6c
	if (!cr6.lt) goto loc_82BADE6C;
	// rlwinm r28,r29,2,0,29
	r28.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
loc_82BADE24:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// lwzx r4,r28,r11
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + r11.u32);
	// beq cr6,0x82bade48
	if (cr6.eq) goto loc_82BADE48;
	// bl 0x82bae190
	sub_82BAE190(ctx, base);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x82bade54
	goto loc_82BADE54;
loc_82BADE48:
	// bl 0x82bae190
	sub_82BAE190(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r31,20(r27)
	PPC_STORE_U32(r27.u32 + 20, r31.u32);
loc_82BADE54:
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// lwzx r10,r11,r30
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// blt cr6,0x82bade24
	if (cr6.lt) goto loc_82BADE24;
loc_82BADE6C:
	// lwz r10,344(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(344) );
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// lwzx r29,r11,r30
	r29.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r26,r10
	cr6.compare<int32_t>(r26.s32, ctx.r10.s32, xer);
	// blt cr6,0x82badd90
	if (cr6.lt) goto loc_82BADD90;
loc_82BADE84:
	// lwz r11,232(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(232) );
	// mr r21,r17
	r21.u64 = r17.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82bae030
	if (!cr6.gt) goto loc_82BAE030;
	// mr r22,r17
	r22.u64 = r17.u64;
	// li r23,56
	r23.s64 = 56;
loc_82BADE9C:
	// lwz r11,32(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(32) );
	// lwz r9,224(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(224) );
	// addi r3,r11,76
	ctx.r3.s64 = r11.s64 + 76;
	// lwz r10,32(r14)
	ctx.r10.u64 = PPC_LOAD_U32(r14.u32 + int32_t(32) );
	// lwzx r8,r9,r22
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + r22.u32);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rlwinm r11,r8,2,0,29
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(4) );
	// lwzx r4,r11,r10
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lhz r5,40(r25)
	ctx.r5.u64 = PPC_LOAD_U16(r25.u32 + 40);
	// lbz r11,42(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 42);
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// add r4,r11,r5
	ctx.r4.u64 = r11.u64 + ctx.r5.u64;
	// extsh r24,r4
	r24.s64 = ctx.r4.s16;
	// cmpw cr6,r26,r24
	cr6.compare<int32_t>(r26.s32, r24.s32, xer);
	// bge cr6,0x82bae01c
	if (!cr6.lt) goto loc_82BAE01C;
loc_82BADEE8:
	// lwz r10,8(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + int32_t(8) );
	// rlwinm r11,r26,5,0,26
	r11.u64 = rotl64(r26.u32 | (r26.u64 << 32), 5) & 0xFFFFFFE0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// add r28,r11,r10
	r28.u64 = r11.u64 + ctx.r10.u64;
	// beq cr6,0x82badf20
	if (cr6.eq) goto loc_82BADF20;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,56
	ctx.r4.s64 = 56;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// li r7,455
	ctx.r7.s64 = 455;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// stw r3,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r3.u32);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// b 0x82badf40
	goto loc_82BADF40;
loc_82BADF20:
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,56
	ctx.r4.s64 = 56;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// li r7,460
	ctx.r7.s64 = 460;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
loc_82BADF40:
	// addi r11,r27,24
	r11.s64 = r27.s64 + 24;
	// stw r25,12(r27)
	PPC_STORE_U32(r27.u32 + 12, r25.u32);
	// stw r26,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r26.u32);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// stw r11,16(r27)
	PPC_STORE_U32(r27.u32 + 16, r11.u32);
	// li r9,8
	ctx.r9.s64 = 8;
	// stw r23,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r23.u32);
	// stw r17,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r17.u32);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_82BADF64:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82badf64
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BADF64;
	// lwz r11,16(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(16) );
	// mr r29,r17
	r29.u64 = r17.u64;
	// lwz r31,28(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + int32_t(28) );
	// mr r30,r17
	r30.u64 = r17.u64;
	// clrlwi r10,r11,5
	ctx.r10.u64 = r11.u32 & 0x7FFFFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bae008
	if (cr6.eq) goto loc_82BAE008;
loc_82BADF94:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// beq cr6,0x82badfb4
	if (cr6.eq) goto loc_82BADFB4;
	// bl 0x82bae190
	sub_82BAE190(ctx, base);
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// b 0x82badfc8
	goto loc_82BADFC8;
loc_82BADFB4:
	// bl 0x82bae190
	sub_82BAE190(ctx, base);
	// lwz r11,16(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(16) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r29,20(r27)
	PPC_STORE_U32(r27.u32 + 20, r29.u32);
	// stw r17,28(r11)
	PPC_STORE_U32(r11.u32 + 28, r17.u32);
loc_82BADFC8:
	// lhz r11,4(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// lhz r9,6(r31)
	ctx.r9.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r8,16(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + int32_t(16) );
	// rlwinm r11,r9,31,1,31
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r6,r8,5
	ctx.r6.u64 = ctx.r8.u32 & 0x7FFFFFF;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplw cr6,r30,r6
	cr6.compare<uint32_t>(r30.u32, ctx.r6.u32, xer);
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + r11.u64;
	// rlwinm r11,r5,1,0,30
	r11.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r11,r31
	r31.u64 = r11.u64 + r31.u64;
	// blt cr6,0x82badf94
	if (cr6.lt) goto loc_82BADF94;
loc_82BAE008:
	// addi r11,r26,1
	r11.s64 = r26.s64 + 1;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// mr r26,r11
	r26.u64 = r11.u64;
	// cmpw cr6,r11,r24
	cr6.compare<int32_t>(r11.s32, r24.s32, xer);
	// blt cr6,0x82badee8
	if (cr6.lt) goto loc_82BADEE8;
loc_82BAE01C:
	// lwz r11,232(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(232) );
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r22,r22,4
	r22.s64 = r22.s64 + 4;
	// cmpw cr6,r21,r11
	cr6.compare<int32_t>(r21.s32, r11.s32, xer);
	// blt cr6,0x82bade9c
	if (cr6.lt) goto loc_82BADE9C;
loc_82BAE030:
	// lwz r7,84(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82bae0a0
	if (cr6.eq) goto loc_82BAE0A0;
loc_82BAE040:
	// lwz r10,20(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(20) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bae094
	if (cr6.eq) goto loc_82BAE094;
loc_82BAE04C:
	// cmpwi cr6,r15,0
	cr6.compare<int32_t>(r15.s32, 0, xer);
	// ble cr6,0x82bae088
	if (!cr6.gt) goto loc_82BAE088;
	// mr r11,r17
	r11.u64 = r17.u64;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
loc_82BAE05C:
	// lwz r6,32(r16)
	ctx.r6.u64 = PPC_LOAD_U32(r16.u32 + int32_t(32) );
	// lwz r5,8(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// lwzx r4,r6,r11
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	// cmplw cr6,r5,r4
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r4.u32, xer);
	// bne cr6,0x82bae07c
	if (!cr6.eq) goto loc_82BAE07C;
	// lwz r6,32(r14)
	ctx.r6.u64 = PPC_LOAD_U32(r14.u32 + int32_t(32) );
	// lwzx r5,r6,r11
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + r11.u32);
	// stw r5,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r5.u32);
loc_82BAE07C:
	// addic. r9,r9,-1
	xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x82bae05c
	if (!cr0.eq) goto loc_82BAE05C;
loc_82BAE088:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bae04c
	if (!cr6.eq) goto loc_82BAE04C;
loc_82BAE094:
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82bae040
	if (!cr6.eq) goto loc_82BAE040;
loc_82BAE0A0:
	// lwz r11,28(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(28) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bae0d0
	if (cr6.eq) goto loc_82BAE0D0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bae0c8
	if (cr6.eq) goto loc_82BAE0C8;
loc_82BAE0B8:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bae0b8
	if (!cr6.eq) goto loc_82BAE0B8;
loc_82BAE0C8:
	// stw r7,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r7.u32);
	// b 0x82bae0d4
	goto loc_82BAE0D4;
loc_82BAE0D0:
	// stw r7,28(r18)
	PPC_STORE_U32(r18.u32 + 28, ctx.r7.u32);
loc_82BAE0D4:
	// stw r17,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r17.u32);
	// lwz r31,228(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(228) );
	// lwz r30,224(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(224) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r17.u32);
	// stw r17,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r17.u32);
	// stw r17,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, r17.u32);
	// lwz r31,292(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(292) );
	// lwz r30,288(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r17.u32);
	// stw r17,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r17.u32);
	// stw r17,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r17.u32);
	// lwz r31,132(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r17.u32);
	// stw r17,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r17.u32);
	// stw r17,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r17.u32);
	// lwz r31,180(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(180) );
	// lwz r30,176(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r17.u32);
	// stw r17,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r17.u32);
	// stw r17,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r17.u32);
	// lwz r31,340(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(340) );
	// lwz r30,336(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(336) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BAD9D8) {
	__imp__sub_82BAD9D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAE190) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r28,r9,-12328
	r28.s64 = ctx.r9.s64 + -12328;
	// lhz r11,4(r27)
	r11.u64 = PPC_LOAD_U16(r27.u32 + 4);
	// addi r26,r7,-12480
	r26.s64 = ctx.r7.s64 + -12480;
	// lhz r8,6(r27)
	ctx.r8.u64 = PPC_LOAD_U16(r27.u32 + 6);
	// rlwinm r11,r11,31,1,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r8,31,1,31
	ctx.r10.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r6,r11,1
	ctx.r6.s64 = r11.s64 + 1;
	// rlwinm r11,r6,2,0,29
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r30,r5,1,0,30
	r30.u64 = rotl64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r31,r30,8
	r31.s64 = r30.s64 + 8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r7,572
	ctx.r7.s64 = 572;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
	// addi r31,r28,8
	r31.s64 = r28.s64 + 8;
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lhz r11,14(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 14);
	// rlwinm r10,r11,0,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFFFE;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bae2d8
	if (cr6.eq) goto loc_82BAE2D8;
	// lwz r3,32(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// li r5,4
	ctx.r5.s64 = 4;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bae2d8
	if (!cr6.eq) goto loc_82BAE2D8;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// lwz r30,0(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(44) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(20) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// lhz r6,6(r31)
	ctx.r6.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwinm r11,r7,31,1,31
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r5,r6,0,0,30
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// rlwinm r11,r4,3,0,28
	r11.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// beq cr6,0x82bae2d8
	if (cr6.eq) goto loc_82BAE2D8;
loc_82BAE2A0:
	// lhz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// rlwinm r8,r10,0,17,17
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4000;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bae2c0
	if (cr6.eq) goto loc_82BAE2C0;
	// clrlwi r8,r10,16
	ctx.r8.u64 = ctx.r10.u32 & 0xFFFF;
	// subf r7,r3,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r3.s64;
	// rlwimi r7,r10,0,16,17
	ctx.r7.u64 = (rotl32(ctx.r10.u32, 0) & 0xC000) | (ctx.r7.u64 & 0xFFFFFFFFFFFF3FFF);
	// sth r7,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r7.u16);
loc_82BAE2C0:
	// lhz r10,6(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// rlwinm r8,r10,31,1,31
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// cmpw cr6,r9,r8
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, xer);
	// blt cr6,0x82bae2a0
	if (cr6.lt) goto loc_82BAE2A0;
loc_82BAE2D8:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAE190) {
	__imp__sub_82BAE190(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAE2E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// lis r9,-32245
	ctx.r9.s64 = -2113208320;
	// lis r8,-32245
	ctx.r8.s64 = -2113208320;
	// lfs f0,-16596(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16596);
	f0.f64 = double(temp.f32);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stfs f0,44(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// addi r6,r9,-6344
	ctx.r6.s64 = ctx.r9.s64 + -6344;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// addi r5,r8,-6332
	ctx.r5.s64 = ctx.r8.s64 + -6332;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAE2E8) {
	__imp__sub_82BAE2E8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAE368) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82bae3c8
	if (!cr6.eq) goto loc_82BAE3C8;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
loc_82BAE3C8:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r10.u32);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r7,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r7.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAE368) {
	__imp__sub_82BAE368(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAE410) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82bae460
	sub_82BAE460(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bae448
	if (cr6.eq) goto loc_82BAE448;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAE448:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAE410) {
	__imp__sub_82BAE410(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAE460) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r7,r10,-12040
	ctx.r7.s64 = ctx.r10.s64 + -12040;
	// stw r7,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r7.u32);
	// lwz r11,44(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(44) );
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,36(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r11,r8
	ctx.r6.u64 = r11.u64 + ctx.r8.u64;
	// rlwinm r29,r6,3,0,28
	r29.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r5,32(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x82bae4e8
	if (cr6.eq) goto loc_82BAE4E8;
loc_82BAE4BC:
	// lwz r30,32(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r28,4(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// rotlwi r11,r29,0
	r11.u64 = rotl32(r29.u32, 0);
	// stw r29,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r29.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bae4bc
	if (!cr6.eq) goto loc_82BAE4BC;
loc_82BAE4E8:
	// lis r11,-32245
	r11.s64 = -2113208320;
	// addi r10,r11,2988
	ctx.r10.s64 = r11.s64 + 2988;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BAE460) {
	__imp__sub_82BAE460(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAE500) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-512(r1)
	ea = -512 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r18,r3
	r18.u64 = ctx.r3.u64;
	// mr r14,r4
	r14.u64 = ctx.r4.u64;
	// mr r15,r5
	r15.u64 = ctx.r5.u64;
	// lwz r3,4(r18)
	ctx.r3.u64 = PPC_LOAD_U32(r18.u32 + int32_t(4) );
	// lwz r30,40(r14)
	r30.u64 = PPC_LOAD_U32(r14.u32 + int32_t(40) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// li r17,0
	r17.s64 = 0;
	// lis r8,-32245
	ctx.r8.s64 = -2113208320;
	// stw r17,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r17.u32);
	// lis r7,-32245
	ctx.r7.s64 = -2113208320;
	// stw r17,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r17.u32);
	// addi r31,r8,-6344
	r31.s64 = ctx.r8.s64 + -6344;
	// lfs f31,-16596(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -16596);
	f31.f64 = double(temp.f32);
	// stw r17,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r17.u32);
	// stfs f31,236(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 236, temp.u32);
	// addi r29,r7,-6332
	r29.s64 = ctx.r7.s64 + -6332;
	// mr r16,r3
	r16.u64 = ctx.r3.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x823f9ae0
	sub_823F9AE0(ctx, base);
	// stfs f31,188(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stw r17,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r17.u32);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// stw r17,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r17.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r17,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r17.u32);
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// stfs f31,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stw r17,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r17.u32);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// stw r17,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r17.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r17,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r17.u32);
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// bl 0x82bae2e8
	sub_82BAE2E8(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// bl 0x82bae2e8
	sub_82BAE2E8(ctx, base);
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82bae820
	if (!cr6.gt) goto loc_82BAE820;
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// mr r20,r17
	r20.u64 = r17.u64;
	// mr r19,r30
	r19.u64 = r30.u64;
loc_82BAE630:
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + int32_t(32) );
	// mr r21,r17
	r21.u64 = r17.u64;
	// lwz r10,12(r18)
	ctx.r10.u64 = PPC_LOAD_U32(r18.u32 + int32_t(12) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwzx r24,r20,r11
	r24.u64 = PPC_LOAD_U32(r20.u32 + r11.u32);
	// ble cr6,0x82bae814
	if (!cr6.gt) goto loc_82BAE814;
	// mr r22,r17
	r22.u64 = r17.u64;
loc_82BAE64C:
	// lwz r11,8(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(8) );
	// mr r26,r17
	r26.u64 = r17.u64;
	// add r25,r11,r22
	r25.u64 = r11.u64 + r22.u64;
	// lhz r11,4(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 4);
	// lhz r10,6(r25)
	ctx.r10.u64 = PPC_LOAD_U16(r25.u32 + 6);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82bae7d0
	if (!cr6.lt) goto loc_82BAE7D0;
	// rlwinm r27,r11,3,0,28
	r27.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r23,r11,r10
	r23.s64 = ctx.r10.s64 - r11.s64;
loc_82BAE67C:
	// lwz r11,16(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(16) );
	// mr r8,r17
	ctx.r8.u64 = r17.u64;
	// add r28,r11,r27
	r28.u64 = r11.u64 + r27.u64;
	// lhz r11,4(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 4);
	// lhz r10,6(r28)
	ctx.r10.u64 = PPC_LOAD_U16(r28.u32 + 6);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82bae738
	if (!cr6.lt) goto loc_82BAE738;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r29,r11,r10
	r29.s64 = ctx.r10.s64 - r11.s64;
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// rlwinm r30,r11,3,0,28
	r30.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
loc_82BAE6B4:
	// lwz r11,24(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(24) );
	// add r31,r11,r30
	r31.u64 = r11.u64 + r30.u64;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// cmplw cr6,r11,r24
	cr6.compare<uint32_t>(r11.u32, r24.u32, xer);
	// bne cr6,0x82bae72c
	if (!cr6.eq) goto loc_82BAE72C;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// cmpw cr6,r7,r11
	cr6.compare<int32_t>(ctx.r7.s32, r11.s32, xer);
	// bne cr6,0x82bae710
	if (!cr6.eq) goto loc_82BAE710;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,240(r1)
	PPC_STORE_U64(ctx.r1.u32 + 240, r11.u64);
	// lfd f13,240(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 240);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f9.u64);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
loc_82BAE710:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// li r8,1
	ctx.r8.s64 = 1;
	// stwx r31,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + r11.u32, r31.u32);
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// stw r7,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r7.u32);
loc_82BAE72C:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r30,r30,24
	r30.s64 = r30.s64 + 24;
	// bne 0x82bae6b4
	if (!cr0.eq) goto loc_82BAE6B4;
loc_82BAE738:
	// clrlwi r11,r8,24
	r11.u64 = ctx.r8.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bae7c0
	if (cr6.eq) goto loc_82BAE7C0;
	// stw r7,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r7.u32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(180) );
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(184) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82bae79c
	if (!cr6.eq) goto loc_82BAE79C;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,188(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,248(r1)
	PPC_STORE_U64(ctx.r1.u32 + 248, r11.u64);
	// lfd f13,248(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 248);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f9.u64);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
loc_82BAE79C:
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(184) );
	// li r26,1
	r26.s64 = 1;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r28.u32);
	// lwz r8,184(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(184) );
	// addi r6,r8,1
	ctx.r6.s64 = ctx.r8.s64 + 1;
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// stw r6,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r6.u32);
loc_82BAE7C0:
	// addic. r23,r23,-1
	xer.ca = r23.u32 > 0;
	r23.s64 = r23.s64 + -1;
	cr0.compare<int32_t>(r23.s32, 0, xer);
	// addi r27,r27,8
	r27.s64 = r27.s64 + 8;
	// bne 0x82bae67c
	if (!cr0.eq) goto loc_82BAE67C;
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82BAE7D0:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bae800
	if (cr6.eq) goto loc_82BAE800;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x82284810
	sub_82284810(ctx, base);
	// lwz r11,184(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(184) );
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
loc_82BAE800:
	// lwz r11,12(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(12) );
	// addi r21,r21,1
	r21.s64 = r21.s64 + 1;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// cmpw cr6,r21,r11
	cr6.compare<int32_t>(r21.s32, r11.s32, xer);
	// blt cr6,0x82bae64c
	if (cr6.lt) goto loc_82BAE64C;
loc_82BAE814:
	// addic. r19,r19,-1
	xer.ca = r19.u32 > 0;
	r19.s64 = r19.s64 + -1;
	cr0.compare<int32_t>(r19.s32, 0, xer);
	// addi r20,r20,4
	r20.s64 = r20.s64 + 4;
	// bne 0x82bae630
	if (!cr0.eq) goto loc_82BAE630;
loc_82BAE820:
	// lwz r8,232(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(232) );
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// mr r28,r17
	r28.u64 = r17.u64;
	// mr r23,r17
	r23.u64 = r17.u64;
	// mr r31,r17
	r31.u64 = r17.u64;
	// mr r19,r17
	r19.u64 = r17.u64;
	// mr r20,r17
	r20.u64 = r17.u64;
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// addi r22,r9,-12480
	r22.s64 = ctx.r9.s64 + -12480;
	// addi r21,r11,-12160
	r21.s64 = r11.s64 + -12160;
	// ble cr6,0x82bae9b4
	if (!cr6.gt) goto loc_82BAE9B4;
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// mr r25,r17
	r25.u64 = r17.u64;
loc_82BAE85C:
	// lwz r9,336(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(336) );
	// mr r24,r10
	r24.u64 = ctx.r10.u64;
	// lwzx r8,r25,r9
	ctx.r8.u64 = PPC_LOAD_U32(r25.u32 + ctx.r9.u32);
	// cmpw cr6,r10,r8
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, xer);
	// bge cr6,0x82bae998
	if (!cr6.lt) goto loc_82BAE998;
	// rlwinm r27,r23,2,0,29
	r27.u64 = rotl64(r23.u32 | (r23.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r26,r10,2,0,29
	r26.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_82BAE878:
	// lwzx r11,r11,r27
	r11.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// subf r29,r28,r11
	r29.s64 = r11.s64 - r28.s64;
	// rlwinm r11,r29,1,0,30
	r11.u64 = rotl64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r29,r11
	ctx.r10.u64 = r29.u64 + r11.u64;
	// rlwinm r11,r10,3,0,28
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r30,r11,20
	r30.s64 = r11.s64 + 20;
	// beq cr6,0x82bae8bc
	if (cr6.eq) goto loc_82BAE8BC;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// li r7,330
	ctx.r7.s64 = 330;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x82bae8dc
	goto loc_82BAE8DC;
loc_82BAE8BC:
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// li r7,335
	ctx.r7.s64 = 335;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r19,r31
	r19.u64 = r31.u64;
loc_82BAE8DC:
	// sth r29,16(r31)
	PPC_STORE_U16(r31.u32 + 16, r29.u16);
	// addi r9,r31,20
	ctx.r9.s64 = r31.s64 + 20;
	// lwz r10,176(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwzx r8,r26,r10
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + ctx.r10.u32);
	// lhz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r8.u32 + 4);
	// sth r7,18(r31)
	PPC_STORE_U16(r31.u32 + 18, ctx.r7.u16);
	// lwz r5,176(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// lwzx r4,r26,r5
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + ctx.r5.u32);
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// stw r3,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r3.u32);
	// lwz r11,224(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(224) );
	// lwzx r10,r25,r11
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r17,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r17.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// lwzx r8,r11,r27
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmpw cr6,r28,r8
	cr6.compare<int32_t>(r28.s32, ctx.r8.s32, xer);
	// bge cr6,0x82bae974
	if (!cr6.lt) goto loc_82BAE974;
	// rlwinm r8,r28,2,0,29
	ctx.r8.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
loc_82BAE930:
	// lwz r11,128(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// li r7,6
	ctx.r7.s64 = 6;
	// lwzx r11,r11,r8
	r11.u64 = PPC_LOAD_U32(r11.u32 + ctx.r8.u32);
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
loc_82BAE944:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x82bae944
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82BAE944;
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lwzx r10,r11,r27
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// cmpw cr6,r6,r10
	cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, xer);
	// blt cr6,0x82bae930
	if (cr6.lt) goto loc_82BAE930;
loc_82BAE974:
	// lwz r9,336(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(336) );
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// lwzx r28,r11,r27
	r28.u64 = PPC_LOAD_U32(r11.u32 + r27.u32);
	// addi r23,r23,1
	r23.s64 = r23.s64 + 1;
	// addi r27,r27,4
	r27.s64 = r27.s64 + 4;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// lwzx r10,r25,r9
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + ctx.r9.u32);
	// cmpw cr6,r24,r10
	cr6.compare<int32_t>(r24.s32, ctx.r10.s32, xer);
	// blt cr6,0x82bae878
	if (cr6.lt) goto loc_82BAE878;
loc_82BAE998:
	// lwz r8,232(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(232) );
	// addi r20,r20,1
	r20.s64 = r20.s64 + 1;
	// lwzx r10,r25,r9
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + ctx.r9.u32);
	// addi r25,r25,4
	r25.s64 = r25.s64 + 4;
	// cmpw cr6,r20,r8
	cr6.compare<int32_t>(r20.s32, ctx.r8.s32, xer);
	// blt cr6,0x82bae85c
	if (cr6.lt) goto loc_82BAE85C;
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82BAE9B4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82baea98
	if (!cr6.gt) goto loc_82BAEA98;
	// mr r29,r17
	r29.u64 = r17.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
loc_82BAE9C4:
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + int32_t(32) );
	// li r5,4
	ctx.r5.s64 = 4;
	// lwz r10,0(r16)
	ctx.r10.u64 = PPC_LOAD_U32(r16.u32 + int32_t(0) );
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lwzx r4,r29,r11
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(76) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82baea88
	if (cr6.eq) goto loc_82BAEA88;
	// lwz r30,32(r18)
	r30.u64 = PPC_LOAD_U32(r18.u32 + int32_t(32) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82baea88
	if (cr6.eq) goto loc_82BAEA88;
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + int32_t(32) );
	// lwzx r11,r29,r11
	r11.u64 = PPC_LOAD_U32(r29.u32 + r11.u32);
loc_82BAEA04:
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x82baea20
	if (cr6.eq) goto loc_82BAEA20;
	// lwz r30,0(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x82baea04
	if (!cr6.eq) goto loc_82BAEA04;
	// b 0x82baea88
	goto loc_82BAEA88;
loc_82BAEA20:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baea50
	if (cr6.eq) goto loc_82BAEA50;
	// lwz r27,4(r30)
	r27.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// li r7,375
	ctx.r7.s64 = 375;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x82baea74
	goto loc_82BAEA74;
loc_82BAEA50:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r22
	ctx.r5.u64 = r22.u64;
	// mr r6,r21
	ctx.r6.u64 = r21.u64;
	// li r7,380
	ctx.r7.s64 = 380;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r19,r31
	r19.u64 = r31.u64;
loc_82BAEA74:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// stw r17,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r17.u32);
loc_82BAEA88:
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// bne 0x82bae9c4
	if (!cr0.eq) goto loc_82BAE9C4;
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82BAEA98:
	// mr r8,r19
	ctx.r8.u64 = r19.u64;
	// cmplwi cr6,r19,0
	cr6.compare<uint32_t>(r19.u32, 0, xer);
	// beq cr6,0x82baeb50
	if (cr6.eq) goto loc_82BAEB50;
loc_82BAEAA4:
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82baeb44
	if (!cr6.gt) goto loc_82BAEB44;
	// addi r6,r8,20
	ctx.r6.s64 = ctx.r8.s64 + 20;
	// mr r10,r17
	ctx.r10.u64 = r17.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
loc_82BAEAB8:
	// lwz r11,32(r14)
	r11.u64 = PPC_LOAD_U32(r14.u32 + int32_t(32) );
	// lwz r9,8(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(8) );
	// lwzx r7,r10,r11
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + r11.u32);
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// bne cr6,0x82baead8
	if (!cr6.eq) goto loc_82BAEAD8;
	// lwz r11,32(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + int32_t(32) );
	// lwzx r9,r11,r10
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// stw r9,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r9.u32);
loc_82BAEAD8:
	// lhz r4,16(r8)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r8.u32 + 16);
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// mr r9,r17
	ctx.r9.u64 = r17.u64;
	// extsh r3,r4
	ctx.r3.s64 = ctx.r4.s16;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x82baeb38
	if (!cr6.gt) goto loc_82BAEB38;
loc_82BAEAF0:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r4,r7
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, xer);
	// bne cr6,0x82baeb08
	if (!cr6.eq) goto loc_82BAEB08;
	// lwz r4,32(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + int32_t(32) );
	// lwzx r3,r4,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// stw r3,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r3.u32);
loc_82BAEB08:
	// lwz r4,20(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// cmplw cr6,r4,r7
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, xer);
	// bne cr6,0x82baeb20
	if (!cr6.eq) goto loc_82BAEB20;
	// lwz r4,32(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + int32_t(32) );
	// lwzx r3,r4,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r10.u32);
	// stw r3,20(r11)
	PPC_STORE_U32(r11.u32 + 20, ctx.r3.u32);
loc_82BAEB20:
	// lhz r4,16(r8)
	ctx.r4.u64 = PPC_LOAD_U16(ctx.r8.u32 + 16);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// extsh r3,r4
	ctx.r3.s64 = ctx.r4.s16;
	// cmpw cr6,r9,r3
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r3.s32, xer);
	// blt cr6,0x82baeaf0
	if (cr6.lt) goto loc_82BAEAF0;
loc_82BAEB38:
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x82baeab8
	if (!cr0.eq) goto loc_82BAEAB8;
loc_82BAEB44:
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x82baeaa4
	if (!cr6.eq) goto loc_82BAEAA4;
loc_82BAEB50:
	// lwz r11,32(r18)
	r11.u64 = PPC_LOAD_U32(r18.u32 + int32_t(32) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82baeb80
	if (cr6.eq) goto loc_82BAEB80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82baeb78
	if (cr6.eq) goto loc_82BAEB78;
loc_82BAEB68:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82baeb68
	if (!cr6.eq) goto loc_82BAEB68;
loc_82BAEB78:
	// stw r19,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r19.u32);
	// b 0x82baeb84
	goto loc_82BAEB84;
loc_82BAEB80:
	// stw r19,32(r18)
	PPC_STORE_U32(r18.u32 + 32, r19.u32);
loc_82BAEB84:
	// stw r17,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r17.u32);
	// lwz r31,340(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(340) );
	// lwz r30,336(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(336) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, r17.u32);
	// stw r17,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, r17.u32);
	// stw r17,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, r17.u32);
	// lwz r31,292(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(292) );
	// lwz r30,288(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(288) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r17.u32);
	// stw r17,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r17.u32);
	// stw r17,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r17.u32);
	// lwz r31,132(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r17.u32);
	// stw r17,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r17.u32);
	// stw r17,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r17.u32);
	// lwz r31,180(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(180) );
	// lwz r30,176(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(176) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r17,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r17.u32);
	// stw r17,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r17.u32);
	// stw r17,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r17.u32);
	// lwz r31,228(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(228) );
	// lwz r30,224(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(224) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BAE500) {
	__imp__sub_82BAE500(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAEC40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82baec90
	sub_82BAEC90(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82baec78
	if (cr6.eq) goto loc_82BAEC78;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BAEC78:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAEC40) {
	__imp__sub_82BAEC40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAEC90) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r8,r10,6416
	ctx.r8.s64 = ctx.r10.s64 + 6416;
	// stw r8,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r8.u32);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// lwz r9,28(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r29,8(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// add r7,r11,r10
	ctx.r7.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r7,3,0,28
	r11.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r11,r9
	r31.u64 = r11.u64 + ctx.r9.u64;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// addi r5,r6,-12208
	ctx.r5.s64 = ctx.r6.s64 + -12208;
	// stw r5,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r5.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BAEC90) {
	__imp__sub_82BAEC90(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAECF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bbc
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r20,r4
	r20.u64 = ctx.r4.u64;
	// li r25,0
	r25.s64 = 0;
	// mr r19,r5
	r19.u64 = ctx.r5.u64;
	// mr r28,r25
	r28.u64 = r25.u64;
	// lwz r3,4(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + int32_t(4) );
	// mr r18,r25
	r18.u64 = r25.u64;
	// lwz r31,40(r20)
	r31.u64 = PPC_LOAD_U32(r20.u32 + int32_t(40) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// stw r25,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r25.u32);
	// lis r8,-32245
	ctx.r8.s64 = -2113208320;
	// stw r25,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r25.u32);
	// lis r7,-32245
	ctx.r7.s64 = -2113208320;
	// stw r25,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r25.u32);
	// mr r17,r3
	r17.u64 = ctx.r3.u64;
	// addi r6,r8,-6344
	ctx.r6.s64 = ctx.r8.s64 + -6344;
	// lwz r30,12(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + int32_t(12) );
	// lfs f0,-16596(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -16596);
	f0.f64 = double(temp.f32);
	// addi r5,r7,-6332
	ctx.r5.s64 = ctx.r7.s64 + -6332;
	// stfs f0,140(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x82baef78
	if (!cr6.gt) goto loc_82BAEF78;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// mr r24,r25
	r24.u64 = r25.u64;
	// mr r21,r31
	r21.u64 = r31.u64;
	// addi r23,r11,-12480
	r23.s64 = r11.s64 + -12480;
	// addi r22,r10,-12024
	r22.s64 = ctx.r10.s64 + -12024;
loc_82BAED98:
	// lwz r11,32(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(32) );
	// mr r30,r25
	r30.u64 = r25.u64;
	// lwz r10,12(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(12) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwzx r27,r24,r11
	r27.u64 = PPC_LOAD_U32(r24.u32 + r11.u32);
	// ble cr6,0x82baedec
	if (!cr6.gt) goto loc_82BAEDEC;
	// mr r31,r25
	r31.u64 = r25.u64;
loc_82BAEDB4:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(8) );
	// add r11,r31,r11
	r11.u64 = r31.u64 + r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r10,r27
	cr6.compare<uint32_t>(ctx.r10.u32, r27.u32, xer);
	// bne cr6,0x82baedd8
	if (!cr6.eq) goto loc_82BAEDD8;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
loc_82BAEDD8:
	// lwz r11,12(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(12) );
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x82baedb4
	if (cr6.lt) goto loc_82BAEDB4;
loc_82BAEDEC:
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + int32_t(0) );
	// li r5,4
	ctx.r5.s64 = 4;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82baee74
	if (cr6.eq) goto loc_82BAEE74;
	// lwz r29,24(r26)
	r29.u64 = PPC_LOAD_U32(r26.u32 + int32_t(24) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82baee74
	if (cr6.eq) goto loc_82BAEE74;
loc_82BAEE20:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// addi r31,r29,12
	r31.s64 = r29.s64 + 12;
	// mr r30,r25
	r30.u64 = r25.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82baee68
	if (!cr6.gt) goto loc_82BAEE68;
loc_82BAEE38:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x82baee50
	if (!cr6.eq) goto loc_82BAEE50;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82bae368
	sub_82BAE368(ctx, base);
loc_82BAEE50:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// blt cr6,0x82baee38
	if (cr6.lt) goto loc_82BAEE38;
loc_82BAEE68:
	// lwz r29,0(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82baee20
	if (!cr6.eq) goto loc_82BAEE20;
loc_82BAEE74:
	// lwz r11,136(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82baef48
	if (!cr6.gt) goto loc_82BAEF48;
	// rlwinm r11,r11,4,0,27
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// addi r31,r11,12
	r31.s64 = r11.s64 + 12;
	// beq cr6,0x82baeeb4
	if (cr6.eq) goto loc_82BAEEB4;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// li r7,231
	ctx.r7.s64 = 231;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// stw r3,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r3.u32);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// b 0x82baeed4
	goto loc_82BAEED4;
loc_82BAEEB4:
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// li r7,236
	ctx.r7.s64 = 236;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r18,r28
	r18.u64 = r28.u64;
loc_82BAEED4:
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
	// addi r11,r28,12
	r11.s64 = r28.s64 + 12;
	// stw r25,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r25.u32);
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// lwz r9,136(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// stw r9,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r9.u32);
	// lwz r8,32(r19)
	ctx.r8.u64 = PPC_LOAD_U32(r19.u32 + int32_t(32) );
	// lwz r7,136(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(136) );
	// cmpwi cr6,r7,0
	cr6.compare<int32_t>(ctx.r7.s32, 0, xer);
	// lwzx r8,r8,r24
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + r24.u32);
	// ble cr6,0x82baef48
	if (!cr6.gt) goto loc_82BAEF48;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
loc_82BAEF04:
	// lwz r7,128(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwzx r6,r9,r7
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// lwz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// stw r5,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r5.u32);
	// lwz r4,4(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(4) );
	// stw r4,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r4.u32);
	// lwz r3,8(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(8) );
	// stw r3,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r3.u32);
	// lwz r7,12(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(12) );
	// stw r7,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r7.u32);
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lwz r6,8(r28)
	ctx.r6.u64 = PPC_LOAD_U32(r28.u32 + int32_t(8) );
	// cmpw cr6,r10,r6
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r6.s32, xer);
	// blt cr6,0x82baef04
	if (cr6.lt) goto loc_82BAEF04;
loc_82BAEF48:
	// stw r25,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r25.u32);
	// lwz r31,132(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// addic. r21,r21,-1
	xer.ca = r21.u32 > 0;
	r21.s64 = r21.s64 + -1;
	cr0.compare<int32_t>(r21.s32, 0, xer);
	// stw r25,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r25.u32);
	// addi r24,r24,4
	r24.s64 = r24.s64 + 4;
	// stw r25,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r25.u32);
	// bne 0x82baed98
	if (!cr0.eq) goto loc_82BAED98;
loc_82BAEF78:
	// lwz r11,24(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(24) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82baefa8
	if (cr6.eq) goto loc_82BAEFA8;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82baefa0
	if (cr6.eq) goto loc_82BAEFA0;
loc_82BAEF90:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82baef90
	if (!cr6.eq) goto loc_82BAEF90;
loc_82BAEFA0:
	// stw r18,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r18.u32);
	// b 0x82baefac
	goto loc_82BAEFAC;
loc_82BAEFA8:
	// stw r18,24(r26)
	PPC_STORE_U32(r26.u32 + 24, r18.u32);
loc_82BAEFAC:
	// stw r25,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r25.u32);
	// lwz r31,132(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(132) );
	// lwz r30,128(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(128) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c0c
	return;
}

PPC_WEAK_FUNC(sub_82BAECF0) {
	__imp__sub_82BAECF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAEFD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// li r3,-1
	ctx.r3.s64 = -1;
	// li r11,0
	r11.s64 = 0;
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(12) );
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blelr cr6
	if (!cr6.gt) return;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
loc_82BAEFEC:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r8,r4
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r4.u32, xer);
	// beq cr6,0x82baf00c
	if (cr6.eq) goto loc_82BAF00C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x82baefec
	if (cr6.lt) goto loc_82BAEFEC;
	// blr 
	return;
loc_82BAF00C:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAEFD0) {
	__imp__sub_82BAEFD0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF018) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// rlwinm r10,r4,3,0,28
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r9,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r9.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF018) {
	__imp__sub_82BAF018(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF038) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r28,0
	r28.s64 = 0;
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// lwz r30,32(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82baf0c8
	if (cr6.eq) goto loc_82BAF0C8;
loc_82BAF064:
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baf0ac
	if (cr6.eq) goto loc_82BAF0AC;
	// stw r28,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r28.u32);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lwz r27,40(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r26,36(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r5,r27,3,0,28
	ctx.r5.u64 = rotl64(r27.u32 | (r27.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r28,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r28.u32);
	// stw r28,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r28.u32);
	// lwz r31,0(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// li r5,52
	ctx.r5.s64 = 52;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAF0AC:
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(40) );
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// lwz r10,32(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x82baf064
	if (!cr6.eq) goto loc_82BAF064;
loc_82BAF0C8:
	// stw r28,40(r29)
	PPC_STORE_U32(r29.u32 + 40, r28.u32);
	// lwz r31,36(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(36) );
	// lwz r30,32(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r28,36(r29)
	PPC_STORE_U32(r29.u32 + 36, r28.u32);
	// stw r28,32(r29)
	PPC_STORE_U32(r29.u32 + 32, r28.u32);
	// stw r28,40(r29)
	PPC_STORE_U32(r29.u32 + 40, r28.u32);
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r28,36(r29)
	PPC_STORE_U32(r29.u32 + 36, r28.u32);
	// stw r28,32(r29)
	PPC_STORE_U32(r29.u32 + 32, r28.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAF038) {
	__imp__sub_82BAF038(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF110) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82baf180
	if (cr6.eq) goto loc_82BAF180;
loc_82BAF144:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r27,r9
	cr6.compare<uint32_t>(r27.u32, ctx.r9.u32, xer);
	// beq cr6,0x82baf174
	if (cr6.eq) goto loc_82BAF174;
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82baf144
	if (!cr6.eq) goto loc_82BAF144;
	// b 0x82baf180
	goto loc_82BAF180;
loc_82BAF174:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addic. r30,r11,4
	xer.ca = r11.u32 > 4294967291;
	r30.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// bne 0x82baf2a0
	if (!cr0.eq) goto loc_82BAF2A0;
loc_82BAF180:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r30,r11,-11872
	r30.s64 = r11.s64 + -11872;
	// addi r29,r10,-11752
	r29.s64 = ctx.r10.s64 + -11752;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,52
	ctx.r4.s64 = 52;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r7,78
	ctx.r7.s64 = 78;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lis r11,-32245
	r11.s64 = -2113208320;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// addi r28,r11,-6332
	r28.s64 = r11.s64 + -6332;
	// beq cr6,0x82baf208
	if (cr6.eq) goto loc_82BAF208;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r11,0
	r11.s64 = 0;
	// lis r9,-32245
	ctx.r9.s64 = -2113208320;
	// stw r11,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r11.u32);
	// addi r29,r30,4
	r29.s64 = r30.s64 + 4;
	// stw r11,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r11.u32);
	// addi r6,r9,-6344
	ctx.r6.s64 = ctx.r9.s64 + -6344;
	// lfs f0,-16596(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16596);
	f0.f64 = double(temp.f32);
	// stw r11,44(r30)
	PPC_STORE_U32(r30.u32 + 44, r11.u32);
	// stfs f0,48(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 48, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82baf4b0
	sub_82BAF4B0(ctx, base);
	// b 0x82baf20c
	goto loc_82BAF20C;
loc_82BAF208:
	// li r30,0
	r30.s64 = 0;
loc_82BAF20C:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// addi r6,r11,-11724
	ctx.r6.s64 = r11.s64 + -11724;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82baf26c
	if (!cr6.eq) goto loc_82BAF26C;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
loc_82BAF26C:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r30,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, r30.u32);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r11,-4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(-4) );
	// addi r30,r11,4
	r30.s64 = r11.s64 + 4;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
loc_82BAF2A0:
	// lwz r11,36(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(36) );
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// stw r26,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r26.u32);
	// stw r25,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r25.u32);
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82baf2f0
	if (!cr6.eq) goto loc_82BAF2F0;
	// extsw r11,r11
	r11.s64 = r11.s32;
	// lfs f0,44(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 44);
	f0.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.f9.u64);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// addi r4,r11,1
	ctx.r4.s64 = r11.s64 + 1;
	// bl 0x82baf4b0
	sub_82BAF4B0(ctx, base);
loc_82BAF2F0:
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// stdx r9,r8,r10
	PPC_STORE_U64(ctx.r8.u32 + ctx.r10.u32, ctx.r9.u64);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// stw r7,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r7.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BAF110) {
	__imp__sub_82BAF110(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF318) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// li r27,0
	r27.s64 = 0;
	// mr r29,r27
	r29.u64 = r27.u64;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r27.u8);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x82baf380
	if (cr6.eq) goto loc_82BAF380;
loc_82BAF350:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// beq cr6,0x82baf38c
	if (cr6.eq) goto loc_82BAF38C;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x82baf350
	if (!cr6.eq) goto loc_82BAF350;
loc_82BAF380:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_82BAF38C:
	// lwz r28,0(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// addi r31,r28,4
	r31.s64 = r28.s64 + 4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baf380
	if (cr6.eq) goto loc_82BAF380;
	// lwz r4,40(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// cmplw cr6,r7,r9
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, xer);
	// beq cr6,0x82baf380
	if (cr6.eq) goto loc_82BAF380;
loc_82BAF3BC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r9,r5
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, xer);
	// bne cr6,0x82baf3d4
	if (!cr6.eq) goto loc_82BAF3D4;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmplw cr6,r9,r6
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, xer);
	// beq cr6,0x82baf400
	if (cr6.eq) goto loc_82BAF400;
loc_82BAF3D4:
	// lwz r9,40(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// bne cr6,0x82baf3bc
	if (!cr6.eq) goto loc_82BAF3BC;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_82BAF400:
	// subf r11,r10,r4
	r11.s64 = ctx.r4.s64 - ctx.r10.s64;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// add r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 + ctx.r7.u64;
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r4,r3,8
	ctx.r4.s64 = ctx.r3.s64 + 8;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// addic. r11,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	r11.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bne 0x82baf4a0
	if (!cr0.eq) goto loc_82BAF4A0;
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = rotl64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// add r3,r10,r9
	ctx.r3.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// addi r4,r3,4
	ctx.r4.s64 = ctx.r3.s64 + 4;
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82caa2e0
	sub_82CAA2E0(ctx, base);
	// lwz r11,40(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// stw r9,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r9.u32);
	// beq cr6,0x82baf498
	if (cr6.eq) goto loc_82BAF498;
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// lwz r30,36(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r29,32(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// rlwinm r5,r30,3,0,28
	ctx.r5.u64 = rotl64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r27,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r27.u32);
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r5,52
	ctx.r5.s64 = 52;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BAF498:
	// li r11,1
	r11.s64 = 1;
	// stb r11,0(r26)
	PPC_STORE_U8(r26.u32 + 0, r11.u8);
loc_82BAF4A0:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAF318) {
	__imp__sub_82BAF318(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF4B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// ble cr6,0x82baf538
	if (!cr6.gt) goto loc_82BAF538;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// rlwinm r29,r30,3,0,28
	r29.u64 = rotl64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r28,r11,-13456
	r28.s64 = r11.s64 + -13456;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r7,141
	ctx.r7.s64 = 141;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,32(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r5,r10,3,0,28
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r29,36(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r27,32(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// rlwinm r5,r29,3,0,28
	ctx.r5.u64 = rotl64(r29.u32 | (r29.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r28,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r28.u32);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
loc_82BAF538:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BAF4B0) {
	__imp__sub_82BAF4B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF540) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// stw r26,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, r26.u32);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// blt cr6,0x82baf68c
	if (cr6.lt) goto loc_82BAF68C;
	// lbz r11,42(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 42);
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// bge cr6,0x82baf68c
	if (!cr6.lt) goto loc_82BAF68C;
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82baf5b4
	if (cr6.eq) goto loc_82BAF5B4;
	// lwz r3,-72(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-72) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,116(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(116) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r4,r1,172
	ctx.r4.s64 = ctx.r1.s64 + 172;
	// addi r3,r3,192
	ctx.r3.s64 = ctx.r3.s64 + 192;
	// bl 0x82284810
	sub_82284810(ctx, base);
loc_82BAF5B4:
	// lwz r3,-72(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-72) );
	// lhz r27,40(r31)
	r27.u64 = PPC_LOAD_U16(r31.u32 + 40);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// add r29,r27,r28
	r29.u64 = r27.u64 + r28.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,-72(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-72) );
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r6,40(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(40) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,-72(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-72) );
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,40(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(40) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r6,8(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(8) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r4,43(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 43);
	// stb r28,43(r31)
	PPC_STORE_U8(r31.u32 + 43, r28.u8);
	// addi r11,r31,48
	r11.s64 = r31.s64 + 48;
	// li r10,0
	ctx.r10.s64 = 0;
	// stb r4,44(r31)
	PPC_STORE_U8(r31.u32 + 44, ctx.r4.u8);
	// lbz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U8(r31.u32 + 64);
	// ori r11,r3,16
	r11.u64 = ctx.r3.u64 | 16;
	// stw r10,56(r31)
	PPC_STORE_U32(r31.u32 + 56, ctx.r10.u32);
	// stb r11,64(r31)
	PPC_STORE_U8(r31.u32 + 64, r11.u8);
	// stw r10,48(r31)
	PPC_STORE_U32(r31.u32 + 48, ctx.r10.u32);
	// lwz r3,-72(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(-72) );
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r9,116(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(116) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r4,r1,172
	ctx.r4.s64 = ctx.r1.s64 + 172;
	// addi r3,r3,240
	ctx.r3.s64 = ctx.r3.s64 + 240;
	// bl 0x82284810
	sub_82284810(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_82BAF68C:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r4,r10,-11700
	ctx.r4.s64 = ctx.r10.s64 + -11700;
	// lwz r9,19068(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAF540) {
	__imp__sub_82BAF540(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF6B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,-72(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-72) );
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// lwz r7,40(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(40) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lhz r5,40(r30)
	ctx.r5.u64 = PPC_LOAD_U16(r30.u32 + 40);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lbz r6,42(r30)
	ctx.r6.u64 = PPC_LOAD_U8(r30.u32 + 42);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r5,r3,24
	ctx.r5.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r5,1
	cr6.compare<uint32_t>(ctx.r5.u32, 1, xer);
	// blt cr6,0x82baf74c
	if (cr6.lt) goto loc_82BAF74C;
	// lbz r11,42(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 42);
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// bge cr6,0x82baf74c
	if (!cr6.lt) goto loc_82BAF74C;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82BAF74C:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r4,r10,-11668
	ctx.r4.s64 = ctx.r10.s64 + -11668;
	// lwz r9,19068(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BAF6B0) {
	__imp__sub_82BAF6B0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF770) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r3,43(r3)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 43);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF770) {
	__imp__sub_82BAF770(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF7A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r3,42(r3)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 42);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF7A0) {
	__imp__sub_82BAF7A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF7D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r9,43(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 43);
	// lbz r10,42(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 42);
	// add r11,r9,r30
	r11.u64 = ctx.r9.u64 + r30.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bgt cr6,0x82baf818
	if (cr6.gt) goto loc_82BAF818;
	// li r11,1
	r11.s64 = 1;
	// b 0x82baf824
	goto loc_82BAF824;
loc_82BAF818:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82baf824
	if (cr6.lt) goto loc_82BAF824;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAF824:
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// beq cr6,0x82baf84c
	if (cr6.eq) goto loc_82BAF84C;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAF84C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BAF7D0) {
	__imp__sub_82BAF7D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF858) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r9,43(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 43);
	// lbz r10,42(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 42);
	// subf r11,r29,r9
	r11.s64 = ctx.r9.s64 - r29.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bgt cr6,0x82baf8a0
	if (cr6.gt) goto loc_82BAF8A0;
	// li r11,1
	r11.s64 = 1;
	// b 0x82baf8ac
	goto loc_82BAF8AC;
loc_82BAF8A0:
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// blt cr6,0x82baf8ac
	if (cr6.lt) goto loc_82BAF8AC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
loc_82BAF8AC:
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// beq cr6,0x82baf8d4
	if (cr6.eq) goto loc_82BAF8D4;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BAF8D4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BAF858) {
	__imp__sub_82BAF858(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF8E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// lwz r7,8(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(8) );
	// lbz r5,44(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 44);
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF8E0) {
	__imp__sub_82BAF8E0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF940) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// lbz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 16);
	// stw r31,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, r31.u32);
	// ori r7,r8,16
	ctx.r7.u64 = ctx.r8.u64 | 16;
	// stw r31,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r31.u32);
	// stb r7,16(r9)
	PPC_STORE_U8(ctx.r9.u32 + 16, ctx.r7.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF940) {
	__imp__sub_82BAF940(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF990) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r9,64(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 64);
	// rlwinm r3,r9,30,31,31
	ctx.r3.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF990) {
	__imp__sub_82BAF990(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAF9C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r9,64(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 64);
	// rlwinm r3,r9,31,31,31
	ctx.r3.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAF9C8) {
	__imp__sub_82BAF9C8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFA00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// rlwimi r27,r28,1,0,30
	r27.u64 = (rotl32(r28.u32, 1) & 0xFFFFFFFE) | (r27.u64 & 0xFFFFFFFF00000001);
	// li r11,0
	r11.s64 = 0;
	// rlwinm r8,r27,5,0,26
	ctx.r8.u64 = rotl64(r27.u32 | (r27.u64 << 32), 5) & 0xFFFFFFE0;
	// li r7,-1
	ctx.r7.s64 = -1;
	// lbz r5,16(r9)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r9.u32 + 16);
	// addi r3,r31,-76
	ctx.r3.s64 = r31.s64 + -76;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, r11.u32);
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// clrlwi r4,r5,31
	ctx.r4.u64 = ctx.r5.u32 & 0x1;
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, r11.u32);
	// stw r29,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r29.u32);
	// li r5,16384
	ctx.r5.s64 = 16384;
	// or r11,r8,r4
	r11.u64 = ctx.r8.u64 | ctx.r4.u64;
	// stw r7,12(r9)
	PPC_STORE_U32(ctx.r9.u32 + 12, ctx.r7.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// ori r8,r10,10
	ctx.r8.u64 = ctx.r10.u64 | 10;
	// stb r8,16(r9)
	PPC_STORE_U8(ctx.r9.u32 + 16, ctx.r8.u8);
	// lwz r7,-76(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-76) );
	// lwz r11,80(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(80) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAFA00) {
	__imp__sub_82BAFA00(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFA98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r9,r3,48
	ctx.r9.s64 = ctx.r3.s64 + 48;
	// clrlwi r8,r31,24
	ctx.r8.u64 = r31.u32 & 0xFF;
	// lbz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r9.u32 + 16);
	// rlwimi r7,r8,2,29,29
	ctx.r7.u64 = (rotl32(ctx.r8.u32, 2) & 0x4) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFFB);
	// stb r7,16(r9)
	PPC_STORE_U8(ctx.r9.u32 + 16, ctx.r7.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAFA98) {
	__imp__sub_82BAFA98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFAE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,-72(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(-72) );
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// lwz r7,40(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(40) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lbz r5,43(r31)
	ctx.r5.u64 = PPC_LOAD_U8(r31.u32 + 43);
	// lhz r4,40(r31)
	ctx.r4.u64 = PPC_LOAD_U16(r31.u32 + 40);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BAFAE8) {
	__imp__sub_82BAFAE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFB58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82bafb90
	if (!cr6.gt) goto loc_82BAFB90;
loc_82BAFB78:
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// addi r9,r9,2
	ctx.r9.s64 = ctx.r9.s64 + 2;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r9,r11
	r11.u64 = ctx.r9.u64 + r11.u64;
	// bne 0x82bafb78
	if (!cr0.eq) goto loc_82BAFB78;
loc_82BAFB90:
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82bafbd8
	if (cr6.eq) goto loc_82BAFBD8;
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// addi r30,r11,8
	r30.s64 = r11.s64 + 8;
	// li r31,0
	r31.s64 = 0;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// ble cr6,0x82bafbd8
	if (!cr6.gt) goto loc_82BAFBD8;
loc_82BAFBB0:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bl 0x82bafbe8
	sub_82BAFBE8(ctx, base);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmpw cr6,r31,r10
	cr6.compare<int32_t>(r31.s32, ctx.r10.s32, xer);
	// blt cr6,0x82bafbb0
	if (cr6.lt) goto loc_82BAFBB0;
loc_82BAFBD8:
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BAFB58) {
	__imp__sub_82BAFB58(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFBE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// cmpw cr6,r10,r11
	cr6.compare<int32_t>(ctx.r10.s32, r11.s32, xer);
	// bne cr6,0x82bafca8
	if (!cr6.eq) goto loc_82BAFCA8;
	// extsw r10,r11
	ctx.r10.s64 = r11.s32;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 44);
	f0.f64 = double(temp.f32);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fmuls f10,f0,f11
	ctx.f10.f64 = double(float(f0.f64 * ctx.f11.f64));
	// fctiwz f9,f10
	ctx.f9.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f9.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// addi r30,r10,1
	r30.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// ble cr6,0x82bafca8
	if (!cr6.gt) goto loc_82BAFCA8;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// rlwinm r28,r30,2,0,29
	r28.u64 = rotl64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r27,r11,-13456
	r27.s64 = r11.s64 + -13456;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// li r7,141
	ctx.r7.s64 = 141;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r4,32(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r28,36(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r26,32(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
loc_82BAFCA8:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r8,r9
	PPC_STORE_U32(ctx.r8.u32 + ctx.r9.u32, ctx.r10.u32);
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r7,r11,1
	ctx.r7.s64 = r11.s64 + 1;
	// add r3,r9,r10
	ctx.r3.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r7,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r7.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAFBE8) {
	__imp__sub_82BAFBE8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFCE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// lwz r7,36(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(36) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r5,4
	ctx.r5.s64 = 4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,76(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(76) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bafe70
	if (!cr6.eq) goto loc_82BAFE70;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// rlwinm r31,r26,5,0,26
	r31.u64 = rotl64(r26.u32 | (r26.u64 << 32), 5) & 0xFFFFFFE0;
	// lwz r9,0(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// add r8,r10,r31
	ctx.r8.u64 = ctx.r10.u64 + r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r9,56(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(56) );
	// lwz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// lwz r6,16(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(16) );
	// lwz r5,24(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// rlwinm r8,r7,5,31,31
	ctx.r8.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 5) & 0x1;
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// rlwinm r7,r6,4,31,31
	ctx.r7.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0x1;
	// lwz r27,28(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// rlwinm r6,r6,2,30,31
	ctx.r6.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0x3;
	// subf r5,r10,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r10.s64;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r8,r11,r31
	ctx.r8.u64 = r11.u64 + r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r11,60(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(60) );
	// lwz r6,16(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(16) );
	// rlwinm r5,r6,3,31,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0x1;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// li r29,0
	r29.s64 = 0;
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// lwz r9,16(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(16) );
	// clrlwi r8,r9,5
	ctx.r8.u64 = ctx.r9.u32 & 0x7FFFFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bafe14
	if (cr6.eq) goto loc_82BAFE14;
loc_82BAFDE8:
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bb0148
	sub_82BB0148(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// add r11,r11,r31
	r11.u64 = r11.u64 + r31.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// clrlwi r9,r10,5
	ctx.r9.u64 = ctx.r10.u32 & 0x7FFFFFF;
	// cmplw cr6,r29,r9
	cr6.compare<uint32_t>(r29.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bafde8
	if (cr6.lt) goto loc_82BAFDE8;
loc_82BAFE14:
	// lwz r29,28(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82baff1c
	if (cr6.eq) goto loc_82BAFF1C;
loc_82BAFE20:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bne cr6,0x82bafe5c
	if (!cr6.eq) goto loc_82BAFE5C;
	// lwz r11,12(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(12) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bafe5c
	if (!cr6.eq) goto loc_82BAFE5C;
	// lwz r31,20(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(20) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bafe5c
	if (cr6.eq) goto loc_82BAFE5C;
loc_82BAFE44:
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bb0148
	sub_82BB0148(ctx, base);
	// lwz r31,0(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82bafe44
	if (!cr6.eq) goto loc_82BAFE44;
loc_82BAFE5C:
	// lwz r29,0(r29)
	r29.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x82bafe20
	if (!cr6.eq) goto loc_82BAFE20;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_82BAFE70:
	// lwz r27,28(r30)
	r27.u64 = PPC_LOAD_U32(r30.u32 + int32_t(28) );
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82baff1c
	if (cr6.eq) goto loc_82BAFF1C;
loc_82BAFE7C:
	// lwz r11,12(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(12) );
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x82baff10
	if (!cr6.eq) goto loc_82BAFF10;
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// cmpw cr6,r11,r26
	cr6.compare<int32_t>(r11.s32, r26.s32, xer);
	// bne cr6,0x82baff10
	if (!cr6.eq) goto loc_82BAFF10;
	// lwz r31,16(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + int32_t(16) );
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// lwz r5,20(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// rlwinm r8,r10,5,31,31
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x1;
	// lwz r11,56(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(56) );
	// rlwinm r7,r10,4,31,31
	ctx.r7.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0x1;
	// rlwinm r6,r10,2,30,31
	ctx.r6.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0x3;
	// subf r5,r5,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r5.s64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rlwinm r5,r9,3,31,31
	ctx.r5.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0x1;
	// lwz r8,60(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(60) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r31,20(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + int32_t(20) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82baff10
	if (cr6.eq) goto loc_82BAFF10;
loc_82BAFEF8:
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bb0148
	sub_82BB0148(ctx, base);
	// lwz r31,0(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82bafef8
	if (!cr6.eq) goto loc_82BAFEF8;
loc_82BAFF10:
	// lwz r27,0(r27)
	r27.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x82bafe7c
	if (!cr6.eq) goto loc_82BAFE7C;
loc_82BAFF1C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BAFCE0) {
	__imp__sub_82BAFCE0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BAFF28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// li r28,0
	r28.s64 = 0;
	// li r5,16
	ctx.r5.s64 = 16;
	// li r4,0
	ctx.r4.s64 = 0;
	// stb r28,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r28.u8);
	// addi r3,r1,81
	ctx.r3.s64 = ctx.r1.s64 + 81;
	// mr r25,r6
	r25.u64 = ctx.r6.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// clrlwi r26,r25,24
	r26.u64 = r25.u32 & 0xFF;
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x82baffac
	if (cr6.eq) goto loc_82BAFFAC;
	// mr r31,r28
	r31.u64 = r28.u64;
loc_82BAFF6C:
	// add r11,r31,r30
	r11.u64 = r31.u64 + r30.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// li r5,16
	ctx.r5.s64 = 16;
	// rlwinm r11,r11,5,0,26
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82188cf0
	sub_82188CF0(ctx, base);
	// cmplw cr6,r27,r3
	cr6.compare<uint32_t>(r27.u32, ctx.r3.u32, xer);
	// beq cr6,0x82bb000c
	if (cr6.eq) goto loc_82BB000C;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// clrlwi r28,r11,24
	r28.u64 = r11.u32 & 0xFF;
	// mr r31,r28
	r31.u64 = r28.u64;
	// cmplw cr6,r28,r26
	cr6.compare<uint32_t>(r28.u32, r26.u32, xer);
	// blt cr6,0x82baff6c
	if (cr6.lt) goto loc_82BAFF6C;
loc_82BAFFAC:
	// lwz r31,28(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(28) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bb0018
	if (cr6.eq) goto loc_82BB0018;
loc_82BAFFB8:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82baffb8
	if (cr6.eq) goto loc_82BAFFB8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// bgt cr6,0x82baffb8
	if (cr6.gt) goto loc_82BAFFB8;
	// add r10,r26,r30
	ctx.r10.u64 = r26.u64 + r30.u64;
	// cmpw cr6,r11,r10
	cr6.compare<int32_t>(r11.s32, ctx.r10.s32, xer);
	// bge cr6,0x82baffb8
	if (!cr6.lt) goto loc_82BAFFB8;
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82188cf0
	sub_82188CF0(ctx, base);
	// cmplw cr6,r27,r3
	cr6.compare<uint32_t>(r27.u32, ctx.r3.u32, xer);
	// bne cr6,0x82baffb8
	if (!cr6.eq) goto loc_82BAFFB8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c2c
	return;
loc_82BB000C:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c2c
	return;
loc_82BB0018:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BAFF28) {
	__imp__sub_82BAFF28(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0028) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(44) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// rlwinm r26,r31,5,0,26
	r26.u64 = rotl64(r31.u32 | (r31.u64 << 32), 5) & 0xFFFFFFE0;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// add r11,r26,r11
	r11.u64 = r26.u64 + r11.u64;
	// li r28,0
	r28.s64 = 0;
	// lwz r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(16) );
	// lwz r31,28(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// clrlwi r8,r9,5
	ctx.r8.u64 = ctx.r9.u32 & 0x7FFFFFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bb0124
	if (cr6.eq) goto loc_82BB0124;
loc_82BB0078:
	// lhz r11,4(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// lhz r10,6(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// rlwinm r11,r11,31,1,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r9,r10,0,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// rlwinm r11,r8,3,0,28
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r11,r31
	r29.u64 = r11.u64 + r31.u64;
	// beq cr6,0x82bb00f4
	if (cr6.eq) goto loc_82BB00F4;
	// li r30,0
	r30.s64 = 0;
loc_82BB00A0:
	// rlwinm r11,r30,1,0,30
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	r11.u64 = PPC_LOAD_U16(r11.u32 + r29.u32);
	// rlwinm r10,r11,0,17,17
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x4000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb00d8
	if (cr6.eq) goto loc_82BB00D8;
	// rlwinm r10,r11,0,0,16
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFF8000;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb00d8
	if (cr6.eq) goto loc_82BB00D8;
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// clrlwi r4,r11,18
	ctx.r4.u64 = r11.u32 & 0x3FFF;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(12) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BB00D8:
	// lhz r10,6(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// addi r9,r30,1
	ctx.r9.s64 = r30.s64 + 1;
	// clrlwi r11,r9,16
	r11.u64 = ctx.r9.u32 & 0xFFFF;
	// rlwinm r8,r10,31,1,31
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x82bb00a0
	if (cr6.lt) goto loc_82BB00A0;
loc_82BB00F4:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// addi r28,r28,1
	r28.s64 = r28.s64 + 1;
	// lhz r10,6(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// add r9,r26,r11
	ctx.r9.u64 = r26.u64 + r11.u64;
	// rlwinm r11,r10,31,1,31
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// lwz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(16) );
	// rlwinm r11,r8,1,0,29
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFC;
	// clrlwi r6,r7,5
	ctx.r6.u64 = ctx.r7.u32 & 0x7FFFFFF;
	// add r31,r11,r29
	r31.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r28,r6
	cr6.compare<uint32_t>(r28.u32, ctx.r6.u32, xer);
	// blt cr6,0x82bb0078
	if (cr6.lt) goto loc_82BB0078;
loc_82BB0124:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BB0028) {
	__imp__sub_82BB0028(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0130) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// add r11,r4,r5
	r11.u64 = ctx.r4.u64 + ctx.r5.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// rlwinm r11,r11,5,0,26
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB0130) {
	__imp__sub_82BB0130(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0148) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r9
	ctx.r3.u64 = ctx.r9.u64;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// lwz r7,44(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(44) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,4(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// lwz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(0) );
	// lwz r4,48(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(48) );
	// mtctr r4
	ctr.u64 = ctx.r4.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r28,0(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lhz r10,6(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 6);
	// li r5,8192
	ctx.r5.s64 = 8192;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r9,80(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(80) );
	// clrlwi r6,r10,31
	ctx.r6.u64 = ctx.r10.u32 & 0x1;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r8,4(r29)
	ctx.r8.u64 = PPC_LOAD_U16(r29.u32 + 4);
	// addi r27,r29,8
	r27.s64 = r29.s64 + 8;
	// rlwinm r7,r8,0,0,30
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82bb0230
	if (cr6.eq) goto loc_82BB0230;
	// li r30,0
	r30.s64 = 0;
loc_82BB01EC:
	// rlwinm r11,r30,3,0,28
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r11,r11,r27
	r11.u64 = r11.u64 + r27.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,72(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(72) );
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// rlwinm r5,r8,27,5,31
	ctx.r5.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x7FFFFFF;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r6,4(r29)
	ctx.r6.u64 = PPC_LOAD_U16(r29.u32 + 4);
	// addi r7,r30,1
	ctx.r7.s64 = r30.s64 + 1;
	// rlwinm r5,r6,31,1,31
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r30,r7,16
	r30.u64 = ctx.r7.u32 & 0xFFFF;
	// cmplw cr6,r30,r5
	cr6.compare<uint32_t>(r30.u32, ctx.r5.u32, xer);
	// blt cr6,0x82bb01ec
	if (cr6.lt) goto loc_82BB01EC;
loc_82BB0230:
	// lhz r11,4(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 4);
	// lhz r10,6(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 6);
	// rlwinm r11,r11,2,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFF8;
	// rlwinm r9,r10,0,0,30
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFE;
	// add r31,r11,r27
	r31.u64 = r11.u64 + r27.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb02b0
	if (cr6.eq) goto loc_82BB02B0;
	// li r30,0
	r30.s64 = 0;
loc_82BB0250:
	// rlwinm r11,r30,1,0,30
	r11.u64 = rotl64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lhzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + r31.u32);
	// rlwinm r9,r10,0,17,17
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// rlwinm r6,r9,17,15,31
	ctx.r6.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 17) & 0x1FFFF;
	// clrlwi r5,r9,18
	ctx.r5.u64 = ctx.r9.u32 & 0x3FFF;
	// beq cr6,0x82bb0280
	if (cr6.eq) goto loc_82BB0280;
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// b 0x82bb0288
	goto loc_82BB0288;
loc_82BB0280:
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
loc_82BB0288:
	// lwz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lhz r10,6(r29)
	ctx.r10.u64 = PPC_LOAD_U16(r29.u32 + 6);
	// addi r9,r30,1
	ctx.r9.s64 = r30.s64 + 1;
	// rlwinm r8,r10,31,1,31
	ctx.r8.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// clrlwi r11,r9,16
	r11.u64 = ctx.r9.u32 & 0xFFFF;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x82bb0250
	if (cr6.lt) goto loc_82BB0250;
loc_82BB02B0:
	// lhz r11,6(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 6);
	// rlwinm r11,r11,31,1,31
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r11,1
	ctx.r10.s64 = r11.s64 + 1;
	// rlwinm r11,r10,1,0,29
	r11.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFC;
	// add r3,r11,r31
	ctx.r3.u64 = r11.u64 + r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BB0148) {
	__imp__sub_82BB0148(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB02D0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// lwz r28,32(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + int32_t(32) );
	// beq cr6,0x82bb03d0
	if (cr6.eq) goto loc_82BB03D0;
	// lwz r9,12(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + int32_t(12) );
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x82bb0388
	if (!cr6.gt) goto loc_82BB0388;
loc_82BB0308:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r8,r31
	cr6.compare<uint32_t>(ctx.r8.u32, r31.u32, xer);
	// beq cr6,0x82bb0328
	if (cr6.eq) goto loc_82BB0328;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82bb0308
	if (cr6.lt) goto loc_82BB0308;
	// b 0x82bb0388
	goto loc_82BB0388;
loc_82BB0328:
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// lhz r7,6(r11)
	ctx.r7.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// extsh r6,r9
	ctx.r6.s64 = ctx.r9.s16;
	// lwz r8,16(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// extsh r9,r7
	ctx.r9.s64 = ctx.r7.s16;
	// rlwinm r11,r6,3,0,28
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// ble cr6,0x82bb0388
	if (!cr6.gt) goto loc_82BB0388;
loc_82BB0350:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplw cr6,r8,r27
	cr6.compare<uint32_t>(ctx.r8.u32, r27.u32, xer);
	// beq cr6,0x82bb0370
	if (cr6.eq) goto loc_82BB0370;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// blt cr6,0x82bb0350
	if (cr6.lt) goto loc_82BB0350;
	// b 0x82bb0388
	goto loc_82BB0388;
loc_82BB0370:
	// lhz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U16(r11.u32 + 6);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lhz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 4);
	// extsh r5,r10
	ctx.r5.s64 = ctx.r10.s16;
	// extsh r4,r9
	ctx.r4.s64 = ctx.r9.s16;
	// bl 0x82bb0568
	sub_82BB0568(ctx, base);
loc_82BB0388:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82bb0450
	if (cr6.eq) goto loc_82BB0450;
loc_82BB0390:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(8) );
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x82bb03bc
	if (!cr6.eq) goto loc_82BB03BC;
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(12) );
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x82bb03bc
	if (!cr6.eq) goto loc_82BB03BC;
	// lhz r11,16(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 16);
	// addi r4,r28,20
	ctx.r4.s64 = r28.s64 + 20;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// bl 0x82bb0628
	sub_82BB0628(ctx, base);
loc_82BB03BC:
	// lwz r28,0(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82bb0390
	if (!cr6.eq) goto loc_82BB0390;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BB03D0:
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(20) );
	// li r30,0
	r30.s64 = 0;
	// lwz r31,16(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82bb041c
	if (!cr6.gt) goto loc_82BB041C;
loc_82BB03E4:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x82bb0408
	if (!cr6.eq) goto loc_82BB0408;
	// lhz r11,6(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 6);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lhz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 4);
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// extsh r4,r10
	ctx.r4.s64 = ctx.r10.s16;
	// bl 0x82bb0568
	sub_82BB0568(ctx, base);
loc_82BB0408:
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(20) );
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// cmpw cr6,r30,r11
	cr6.compare<int32_t>(r30.s32, r11.s32, xer);
	// blt cr6,0x82bb03e4
	if (cr6.lt) goto loc_82BB03E4;
loc_82BB041C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82bb0450
	if (cr6.eq) goto loc_82BB0450;
loc_82BB0424:
	// lwz r11,12(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(12) );
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bne cr6,0x82bb0444
	if (!cr6.eq) goto loc_82BB0444;
	// lhz r11,16(r28)
	r11.u64 = PPC_LOAD_U16(r28.u32 + 16);
	// addi r4,r28,20
	ctx.r4.s64 = r28.s64 + 20;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// extsh r5,r11
	ctx.r5.s64 = r11.s16;
	// bl 0x82bb0628
	sub_82BB0628(ctx, base);
loc_82BB0444:
	// lwz r28,0(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x82bb0424
	if (!cr6.eq) goto loc_82BB0424;
loc_82BB0450:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BB02D0) {
	__imp__sub_82BB02D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0458) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x82bb0528
	if (cr6.eq) goto loc_82BB0528;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// li r5,4
	ctx.r5.s64 = 4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb0528
	if (cr6.eq) goto loc_82BB0528;
	// lwz r9,32(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb0560
	if (cr6.eq) goto loc_82BB0560;
loc_82BB04C0:
	// lwz r11,40(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(40) );
	// addi r8,r9,20
	ctx.r8.s64 = ctx.r9.s64 + 20;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bne cr6,0x82bb0514
	if (!cr6.eq) goto loc_82BB0514;
	// lhz r11,18(r9)
	r11.u64 = PPC_LOAD_U16(ctx.r9.u32 + 18);
	// extsh r11,r11
	r11.s64 = r11.s16;
	// cmpw cr6,r31,r11
	cr6.compare<int32_t>(r31.s32, r11.s32, xer);
	// blt cr6,0x82bb0514
	if (cr6.lt) goto loc_82BB0514;
	// lhz r10,16(r9)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r9.u32 + 16);
	// extsh r10,r10
	ctx.r10.s64 = ctx.r10.s16;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + r11.u64;
	// cmpw cr6,r31,r7
	cr6.compare<int32_t>(r31.s32, ctx.r7.s32, xer);
	// bge cr6,0x82bb0514
	if (!cr6.lt) goto loc_82BB0514;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r8
	r11.u64 = r11.u64 + ctx.r8.u64;
	// lbz r10,18(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 18);
	// rlwimi r10,r29,7,17,24
	ctx.r10.u64 = (rotl32(r29.u32, 7) & 0x7F80) | (ctx.r10.u64 & 0xFFFFFFFFFFFF807F);
	// stb r10,18(r11)
	PPC_STORE_U8(r11.u32 + 18, ctx.r10.u8);
loc_82BB0514:
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(0) );
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bb04c0
	if (!cr6.eq) goto loc_82BB04C0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82BB0528:
	// rlwinm r11,r31,1,0,30
	r11.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(24) );
	// clrlwi r9,r29,24
	ctx.r9.u64 = r29.u32 & 0xFF;
	// add r8,r31,r11
	ctx.r8.u64 = r31.u64 + r11.u64;
	// cntlzw r7,r9
	ctx.r7.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r11,r8,3,0,28
	r11.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r6,r7,2,24,24
	ctx.r6.u64 = rotl64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0x80;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// xori r5,r6,128
	ctx.r5.u64 = ctx.r6.u64 ^ 128;
	// addi r10,r11,18
	ctx.r10.s64 = r11.s64 + 18;
	// lbz r4,18(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 18);
	// clrlwi r3,r4,25
	ctx.r3.u64 = ctx.r4.u32 & 0x7F;
	// or r10,r5,r3
	ctx.r10.u64 = ctx.r5.u64 | ctx.r3.u64;
	// stb r10,18(r11)
	PPC_STORE_U8(r11.u32 + 18, ctx.r10.u8);
loc_82BB0560:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BB0458) {
	__imp__sub_82BB0458(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0568) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r11,r31,1,0,30
	r11.u64 = rotl64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r10,24(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(24) );
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// add r9,r31,r11
	ctx.r9.u64 = r31.u64 + r11.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// rlwinm r11,r9,3,0,28
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r31,r11,r10
	r31.u64 = r11.u64 + ctx.r10.u64;
	// ble cr6,0x82bb0620
	if (!cr6.gt) goto loc_82BB0620;
loc_82BB05B4:
	// lbz r11,18(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 18);
	// rlwinm r10,r11,0,0,24
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb0614
	if (cr6.eq) goto loc_82BB0614;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb0614
	if (cr6.eq) goto loc_82BB0614;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r7,r31,12
	ctx.r7.s64 = r31.s64 + 12;
	// addi r6,r31,8
	ctx.r6.s64 = r31.s64 + 8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BB0614:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r31,r31,24
	r31.s64 = r31.s64 + 24;
	// bne 0x82bb05b4
	if (!cr0.eq) goto loc_82BB05B4;
loc_82BB0620:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BB0568) {
	__imp__sub_82BB0568(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0628) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r3,4(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// ble cr6,0x82bb06cc
	if (!cr6.gt) goto loc_82BB06CC;
loc_82BB0660:
	// lbz r11,18(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 18);
	// rlwinm r10,r11,0,0,24
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0xFFFFFF80;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb06c0
	if (cr6.eq) goto loc_82BB06C0;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(20) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb06c0
	if (cr6.eq) goto loc_82BB06C0;
	// lwz r3,4(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + int32_t(4) );
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r7,r31,12
	ctx.r7.s64 = r31.s64 + 12;
	// addi r6,r31,8
	ctx.r6.s64 = r31.s64 + 8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BB06C0:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r31,r31,24
	r31.s64 = r31.s64 + 24;
	// bne 0x82bb0660
	if (!cr0.eq) goto loc_82BB0660;
loc_82BB06CC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BB0628) {
	__imp__sub_82BB0628(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB06D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bb0770
	if (cr6.eq) goto loc_82BB0770;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// li r5,4
	ctx.r5.s64 = 4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r8,76(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(76) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82bb0770
	if (cr6.eq) goto loc_82BB0770;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(24) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb07a0
	if (cr6.eq) goto loc_82BB07A0;
loc_82BB0740:
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(12) );
	// addi r11,r10,12
	r11.s64 = ctx.r10.s64 + 12;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// beq cr6,0x82bb0764
	if (cr6.eq) goto loc_82BB0764;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bb0740
	if (!cr6.eq) goto loc_82BB0740;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_82BB0764:
	// rlwinm r10,r29,4,0,27
	ctx.r10.u64 = rotl64(r29.u32 | (r29.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// b 0x82bb077c
	goto loc_82BB077C;
loc_82BB0770:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// rlwinm r11,r29,4,0,27
	r11.u64 = rotl64(r29.u32 | (r29.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
loc_82BB077C:
	// clrlwi r9,r28,24
	ctx.r9.u64 = r28.u32 & 0xFF;
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// addi r10,r11,12
	ctx.r10.s64 = r11.s64 + 12;
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// clrlwi r4,r6,1
	ctx.r4.u64 = ctx.r6.u32 & 0x7FFFFFFF;
	// rlwinm r7,r8,26,0,0
	ctx.r7.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 26) & 0x80000000;
	// xoris r5,r7,32768
	ctx.r5.u64 = ctx.r7.u64 ^ 2147483648;
	// or r3,r5,r4
	ctx.r3.u64 = ctx.r5.u64 | ctx.r4.u64;
	// stw r3,12(r11)
	PPC_STORE_U32(r11.u32 + 12, ctx.r3.u32);
loc_82BB07A0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BB06D8) {
	__imp__sub_82BB06D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB07A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// fmr f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f1.f64;
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// addi r11,r11,-19232
	r11.s64 = r11.s64 + -19232;
	// li r10,0
	ctx.r10.s64 = 0;
	// lfs f0,1320(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 1320);
	f0.f64 = double(temp.f32);
	// fmuls f10,f2,f0
	ctx.f10.f64 = double(float(ctx.f2.f64 * f0.f64));
	// lfs f13,-8632(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8632);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f3,f0
	ctx.f9.f64 = double(float(ctx.f3.f64 * f0.f64));
	// lfs f1,-8236(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8236);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f8,f12,f0
	ctx.f8.f64 = double(float(ctx.f12.f64 * f0.f64));
	// stfs f1,80(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fsubs f11,f12,f5
	ctx.f11.f64 = static_cast<float>(ctx.f12.f64 - ctx.f5.f64);
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// stw r10,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r10.u32);
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fsubs f7,f10,f12
	ctx.f7.f64 = static_cast<float>(ctx.f10.f64 - ctx.f12.f64);
	// fnmsubs f6,f2,f13,f8
	ctx.f6.f64 = -double(std::fma(float(ctx.f2.f64), float(ctx.f13.f64), -float(ctx.f8.f64)));
	// fsubs f13,f10,f8
	ctx.f13.f64 = static_cast<float>(ctx.f10.f64 - ctx.f8.f64);
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fsubs f5,f7,f9
	ctx.f5.f64 = static_cast<float>(ctx.f7.f64 - ctx.f9.f64);
	// fadds f0,f6,f9
	f0.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// stfs f0,104(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fadds f12,f5,f4
	ctx.f12.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// stfs f12,108(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fcmpu cr6,f12,f1
	cr6.compare(ctx.f12.f64, ctx.f1.f64);
	// bne cr6,0x82bb08e0
	if (!cr6.eq) goto loc_82BB08E0;
	// fcmpu cr6,f0,f1
	cr6.compare(f0.f64, ctx.f1.f64);
	// bne cr6,0x82bb0848
	if (!cr6.eq) goto loc_82BB0848;
	// fcmpu cr6,f13,f1
	cr6.compare(ctx.f13.f64, ctx.f1.f64);
	// beq cr6,0x82bb0900
	if (cr6.eq) goto loc_82BB0900;
	// fdivs f0,f11,f13
	f0.f64 = double(float(ctx.f11.f64 / ctx.f13.f64));
	// fneg f1,f0
	ctx.f1.u64 = f0.u64 ^ 0x8000000000000000;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
loc_82BB0848:
	// lfs f12,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fdivs f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 / f0.f64));
	// fmuls f10,f0,f12
	ctx.f10.f64 = double(float(f0.f64 * ctx.f12.f64));
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// lfs f12,832(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 832);
	ctx.f12.f64 = double(temp.f32);
	// fdivs f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 / ctx.f10.f64));
	// fmsubs f0,f13,f13,f11
	f0.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f13.f64), -float(ctx.f11.f64)));
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// ble cr6,0x82bb0898
	if (!cr6.gt) goto loc_82BB0898;
	// lis r9,-32247
	ctx.r9.s64 = -2113339392;
	// lfs f12,2384(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2384);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// bge cr6,0x82bb0898
	if (!cr6.lt) goto loc_82BB0898;
	// li r11,1
	r11.s64 = 1;
	// fneg f0,f13
	f0.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// b 0x82bb08f4
	goto loc_82BB08F4;
loc_82BB0898:
	// fcmpu cr6,f0,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f1.f64);
	// bge cr6,0x82bb08b0
	if (!cr6.lt) goto loc_82BB08B0;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// b 0x82bb08f4
	goto loc_82BB08F4;
loc_82BB08B0:
	// fcmpu cr6,f0,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f1.f64);
	// ble cr6,0x82bb08d4
	if (!cr6.gt) goto loc_82BB08D4;
	// fsqrts f0,f0
	f0.f64 = double(simd::sqrt_f32(float(f0.f64)));
	// li r11,2
	r11.s64 = 2;
	// fneg f12,f0
	ctx.f12.u64 = f0.u64 ^ 0x8000000000000000;
	// fsubs f11,f0,f13
	ctx.f11.f64 = static_cast<float>(f0.f64 - ctx.f13.f64);
	// stfs f11,80(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fsubs f10,f12,f13
	ctx.f10.f64 = static_cast<float>(ctx.f12.f64 - ctx.f13.f64);
	// stfs f10,84(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
loc_82BB08D4:
	// srawi r11,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	r11.s64 = r11.s32 >> 1;
	// addze r9,r11
	temp.s64 = r11.s64 + xer.ca;
	xer.ca = temp.u32 < r11.u32;
	ctx.r9.s64 = temp.s64;
	// b 0x82bb08f4
	goto loc_82BB08F4;
loc_82BB08E0:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82bb0a38
	sub_82BB0A38(ctx, base);
	// li r11,3
	r11.s64 = 3;
	// divw r9,r3,r11
	ctx.r9.s32 = ctx.r3.s32 / r11.s32;
loc_82BB08F4:
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f1,r8,r10
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	ctx.f1.f64 = double(temp.f32);
loc_82BB0900:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB07A8) {
	__imp__sub_82BB07A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0910) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = rotl64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// rlwinm r10,r11,0,1,1
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 0) & 0x40000000;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82bb0a2c
	if (cr6.eq) goto loc_82BB0A2C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,16(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// rlwinm r11,r11,3,0,28
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r29,r11,r9
	r29.u64 = r11.u64 + ctx.r9.u64;
	// lwz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(32) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r7,12(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(12) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r6,12(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r5,4
	cr6.compare<int32_t>(ctx.r5.s32, 4, xer);
	// ble cr6,0x82bb0a24
	if (!cr6.gt) goto loc_82BB0A24;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lwz r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = rotl64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r8,-27468
	ctx.r7.s64 = ctx.r8.s64 + -27468;
	// add r6,r11,r9
	ctx.r6.u64 = r11.u64 + ctx.r9.u64;
	// lfs f12,-27468(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -27468);
	ctx.f12.f64 = double(temp.f32);
	// rlwinm r11,r6,3,0,28
	r11.u64 = rotl64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// lfs f10,9708(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 9708);
	ctx.f10.f64 = double(temp.f32);
	// addi r10,r11,24
	ctx.r10.s64 = r11.s64 + 24;
	// lfs f13,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,28(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f0,f11,f13
	f0.f64 = static_cast<float>(ctx.f11.f64 - ctx.f13.f64);
	// fmuls f0,f0,f10
	f0.f64 = double(float(f0.f64 * ctx.f10.f64));
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// beq cr6,0x82bb09e8
	if (cr6.eq) goto loc_82BB09E8;
	// lfs f9,12(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f9,f13
	ctx.f8.f64 = static_cast<float>(ctx.f9.f64 - ctx.f13.f64);
	// fdivs f13,f8,f0
	ctx.f13.f64 = double(float(ctx.f8.f64 / f0.f64));
	// b 0x82bb09ec
	goto loc_82BB09EC;
loc_82BB09E8:
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
loc_82BB09EC:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f12.f64);
	// beq cr6,0x82bb0a00
	if (cr6.eq) goto loc_82BB0A00;
	// lfs f12,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f11,f12
	ctx.f11.f64 = static_cast<float>(ctx.f11.f64 - ctx.f12.f64);
	// fdivs f12,f11,f0
	ctx.f12.f64 = double(float(ctx.f11.f64 / f0.f64));
loc_82BB0A00:
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// lfs f11,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f9,f11,f0
	ctx.f9.f64 = static_cast<float>(ctx.f11.f64 - f0.f64);
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmadds f7,f8,f13,f0
	ctx.f7.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f13.f64), float(f0.f64)));
	// stfs f7,12(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 12, temp.u32);
	// lfs f6,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f5,f8,f12,f6
	ctx.f5.f64 = -double(std::fma(float(ctx.f8.f64), float(ctx.f12.f64), -float(ctx.f6.f64)));
	// stfs f5,20(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
loc_82BB0A24:
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// stfs f0,4(r29)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r29.u32 + 4, temp.u32);
loc_82BB0A2C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BB0910) {
	__imp__sub_82BB0910(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0A38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r12,r1,-24
	r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82ca74fc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lfs f12,12(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r31,r11,-8700
	r31.s64 = r11.s64 + -8700;
	// lfs f10,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// lfs f13,836(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 836);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-18756(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -18756);
	f0.f64 = double(temp.f32);
	// fdivs f8,f0,f12
	ctx.f8.f64 = double(float(f0.f64 / ctx.f12.f64));
	// lfs f26,-9060(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -9060);
	f26.f64 = double(temp.f32);
	// lfs f0,-19152(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -19152);
	f0.f64 = double(temp.f32);
	// lfs f12,832(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 832);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f25,f11,f8
	f25.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// fmuls f7,f10,f8
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// fmuls f6,f25,f25
	ctx.f6.f64 = double(float(f25.f64 * f25.f64));
	// fmuls f5,f7,f25
	ctx.f5.f64 = double(float(ctx.f7.f64 * f25.f64));
	// fnmsubs f4,f6,f26,f7
	ctx.f4.f64 = -double(std::fma(float(ctx.f6.f64), float(f26.f64), -float(ctx.f7.f64)));
	// fmuls f3,f6,f25
	ctx.f3.f64 = double(float(ctx.f6.f64 * f25.f64));
	// fmuls f2,f5,f26
	ctx.f2.f64 = double(float(ctx.f5.f64 * f26.f64));
	// fmuls f31,f4,f26
	f31.f64 = double(float(ctx.f4.f64 * f26.f64));
	// fmsubs f1,f3,f13,f2
	ctx.f1.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f13.f64), -float(ctx.f2.f64)));
	// fmuls f13,f31,f31
	ctx.f13.f64 = double(float(f31.f64 * f31.f64));
	// fmadds f11,f8,f9,f1
	ctx.f11.f64 = double(std::fma(float(ctx.f8.f64), float(ctx.f9.f64), float(ctx.f1.f64)));
	// fmuls f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 * f31.f64));
	// fmuls f30,f11,f0
	f30.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fmadds f0,f30,f30,f13
	f0.f64 = double(std::fma(float(f30.f64), float(f30.f64), float(ctx.f13.f64)));
	// fcmpu cr6,f0,f12
	cr6.compare(f0.f64, ctx.f12.f64);
	// ble cr6,0x82bb0b5c
	if (!cr6.gt) goto loc_82BB0B5C;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// lfs f11,2384(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2384);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f0,f11
	cr6.compare(f0.f64, ctx.f11.f64);
	// bge cr6,0x82bb0b5c
	if (!cr6.lt) goto loc_82BB0B5C;
	// fcmpu cr6,f30,f12
	cr6.compare(f30.f64, ctx.f12.f64);
	// ble cr6,0x82bb0afc
	if (!cr6.gt) goto loc_82BB0AFC;
	// fcmpu cr6,f30,f11
	cr6.compare(f30.f64, ctx.f11.f64);
	// bge cr6,0x82bb0afc
	if (!cr6.lt) goto loc_82BB0AFC;
	// lfs f0,-18768(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -18768);
	f0.f64 = double(temp.f32);
	// stfs f0,0(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// b 0x82bb0c7c
	goto loc_82BB0C7C;
loc_82BB0AFC:
	// fneg f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = f30.u64 ^ 0x8000000000000000;
	// lfs f0,-18768(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -18768);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// ble cr6,0x82bb0b20
	if (!cr6.gt) goto loc_82BB0B20;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f2,824(r11)
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + 824);
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// b 0x82bb0b40
	goto loc_82BB0B40;
loc_82BB0B20:
	// fcmpu cr6,f1,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f0.f64);
	// bge cr6,0x82bb0b40
	if (!cr6.lt) goto loc_82BB0B40;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// fneg f1,f1
	ctx.f1.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// lfd f2,824(r11)
	ctx.f2.u64 = PPC_LOAD_U64(r11.u32 + 824);
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_82BB0B40:
	// lfs f13,-10532(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + -10532);
	ctx.f13.f64 = double(temp.f32);
	// fneg f12,f0
	ctx.f12.u64 = f0.u64 ^ 0x8000000000000000;
	// fmuls f11,f0,f13
	ctx.f11.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f11,0(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// stfs f12,4(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r30.u32 + 4, temp.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// b 0x82bb0c80
	goto loc_82BB0C80;
loc_82BB0B5C:
	// lfs f29,-18768(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + -18768);
	f29.f64 = double(temp.f32);
	// fcmpu cr6,f0,f29
	cr6.compare(f0.f64, f29.f64);
	// bge cr6,0x82bb0bec
	if (!cr6.lt) goto loc_82BB0BEC;
	// fneg f13,f13
	ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// lfs f0,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	f0.f64 = double(temp.f32);
	// fsqrts f12,f13
	ctx.f12.f64 = double(simd::sqrt_f32(float(ctx.f13.f64)));
	// fdivs f11,f0,f12
	ctx.f11.f64 = double(float(f0.f64 / ctx.f12.f64));
	// fmuls f1,f11,f30
	ctx.f1.f64 = double(float(ctx.f11.f64 * f30.f64));
	// bl 0x82260900
	sub_82260900(ctx, base);
	// frsp f10,f1
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// lfs f0,-10532(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + -10532);
	f0.f64 = double(temp.f32);
	// fneg f9,f31
	ctx.f9.u64 = f31.u64 ^ 0x8000000000000000;
	// fmuls f30,f10,f26
	f30.f64 = double(float(ctx.f10.f64 * f26.f64));
	// fsqrts f8,f9
	ctx.f8.f64 = double(simd::sqrt_f32(float(ctx.f9.f64)));
	// fmuls f29,f8,f0
	f29.f64 = double(float(ctx.f8.f64 * f0.f64));
	// fmr f1,f30
	ctx.f1.f64 = f30.f64;
	// bl 0x82239e88
	sub_82239E88(ctx, base);
	// frsp f7,f1
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f1.f64));
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f31,820(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 820);
	f31.f64 = double(temp.f32);
	// fadds f1,f30,f31
	ctx.f1.f64 = double(float(f30.f64 + f31.f64));
	// fmuls f6,f7,f29
	ctx.f6.f64 = double(float(ctx.f7.f64 * f29.f64));
	// stfs f6,0(r30)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
	// bl 0x82239e88
	sub_82239E88(ctx, base);
	// frsp f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = double(float(ctx.f1.f64));
	// fsubs f1,f30,f31
	ctx.f1.f64 = static_cast<float>(f30.f64 - f31.f64);
	// fmuls f4,f5,f29
	ctx.f4.f64 = double(float(ctx.f5.f64 * f29.f64));
	// fneg f3,f4
	ctx.f3.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// stfs f3,4(r30)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(r30.u32 + 4, temp.u32);
	// bl 0x82239e88
	sub_82239E88(ctx, base);
	// frsp f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = double(float(ctx.f1.f64));
	// li r3,3
	ctx.r3.s64 = 3;
	// fmuls f1,f2,f29
	ctx.f1.f64 = double(float(ctx.f2.f64 * f29.f64));
	// fneg f0,f1
	f0.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// stfs f0,8(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 8, temp.u32);
	// b 0x82bb0c80
	goto loc_82BB0C80;
loc_82BB0BEC:
	// fsqrts f28,f0
	ctx.fpscr.disableFlushMode();
	f28.f64 = double(simd::sqrt_f32(float(f0.f64)));
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfd f31,824(r11)
	f31.u64 = PPC_LOAD_U64(r11.u32 + 824);
	// fsubs f1,f28,f30
	ctx.f1.f64 = static_cast<float>(f28.f64 - f30.f64);
	// fcmpu cr6,f1,f29
	cr6.compare(ctx.f1.f64, f29.f64);
	// ble cr6,0x82bb0c14
	if (!cr6.gt) goto loc_82BB0C14;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f27,f1
	ctx.fpscr.disableFlushMode();
	f27.f64 = double(float(ctx.f1.f64));
	// b 0x82bb0c38
	goto loc_82BB0C38;
loc_82BB0C14:
	// fcmpu cr6,f1,f29
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f29.f64);
	// bge cr6,0x82bb0c34
	if (!cr6.lt) goto loc_82BB0C34;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fneg f1,f1
	ctx.f1.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// fneg f27,f0
	f27.u64 = f0.u64 ^ 0x8000000000000000;
	// b 0x82bb0c38
	goto loc_82BB0C38;
loc_82BB0C34:
	// fmr f27,f29
	ctx.fpscr.disableFlushMode();
	f27.f64 = f29.f64;
loc_82BB0C38:
	// fadds f1,f28,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(f28.f64 + f30.f64));
	// fcmpu cr6,f1,f29
	cr6.compare(ctx.f1.f64, f29.f64);
	// ble cr6,0x82bb0c54
	if (!cr6.gt) goto loc_82BB0C54;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f29,f1
	ctx.fpscr.disableFlushMode();
	f29.f64 = double(float(ctx.f1.f64));
	// b 0x82bb0c70
	goto loc_82BB0C70;
loc_82BB0C54:
	// fcmpu cr6,f1,f29
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, f29.f64);
	// bge cr6,0x82bb0c70
	if (!cr6.lt) goto loc_82BB0C70;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fneg f1,f1
	ctx.f1.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// fneg f29,f0
	f29.u64 = f0.u64 ^ 0x8000000000000000;
loc_82BB0C70:
	// fneg f0,f29
	ctx.fpscr.disableFlushMode();
	f0.u64 = f29.u64 ^ 0x8000000000000000;
	// fadds f13,f0,f27
	ctx.f13.f64 = double(float(f0.f64 + f27.f64));
	// stfs f13,0(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r30.u32 + 0, temp.u32);
loc_82BB0C7C:
	// li r3,1
	ctx.r3.s64 = 1;
loc_82BB0C80:
	// fmuls f0,f25,f26
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f25.f64 * f26.f64));
	// li r9,0
	ctx.r9.s64 = 0;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// blt cr6,0x82bb0ce0
	if (cr6.lt) goto loc_82BB0CE0;
	// addi r10,r3,-4
	ctx.r10.s64 = ctx.r3.s64 + -4;
	// addi r11,r30,8
	r11.s64 = r30.s64 + 8;
	// rlwinm r10,r10,30,2,31
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x3FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_82BB0CA4:
	// lfs f13,-8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -8);
	ctx.f13.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lfs f12,-4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -4);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f13,f0
	ctx.f11.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f9,f12,f0
	ctx.f9.f64 = static_cast<float>(ctx.f12.f64 - f0.f64);
	// lfs f8,4(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 4);
	ctx.f8.f64 = double(temp.f32);
	// fsubs f7,f10,f0
	ctx.f7.f64 = static_cast<float>(ctx.f10.f64 - f0.f64);
	// fsubs f6,f8,f0
	ctx.f6.f64 = static_cast<float>(ctx.f8.f64 - f0.f64);
	// stfs f11,-8(r11)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r11.u32 + -8, temp.u32);
	// stfs f9,-4(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + -4, temp.u32);
	// stfs f7,0(r11)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// stfs f6,4(r11)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(r11.u32 + 4, temp.u32);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// bne 0x82bb0ca4
	if (!cr0.eq) goto loc_82BB0CA4;
loc_82BB0CE0:
	// cmpw cr6,r9,r3
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r3.s32, xer);
	// bge cr6,0x82bb0d0c
	if (!cr6.lt) goto loc_82BB0D0C;
	// rlwinm r11,r9,2,0,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r9,r3
	ctx.r10.s64 = ctx.r3.s64 - ctx.r9.s64;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
loc_82BB0CF4:
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// fsubs f12,f13,f0
	ctx.f12.f64 = static_cast<float>(ctx.f13.f64 - f0.f64);
	// stfs f12,0(r11)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x82bb0cf4
	if (!cr0.eq) goto loc_82BB0CF4;
loc_82BB0D0C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// addi r12,r1,-24
	r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82ca7548
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB0A38) {
	__imp__sub_82BB0A38(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0D30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-10688
	ctx.r9.s64 = r11.s64 + -10688;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82bb0d64
	if (cr6.eq) goto loc_82BB0D64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BB0D64:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB0D30) {
	__imp__sub_82BB0D30(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB0D78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stfd f31,-80(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -80, f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r30,0
	r30.s64 = 0;
	// addi r10,r11,7796
	ctx.r10.s64 = r11.s64 + 7796;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r27,r31,24
	r27.s64 = r31.s64 + 24;
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// sth r30,12(r31)
	PPC_STORE_U16(r31.u32 + 12, r30.u16);
	// li r4,0
	ctx.r4.s64 = 0;
	// sth r30,14(r31)
	PPC_STORE_U16(r31.u32 + 14, r30.u16);
	// lfs f31,-27468(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27468);
	f31.f64 = double(temp.f32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// addi r6,r8,-11500
	ctx.r6.s64 = ctx.r8.s64 + -11500;
	// sth r30,20(r31)
	PPC_STORE_U16(r31.u32 + 20, r30.u16);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// sth r30,22(r31)
	PPC_STORE_U16(r31.u32 + 22, r30.u16);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// addi r29,r31,8
	r29.s64 = r31.s64 + 8;
	// addi r28,r31,16
	r28.s64 = r31.s64 + 16;
	// bl 0x82bb2cb8
	sub_82BB2CB8(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// addi r26,r31,72
	r26.s64 = r31.s64 + 72;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r6,r7,-11480
	ctx.r6.s64 = ctx.r7.s64 + -11480;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82bb2cb8
	sub_82BB2CB8(ctx, base);
	// lis r6,-31951
	ctx.r6.s64 = -2093940736;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,19040(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(19040) );
	// bl 0x82bc7fb0
	sub_82BC7FB0(ctx, base);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,16(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(16) );
	// li r4,50
	ctx.r4.s64 = 50;
	// bl 0x82bc62b8
	sub_82BC62B8(ctx, base);
	// lis r5,-31951
	ctx.r5.s64 = -2093940736;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r4,19044(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(19044) );
	// mtctr r4
	ctr.u64 = ctx.r4.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r25,2
	r25.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// stw r24,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r24.u32);
	// stw r25,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r25.u32);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5f48
	sub_82BC5F48(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5cd8
	sub_82BC5CD8(ctx, base);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5f48
	sub_82BC5F48(ctx, base);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// stw r27,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r27.u32);
	// stw r25,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r25.u32);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r7.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5f48
	sub_82BC5F48(ctx, base);
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r3,8(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(8) );
	// li r5,5
	ctx.r5.s64 = 5;
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// stw r26,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r26.u32);
	// stw r25,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r25.u32);
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, r11.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5f48
	sub_82BC5F48(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5cd8
	sub_82BC5CD8(ctx, base);
	// li r5,6
	ctx.r5.s64 = 6;
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5f48
	sub_82BC5F48(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82bbb1c8
	sub_82BBB1C8(ctx, base);
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r10,10316
	ctx.r5.s64 = ctx.r10.s64 + 10316;
	// bl 0x821e1408
	sub_821E1408(ctx, base);
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// li r5,10
	ctx.r5.s64 = 10;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r9,10116
	ctx.r4.s64 = ctx.r9.s64 + 10116;
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// lwz r27,4(r31)
	r27.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
	// lwz r3,-16(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-16) );
	// bl 0x821cf230
	sub_821CF230(ctx, base);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(8) );
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// stw r7,-8(r11)
	PPC_STORE_U32(r11.u32 + -8, ctx.r7.u32);
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(4) );
	// stw r6,-4(r11)
	PPC_STORE_U32(r11.u32 + -4, ctx.r6.u32);
	// bl 0x8221eb58
	sub_8221EB58(ctx, base);
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lis r5,15
	ctx.r5.s64 = 983040;
	// ld r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// lis r9,-32068
	ctx.r9.s64 = -2101608448;
	// ld r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// ori r10,r5,16960
	ctx.r10.u64 = ctx.r5.u64 | 16960;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// addi r4,r9,24864
	ctx.r4.s64 = ctx.r9.s64 + 24864;
	// mulld r10,r8,r10
	ctx.r10.s64 = ctx.r8.s64 * ctx.r10.s64;
	// lwz r9,8(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(8) );
	// li r11,3
	r11.s64 = 3;
	// divd r8,r10,r3
	ctx.r8.s64 = ctx.r10.s64 / ctx.r3.s64;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// extsw r11,r8
	r11.s64 = ctx.r8.s32;
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, r11.u64);
	// lfd f0,112(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + int32_t(8) );
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r10,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, ctx.r10.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r9,32(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(32) );
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r11,r11,-16
	r11.s64 = r11.s64 + -16;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// subf r6,r9,r11
	ctx.r6.s64 = r11.s64 - ctx.r9.s64;
	// bl 0x822c05f8
	sub_822C05F8(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r8,12(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// cmplw cr6,r7,r8
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r8.u32, xer);
	// bge cr6,0x82bb1020
	if (!cr6.lt) goto loc_82BB1020;
loc_82BB0FFC:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// stw r30,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r30.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// rotlwi r8,r9,0
	ctx.r8.u64 = rotl32(ctx.r9.u32, 0);
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x82bb0ffc
	if (cr6.lt) goto loc_82BB0FFC;
loc_82BB1020:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// bl 0x82bb2528
	sub_82BB2528(ctx, base);
	// lwz r9,0(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + int32_t(0) );
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r8,76(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(76) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82baa238
	sub_82BAA238(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82ba8ac8
	sub_82BA8AC8(ctx, base);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bb1098
	if (cr6.eq) goto loc_82BB1098;
	// lhz r10,94(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 94);
	// mr r29,r11
	r29.u64 = r11.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// addi r27,r11,1
	r27.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r30,94(r1)
	PPC_STORE_U16(ctx.r1.u32 + 94, r30.u16);
	// sth r30,92(r1)
	PPC_STORE_U16(ctx.r1.u32 + 92, r30.u16);
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r30.u32);
loc_82BB1098:
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bb10d0
	if (cr6.eq) goto loc_82BB10D0;
	// lhz r10,102(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 102);
	// mr r29,r11
	r29.u64 = r11.u64;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// addi r27,r11,1
	r27.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r30,102(r1)
	PPC_STORE_U16(ctx.r1.u32 + 102, r30.u16);
	// sth r30,100(r1)
	PPC_STORE_U16(ctx.r1.u32 + 100, r30.u16);
	// stw r30,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r30.u32);
loc_82BB10D0:
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// sth r30,84(r1)
	PPC_STORE_U16(ctx.r1.u32 + 84, r30.u16);
	// li r5,-1
	ctx.r5.s64 = -1;
	// sth r30,86(r1)
	PPC_STORE_U16(ctx.r1.u32 + 86, r30.u16);
	// addi r4,r11,3224
	ctx.r4.s64 = r11.s64 + 3224;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82ba8ac8
	sub_82BA8AC8(ctx, base);
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bb1120
	if (cr6.eq) goto loc_82BB1120;
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BB1120:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f31,-80(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// b 0x82ca2c28
	return;
}

PPC_WEAK_FUNC(sub_82BB0D78) {
	__imp__sub_82BB0D78(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1130) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82bb1180
	sub_82BB1180(ctx, base);
	// clrlwi r11,r30,31
	r11.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82bb1168
	if (cr6.eq) goto loc_82BB1168;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BB1168:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB1130) {
	__imp__sub_82BB1130(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1180) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r9,r11,7796
	ctx.r9.s64 = r11.s64 + 7796;
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(4) );
	// stw r9,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r9.u32);
	// lwz r8,16(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(16) );
	// lwz r31,100(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(100) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,32(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82bcc6f0
	sub_82BCC6F0(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bca948
	sub_82BCA948(ctx, base);
	// li r27,0
	r27.s64 = 0;
	// stw r27,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r27.u32);
loc_82BB11C8:
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lis r10,-32067
	ctx.r10.s64 = -2101542912;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r10,-32472
	ctx.r4.s64 = ctx.r10.s64 + -32472;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// sth r27,52(r31)
	PPC_STORE_U16(r31.u32 + 52, r27.u16);
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// stw r9,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r9.u32);
	// bl 0x822c0568
	sub_822C0568(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82bb11c8
	if (!cr6.eq) goto loc_82BB11C8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc7d80
	sub_82BC7D80(ctx, base);
	// lwz r10,64(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(64) );
	// lwz r11,56(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(56) );
	// addi r30,r29,24
	r30.s64 = r29.s64 + 24;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// extsw r31,r11
	r31.s64 = r11.s32;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bb1254
	if (cr6.eq) goto loc_82BB1254;
loc_82BB1224:
	// lwz r28,0(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r5,24
	ctx.r5.s64 = 24;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(40) );
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// bne cr6,0x82bb1224
	if (!cr6.eq) goto loc_82BB1224;
loc_82BB1254:
	// lwz r10,112(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(112) );
	// addi r31,r29,72
	r31.s64 = r29.s64 + 72;
	// lwz r11,104(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(104) );
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = rotl64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// extsw r28,r11
	r28.s64 = r11.s32;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r28,r9
	cr6.compare<uint32_t>(r28.u32, ctx.r9.u32, xer);
	// beq cr6,0x82bb12a4
	if (cr6.eq) goto loc_82BB12A4;
loc_82BB1274:
	// lwz r26,0(r28)
	r26.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// li r5,24
	ctx.r5.s64 = 24;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// lwz r11,40(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(40) );
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r28,r28,4
	r28.s64 = r28.s64 + 4;
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1274
	if (!cr6.eq) goto loc_82BB1274;
loc_82BB12A4:
	// stw r27,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r27.u32);
	// lwz r28,36(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + int32_t(36) );
	// lwz r26,32(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// rlwinm r5,r28,2,0,29
	ctx.r5.u64 = rotl64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r27,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r27.u32);
	// stw r27,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r27.u32);
	// stw r27,40(r30)
	PPC_STORE_U32(r30.u32 + 40, r27.u32);
	// lwz r31,36(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(36) );
	// lwz r28,32(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + int32_t(32) );
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rlwinm r5,r31,2,0,29
	ctx.r5.u64 = rotl64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// stw r27,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r27.u32);
	// stw r27,32(r30)
	PPC_STORE_U32(r30.u32 + 32, r27.u32);
	// lwz r31,16(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(16) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bb131c
	if (cr6.eq) goto loc_82BB131C;
	// lhz r11,22(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 22);
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r27,22(r29)
	PPC_STORE_U16(r29.u32 + 22, r27.u16);
	// sth r27,20(r29)
	PPC_STORE_U16(r29.u32 + 20, r27.u16);
	// stw r27,16(r29)
	PPC_STORE_U32(r29.u32 + 16, r27.u32);
loc_82BB131C:
	// lwz r31,8(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + int32_t(8) );
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bb134c
	if (cr6.eq) goto loc_82BB134C;
	// lhz r11,14(r29)
	r11.u64 = PPC_LOAD_U16(r29.u32 + 14);
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// sth r27,14(r29)
	PPC_STORE_U16(r29.u32 + 14, r27.u16);
	// sth r27,12(r29)
	PPC_STORE_U16(r29.u32 + 12, r27.u16);
	// stw r27,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r27.u32);
loc_82BB134C:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r10,r11,-10688
	ctx.r10.s64 = r11.s64 + -10688;
	// stw r10,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r10.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

PPC_WEAK_FUNC(sub_82BB1180) {
	__imp__sub_82BB1180(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1360) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// li r9,2
	ctx.r9.s64 = 2;
	// li r5,3
	ctx.r5.s64 = 3;
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// lwz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, r11.u32);
	// stw r9,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r9.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// b 0x82bc5f48
	sub_82BC5F48(ctx, base);
	return;
}

PPC_WEAK_FUNC(sub_82BB1360) {
	__imp__sub_82BB1360(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1398) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r31,0
	r31.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// sth r31,86(r1)
	PPC_STORE_U16(ctx.r1.u32 + 86, r31.u16);
	// sth r31,84(r1)
	PPC_STORE_U16(ctx.r1.u32 + 84, r31.u16);
	// beq cr6,0x82bb13d8
	if (cr6.eq) goto loc_82BB13D8;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
loc_82BB13D8:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,16
	ctx.r3.s64 = r30.s64 + 16;
	// bl 0x82ba8ac8
	sub_82BA8AC8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bb1404
	if (cr6.eq) goto loc_82BB1404;
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BB1404:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB1398) {
	__imp__sub_82BB1398(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1420) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bb1e58
	sub_82BB1E58(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba8798
	sub_82BA8798(ctx, base);
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba89b0
	sub_82BA89B0(ctx, base);
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r25,4(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,16(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(16) );
	// lwz r10,8(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// lwz r9,12(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(12) );
	// subf r8,r9,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
	// lwz r3,92(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// srawi r26,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	r26.s64 = ctx.r8.s32 >> 3;
	// bl 0x821e1498
	sub_821E1498(ctx, base);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lwz r7,8(r25)
	ctx.r7.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// lis r28,-31950
	r28.s64 = -2093875200;
	// addi r29,r11,3488
	r29.s64 = r11.s64 + 3488;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// stw r6,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// stw r3,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r3.u32);
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,8(r25)
	PPC_STORE_U32(r25.u32 + 8, r11.u32);
	// lwz r5,-10068(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-10068) );
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5cd8
	sub_82BC5CD8(ctx, base);
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x822a5e98
	sub_822A5E98(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bb14f0
	if (!cr6.eq) goto loc_82BB14F0;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r11,r11,3224
	r11.s64 = r11.s64 + 3224;
loc_82BB14F0:
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc7950
	sub_82BC7950(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// li r25,0
	r25.s64 = 0;
	// beq cr6,0x82bb15c8
	if (cr6.eq) goto loc_82BB15C8;
	// lwz r30,4(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-4) );
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82bb155c
	if (cr6.eq) goto loc_82BB155C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bcbdc8
	sub_82BCBDC8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82bb1538
	if (!cr6.eq) goto loc_82BB1538;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x82bb1564
	goto loc_82BB1564;
loc_82BB1538:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(68) );
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bb1554
	if (cr6.lt) goto loc_82BB1554;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8227b8b8
	sub_8227B8B8(ctx, base);
loc_82BB1554:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
loc_82BB155C:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
loc_82BB1564:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// li r3,2
	ctx.r3.s64 = 2;
	// lwz r10,19068(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bge cr6,0x82bb15b0
	if (!cr6.lt) goto loc_82BB15B0;
loc_82BB158C:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// stw r25,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r25.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// rotlwi r8,r9,0
	ctx.r8.u64 = rotl32(ctx.r9.u32, 0);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x82bb158c
	if (cr6.lt) goto loc_82BB158C;
loc_82BB15B0:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// bl 0x82ba8800
	sub_82BA8800(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
loc_82BB15C8:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lis r11,-32068
	r11.s64 = -2101608448;
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r4,r11,24864
	ctx.r4.s64 = r11.s64 + 24864;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(32) );
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// subf r6,r10,r11
	ctx.r6.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x822c05f8
	sub_822C05F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82bb16c0
	if (cr6.eq) goto loc_82BB16C0;
	// lwz r30,4(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-4) );
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82bb1654
	if (cr6.eq) goto loc_82BB1654;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bcbdc8
	sub_82BCBDC8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82bb1630
	if (!cr6.eq) goto loc_82BB1630;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x82bb165c
	goto loc_82BB165C;
loc_82BB1630:
	// lwz r11,16(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(16) );
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(68) );
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bb164c
	if (cr6.lt) goto loc_82BB164C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8227b8b8
	sub_8227B8B8(ctx, base);
loc_82BB164C:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
loc_82BB1654:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
loc_82BB165C:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// li r3,2
	ctx.r3.s64 = 2;
	// lwz r10,19068(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bge cr6,0x82bb16a8
	if (!cr6.lt) goto loc_82BB16A8;
loc_82BB1684:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// stw r25,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r25.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(8) );
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// rotlwi r8,r9,0
	ctx.r8.u64 = rotl32(ctx.r9.u32, 0);
	// stw r9,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r9.u32);
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// blt cr6,0x82bb1684
	if (cr6.lt) goto loc_82BB1684;
loc_82BB16A8:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(12) );
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// bl 0x82ba8800
	sub_82BA8800(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
loc_82BB16C0:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,-10068(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-10068) );
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5c88
	sub_82BC5C88(ctx, base);
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r10,-11556
	ctx.r4.s64 = ctx.r10.s64 + -11556;
	// lwz r5,-10072(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-10072) );
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r9,2
	ctx.r9.s64 = 2;
	// li r4,-3
	ctx.r4.s64 = -3;
	// lwz r7,8(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(8) );
	// stw r30,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, r30.u32);
	// stw r9,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r9.u32);
	// lwz r11,8(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(8) );
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// stw r6,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r6.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x822a5e98
	sub_822A5E98(ctx, base);
	// li r4,-2
	ctx.r4.s64 = -2;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc77f0
	sub_82BC77F0(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,-10068(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + int32_t(-10068) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc5c88
	sub_82BC5C88(ctx, base);
	// lis r5,-31950
	ctx.r5.s64 = -2093875200;
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,-11572
	ctx.r4.s64 = ctx.r4.s64 + -11572;
	// lwz r5,-10064(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + int32_t(-10064) );
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// extsw r3,r29
	ctx.r3.s64 = r29.s32;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r11,3
	r11.s64 = 3;
	// std r3,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r3.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// li r4,-3
	ctx.r4.s64 = -3;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x822a5e98
	sub_822A5E98(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bc55b8
	sub_82BC55B8(ctx, base);
	// lwz r7,0(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r6,32(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(32) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// lwz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(44) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,16
	ctx.r5.s64 = 16;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r8,80(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(80) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// li r5,32
	ctx.r5.s64 = 32;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,80(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(80) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba8800
	sub_82BA8800(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BB1420) {
	__imp__sub_82BB1420(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1830) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r28,r10,-11588
	r28.s64 = ctx.r10.s64 + -11588;
	// lwz r9,40(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(40) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// bl 0x821e15a0
	sub_821E15A0(ctx, base);
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,32
	ctx.r5.s64 = 32;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r7,80(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(80) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

PPC_WEAK_FUNC(sub_82BB1830) {
	__imp__sub_82BB1830(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB18A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(0) );
	// bl 0x82bb1e58
	sub_82BB1E58(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + int32_t(0) );
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r8,32(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(32) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r6,40(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(40) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// bge cr6,0x82bb19b0
	if (!cr6.lt) goto loc_82BB19B0;
	// li r19,0
	r19.s64 = 0;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// stw r19,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r19.u32);
	// li r5,-1
	ctx.r5.s64 = -1;
	// sth r19,100(r1)
	PPC_STORE_U16(ctx.r1.u32 + 100, r19.u16);
	// addi r4,r11,-11460
	ctx.r4.s64 = r11.s64 + -11460;
	// sth r19,102(r1)
	PPC_STORE_U16(ctx.r1.u32 + 102, r19.u16);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,-7424(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(-7424) );
	// bl 0x822844b0
	sub_822844B0(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// addi r31,r11,3224
	r31.s64 = r11.s64 + 3224;
	// beq cr6,0x82bb197c
	if (cr6.eq) goto loc_82BB197C;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb1970
	if (cr6.eq) goto loc_82BB1970;
	// rotlwi r4,r10,0
	ctx.r4.u64 = rotl32(ctx.r10.u32, 0);
loc_82BB1970:
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ba89b0
	sub_82BA89B0(ctx, base);
loc_82BB197C:
	// lwz r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(96) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82bb198c
	if (!cr6.eq) goto loc_82BB198C;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
loc_82BB198C:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// li r3,1
	ctx.r3.s64 = 1;
	// lwz r10,19068(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82ba8800
	sub_82BA8800(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c00
	return;
loc_82BB19B0:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// li r15,1
	r15.s64 = 1;
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r6,r1,92
	ctx.r6.s64 = ctx.r1.s64 + 92;
	// lwz r18,12(r31)
	r18.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// mr r17,r15
	r17.u64 = r15.u64;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lis r9,-32069
	ctx.r9.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r9,10704
	ctx.r4.s64 = ctx.r9.s64 + 10704;
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// lwz r7,12(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(12) );
	// subf r6,r7,r8
	ctx.r6.s64 = ctx.r8.s64 - ctx.r7.s64;
	// srawi r26,r6,3
	xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x7) != 0);
	r26.s64 = ctx.r6.s32 >> 3;
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r26.u32);
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// li r5,2
	ctx.r5.s64 = 2;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// li r4,-10000
	ctx.r4.s64 = -10000;
	// bl 0x821e1548
	sub_821E1548(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x821e1548
	sub_821E1548(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r16,r11,3224
	r16.s64 = r11.s64 + 3224;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(92) );
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// mr r5,r16
	ctx.r5.u64 = r16.u64;
	// beq cr6,0x82bb1a48
	if (cr6.eq) goto loc_82BB1A48;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
loc_82BB1A48:
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x821e1408
	sub_821E1408(ctx, base);
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r10,r11,1876
	ctx.r10.s64 = r11.s64 + 1876;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x82bb1e40
	if (cr6.eq) goto loc_82BB1E40;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// bne cr6,0x82bb1e40
	if (!cr6.eq) goto loc_82BB1E40;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r4,r10,3488
	ctx.r4.s64 = ctx.r10.s64 + 3488;
	// lwz r5,-10068(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-10068) );
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// lwz r7,-24(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(-24) );
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// lwz r6,-20(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(-20) );
	// stw r6,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(8) );
	// addi r5,r11,8
	ctx.r5.s64 = r11.s64 + 8;
	// stw r5,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r5.u32);
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x822a5e98
	sub_822A5E98(ctx, base);
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r3,8(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// addi r31,r29,1
	r31.s64 = r29.s64 + 1;
	// li r19,0
	r19.s64 = 0;
	// cmpw cr6,r31,r18
	cr6.compare<int32_t>(r31.s32, r18.s32, xer);
	// lwz r11,-16(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(-16) );
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// lwz r10,-12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(-12) );
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(8) );
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// stw r9,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r9.u32);
	// bgt cr6,0x82bb1de8
	if (cr6.gt) goto loc_82BB1DE8;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r20,-31951
	r20.s64 = -2093940736;
	// li r28,3
	r28.s64 = 3;
	// lis r26,-31924
	r26.s64 = -2092171264;
	// lis r25,-31924
	r25.s64 = -2092171264;
	// lis r24,-31924
	r24.s64 = -2092171264;
	// lis r23,-31924
	r23.s64 = -2092171264;
	// lis r22,-31924
	r22.s64 = -2092171264;
	// lis r29,-31924
	r29.s64 = -2092171264;
	// addi r21,r11,-11424
	r21.s64 = r11.s64 + -11424;
loc_82BB1B1C:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r10,-7028(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(-7028) );
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1b70
	if (!cr6.eq) goto loc_82BB1B70;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// stfs f0,0(r9)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stw r28,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r28.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// stw r8,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r8.u32);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1B70:
	// lwz r10,-7024(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + int32_t(-7024) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1bec
	if (!cr6.eq) goto loc_82BB1BEC;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82bb5c30
	sub_82BB5C30(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stfs f0,0(r14)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r14.u32 + 0, temp.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lfs f13,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r14)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r14.u32 + 4, temp.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lfs f12,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r14)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r14.u32 + 8, temp.u32);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1BEC:
	// lwz r10,-7020(r23)
	ctx.r10.u64 = PPC_LOAD_U32(r23.u32 + int32_t(-7020) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1c3c
	if (!cr6.eq) goto loc_82BB1C3C;
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f12,104(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lwz r7,8(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// stfs f10,0(r7)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stw r28,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, r28.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1C3C:
	// lwz r10,-7016(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + int32_t(-7016) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1d14
	if (!cr6.eq) goto loc_82BB1D14;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82bb6820
	sub_82BB6820(ctx, base);
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	f0.f64 = double(temp.f32);
	// mr r14,r3
	r14.u64 = ctx.r3.u64;
	// fctidz f13,f0
	ctx.f13.s64 = (f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(f0.f64);
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lbz r11,103(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 103);
	// stb r11,0(r14)
	PPC_STORE_U8(r14.u32 + 0, r11.u8);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(4) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lfs f12,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// fctidz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f12.f64);
	// stfd f11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f11.u64);
	// lbz r8,103(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 103);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stb r8,1(r14)
	PPC_STORE_U8(r14.u32 + 1, ctx.r8.u8);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r7,0(r27)
	ctx.r7.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r11,4(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(4) );
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lfs f10,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// fctidz f9,f10
	ctx.f9.s64 = (ctx.f10.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f10.f64);
	// stfd f9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f9.u64);
	// lbz r10,103(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 103);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stb r10,2(r14)
	PPC_STORE_U8(r14.u32 + 2, ctx.r10.u8);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(4) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lfs f8,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : simd::truncate_f64_to_i64(ctx.f8.f64);
	// stfd f7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f7.u64);
	// lbz r7,103(r1)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r1.u32 + 103);
	// stb r7,3(r14)
	PPC_STORE_U8(r14.u32 + 3, ctx.r7.u8);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1D14:
	// lwz r10,-7012(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(-7012) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1d50
	if (!cr6.eq) goto loc_82BB1D50;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// lwz r7,8(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// rlwinm r6,r8,27,31,31
	ctx.r6.u64 = rotl64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r5,r6,1
	ctx.r5.u64 = ctx.r6.u64 ^ 1;
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// stw r15,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, r15.u32);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(8) );
	// addi r4,r11,8
	ctx.r4.s64 = r11.s64 + 8;
	// stw r4,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r4.u32);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1D50:
	// lwz r10,-7008(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(-7008) );
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb1dc0
	if (!cr6.eq) goto loc_82BB1DC0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// mr r4,r16
	ctx.r4.u64 = r16.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82bb1d74
	if (cr6.eq) goto loc_82BB1D74;
	// rotlwi r4,r10,0
	ctx.r4.u64 = rotl32(ctx.r10.u32, 0);
loc_82BB1D74:
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x82bb1d98
	if (!cr6.eq) goto loc_82BB1D98;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// stw r19,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r19.u32);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1D98:
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
loc_82BB1D9C:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82bb1d9c
	if (!cr6.eq) goto loc_82BB1D9C;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r5,r11,0
	ctx.r5.u64 = rotl32(r11.u32, 0);
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// b 0x82bb1dd4
	goto loc_82BB1DD4;
loc_82BB1DC0:
	// lwz r11,19068(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + int32_t(19068) );
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BB1DD4:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// addi r17,r17,1
	r17.s64 = r17.s64 + 1;
	// cmpw cr6,r31,r18
	cr6.compare<int32_t>(r31.s32, r18.s32, xer);
	// ble cr6,0x82bb1b1c
	if (!cr6.gt) goto loc_82BB1B1C;
	// lwz r26,88(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(88) );
loc_82BB1DE8:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// addic. r4,r26,1
	xer.ca = r26.u32 > 4294967294;
	ctx.r4.s64 = r26.s64 + 1;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x82bb1dfc
	if (!cr0.eq) goto loc_82BB1DFC;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
	// b 0x82bb1e0c
	goto loc_82BB1E0C;
loc_82BB1DFC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc5448
	sub_82BC5448(ctx, base);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// subf r7,r11,r3
	ctx.r7.s64 = ctx.r3.s64 - r11.s64;
loc_82BB1E0C:
	// addi r11,r17,1
	r11.s64 = r17.s64 + 1;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// stw r19,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r19.u32);
	// lis r9,-32068
	ctx.r9.s64 = -2101608448;
	// rlwinm r8,r11,3,0,28
	ctx.r8.u64 = rotl64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r6,32(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// subf r11,r8,r10
	r11.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addi r4,r9,24864
	ctx.r4.s64 = ctx.r9.s64 + 24864;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// subf r6,r6,r11
	ctx.r6.s64 = r11.s64 - ctx.r6.s64;
	// bl 0x822c05f8
	sub_822C05F8(ctx, base);
loc_82BB1E40:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + int32_t(4) );
	// bl 0x82bc55b8
	sub_82BC55B8(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c00
	return;
}

PPC_WEAK_FUNC(sub_82BB18A0) {
	__imp__sub_82BB18A0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1E58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r3,92(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// bl 0x821e1498
	sub_821E1498(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// bl 0x822af338
	sub_822AF338(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r6,r11,-8
	ctx.r6.s64 = r11.s64 + -8;
	// stw r6,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r6.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB1E58) {
	__imp__sub_82BB1E58(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB1ED0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bcc
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r23,0
	r23.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r23,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r23.u32);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// sth r23,84(r1)
	PPC_STORE_U16(ctx.r1.u32 + 84, r23.u16);
	// li r5,-1
	ctx.r5.s64 = -1;
	// sth r23,86(r1)
	PPC_STORE_U16(ctx.r1.u32 + 86, r23.u16);
	// addi r22,r11,3224
	r22.s64 = r11.s64 + 3224;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// subf r8,r10,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r10.s64;
	// srawi r28,r8,3
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	r28.s64 = ctx.r8.s32 >> 3;
	// bl 0x82ba8850
	sub_82BA8850(ctx, base);
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r21,r23
	r21.u64 = r23.u64;
	// lwz r3,92(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + int32_t(92) );
	// bl 0x821e1498
	sub_821E1498(ctx, base);
	// lwz r7,8(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// stw r6,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r6.u32);
	// lwz r5,4(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// stw r5,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r5.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-4) );
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x82bb1f60
	if (cr6.eq) goto loc_82BB1F60;
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x82bb1f70
	if (!cr6.eq) goto loc_82BB1F70;
loc_82BB1F60:
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822af338
	sub_822AF338(ctx, base);
	// mr r21,r3
	r21.u64 = ctx.r3.u64;
loc_82BB1F70:
	// li r27,1
	r27.s64 = 1;
	// cmpwi cr6,r28,1
	cr6.compare<int32_t>(r28.s32, 1, xer);
	// mr r30,r27
	r30.u64 = r27.u64;
	// blt cr6,0x82bb20a4
	if (cr6.lt) goto loc_82BB20A4;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// addi r26,r11,3200
	r26.s64 = r11.s64 + 3200;
	// addi r25,r10,-11344
	r25.s64 = ctx.r10.s64 + -11344;
	// addi r24,r9,-11384
	r24.s64 = ctx.r9.s64 + -11384;
	// addi r29,r8,7616
	r29.s64 = ctx.r8.s64 + 7616;
loc_82BB1FA0:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e1408
	sub_821E1408(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc5448
	sub_82BC5448(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lis r9,-32068
	ctx.r9.s64 = -2101608448;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r4,r9,24864
	ctx.r4.s64 = ctx.r9.s64 + 24864;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r6,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r6.u32);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(32) );
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// addi r10,r11,-16
	ctx.r10.s64 = r11.s64 + -16;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// subf r6,r9,r10
	ctx.r6.s64 = ctx.r10.s64 - ctx.r9.s64;
	// bl 0x822c05f8
	sub_822C05F8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82bb201c
	if (cr6.eq) goto loc_82BB201C;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// b 0x82bb2074
	goto loc_82BB2074;
loc_82BB201C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-4) );
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82bb2064
	if (cr6.eq) goto loc_82BB2064;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bcbdc8
	sub_82BCBDC8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82bb2070
	if (cr6.eq) goto loc_82BB2070;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(68) );
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bb205c
	if (cr6.lt) goto loc_82BB205C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8227b8b8
	sub_8227B8B8(ctx, base);
loc_82BB205C:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
loc_82BB2064:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addic. r4,r11,16
	xer.ca = r11.u32 > 4294967279;
	ctx.r4.s64 = r11.s64 + 16;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x82bb2074
	if (!cr0.eq) goto loc_82BB2074;
loc_82BB2070:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
loc_82BB2074:
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba89b0
	sub_82BA89B0(ctx, base);
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// bgt cr6,0x82bb20a4
	if (cr6.gt) goto loc_82BB20A4;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82ba89b0
	sub_82BA89B0(ctx, base);
	// cmpw cr6,r30,r28
	cr6.compare<int32_t>(r30.s32, r28.s32, xer);
	// ble cr6,0x82bb1fa0
	if (!cr6.gt) goto loc_82BB1FA0;
loc_82BB20A4:
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x82bb20d4
	if (cr6.eq) goto loc_82BB20D4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// beq cr6,0x82bb20c0
	if (cr6.eq) goto loc_82BB20C0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
loc_82BB20C0:
	// lwz r11,0(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + int32_t(0) );
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_82BB20D4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// beq cr6,0x82bb20e4
	if (cr6.eq) goto loc_82BB20E4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
loc_82BB20E4:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r10,19068(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bge cr6,0x82bb212c
	if (!cr6.lt) goto loc_82BB212C;
loc_82BB2108:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// stw r23,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r23.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// rotlwi r9,r10,0
	ctx.r9.u64 = rotl32(ctx.r10.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// lwz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// blt cr6,0x82bb2108
	if (cr6.lt) goto loc_82BB2108;
loc_82BB212C:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// beq cr6,0x82bb2154
	if (cr6.eq) goto loc_82BB2154;
	// lhz r11,86(r1)
	r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 86);
	// addi r31,r11,1
	r31.s64 = r11.s64 + 1;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
loc_82BB2154:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c1c
	return;
}

PPC_WEAK_FUNC(sub_82BB1ED0) {
	__imp__sub_82BB1ED0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2160) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82bb1e58
	sub_82BB1E58(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// lwz r8,80(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(80) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r6,56(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(56) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(24) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(112) );
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// blt cr6,0x82bb23bc
	if (cr6.lt) goto loc_82BB23BC;
	// beq cr6,0x82bb2374
	if (cr6.eq) goto loc_82BB2374;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// blt cr6,0x82bb2268
	if (cr6.lt) goto loc_82BB2268;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// li r11,3
	r11.s64 = 3;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r8,0
	ctx.r8.s64 = 0;
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// li r3,3
	ctx.r3.s64 = 3;
	// std r7,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r7.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// extsw r5,r6
	ctx.r5.s64 = ctx.r6.s32;
	// std r5,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r5.u64);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stfs f9,0(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,8
	ctx.r4.s64 = r11.s64 + 8;
	// stw r4,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r4.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c3c
	return;
loc_82BB2268:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// extsw r9,r11
	ctx.r9.s64 = r11.s32;
	// lwz r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(104) );
	// extsw r5,r8
	ctx.r5.s64 = ctx.r8.s32;
	// lwz r10,108(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(108) );
	// std r9,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r9.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// extsw r4,r6
	ctx.r4.s64 = ctx.r6.s32;
	// std r5,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r5.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// std r4,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r4.u64);
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f8,f10
	ctx.f8.f64 = double(ctx.f10.s64);
	// lfd f12,88(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f0
	ctx.f9.f64 = double(f0.s64);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// frsp f5,f8
	ctx.f5.f64 = double(float(ctx.f8.f64));
	// fcfid f6,f13
	ctx.f6.f64 = double(ctx.f13.s64);
	// frsp f13,f9
	ctx.f13.f64 = double(float(ctx.f9.f64));
	// frsp f7,f11
	ctx.f7.f64 = double(float(ctx.f11.f64));
	// frsp f0,f6
	f0.f64 = double(float(ctx.f6.f64));
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// fdivs f12,f7,f5
	ctx.f12.f64 = double(float(ctx.f7.f64 / ctx.f5.f64));
	// fmuls f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x82bb22e4
	if (!cr6.gt) goto loc_82BB22E4;
	// fmr f13,f0
	ctx.f13.f64 = f0.f64;
	// fdivs f11,f0,f12
	ctx.f11.f64 = double(float(f0.f64 / ctx.f12.f64));
loc_82BB22E4:
	// fctiwz f0,f13
	ctx.fpscr.disableFlushMode();
	f0.u64 = uint64_t(int32_t(std::trunc(ctx.f13.f64)));
	// stfd f0,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, f0.u64);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// extsw r10,r11
	ctx.r10.s64 = r11.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// fctiwz f12,f11
	ctx.f12.u64 = uint64_t(int32_t(std::trunc(ctx.f11.f64)));
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r11,3
	r11.s64 = 3;
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// li r5,6
	ctx.r5.s64 = 6;
	// addi r4,r6,-11640
	ctx.r4.s64 = ctx.r6.s64 + -11640;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// stfd f12,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f12.u64);
	// lwz r8,100(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// fcfid f11,f13
	ctx.f11.f64 = double(ctx.f13.s64);
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// stfs f8,0(r9)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// stfs f7,0(r10)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c3c
	return;
loc_82BB2374:
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(104) );
	// li r11,3
	r11.s64 = 3;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// li r5,5
	ctx.r5.s64 = 5;
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// addi r4,r8,-11632
	ctx.r4.s64 = ctx.r8.s64 + -11632;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lwz r6,108(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(108) );
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f11,96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// b 0x82bb2400
	goto loc_82BB2400;
loc_82BB23BC:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(80) );
	// li r11,3
	r11.s64 = 3;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// li r5,4
	ctx.r5.s64 = 4;
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// addi r4,r8,-11624
	ctx.r4.s64 = ctx.r8.s64 + -11624;
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(f0.s64);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// extsw r9,r6
	ctx.r9.s64 = ctx.r6.s32;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f11,96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
loc_82BB2400:
	// fcfid f10,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(ctx.f11.s64);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// stfs f9,0(r10)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
	// bl 0x8219cda8
	sub_8219CDA8(ctx, base);
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BB2160) {
	__imp__sub_82BB2160(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2440) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82bb1e58
	sub_82BB1E58(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r8,60(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(60) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(0) );
	// lwz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + int32_t(4) );
	// mtctr r6
	ctr.u64 = ctx.r6.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// li r5,164
	ctx.r5.s64 = 164;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lfs f0,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	f0.f64 = double(temp.f32);
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// extsw r11,r4
	r11.s64 = ctx.r4.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fctiwz f8,f10
	ctx.f8.u64 = uint64_t(int32_t(std::trunc(ctx.f10.f64)));
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(84) );
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f7,80(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// li r10,3
	ctx.r10.s64 = 3;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r10,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
	// frsp f9,f11
	ctx.f9.f64 = double(float(ctx.f11.f64));
	// stfs f9,0(r5)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r5.u32 + 0, temp.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// fcfid f6,f7
	ctx.f6.f64 = double(ctx.f7.s64);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// frsp f5,f6
	ctx.f5.f64 = double(float(ctx.f6.f64));
	// stfs f5,0(r11)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(r11.u32 + 0, temp.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2440) {
	__imp__sub_82BB2440(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2528) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32069
	r11.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r11,26840
	ctx.r4.s64 = r11.s64 + 26840;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// addi r5,r10,-11292
	ctx.r5.s64 = ctx.r10.s64 + -11292;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r9,-32069
	ctx.r9.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r9,27584
	ctx.r4.s64 = ctx.r9.s64 + 27584;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r8,-11280
	ctx.r5.s64 = ctx.r8.s64 + -11280;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r7,-32069
	ctx.r7.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r7,27808
	ctx.r4.s64 = ctx.r7.s64 + 27808;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r6,-11264
	ctx.r5.s64 = ctx.r6.s64 + -11264;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32069
	ctx.r4.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,28928
	ctx.r4.s64 = ctx.r4.s64 + 28928;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-11248
	ctx.r5.s64 = r11.s64 + -11248;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r10,30096
	ctx.r4.s64 = ctx.r10.s64 + 30096;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r9,-11232
	ctx.r5.s64 = ctx.r9.s64 + -11232;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r8,-32069
	ctx.r8.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r8,30464
	ctx.r4.s64 = ctx.r8.s64 + 30464;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r7,-11216
	ctx.r5.s64 = ctx.r7.s64 + -11216;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r6,-32069
	ctx.r6.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r6,30864
	ctx.r4.s64 = ctx.r6.s64 + 30864;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r5,-11204
	ctx.r5.s64 = ctx.r5.s64 + -11204;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32069
	ctx.r4.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,31160
	ctx.r4.s64 = ctx.r4.s64 + 31160;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-11188
	ctx.r5.s64 = r11.s64 + -11188;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r10,31592
	ctx.r4.s64 = ctx.r10.s64 + 31592;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r9,-11180
	ctx.r5.s64 = ctx.r9.s64 + -11180;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r8,-32069
	ctx.r8.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r8,32696
	ctx.r4.s64 = ctx.r8.s64 + 32696;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r7,-11156
	ctx.r5.s64 = ctx.r7.s64 + -11156;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r6,-32068
	ctx.r6.s64 = -2101608448;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r6,-32528
	ctx.r4.s64 = ctx.r6.s64 + -32528;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r5,-11128
	ctx.r5.s64 = ctx.r5.s64 + -11128;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32068
	ctx.r4.s64 = -2101608448;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,-30832
	ctx.r4.s64 = ctx.r4.s64 + -30832;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-11120
	ctx.r5.s64 = r11.s64 + -11120;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r10,-32068
	ctx.r10.s64 = -2101608448;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r10,-30288
	ctx.r4.s64 = ctx.r10.s64 + -30288;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r9,-11108
	ctx.r5.s64 = ctx.r9.s64 + -11108;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r8,-32068
	ctx.r8.s64 = -2101608448;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r8,-30248
	ctx.r4.s64 = ctx.r8.s64 + -30248;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r7,-11088
	ctx.r5.s64 = ctx.r7.s64 + -11088;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r6,-32068
	ctx.r6.s64 = -2101608448;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r6,-30208
	ctx.r4.s64 = ctx.r6.s64 + -30208;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r5,-11068
	ctx.r5.s64 = ctx.r5.s64 + -11068;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32068
	ctx.r4.s64 = -2101608448;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,-30168
	ctx.r4.s64 = ctx.r4.s64 + -30168;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-11048
	ctx.r5.s64 = r11.s64 + -11048;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r10,11928
	ctx.r4.s64 = ctx.r10.s64 + 11928;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r9,-11028
	ctx.r5.s64 = ctx.r9.s64 + -11028;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r8,-32069
	ctx.r8.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r8,12016
	ctx.r4.s64 = ctx.r8.s64 + 12016;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r7,-11020
	ctx.r5.s64 = ctx.r7.s64 + -11020;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r6,-32069
	ctx.r6.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r6,12104
	ctx.r4.s64 = ctx.r6.s64 + 12104;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r5,-11012
	ctx.r5.s64 = ctx.r5.s64 + -11012;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32069
	ctx.r4.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,12192
	ctx.r4.s64 = ctx.r4.s64 + 12192;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-11000
	ctx.r5.s64 = r11.s64 + -11000;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r10,12240
	ctx.r4.s64 = ctx.r10.s64 + 12240;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r9,-10988
	ctx.r5.s64 = ctx.r9.s64 + -10988;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r8,-32069
	ctx.r8.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r8,12328
	ctx.r4.s64 = ctx.r8.s64 + 12328;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r7,-10972
	ctx.r5.s64 = ctx.r7.s64 + -10972;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r6,-32069
	ctx.r6.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r6,12416
	ctx.r4.s64 = ctx.r6.s64 + 12416;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r5,-10952
	ctx.r5.s64 = ctx.r5.s64 + -10952;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32069
	ctx.r4.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,7888
	ctx.r4.s64 = ctx.r4.s64 + 7888;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-10936
	ctx.r5.s64 = r11.s64 + -10936;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r10,-32069
	ctx.r10.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r10,8544
	ctx.r4.s64 = ctx.r10.s64 + 8544;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r9,-10928
	ctx.r5.s64 = ctx.r9.s64 + -10928;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r8,-32230
	ctx.r8.s64 = -2112225280;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r8,-3416
	ctx.r4.s64 = ctx.r8.s64 + -3416;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r7,-10912
	ctx.r5.s64 = ctx.r7.s64 + -10912;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r6,-32069
	ctx.r6.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r6,9280
	ctx.r4.s64 = ctx.r6.s64 + 9280;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r5,-10896
	ctx.r5.s64 = ctx.r5.s64 + -10896;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lis r4,-32069
	ctx.r4.s64 = -2101673984;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r4,r4,10864
	ctx.r4.s64 = ctx.r4.s64 + 10864;
	// bl 0x8227b7e0
	sub_8227B7E0(ctx, base);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r4,-10002
	ctx.r4.s64 = -10002;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// addi r5,r11,-10876
	ctx.r5.s64 = r11.s64 + -10876;
	// bl 0x82bc5eb0
	sub_82BC5EB0(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bb46a8
	sub_82BB46A8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bb5c90
	sub_82BB5C90(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bba1d8
	sub_82BBA1D8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + int32_t(4) );
	// bl 0x82bbb3a8
	sub_82BBB3A8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2528) {
	__imp__sub_82BB2528(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB29D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-4) );
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82bb2a30
	if (cr6.eq) goto loc_82BB2A30;
	// bl 0x82bcbdc8
	sub_82BCBDC8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82bb2a0c
	if (!cr6.eq) goto loc_82BB2A0C;
	// li r4,0
	ctx.r4.s64 = 0;
	// b 0x82bb2a38
	goto loc_82BB2A38;
loc_82BB2A0C:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(68) );
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bb2a28
	if (cr6.lt) goto loc_82BB2A28;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8227b8b8
	sub_8227B8B8(ctx, base);
loc_82BB2A28:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
loc_82BB2A30:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
loc_82BB2A38:
	// lis r11,-31951
	r11.s64 = -2093940736;
	// li r3,2
	ctx.r3.s64 = 2;
	// lwz r10,19068(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r9,r11,-8
	ctx.r9.s64 = r11.s64 + -8;
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB29D0) {
	__imp__sub_82BB29D0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2A70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r28,0
	r28.s64 = 0;
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r28
	r30.u64 = r28.u64;
	// bl 0x82bc5870
	sub_82BC5870(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82bb2bc8
	if (cr6.eq) goto loc_82BB2BC8;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
	// lwz r11,-4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-4) );
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82bb2ae8
	if (cr6.eq) goto loc_82BB2AE8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bcbdc8
	sub_82BCBDC8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82bb2ac4
	if (!cr6.eq) goto loc_82BB2AC4;
	// mr r29,r28
	r29.u64 = r28.u64;
	// b 0x82bb2af0
	goto loc_82BB2AF0;
loc_82BB2AC4:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(68) );
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bb2ae0
	if (cr6.lt) goto loc_82BB2AE0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8227b8b8
	sub_8227B8B8(ctx, base);
loc_82BB2AE0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,-8
	ctx.r4.s64 = r11.s64 + -8;
loc_82BB2AE8:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addi r29,r11,16
	r29.s64 = r11.s64 + 16;
loc_82BB2AF0:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r30,r11,-10856
	r30.s64 = r11.s64 + -10856;
	// addi r27,r10,-10748
	r27.s64 = ctx.r10.s64 + -10748;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// li r4,24
	ctx.r4.s64 = 24;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r7,810
	ctx.r7.s64 = 810;
	// bl 0x82ba8f18
	sub_82BA8F18(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lis r8,4919
	ctx.r8.s64 = 322371584;
	// li r7,24
	ctx.r7.s64 = 24;
	// ori r6,r8,61904
	ctx.r6.u64 = ctx.r8.u64 | 61904;
	// addic. r3,r9,16
	xer.ca = ctx.r9.u32 > 4294967279;
	ctx.r3.s64 = ctx.r9.s64 + 16;
	cr0.compare<int32_t>(ctx.r3.s32, 0, xer);
	// stw r6,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r6.u32);
	// stw r7,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r7.u32);
	// beq 0x82bb2b50
	if (cr0.eq) goto loc_82BB2B50;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r5,r11,11952
	ctx.r5.s64 = r11.s64 + 11952;
	// bl 0x82bbc6b8
	sub_82BBC6B8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x82bb2b54
	goto loc_82BB2B54;
loc_82BB2B50:
	// mr r30,r28
	r30.u64 = r28.u64;
loc_82BB2B54:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(20) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82bb2bc8
	if (!cr6.eq) goto loc_82BB2BC8;
	// lis r27,-31951
	r27.s64 = -2093940736;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r4,r11,-10728
	ctx.r4.s64 = r11.s64 + -10728;
	// lwz r10,19068(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,19068(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + int32_t(19068) );
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r3,2
	ctx.r3.s64 = 2;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + int32_t(0) );
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82baa458
	sub_82BAA458(ctx, base);
	// mr r30,r28
	r30.u64 = r28.u64;
loc_82BB2BC8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82bab958
	sub_82BAB958(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82bb2bf8
	if (cr6.eq) goto loc_82BB2BF8;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(0) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82baa458
	sub_82BAA458(ctx, base);
loc_82BB2BF8:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82bb2c2c
	if (!cr6.lt) goto loc_82BB2C2C;
loc_82BB2C08:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r10,r11,8
	ctx.r10.s64 = r11.s64 + 8;
	// rotlwi r9,r10,0
	ctx.r9.u64 = rotl32(ctx.r10.u32, 0);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// lwz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// blt cr6,0x82bb2c08
	if (cr6.lt) goto loc_82BB2C08;
loc_82BB2C2C:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BB2A70) {
	__imp__sub_82BB2A70(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2C40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x82bb2c7c
	if (!cr6.eq) goto loc_82BB2C7C;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82ba9068
	sub_82BA9068(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_82BB2C7C:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// addi r28,r11,-10856
	r28.s64 = r11.s64 + -10856;
	// addi r27,r10,-6812
	r27.s64 = ctx.r10.s64 + -6812;
	// bl 0x82ba8718
	sub_82BA8718(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// li r9,848
	ctx.r9.s64 = 848;
	// bl 0x82ba91c0
	sub_82BA91C0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

PPC_WEAK_FUNC(sub_82BB2C40) {
	__imp__sub_82BB2C40(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2CB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// lfs f0,-16596(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16596);
	f0.f64 = double(temp.f32);
	// stfs f0,44(r31)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r31.u32 + 44, temp.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r11.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r11.u32);
	// bne cr6,0x82bb2cf8
	if (!cr6.eq) goto loc_82BB2CF8;
	// lis r11,-32245
	r11.s64 = -2113208320;
	// addi r6,r11,-6344
	ctx.r6.s64 = r11.s64 + -6344;
loc_82BB2CF8:
	// lis r11,-32245
	r11.s64 = -2113208320;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r5,r11,-6332
	ctx.r5.s64 = r11.s64 + -6332;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca3eb8
	sub_82CA3EB8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bbf648
	sub_82BBF648(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2CB8) {
	__imp__sub_82BB2CB8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2D38) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lfs f0,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	f0.f64 = double(temp.f32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = static_cast<float>(f0.f64 - ctx.f13.f64);
	// lfs f0,-25888(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	f0.f64 = double(temp.f32);
	// fabs f11,f12
	ctx.f11.u64 = ctx.f12.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f11,f0
	cr6.compare(ctx.f11.f64, f0.f64);
	// bge cr6,0x82bb2d8c
	if (!cr6.lt) goto loc_82BB2D8C;
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f13,f12
	ctx.f11.f64 = static_cast<float>(ctx.f13.f64 - ctx.f12.f64);
	// fabs f10,f11
	ctx.f10.u64 = ctx.f11.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f10,f0
	cr6.compare(ctx.f10.f64, f0.f64);
	// bge cr6,0x82bb2d8c
	if (!cr6.lt) goto loc_82BB2D8C;
	// lfs f13,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// li r11,1
	r11.s64 = 1;
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f13,f12
	ctx.f11.f64 = static_cast<float>(ctx.f13.f64 - ctx.f12.f64);
	// fabs f10,f11
	ctx.f10.u64 = ctx.f11.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f10,f0
	cr6.compare(ctx.f10.f64, f0.f64);
	// blt cr6,0x82bb2d90
	if (cr6.lt) goto loc_82BB2D90;
loc_82BB2D8C:
	// li r11,0
	r11.s64 = 0;
loc_82BB2D90:
	// clrlwi r3,r11,24
	ctx.r3.u64 = r11.u32 & 0xFF;
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2D38) {
	__imp__sub_82BB2D38(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2D98) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister f0{};
	PPCRegister temp{};
	// lfs f12,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// fmuls f0,f12,f12
	f0.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// lfs f13,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,2360(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2360);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f9,f13,f13,f0
	ctx.f9.f64 = double(std::fma(float(ctx.f13.f64), float(ctx.f13.f64), float(f0.f64)));
	// fmadds f0,f11,f11,f9
	f0.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f11.f64), float(ctx.f9.f64)));
	// fcmpu cr6,f0,f10
	cr6.compare(f0.f64, ctx.f10.f64);
	// blelr cr6
	if (!cr6.gt) return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lfs f9,-27456(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27456);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f8,f0,f9
	ctx.f8.f64 = static_cast<float>(f0.f64 - ctx.f9.f64);
	// fabs f7,f8
	ctx.f7.u64 = ctx.f8.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f7,f10
	cr6.compare(ctx.f7.f64, ctx.f10.f64);
	// blelr cr6
	if (!cr6.gt) return;
	// fsqrts f0,f0
	f0.f64 = double(simd::sqrt_f32(float(f0.f64)));
	// fdivs f10,f9,f0
	ctx.f10.f64 = double(float(ctx.f9.f64 / f0.f64));
	// fmuls f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// stfs f9,0(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// fmuls f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// stfs f8,4(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// fmuls f7,f11,f10
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f7,8(r3)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2D98) {
	__imp__sub_82BB2D98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2E00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister f0{};
	PPCRegister temp{};
	// lfs f0,16(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 * ctx.f13.f64));
	// lfs f11,32(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 32);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f7,f11,f10,f12
	ctx.f7.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f10.f64), float(ctx.f12.f64)));
	// fmadds f6,f9,f8,f7
	ctx.f6.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f8.f64), float(ctx.f7.f64)));
	// stfs f6,0(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f5,36(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,20(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f3,f13
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// fmadds f1,f5,f10,f2
	ctx.f1.f64 = double(std::fma(float(ctx.f5.f64), float(ctx.f10.f64), float(ctx.f2.f64)));
	// fmr f0,f6
	f0.f64 = ctx.f6.f64;
	// fmadds f12,f4,f8,f1
	ctx.f12.f64 = double(std::fma(float(ctx.f4.f64), float(ctx.f8.f64), float(ctx.f1.f64)));
	// stfs f12,4(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f11,40(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 40);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,24(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 24);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// fmadds f5,f11,f10,f6
	ctx.f5.f64 = double(std::fma(float(ctx.f11.f64), float(ctx.f10.f64), float(ctx.f6.f64)));
	// fmr f4,f12
	ctx.f4.f64 = ctx.f12.f64;
	// fmadds f3,f9,f8,f5
	ctx.f3.f64 = double(std::fma(float(ctx.f9.f64), float(ctx.f8.f64), float(ctx.f5.f64)));
	// stfs f3,8(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// lfs f1,48(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f1.f64 = double(temp.f32);
	// fadds f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 + f0.f64));
	// fmr f2,f3
	ctx.f2.f64 = ctx.f3.f64;
	// stfs f0,0(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f13,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f4
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f4.f64));
	// stfs f12,4(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f11,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f11,f2
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f2.f64));
	// stfs f10,8(r3)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2E00) {
	__imp__sub_82BB2E00(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2E98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,-6992(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6992) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2E98) {
	__imp__sub_82BB2E98(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2EF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,-6988(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6988) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2EF0) {
	__imp__sub_82BB2EF0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2F48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,-6984(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6984) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2F48) {
	__imp__sub_82BB2F48(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2FA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lwz r4,-6976(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6976) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2FA0) {
	__imp__sub_82BB2FA0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB2FD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,-6968(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6968) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB2FD0) {
	__imp__sub_82BB2FD0(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3028) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,-6964(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6964) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB3028) {
	__imp__sub_82BB3028(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3080) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,-6960(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6960) );
	// bl 0x82bb30d8
	sub_82BB30D8(ctx, base);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r8,2
	ctx.r8.s64 = 2;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r7,r11,8
	ctx.r7.s64 = r11.s64 + 8;
	// stw r7,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB3080) {
	__imp__sub_82BB3080(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB30D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// lis r10,-32247
	ctx.r10.s64 = -2113339392;
	// addi r28,r11,1876
	r28.s64 = r11.s64 + 1876;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r30,r10,1744
	r30.s64 = ctx.r10.s64 + 1744;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bge cr6,0x82bb3120
	if (!cr6.lt) goto loc_82BB3120;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x82bb3120
	if (cr6.eq) goto loc_82BB3120;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// beq cr6,0x82bb3130
	if (cr6.eq) goto loc_82BB3130;
loc_82BB3120:
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + int32_t(8) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc68f0
	sub_82BC68F0(ctx, base);
loc_82BB3130:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822af338
	sub_822AF338(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bb1e58
	sub_82BB1E58(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(32) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// li r5,4096
	ctx.r5.s64 = 4096;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// lwz r9,0(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + int32_t(0) );
	// lwz r8,76(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + int32_t(76) );
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x82bb3358
	if (cr6.eq) goto loc_82BB3358;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lwz r11,-6984(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6984) );
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x82bb31f8
	if (!cr6.eq) goto loc_82BB31F8;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x82bb31bc
	if (!cr6.lt) goto loc_82BB31BC;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x82bb31bc
	if (cr6.eq) goto loc_82BB31BC;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x82bb31cc
	if (cr6.eq) goto loc_82BB31CC;
loc_82BB31BC:
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + int32_t(12) );
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc68f0
	sub_82BC68F0(ctx, base);
loc_82BB31CC:
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82188568
	sub_82188568(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lfs f0,-17748(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -17748);
	f0.f64 = double(temp.f32);
	// fmuls f0,f1,f0
	f0.f64 = double(float(ctx.f1.f64 * f0.f64));
	// fctiwz f13,f0
	ctx.f13.u64 = uint64_t(int32_t(std::trunc(f0.f64)));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(100) );
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// b 0x82bb3310
	goto loc_82BB3310;
loc_82BB31F8:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lwz r11,-6976(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6976) );
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x82bb3310
	if (!cr6.eq) goto loc_82BB3310;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x82bb3220
	if (cr6.lt) goto loc_82BB3220;
	// mr r11,r28
	r11.u64 = r28.u64;
loc_82BB3220:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// cmpwi cr6,r10,3
	cr6.compare<int32_t>(ctx.r10.s32, 3, xer);
	// beq cr6,0x82bb3240
	if (cr6.eq) goto loc_82BB3240;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x82bcbd48
	sub_82BCBD48(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82bb3254
	if (cr6.eq) goto loc_82BB3254;
loc_82BB3240:
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc59d0
	sub_82BC59D0(ctx, base);
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// b 0x82bb3310
	goto loc_82BB3310;
loc_82BB3254:
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bc5870
	sub_82BC5870(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x82bb3300
	if (cr6.eq) goto loc_82BB3300;
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,8
	ctx.r4.s64 = r11.s64 + 8;
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// blt cr6,0x82bb3280
	if (cr6.lt) goto loc_82BB3280;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_82BB3280:
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(4) );
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x82bb32e4
	if (cr6.eq) goto loc_82BB32E4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82bcbdc8
	sub_82BCBDC8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x82bb32b0
	if (!cr6.eq) goto loc_82BB32B0;
	// bl 0x82188cf0
	sub_82188CF0(ctx, base);
	// lis r11,-31924
	r11.s64 = -2092171264;
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// lwz r29,-6972(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6972) );
	// b 0x82bb3310
	goto loc_82BB3310;
loc_82BB32B0:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(16) );
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(68) );
	// lwz r9,64(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(64) );
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82bb32cc
	if (cr6.lt) goto loc_82BB32CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8227b8b8
	sub_8227B8B8(ctx, base);
loc_82BB32CC:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + int32_t(12) );
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + int32_t(8) );
	// addi r4,r11,8
	ctx.r4.s64 = r11.s64 + 8;
	// cmplw cr6,r4,r10
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r10.u32, xer);
	// blt cr6,0x82bb32e4
	if (cr6.lt) goto loc_82BB32E4;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_82BB32E4:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + int32_t(0) );
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// bl 0x82188cf0
	sub_82188CF0(ctx, base);
	// lis r11,-31924
	r11.s64 = -2092171264;
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// lwz r29,-6972(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + int32_t(-6972) );
	// b 0x82bb3310
	goto loc_82BB3310;
loc_82BB3300:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r11,-10660
	ctx.r4.s64 = r11.s64 + -10660;
	// bl 0x82bc6a18
	sub_82BC6A18(ctx, base);
loc_82BB3310:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x82bb3368
	sub_82BB3368(ctx, base);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(0) );
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(28) );
	// addi r7,r1,104
	ctx.r7.s64 = ctx.r1.s64 + 104;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c2c
	return;
loc_82BB3358:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c2c
	return;
}

PPC_WEAK_FUNC(sub_82BB30D8) {
	__imp__sub_82BB30D8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3368) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb33a8
	if (cr6.eq) goto loc_82BB33A8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_82BB33A8:
	// li r29,0
	r29.s64 = 0;
loc_82BB33AC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(88) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82bb33fc
	if (cr6.eq) goto loc_82BB33FC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(76) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82bb33fc
	if (cr6.eq) goto loc_82BB33FC;
	// mr r29,r31
	r29.u64 = r31.u64;
loc_82BB33FC:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82bb33ac
	if (cr6.eq) goto loc_82BB33AC;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

PPC_WEAK_FUNC(sub_82BB3368) {
	__imp__sub_82BB3368(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3410) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-10616
	ctx.r9.s64 = r11.s64 + -10616;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x82bb3444
	if (cr6.eq) goto loc_82BB3444;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_82BB3444:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB3410) {
	__imp__sub_82BB3410(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3458) {
	PPC_FUNC_PROLOGUE();
	// stw r4,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r4.u32);
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB3458) {
	__imp__sub_82BB3458(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3468) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(4) );
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, r11.u32);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + int32_t(8) );
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB3468) {
	__imp__sub_82BB3468(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3480) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r4
	r23.u64 = ctx.r4.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// li r24,0
	r24.s64 = 0;
	// lwz r11,1632(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(1632) );
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// ble cr6,0x82bb369c
	if (!cr6.gt) goto loc_82BB369C;
	// addi r25,r23,92
	r25.s64 = r23.s64 + 92;
loc_82BB34AC:
	// lbz r11,212(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 212);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x82bb369c
	if (!cr6.eq) goto loc_82BB369C;
	// lwz r4,-92(r25)
	ctx.r4.u64 = PPC_LOAD_U32(r25.u32 + int32_t(-92) );
	// addi r26,r25,-92
	r26.s64 = r25.s64 + -92;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x82bb3688
	if (cr6.eq) goto loc_82BB3688;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(52) );
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,8(r25)
	ctx.r9.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// rlwinm r11,r9,2,0,29
	r11.u64 = rotl64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r30,0(r25)
	r30.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// add r8,r11,r30
	ctx.r8.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r30,r8
	cr6.compare<uint32_t>(r30.u32, ctx.r8.u32, xer);
	// beq cr6,0x82bb35b0
	if (cr6.eq) goto loc_82BB35B0;
loc_82BB34F8:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r9,56(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(56) );
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bb3530
	if (cr6.eq) goto loc_82BB3530;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,208(r28)
	PPC_STORE_U32(r28.u32 + 208, ctx.r10.u32);
loc_82BB3530:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,236(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(236) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82bb3594
	if (cr6.eq) goto loc_82BB3594;
	// li r31,236
	r31.s64 = 236;
loc_82BB3544:
	// cmpwi cr6,r31,248
	cr6.compare<int32_t>(r31.s32, 248, xer);
	// bge cr6,0x82bb3594
	if (!cr6.lt) goto loc_82BB3594;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwzx r4,r31,r11
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r9,56(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(56) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bb3580
	if (cr6.eq) goto loc_82BB3580;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,208(r28)
	PPC_STORE_U32(r28.u32 + 208, ctx.r10.u32);
loc_82BB3580:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwzx r10,r31,r11
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82bb3544
	if (!cr6.eq) goto loc_82BB3544;
loc_82BB3594:
	// lwz r11,8(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + int32_t(8) );
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + int32_t(0) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb34f8
	if (!cr6.eq) goto loc_82BB34F8;
loc_82BB35B0:
	// lwz r11,148(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + int32_t(148) );
	// addi r27,r26,108
	r27.s64 = r26.s64 + 108;
	// lwz r10,140(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + int32_t(140) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// extsw r30,r10
	r30.s64 = ctx.r10.s32;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x82bb3688
	if (cr6.eq) goto loc_82BB3688;
loc_82BB35D0:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r9,56(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + int32_t(56) );
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(0) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bb3608
	if (cr6.eq) goto loc_82BB3608;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,208(r28)
	PPC_STORE_U32(r28.u32 + 208, ctx.r10.u32);
loc_82BB3608:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,236(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(236) );
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x82bb366c
	if (cr6.eq) goto loc_82BB366C;
	// li r31,236
	r31.s64 = 236;
loc_82BB361C:
	// cmpwi cr6,r31,248
	cr6.compare<int32_t>(r31.s32, 248, xer);
	// bge cr6,0x82bb366c
	if (!cr6.lt) goto loc_82BB366C;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + int32_t(0) );
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// lwzx r4,r31,r11
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r9,56(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + int32_t(56) );
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x82bb3658
	if (cr6.eq) goto loc_82BB3658;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + int32_t(4) );
	// stw r10,208(r28)
	PPC_STORE_U32(r28.u32 + 208, ctx.r10.u32);
loc_82BB3658:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + int32_t(0) );
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// lwzx r10,r31,r11
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + r11.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x82bb361c
	if (!cr6.eq) goto loc_82BB361C;
loc_82BB366C:
	// lwz r11,40(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + int32_t(40) );
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// lwz r10,32(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + int32_t(32) );
	// rlwinm r11,r11,2,0,29
	r11.u64 = rotl64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x82bb35d0
	if (!cr6.eq) goto loc_82BB35D0;
loc_82BB3688:
	// lwz r11,1632(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + int32_t(1632) );
	// addi r24,r24,1
	r24.s64 = r24.s64 + 1;
	// addi r25,r25,204
	r25.s64 = r25.s64 + 204;
	// cmpw cr6,r24,r11
	cr6.compare<int32_t>(r24.s32, r11.s32, xer);
	// blt cr6,0x82bb34ac
	if (cr6.lt) goto loc_82BB34AC;
loc_82BB369C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c24
	return;
}

PPC_WEAK_FUNC(sub_82BB3480) {
	__imp__sub_82BB3480(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB36A8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister f0{};
	PPCRegister temp{};
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	f0.f64 = double(temp.f32);
	// lfs f13,0(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 * ctx.f13.f64));
	// stfs f12,0(r3)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r3.u32 + 0, temp.u32);
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f9,4(r3)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r3.u32 + 4, temp.u32);
	// lfs f8,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f8,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f5,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// stfs f6,8(r3)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r3.u32 + 8, temp.u32);
	// lfs f4,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f2,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f2.f64 = double(temp.f32);
	// stfs f3,16(r3)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// lfs f1,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f0,f1,f2
	f0.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// lfs f13,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,20(r3)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 20, temp.u32);
	// lfs f12,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f12,f13
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f10,32(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,24(r3)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// lfs f9,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f7,36(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	ctx.f7.f64 = double(temp.f32);
	// stfs f8,32(r3)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// lfs f6,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f7,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f6.f64));
	// lfs f4,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f4.f64 = double(temp.f32);
	// stfs f5,36(r3)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r3.u32 + 36, temp.u32);
	// lfs f3,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f4,f3
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// stfs f2,40(r3)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r3.u32 + 40, temp.u32);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB36A8) {
	__imp__sub_82BB36A8(ctx, base);
}

PPC_FUNC_IMPL(__imp__sub_82BB3740) {
	PPC_FUNC_PROLOGUE();
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// addi r12,r1,-24
	r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82ca74d8
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r30,r11,-27456
	r30.s64 = r11.s64 + -27456;
	// lfs f22,40(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 40);
	f22.f64 = double(temp.f32);
	// lfs f21,20(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 20);
	f21.f64 = double(temp.f32);
	// fmuls f16,f21,f22
	f16.f64 = double(float(f21.f64 * f22.f64));
	// lfs f29,0(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 0);
	f29.f64 = double(temp.f32);
	// lfs f0,-12(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + -12);
	f0.f64 = double(temp.f32);
	// fmr f11,f0
	ctx.f11.f64 = f0.f64;
	// fmr f12,f0
	ctx.f12.f64 = f0.f64;
	// fmuls f13,f29,f16
	ctx.f13.f64 = double(float(f29.f64 * f16.f64));
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x82bb3798
	if (cr6.lt) goto loc_82BB3798;
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// b 0x82bb379c
	goto loc_82BB379C;
loc_82BB3798:
	// fmr f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f13.f64;
loc_82BB379C:
	// lfs f24,32(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 32);
	f24.f64 = double(temp.f32);
	// lfs f23,24(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 24);
	f23.f64 = double(temp.f32);
	// fmuls f17,f23,f24
	f17.f64 = double(float(f23.f64 * f24.f64));
	// lfs f30,4(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 4);
	f30.f64 = double(temp.f32);
	// fmuls f13,f30,f17
	ctx.f13.f64 = double(float(f30.f64 * f17.f64));
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x82bb37c0
	if (cr6.lt) goto loc_82BB37C0;
	// fadds f11,f13,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// b 0x82bb37c4
	goto loc_82BB37C4;
loc_82BB37C0:
	// fadds f12,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
loc_82BB37C4:
	// lfs f27,36(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 36);
	f27.f64 = double(temp.f32);
	// lfs f26,16(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 16);
	f26.f64 = double(temp.f32);
	// fmuls f18,f26,f27
	f18.f64 = double(float(f26.f64 * f27.f64));
	// lfs f31,8(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 8);
	f31.f64 = double(temp.f32);
	// fmuls f13,f31,f18
	ctx.f13.f64 = double(float(f31.f64 * f18.f64));
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x82bb37e8
	if (cr6.lt) goto loc_82BB37E8;
	// fadds f11,f13,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// b 0x82bb37ec
	goto loc_82BB37EC;
loc_82BB37E8:
	// fadds f12,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
loc_82BB37EC:
	// fmuls f19,f24,f21
	ctx.fpscr.disableFlushMode();
	f19.f64 = double(float(f24.f64 * f21.f64));
	// fmuls f13,f19,f31
	ctx.f13.f64 = double(float(f19.f64 * f31.f64));
	// fneg f13,f13
	ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x82bb3808
	if (cr6.lt) goto loc_82BB3808;
	// fadds f11,f13,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// b 0x82bb380c
	goto loc_82BB380C;
loc_82BB3808:
	// fadds f12,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
loc_82BB380C:
	// fmuls f20,f26,f22
	ctx.fpscr.disableFlushMode();
	f20.f64 = double(float(f26.f64 * f22.f64));
	// fmuls f13,f20,f30
	ctx.f13.f64 = double(float(f20.f64 * f30.f64));
	// fneg f13,f13
	ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x82bb3828
	if (cr6.lt) goto loc_82BB3828;
	// fadds f11,f13,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// b 0x82bb382c
	goto loc_82BB382C;
loc_82BB3828:
	// fadds f12,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
loc_82BB382C:
	// fmuls f25,f27,f23
	ctx.fpscr.disableFlushMode();
	f25.f64 = double(float(f27.f64 * f23.f64));
	// fmuls f13,f25,f29
	ctx.f13.f64 = double(float(f25.f64 * f29.f64));
	// fneg f13,f13
	ctx.f13.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x82bb3848
	if (cr6.lt) goto loc_82BB3848;
	// fadds f11,f13,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// b 0x82bb384c
	goto loc_82BB384C;
loc_82BB3848:
	// fadds f12,f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
loc_82BB384C:
	// fadds f28,f12,f11
	ctx.fpscr.disableFlushMode();
	f28.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// fcmpu cr6,f28,f0
	cr6.compare(f28.f64, f0.f64);
	// bne cr6,0x82bb3874
	if (!cr6.eq) goto loc_82BB3874;
	// fsubs f13,f11,f12
	ctx.f13.f64 = static_cast<float>(ctx.f11.f64 - ctx.f12.f64);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfs f0,3100(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 3100);
	f0.f64 = double(temp.f32);
	// fdivs f12,f28,f13
	ctx.f12.f64 = double(float(f28.f64 / ctx.f13.f64));
	// fabs f11,f12
	ctx.f11.u64 = ctx.f12.u64 & 0x7FFFFFFFFFFFFFFF;
	// fcmpu cr6,f11,f0
	cr6.compare(ctx.f11.f64, f0.f64);
	// blt cr6,0x82bb3970
	if (cr6.lt) goto loc_82BB3970;
loc_82BB3874:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,-6944
	ctx.r4.s64 = r11.s64 + -6944;
	// li r5,64
	ctx.r5.s64 = 64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// fmuls f13,f31,f26
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f31.f64 * f26.f64));
	// lfs f0,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmuls f4,f31,f27
	ctx.f4.f64 = double(float(f31.f64 * f27.f64));
	// fdivs f9,f0,f28
	ctx.f9.f64 = double(float(f0.f64 / f28.f64));
	// lfs f5,52(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 52);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f0,f30,f24
	f0.f64 = double(float(f30.f64 * f24.f64));
	// lfs f3,48(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 48);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f11,f31,f21
	ctx.f11.f64 = double(float(f31.f64 * f21.f64));
	// lfs f1,56(r31)
	temp.u32 = PPC_LOAD_U32(r31.u32 + 56);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f31,f24
	ctx.f2.f64 = double(float(f31.f64 * f24.f64));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// fsubs f12,f20,f17
	ctx.f12.f64 = static_cast<float>(f20.f64 - f17.f64);
	// li r5,64
	ctx.r5.s64 = 64;
	// fmuls f8,f26,f30
	ctx.f8.f64 = double(float(f26.f64 * f30.f64));
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// fsubs f7,f18,f19
	ctx.f7.f64 = static_cast<float>(f18.f64 - f19.f64);
	// fsubs f10,f16,f25
	ctx.f10.f64 = static_cast<float>(f16.f64 - f25.f64);
	// fmsubs f6,f23,f29,f13
	ctx.f6.f64 = double(std::fma(float(f23.f64), float(f29.f64), -float(ctx.f13.f64)));
	// fmsubs f4,f30,f22,f4
	ctx.f4.f64 = double(std::fma(float(f30.f64), float(f22.f64), -float(ctx.f4.f64)));
	// fmsubs f31,f27,f29,f0
	f31.f64 = double(std::fma(float(f27.f64), float(f29.f64), -float(f0.f64)));
	// fmsubs f30,f30,f23,f11
	f30.f64 = double(std::fma(float(f30.f64), float(f23.f64), -float(ctx.f11.f64)));
	// fmsubs f2,f29,f22,f2
	ctx.f2.f64 = double(std::fma(float(f29.f64), float(f22.f64), -float(ctx.f2.f64)));
	// fmuls f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
	// fmsubs f8,f29,f21,f8
	ctx.f8.f64 = double(std::fma(float(f29.f64), float(f21.f64), -float(ctx.f8.f64)));
	// fmuls f13,f7,f9
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// stfs f13,112(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fmuls f0,f10,f9
	f0.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// stfs f0,80(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// fmuls f7,f6,f9
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// fmuls f6,f4,f9
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f9.f64));
	// fmuls f4,f31,f9
	ctx.f4.f64 = double(float(f31.f64 * ctx.f9.f64));
	// fmuls f10,f30,f9
	ctx.f10.f64 = double(float(f30.f64 * ctx.f9.f64));
	// stfs f10,88(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fmuls f11,f2,f9
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// stfs f11,100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// fneg f12,f12
	ctx.f12.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// fmuls f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f9,120(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// fneg f8,f7
	ctx.f8.u64 = ctx.f7.u64 ^ 0x8000000000000000;
	// stfs f8,104(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fneg f7,f6
	ctx.f7.u64 = ctx.f6.u64 ^ 0x8000000000000000;
	// stfs f7,84(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fneg f6,f4
	ctx.f6.u64 = ctx.f4.u64 ^ 0x8000000000000000;
	// stfs f6,116(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f2,f5,f12
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// fmuls f12,f5,f11
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f11,f5,f8
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f8.f64));
	// fmadds f8,f3,f0,f2
	ctx.f8.f64 = double(std::fma(float(ctx.f3.f64), float(f0.f64), float(ctx.f2.f64)));
	// fmadds f7,f3,f7,f12
	ctx.f7.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f7.f64), float(ctx.f12.f64)));
	// fmadds f5,f3,f10,f11
	ctx.f5.f64 = double(std::fma(float(ctx.f3.f64), float(ctx.f10.f64), float(ctx.f11.f64)));
	// fnmadds f4,f1,f13,f8
	ctx.f4.f64 = -double(std::fma(float(ctx.f1.f64), float(ctx.f13.f64), float(ctx.f8.f64)));
	// stfs f4,128(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fnmadds f3,f1,f6,f7
	ctx.f3.f64 = -double(std::fma(float(ctx.f1.f64), float(ctx.f6.f64), float(ctx.f7.f64)));
	// stfs f3,132(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// fnmadds f2,f1,f9,f5
	ctx.f2.f64 = -double(std::fma(float(ctx.f1.f64), float(ctx.f9.f64), float(ctx.f5.f64)));
	// stfs f2,136(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_82BB3970:
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// addi r12,r1,-24
	r12.s64 = ctx.r1.s64 + -24;
	// bl 0x82ca7524
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + int32_t(-8) );
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

PPC_WEAK_FUNC(sub_82BB3740) {
	__imp__sub_82BB3740(ctx, base);
}


#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_8224193C"))) PPC_WEAK_FUNC(sub_8224193C);
PPC_FUNC_IMPL(__imp__sub_8224193C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82241940"))) PPC_WEAK_FUNC(sub_82241940);
PPC_FUNC_IMPL(__imp__sub_82241940) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r11,24(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82241ab0
	if (ctx.cr6.eq) goto loc_82241AB0;
	// bl 0x822276e8
	ctx.lr = 0x82241968;
	sub_822276E8(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82241ab0
	if (ctx.cr6.eq) goto loc_82241AB0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// bl 0x82227680
	ctx.lr = 0x82241980;
	sub_82227680(ctx, base);
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// rlwinm r10,r11,20,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 20) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82241a80
	if (ctx.cr6.eq) goto loc_82241A80;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822419bc
	if (ctx.cr6.eq) goto loc_822419BC;
	// lbz r11,44(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 44);
	// rotlwi r11,r11,3
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 3);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241a84
	goto loc_82241A84;
loc_822419BC:
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// subf r9,r10,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// srawi. r11,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82241a28
	if (!ctx.cr0.gt) goto loc_82241A28;
loc_822419D8:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,44
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 44, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822419f8
	if (ctx.cr6.lt) goto loc_822419F8;
	// li r7,0
	ctx.r7.s64 = 0;
loc_822419F8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82241a14
	if (ctx.cr6.eq) goto loc_82241A14;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82241a1c
	goto loc_82241A1C;
loc_82241A14:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82241A1C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x822419d8
	if (ctx.cr6.gt) goto loc_822419D8;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82241A28:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82241a6c
	if (ctx.cr6.eq) goto loc_82241A6C;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,44
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 44, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82241a44
	if (ctx.cr6.gt) goto loc_82241A44;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241A44:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82241a6c
	if (!ctx.cr6.eq) goto loc_82241A6C;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241a84
	goto loc_82241A84;
loc_82241A6C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241a84
	goto loc_82241A84;
loc_82241A80:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241A84:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82241ab0
	if (ctx.cr6.eq) goto loc_82241AB0;
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x82241ab0
	if (ctx.cr6.eq) goto loc_82241AB0;
	// lwz r11,24(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 24);
	// li r3,1
	ctx.r3.s64 = 1;
	// lwz r10,92(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 92);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x82241ab4
	if (ctx.cr6.eq) goto loc_82241AB4;
loc_82241AB0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82241AB4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82241ACC"))) PPC_WEAK_FUNC(sub_82241ACC);
PPC_FUNC_IMPL(__imp__sub_82241ACC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82241AD0"))) PPC_WEAK_FUNC(sub_82241AD0);
PPC_FUNC_IMPL(__imp__sub_82241AD0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lfd f0,-27376(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -27376);
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82241AE0"))) PPC_WEAK_FUNC(sub_82241AE0);
PPC_FUNC_IMPL(__imp__sub_82241AE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x82241AE8;
	sub_82CA2BE4(ctx, base);
	// stfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f30.u64);
	// stfd f31,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r30,-31927
	ctx.r30.s64 = -2092367872;
	// lbz r11,52(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 52);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lwz r11,26788(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26788);
	// bne cr6,0x82241b1c
	if (!ctx.cr6.eq) goto loc_82241B1C;
	// lfs f0,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,136(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// stfs f12,48(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 48, temp.u32);
loc_82241B1C:
	// lfs f0,136(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// li r10,-1
	ctx.r10.s64 = -1;
	// lfs f13,68(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 68);
	ctx.f13.f64 = double(temp.f32);
	// clrlwi r28,r4,24
	ctx.r28.u64 = ctx.r4.u32 & 0xFF;
	// fadds f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// stfs f12,68(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 68, temp.u32);
	// stw r10,72(r31)
	PPC_STORE_U32(ctx.r31.u32 + 72, ctx.r10.u32);
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// bne cr6,0x82241b4c
	if (!ctx.cr6.eq) goto loc_82241B4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x827d1278
	ctx.lr = 0x82241B48;
	sub_827D1278(ctx, base);
	// lwz r11,26788(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26788);
loc_82241B4C:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r10,22
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 22, ctx.xer);
	// beq cr6,0x82241b60
	if (ctx.cr6.eq) goto loc_82241B60;
	// cmpwi cr6,r10,23
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 23, ctx.xer);
	// bne cr6,0x82241b6c
	if (!ctx.cr6.eq) goto loc_82241B6C;
loc_82241B60:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x827d2ee8
	ctx.lr = 0x82241B68;
	sub_827D2EE8(ctx, base);
	// lwz r11,26788(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26788);
loc_82241B6C:
	// lfs f0,156(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lfs f13,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// fsubs f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lfs f12,136(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f9,f0,f13
	ctx.f9.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f10,172(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,168(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	ctx.f7.f64 = double(temp.f32);
	// fsubs f6,f0,f13
	ctx.f6.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// addi r29,r10,-27900
	ctx.r29.s64 = ctx.r10.s64 + -27900;
	// fadds f8,f12,f10
	ctx.f8.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// addi r8,r9,368
	ctx.r8.s64 = ctx.r9.s64 + 368;
	// stfs f8,172(r31)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r31.u32 + 172, temp.u32);
	// addi r30,r31,160
	ctx.r30.s64 = ctx.r31.s64 + 160;
	// lfs f31,432(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 432);
	ctx.f31.f64 = double(temp.f32);
	// fdivs f5,f11,f7
	ctx.f5.f64 = double(float(ctx.f11.f64 / ctx.f7.f64));
	// fcmpu cr6,f9,f31
	ctx.cr6.compare(ctx.f9.f64, ctx.f31.f64);
	// mfcr r7
	ctx.r7.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r7.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r7.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r7.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r7.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r7.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r7.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r7.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r7.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r7.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r7.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r7.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r7.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r7.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r7.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r7.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r7.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r7.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r7.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r7.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r7.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r7.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r7.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r7.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r7.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r7.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r7.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r7.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r7.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r7.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r7.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r7.u64 |= ctx.cr7.so ? 0x1 : 0;
	// fcmpu cr6,f6,f31
	ctx.cr6.compare(ctx.f6.f64, ctx.f31.f64);
	// mfcr r6
	ctx.r6.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r6.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r6.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r6.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r6.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r6.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r6.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r6.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r6.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r6.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r6.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r6.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r6.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r6.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r6.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r6.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r6.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r6.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r6.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r6.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r6.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r6.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r6.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r6.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r6.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r6.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r6.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r6.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r6.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r6.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r6.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r6.u64 |= ctx.cr7.so ? 0x1 : 0;
	// fmadds f4,f5,f8,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f8.f64 + ctx.f0.f64));
	// rlwinm r5,r6,27,29,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x4;
	// rlwinm r4,r6,30,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x4;
	// stfs f4,160(r31)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r31.u32 + 160, temp.u32);
	// rlwinm r3,r7,27,29,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x4;
	// or r10,r5,r4
	ctx.r10.u64 = ctx.r5.u64 | ctx.r4.u64;
	// rlwinm r9,r7,30,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x4;
	// or r7,r3,r9
	ctx.r7.u64 = ctx.r3.u64 | ctx.r9.u64;
	// lfsx f2,r8,r10
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	ctx.f2.f64 = double(temp.f32);
	// fsel f1,f2,f13,f0
	ctx.f1.f64 = ctx.f2.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// lfsx f12,r8,r7
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r7.u32);
	ctx.f12.f64 = double(temp.f32);
	// fsel f11,f12,f0,f13
	ctx.f11.f64 = ctx.f12.f64 >= 0.0 ? ctx.f0.f64 : ctx.f13.f64;
	// fsubs f10,f3,f1
	ctx.f10.f64 = double(float(ctx.f3.f64 - ctx.f1.f64));
	// fcmpu cr6,f10,f31
	ctx.cr6.compare(ctx.f10.f64, ctx.f31.f64);
	// mfcr r6
	ctx.r6.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r6.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r6.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r6.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r6.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r6.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r6.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r6.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r6.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r6.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r6.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r6.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r6.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r6.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r6.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r6.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r6.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r6.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r6.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r6.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r6.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r6.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r6.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r6.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r6.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r6.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r6.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r6.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r6.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r6.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r6.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r6.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r5,r6,27,29,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x4;
	// rlwinm r4,r6,30,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x4;
	// or r3,r5,r4
	ctx.r3.u64 = ctx.r5.u64 | ctx.r4.u64;
	// lfsx f9,r8,r3
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r3.u32);
	ctx.f9.f64 = double(temp.f32);
	// fsel f8,f9,f3,f1
	ctx.f8.f64 = ctx.f9.f64 >= 0.0 ? ctx.f3.f64 : ctx.f1.f64;
	// fsubs f7,f8,f11
	ctx.f7.f64 = double(float(ctx.f8.f64 - ctx.f11.f64));
	// fcmpu cr6,f7,f31
	ctx.cr6.compare(ctx.f7.f64, ctx.f31.f64);
	// mfcr r10
	ctx.r10.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r10.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r10.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r10.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r10.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r10.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r10.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r10.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r10.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r10.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r10.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r10.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r10.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r10.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r10.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r10.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r10.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r10.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r10.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r10.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r10.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r10.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r10.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r10.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r10.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r10.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r10.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r10.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r10.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r10.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r10.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r10.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r9,r10,27,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x4;
	// rlwinm r7,r10,30,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x4;
	// or r6,r9,r7
	ctx.r6.u64 = ctx.r9.u64 | ctx.r7.u64;
	// lfsx f6,r8,r6
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r6.u32);
	ctx.f6.f64 = double(temp.f32);
	// fsel f5,f6,f11,f8
	ctx.f5.f64 = ctx.f6.f64 >= 0.0 ? ctx.f11.f64 : ctx.f8.f64;
	// stfs f5,160(r31)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r31.u32 + 160, temp.u32);
	// lwz r5,32(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// lwz r3,28(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 28);
	// bl 0x821ab5b8
	ctx.lr = 0x82241C48;
	sub_821AB5B8(ctx, base);
	// clrlwi r4,r3,24
	ctx.r4.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x82241c9c
	if (ctx.cr6.eq) goto loc_82241C9C;
	// lfs f13,68(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 68);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,8648(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8648);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// blt cr6,0x82241c90
	if (ctx.cr6.lt) goto loc_82241C90;
	// li r11,1
	ctx.r11.s64 = 1;
	// lfs f0,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// stb r11,100(r31)
	PPC_STORE_U8(ctx.r31.u32 + 100, ctx.r11.u8);
	// beq cr6,0x82241c90
	if (ctx.cr6.eq) goto loc_82241C90;
	// lfs f12,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,444(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 444);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,168(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 168, temp.u32);
	// stfs f31,172(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 172, temp.u32);
	// stfs f12,156(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 156, temp.u32);
	// stfs f31,164(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 164, temp.u32);
loc_82241C90:
	// lfs f0,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f0,176(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 176, temp.u32);
loc_82241C9C:
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x8224204c
	if (ctx.cr6.eq) goto loc_8224204C;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bl 0x8222c210
	ctx.lr = 0x82241CC0;
	sub_8222C210(ctx, base);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x82241ce0
	if (ctx.cr6.eq) goto loc_82241CE0;
	// lbz r11,144(r28)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82241ce4
	if (!ctx.cr6.eq) goto loc_82241CE4;
loc_82241CE0:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241CE4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242044
	if (ctx.cr6.eq) goto loc_82242044;
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// addi r27,r31,24
	ctx.r27.s64 = ctx.r31.s64 + 24;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82241d34
	if (ctx.cr6.eq) goto loc_82241D34;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82241dd0
	if (ctx.cr6.eq) goto loc_82241DD0;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82241d34
	if (ctx.cr6.eq) goto loc_82241D34;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825575c8
	ctx.lr = 0x82241D20;
	sub_825575C8(ctx, base);
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82241d38
	if (!ctx.cr6.eq) goto loc_82241D38;
loc_82241D34:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241D38:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242044
	if (ctx.cr6.eq) goto loc_82242044;
	// lwz r4,124(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 124);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 64);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82241D5C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825575c8
	ctx.lr = 0x82241D64;
	sub_825575C8(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r4,124(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 124);
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r7,64(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 64);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82241D80;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r5
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v12,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// vmsum3fp128 v11,v12,v12
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx128 v11,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822d7ea0
	ctx.lr = 0x82241DA8;
	sub_822D7EA0(ctx, base);
	// lfs f30,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f30.f64 = double(temp.f32);
	// fcmpu cr6,f30,f1
	ctx.cr6.compare(ctx.f30.f64, ctx.f1.f64);
	// ble cr6,0x82241ddc
	if (!ctx.cr6.gt) goto loc_82241DDC;
	// lfs f0,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// beq cr6,0x82241e10
	if (ctx.cr6.eq) goto loc_82241E10;
	// lfs f13,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,156(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 156, temp.u32);
	// stfs f31,164(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 164, temp.u32);
	// b 0x82241e04
	goto loc_82241E04;
loc_82241DD0:
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x821940c8
	ctx.lr = 0x82241DD8;
	sub_821940C8(ctx, base);
	// b 0x82241d34
	goto loc_82241D34;
loc_82241DDC:
	// lbz r11,100(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 100);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82241e10
	if (!ctx.cr6.eq) goto loc_82241E10;
	// lfs f13,64(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,164(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// beq cr6,0x82241e10
	if (ctx.cr6.eq) goto loc_82241E10;
	// lfs f12,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,156(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 156, temp.u32);
	// stfs f13,164(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 164, temp.u32);
loc_82241E04:
	// lfs f0,444(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 444);
	ctx.f0.f64 = double(temp.f32);
	// stfs f31,172(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 172, temp.u32);
	// stfs f0,168(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 168, temp.u32);
loc_82241E10:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// addi r11,r11,-27
	ctx.r11.s64 = ctx.r11.s64 + -27;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bgt cr6,0x82241ea4
	if (ctx.cr6.gt) goto loc_82241EA4;
	// lis r12,-32220
	ctx.r12.s64 = -2111569920;
	// addi r12,r12,7736
	ctx.r12.s64 = ctx.r12.s64 + 7736;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r11.u64) {
	case 0:
		goto loc_82241E9C;
	case 1:
		goto loc_82241E9C;
	case 2:
		goto loc_82241EA4;
	case 3:
		goto loc_82241EA4;
	case 4:
		goto loc_82241EA4;
	case 5:
		goto loc_82241EA4;
	case 6:
		goto loc_82241EA4;
	case 7:
		goto loc_82241EA4;
	case 8:
		goto loc_82241EA4;
	case 9:
		goto loc_82241EA4;
	case 10:
		goto loc_82241EA4;
	case 11:
		goto loc_82241EA4;
	case 12:
		goto loc_82241EA4;
	case 13:
		goto loc_82241EA4;
	case 14:
		goto loc_82241EA4;
	case 15:
		goto loc_82241EA4;
	case 16:
		goto loc_82241EA4;
	case 17:
		goto loc_82241EA4;
	case 18:
		goto loc_82241EA4;
	case 19:
		goto loc_82241EA4;
	case 20:
		goto loc_82241EA4;
	case 21:
		goto loc_82241EA4;
	case 22:
		goto loc_82241EA4;
	case 23:
		goto loc_82241E9C;
	case 24:
		goto loc_82241E9C;
	default:
		__builtin_unreachable();
	}
	// lwz r17,7836(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7836);
	// lwz r17,7836(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7836);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7844(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7844);
	// lwz r17,7836(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7836);
	// lwz r17,7836(r4)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r4.u32 + 7836);
loc_82241E9C:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241ea8
	goto loc_82241EA8;
loc_82241EA4:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241EA8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242040
	if (ctx.cr6.eq) goto loc_82242040;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lfs f0,-23432(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23432);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f30,f0
	ctx.cr6.compare(ctx.f30.f64, ctx.f0.f64);
	// bge cr6,0x82242040
	if (!ctx.cr6.lt) goto loc_82242040;
	// lfs f13,132(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 132);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// blt cr6,0x82242040
	if (ctx.cr6.lt) goto loc_82242040;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// bl 0x825575c8
	ctx.lr = 0x82241EDC;
	sub_825575C8(ctx, base);
	// lwz r11,52(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// rlwinm r10,r11,25,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 25) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82241fe0
	if (ctx.cr6.eq) goto loc_82241FE0;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82241f18
	if (ctx.cr6.eq) goto loc_82241F18;
	// lbz r10,135(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 135);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241fe4
	goto loc_82241FE4;
loc_82241F18:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// subf r9,r10,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82241f88
	if (!ctx.cr0.gt) goto loc_82241F88;
loc_82241F38:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,135
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 135, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82241f58
	if (ctx.cr6.lt) goto loc_82241F58;
	// li r7,0
	ctx.r7.s64 = 0;
loc_82241F58:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82241f74
	if (ctx.cr6.eq) goto loc_82241F74;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82241f7c
	goto loc_82241F7C;
loc_82241F74:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82241F7C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82241f38
	if (ctx.cr6.gt) goto loc_82241F38;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82241F88:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82241fcc
	if (ctx.cr6.eq) goto loc_82241FCC;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,135
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 135, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82241fa4
	if (ctx.cr6.gt) goto loc_82241FA4;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241FA4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82241fcc
	if (!ctx.cr6.eq) goto loc_82241FCC;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241fe4
	goto loc_82241FE4;
loc_82241FCC:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82241fe4
	goto loc_82241FE4;
loc_82241FE0:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82241FE4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242040
	if (ctx.cr6.eq) goto loc_82242040;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,3224
	ctx.r4.s64 = ctx.r11.s64 + 3224;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x82242004;
	sub_8222CF18(ctx, base);
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r4,r10,-4260
	ctx.r4.s64 = ctx.r10.s64 + -4260;
	// bl 0x82bfc950
	ctx.lr = 0x82242014;
	sub_82BFC950(ctx, base);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x821d9258
	ctx.lr = 0x82242028;
	sub_821D9258(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822ade08
	ctx.lr = 0x82242030;
	sub_822ADE08(ctx, base);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x82289530
	ctx.lr = 0x82242038;
	sub_82289530(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x82242040;
	sub_82214F08(ctx, base);
loc_82242040:
	// stfs f30,132(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r31.u32 + 132, temp.u32);
loc_82242044:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82261b30
	ctx.lr = 0x8224204C;
	sub_82261B30(ctx, base);
loc_8224204C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// lfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lfd f31,-56(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_82242060"))) PPC_WEAK_FUNC(sub_82242060);
PPC_FUNC_IMPL(__imp__sub_82242060) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82242068;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r29,1
	ctx.r29.s64 = 1;
	// cmpwi cr6,r4,8000
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 8000, ctx.xer);
	// bgt cr6,0x822420f8
	if (ctx.cr6.gt) goto loc_822420F8;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r10,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 3;
	// add r8,r10,r4
	ctx.r8.u64 = ctx.r10.u64 + ctx.r4.u64;
	// cmpwi cr6,r8,8000
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 8000, ctx.xer);
	// bgt cr6,0x822420f8
	if (ctx.cr6.gt) goto loc_822420F8;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x82242104
	if (!ctx.cr6.gt) goto loc_82242104;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// rlwinm r30,r4,3,0,28
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmpw cr6,r9,r30
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r30.s32, ctx.xer);
	// bgt cr6,0x822420d0
	if (ctx.cr6.gt) goto loc_822420D0;
	// lwz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// cmpw cr6,r4,r11
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x822420c8
	if (ctx.cr6.gt) goto loc_822420C8;
	// rlwinm r4,r11,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x822420cc
	goto loc_822420CC;
loc_822420C8:
	// add r4,r11,r4
	ctx.r4.u64 = ctx.r11.u64 + ctx.r4.u64;
loc_822420CC:
	// bl 0x82a2e140
	ctx.lr = 0x822420D0;
	sub_82A2E140(ctx, base);
loc_822420D0:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x82242108
	if (!ctx.cr6.lt) goto loc_82242108;
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_822420F8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82242104:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
loc_82242108:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82242110"))) PPC_WEAK_FUNC(sub_82242110);
PPC_FUNC_IMPL(__imp__sub_82242110) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82242118;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lis r11,-32241
	ctx.r11.s64 = -2112946176;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// addi r9,r11,7532
	ctx.r9.s64 = ctx.r11.s64 + 7532;
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// mr r28,r7
	ctx.r28.u64 = ctx.r7.u64;
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r9,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r9.u32);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// beq cr6,0x82242154
	if (ctx.cr6.eq) goto loc_82242154;
	// bl 0x821fc048
	ctx.lr = 0x82242154;
	sub_821FC048(ctx, base);
loc_82242154:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821fc270
	ctx.lr = 0x8224215C;
	sub_821FC270(ctx, base);
	// bl 0x822421d8
	ctx.lr = 0x82242160;
	sub_822421D8(ctx, base);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82242180;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822421cc
	if (ctx.cr6.eq) goto loc_822421CC;
	// bl 0x82232298
	ctx.lr = 0x82242190;
	sub_82232298(ctx, base);
	// lis r8,-31946
	ctx.r8.s64 = -2093613056;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// lwz r3,412(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 412);
	// li r8,0
	ctx.r8.s64 = 0;
	// lfs f1,-27468(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -27468);
	ctx.f1.f64 = double(temp.f32);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x822069c0
	ctx.lr = 0x822421CC;
	sub_822069C0(ctx, base);
loc_822421CC:
	// bl 0x821fc410
	ctx.lr = 0x822421D0;
	sub_821FC410(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_822421D8"))) PPC_WEAK_FUNC(sub_822421D8);
PPC_FUNC_IMPL(__imp__sub_822421D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lis r30,-31946
	ctx.r30.s64 = -2093613056;
	// addi r31,r11,-20516
	ctx.r31.s64 = ctx.r11.s64 + -20516;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242230
	if (ctx.cr6.eq) goto loc_82242230;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,412(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 412);
	// bl 0x82264550
	ctx.lr = 0x82242210;
	sub_82264550(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82242230
	if (ctx.cr6.eq) goto loc_82242230;
	// bl 0x821fc1f0
	ctx.lr = 0x82242220;
	sub_821FC1F0(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_82242230:
	// lwz r11,412(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 412);
	// lis r7,-31926
	ctx.r7.s64 = -2092302336;
	// addi r6,r7,23976
	ctx.r6.s64 = ctx.r7.s64 + 23976;
	// lfs f0,12656(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12656);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,12652(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12652);
	ctx.f13.f64 = double(temp.f32);
	// fctidz f12,f0
	ctx.f12.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f0.f64));
	// lfs f11,12660(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12660);
	ctx.f11.f64 = double(temp.f32);
	// fctidz f10,f13
	ctx.f10.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f13.f64));
	// lfs f9,12648(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12648);
	ctx.f9.f64 = double(temp.f32);
	// fctidz f8,f11
	ctx.f8.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f11.f64));
	// stfd f12,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f12.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stfd f10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f10.u64);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stfd f8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f8.u64);
	// lwz r8,84(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// fctidz f7,f9
	ctx.f7.s64 = (ctx.f9.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f9.f64));
	// stfd f7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f7.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r10,23976(r7)
	PPC_STORE_U32(ctx.r7.u32 + 23976, ctx.r10.u32);
	// add r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r11,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, ctx.r11.u32);
	// stw r10,12(r6)
	PPC_STORE_U32(ctx.r6.u32 + 12, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822422AC"))) PPC_WEAK_FUNC(sub_822422AC);
PPC_FUNC_IMPL(__imp__sub_822422AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822422B0"))) PPC_WEAK_FUNC(sub_822422B0);
PPC_FUNC_IMPL(__imp__sub_822422B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be0
	ctx.lr = 0x822422B8;
	sub_82CA2BE0(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// li r28,28
	ctx.r28.s64 = 28;
	// lwz r10,60(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 60);
	// lwz r11,16(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	// mulli r10,r10,28
	ctx.r10.s64 = ctx.r10.s64 * 28;
	// lwz r9,48(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r29,r11,8
	ctx.r29.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x822423b0
	if (!ctx.cr6.eq) goto loc_822423B0;
	// lwz r11,8(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r8,64(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 64);
	// subf r7,r10,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r10.s64;
	// divw r6,r7,r28
	ctx.r6.s32 = ctx.r7.s32 / ctx.r28.s32;
	// cmplw cr6,r8,r6
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, ctx.xer);
	// blt cr6,0x82242328
	if (ctx.cr6.lt) goto loc_82242328;
	// lwz r11,8(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// lwz r11,8(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// addi r9,r11,8
	ctx.r9.s64 = ctx.r11.s64 + 8;
	// stw r9,8(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8, ctx.r9.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c30
	// ERROR 82CA2C30
	return;
loc_82242328:
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822423b0
	if (!ctx.cr6.eq) goto loc_822423B0;
	// lwz r10,64(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 64);
	// addi r31,r30,64
	ctx.r31.s64 = ctx.r30.s64 + 64;
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// mulli r8,r10,28
	ctx.r8.s64 = ctx.r10.s64 * 28;
	// lwzx r27,r8,r11
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r11.u32);
	// subf r7,r11,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r11.s64;
	// divw r6,r7,r28
	ctx.r6.s32 = ctx.r7.s32 / ctx.r28.s32;
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// bge cr6,0x822423b0
	if (!ctx.cr6.lt) goto loc_822423B0;
loc_82242358:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mulli r9,r10,28
	ctx.r9.s64 = ctx.r10.s64 * 28;
	// lwzx r8,r9,r11
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// cmpw cr6,r8,r27
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r27.s32, ctx.xer);
	// bne cr6,0x822423b0
	if (!ctx.cr6.eq) goto loc_822423B0;
	// addi r3,r30,40
	ctx.r3.s64 = ctx.r30.s64 + 40;
	// lwz r11,44(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 44);
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x825f7b10
	ctx.lr = 0x82242388;
	sub_825F7B10(ctx, base);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// rotlwi r9,r10,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r8,8(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r11.s64;
	// divw r6,r7,r28
	ctx.r6.s32 = ctx.r7.s32 / ctx.r28.s32;
	// cmplw cr6,r9,r6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, ctx.xer);
	// blt cr6,0x82242358
	if (ctx.cr6.lt) goto loc_82242358;
loc_822423B0:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lwz r11,48(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lwz r10,26912(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26912);
	// lwz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r7,120(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 120);
	// mulli r10,r7,9377
	ctx.r10.s64 = ctx.r7.s64 * 9377;
	// addi r6,r10,9439
	ctx.r6.s64 = ctx.r10.s64 + 9439;
	// rotlwi r10,r6,19
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 19);
	// stw r10,120(r9)
	PPC_STORE_U32(ctx.r9.u32 + 120, ctx.r10.u32);
	// beq cr6,0x822423f4
	if (ctx.cr6.eq) goto loc_822423F4;
	// divwu r9,r10,r11
	ctx.r9.u32 = ctx.r10.u32 / ctx.r11.u32;
	// mullw r8,r9,r11
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// subf r4,r8,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r8.s64;
	// b 0x822423f8
	goto loc_822423F8;
loc_822423F4:
	// li r4,0
	ctx.r4.s64 = 0;
loc_822423F8:
	// lwz r11,44(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 44);
	// addi r31,r30,40
	ctx.r31.s64 = ctx.r30.s64 + 40;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r31.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// bl 0x824e0e40
	ctx.lr = 0x82242414;
	sub_824E0E40(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82242424
	if (!ctx.cr6.eq) goto loc_82242424;
	// twi 31,r0,22
loc_82242424:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x82242438
	if (!ctx.cr6.eq) goto loc_82242438;
	// twi 31,r0,22
loc_82242438:
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r31,r11,1
	ctx.r31.s64 = ctx.r11.s64 + 1;
	// bl 0x82a666a8
	ctx.lr = 0x82242450;
	sub_82A666A8(ctx, base);
	// extsw r10,r31
	ctx.r10.s64 = ctx.r31.s32;
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// li r11,3
	ctx.r11.s64 = 3;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// li r3,1
	ctx.r3.s64 = 1;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// stfs f12,0(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 0, temp.u32);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r11.u32);
	// lwz r11,8(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 8);
	// addi r8,r11,8
	ctx.r8.s64 = ctx.r11.s64 + 8;
	// stw r8,8(r26)
	PPC_STORE_U32(ctx.r26.u32 + 8, ctx.r8.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c30
	// ERROR 82CA2C30
	return;
}

__attribute__((alias("__imp__sub_8224248C"))) PPC_WEAK_FUNC(sub_8224248C);
PPC_FUNC_IMPL(__imp__sub_8224248C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82242490"))) PPC_WEAK_FUNC(sub_82242490);
PPC_FUNC_IMPL(__imp__sub_82242490) {
	PPC_FUNC_PROLOGUE();
	// lwz r4,32(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x822424f8
	if (ctx.cr6.eq) goto loc_822424F8;
	// lwz r9,16(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// rlwinm r11,r4,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x822424f8
	if (!ctx.cr6.eq) goto loc_822424F8;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplwi cr6,r4,1
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 1, ctx.xer);
	// ble cr6,0x822424f4
	if (!ctx.cr6.gt) goto loc_822424F4;
loc_822424C0:
	// add r11,r3,r4
	ctx.r11.u64 = ctx.r3.u64 + ctx.r4.u64;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bne cr6,0x822424e4
	if (!ctx.cr6.eq) goto loc_822424E4;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// b 0x822424e8
	goto loc_822424E8;
loc_822424E4:
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_822424E8:
	// subf r11,r3,r4
	ctx.r11.s64 = ctx.r4.s64 - ctx.r3.s64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bgt cr6,0x822424c0
	if (ctx.cr6.gt) goto loc_822424C0;
loc_822424F4:
	// blr 
	return;
loc_822424F8:
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// addi r9,r11,-26804
	ctx.r9.s64 = ctx.r11.s64 + -26804;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x82242514
	if (!ctx.cr6.eq) goto loc_82242514;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// blr 
	return;
loc_82242514:
	// b 0x82a2fd80
	sub_82A2FD80(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82242518"))) PPC_WEAK_FUNC(sub_82242518);
PPC_FUNC_IMPL(__imp__sub_82242518) {
	PPC_FUNC_PROLOGUE();
	// ld r10,96(r4)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r4.u32 + 96);
	// addi r11,r4,96
	ctx.r11.s64 = ctx.r4.s64 + 96;
	// ld r9,104(r4)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r4.u32 + 104);
	// std r10,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r10.u64);
	// std r9,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, ctx.r9.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82242530"))) PPC_WEAK_FUNC(sub_82242530);
PPC_FUNC_IMPL(__imp__sub_82242530) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// li r11,12
	ctx.r11.s64 = 12;
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// li r5,0
	ctx.r5.s64 = 0;
	// subf r10,r9,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r9.s64;
	// stw r5,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r5.u32);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// divw. r11,r10,r11
	ctx.r11.s32 = ctx.r10.s32 / ctx.r11.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x822425ac
	if (!ctx.cr0.gt) goto loc_822425AC;
loc_82242554:
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r7,1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 1, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8224257c
	if (ctx.cr6.lt) goto loc_8224257C;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
loc_8224257C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82242598
	if (ctx.cr6.eq) goto loc_82242598;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x822425a0
	goto loc_822425A0;
loc_82242598:
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_822425A0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82242554
	if (ctx.cr6.gt) goto loc_82242554;
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
loc_822425AC:
	// cmplw cr6,r9,r6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x822425e4
	if (ctx.cr6.eq) goto loc_822425E4;
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x822425c8
	if (ctx.cr6.gt) goto loc_822425C8;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
loc_822425C8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822425e4
	if (!ctx.cr6.eq) goto loc_822425E4;
	// ld r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r11.u64);
	// lwz r11,-12(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// b 0x822425e8
	goto loc_822425E8;
loc_822425E4:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_822425E8:
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x82242600
	if (ctx.cr6.eq) goto loc_82242600;
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82242604
	goto loc_82242604;
loc_82242600:
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
loc_82242604:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242618
	if (ctx.cr6.eq) goto loc_82242618;
	// lfs f1,16(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
loc_82242618:
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f1,-27468(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27468);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82242624"))) PPC_WEAK_FUNC(sub_82242624);
PPC_FUNC_IMPL(__imp__sub_82242624) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82242628"))) PPC_WEAK_FUNC(sub_82242628);
PPC_FUNC_IMPL(__imp__sub_82242628) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82242630;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// lwz r11,10896(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10896);
	// lwz r10,10908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10908);
	// subf r9,r30,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r30.s64;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x82242714
	if (!ctx.cr6.lt) goto loc_82242714;
	// clrlwi. r11,r6,24
	ctx.r11.u64 = ctx.r6.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82242680
	if (!ctx.cr0.eq) goto loc_82242680;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82242680
	if (!ctx.cr6.eq) goto loc_82242680;
	// lwz r11,13232(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 13232);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82242714
	if (!ctx.cr6.eq) goto loc_82242714;
	// bl 0x821e8d20
	ctx.lr = 0x82242680;
	sub_821E8D20(ctx, base);
loc_82242680:
	// lwz r11,10896(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10896);
	// lwz r10,10908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10908);
	// subf r9,r30,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r30.s64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// cmplw cr6,r9,r10
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x82242714
	if (!ctx.cr6.lt) goto loc_82242714;
	// lwz r10,256(r13)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r13.u32 + 256);
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r31.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// lwz r10,88(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r11.u32);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// mftb r11
	ctx.r11.u64 = __rdtsc();
	// lwz r10,10896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10896);
	// lwz r9,10908(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10908);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// subf r11,r30,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r30.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x8224270c
	if (!ctx.cr6.lt) goto loc_8224270C;
loc_822426E0:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82b9bf90
	ctx.lr = 0x822426E8;
	sub_82B9BF90(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq 0x8224270c
	if (ctx.cr0.eq) goto loc_8224270C;
	// lwz r11,10896(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10896);
	// lwz r10,10908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10908);
	// subf r9,r30,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r30.s64;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x822426e0
	if (ctx.cr6.lt) goto loc_822426E0;
loc_8224270C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82b9bec8
	ctx.lr = 0x82242714;
	sub_82B9BEC8(ctx, base);
loc_82242714:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8224271C"))) PPC_WEAK_FUNC(sub_8224271C);
PPC_FUNC_IMPL(__imp__sub_8224271C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82242720"))) PPC_WEAK_FUNC(sub_82242720);
PPC_FUNC_IMPL(__imp__sub_82242720) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x82242728;
	sub_82CA2BE4(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// bl 0x8221d118
	ctx.lr = 0x82242740;
	sub_8221D118(ctx, base);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f11,84(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lfs f13,88(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,-27456
	ctx.r11.s64 = ctx.r11.s64 + -27456;
	// lfs f9,80(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	ctx.f9.f64 = double(temp.f32);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// fsubs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lfs f0,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// addi r5,r10,32
	ctx.r5.s64 = ctx.r10.s64 + 32;
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f12,-396(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -396);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// lis r4,-31927
	ctx.r4.s64 = -2092367872;
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r12,1
	ctx.r12.s64 = 1;
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v12,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// fsubs f7,f11,f0
	ctx.f7.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r5.u32 + 0);
	// ld r6,8(r5)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r5.u32 + 8);
	// addi r30,r1,112
	ctx.r30.s64 = ctx.r1.s64 + 112;
	// ld r10,5520(r4)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r4.u32 + 5520);
	// rldicr r12,r12,56,63
	ctx.r12.u64 = __builtin_rotateleft64(ctx.r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// lis r28,-31946
	ctx.r28.s64 = -2093613056;
	// and r5,r10,r12
	ctx.r5.u64 = ctx.r10.u64 & ctx.r12.u64;
	// std r9,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r9.u64);
	// cmpldi cr6,r5,0
	ctx.cr6.compare<uint64_t>(ctx.r5.u64, 0, ctx.xer);
	// std r6,8(r30)
	PPC_STORE_U64(ctx.r30.u32 + 8, ctx.r6.u64);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fdivs f5,f10,f6
	ctx.f5.f64 = double(float(ctx.f10.f64 / ctx.f6.f64));
	// stfs f5,96(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v11,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vrlimi128 v13,v0,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 78), 3));
	// stvx128 v13,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x82242830
	if (ctx.cr6.eq) goto loc_82242830;
	// lwz r10,412(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 412);
	// li r12,1
	ctx.r12.s64 = 1;
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// lfs f13,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f12.f64 = double(temp.f32);
	// rldicr r12,r12,56,63
	ctx.r12.u64 = __builtin_rotateleft64(ctx.r12.u64, 56) & 0xFFFFFFFFFFFFFFFF;
	// lfs f11,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f11.f64 = double(temp.f32);
	// stfs f0,6480(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 6480, temp.u32);
	// stfs f13,6484(r9)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r9.u32 + 6484, temp.u32);
	// stfs f12,6488(r9)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r9.u32 + 6488, temp.u32);
	// stfs f11,6492(r9)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r9.u32 + 6492, temp.u32);
	// ld r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// or r7,r8,r12
	ctx.r7.u64 = ctx.r8.u64 | ctx.r12.u64;
	// std r7,8(r9)
	PPC_STORE_U64(ctx.r9.u32 + 8, ctx.r7.u64);
loc_82242830:
	// li r10,68
	ctx.r10.s64 = 68;
	// lfs f0,17804(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 17804);
	ctx.f0.f64 = double(temp.f32);
	// li r9,64
	ctx.r9.s64 = 64;
	// lfs f13,8224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8224);
	ctx.f13.f64 = double(temp.f32);
	// li r8,76
	ctx.r8.s64 = 76;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// li r7,72
	ctx.r7.s64 = 72;
	// stfs f13,100(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// li r3,398
	ctx.r3.s64 = 398;
	// lvlx v0,r31,r10
	temp.u32 = ctx.r31.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,r31,r9
	temp.u32 = ctx.r31.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,r31,r8
	temp.u32 = ctx.r31.u32 + ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,r31,r7
	temp.u32 = ctx.r31.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vor v1,v13,v13
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v13.u8));
	// vrlimi128 v1,v11,3,2
	_mm_store_ps(ctx.v1.f32, _mm_blend_ps(_mm_load_ps(ctx.v1.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// bl 0x82210418
	ctx.lr = 0x82242878;
	sub_82210418(ctx, base);
	// li r3,305
	ctx.r3.s64 = 305;
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x82204638
	ctx.lr = 0x82242884;
	sub_82204638(ctx, base);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// li r11,48
	ctx.r11.s64 = 48;
	// li r3,361
	ctx.r3.s64 = 361;
	// lvx128 v10,r0,r6
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,124(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f13,f10,f0
	ctx.f13.f64 = double(float(ctx.f10.f64 / ctx.f0.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v9,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v8,v9,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// lvx128 v7,r31,r11
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v1,v7,v8
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v8.f32)));
	// bl 0x821c56c0
	ctx.lr = 0x822428C0;
	sub_821C56C0(ctx, base);
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// addi r11,r11,-16224
	ctx.r11.s64 = ctx.r11.s64 + -16224;
	// addi r8,r10,28104
	ctx.r8.s64 = ctx.r10.s64 + 28104;
	// lis r9,-31924
	ctx.r9.s64 = -2092171264;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r3,r9,-32624
	ctx.r3.s64 = ctx.r9.s64 + -32624;
	// lbz r7,2757(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2757);
	// addi r29,r10,3056
	ctx.r29.s64 = ctx.r10.s64 + 3056;
	// lis r30,-31943
	ctx.r30.s64 = -2093416448;
	// mulli r7,r7,8200
	ctx.r7.s64 = ctx.r7.s64 * 8200;
	// add r7,r7,r3
	ctx.r7.u64 = ctx.r7.u64 + ctx.r3.u64;
	// lwz r9,12(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r10,2740(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2740);
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// addi r10,r7,-8200
	ctx.r10.s64 = ctx.r7.s64 + -8200;
	// beq cr6,0x8224298c
	if (ctx.cr6.eq) goto loc_8224298C;
	// lwz r7,8192(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r6,2744(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2744);
	// and r4,r7,r6
	ctx.r4.u64 = ctx.r7.u64 & ctx.r6.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x82242958
	if (!ctx.cr6.eq) goto loc_82242958;
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r7,r6
	ctx.r9.u64 = ctx.r7.u64 | ctx.r6.u64;
	// addi r7,r11,2736
	ctx.r7.s64 = ctx.r11.s64 + 2736;
	// rlwinm r6,r4,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,2744(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2744, ctx.r9.u32);
	// stwx r7,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r7.u32);
	// lwz r9,2740(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2740);
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r7,r4,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r9,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r9.u32);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r6,r7,1
	ctx.r6.s64 = ctx.r7.s64 + 1;
	// lwz r9,12(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
loc_82242958:
	// lbz r10,2756(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2756);
	// stw r5,2740(r11)
	PPC_STORE_U32(ctx.r11.u32 + 2740, ctx.r5.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8224298c
	if (!ctx.cr6.eq) goto loc_8224298C;
	// lwz r10,1000(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 1000);
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r7,r11,2736
	ctx.r7.s64 = ctx.r11.s64 + 2736;
	// rlwinm r6,r10,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r8,2756(r11)
	PPC_STORE_U8(ctx.r11.u32 + 2756, ctx.r8.u8);
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// stw r5,1000(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1000, ctx.r5.u32);
	// stwx r7,r6,r29
	PPC_STORE_U32(ctx.r6.u32 + ctx.r29.u32, ctx.r7.u32);
	// b 0x82242990
	goto loc_82242990;
loc_8224298C:
	// lwz r5,1000(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 1000);
loc_82242990:
	// lbz r10,3237(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3237);
	// lwz r8,3220(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3220);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// cmplw cr6,r8,r9
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82242a24
	if (ctx.cr6.eq) goto loc_82242A24;
	// lwz r8,8192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r7,3224(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3224);
	// and r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 & ctx.r7.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x822429f8
	if (!ctx.cr6.eq) goto loc_822429F8;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// addi r4,r11,3216
	ctx.r4.s64 = ctx.r11.s64 + 3216;
	// rlwinm r7,r6,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,3224(r11)
	PPC_STORE_U32(ctx.r11.u32 + 3224, ctx.r8.u32);
	// stwx r4,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r4.u32);
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r7,r6,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,3220(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3220);
	// add r4,r7,r10
	ctx.r4.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r8,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r8.u32);
	// lwz r8,8196(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stw r8,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r8.u32);
loc_822429F8:
	// lbz r10,3236(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3236);
	// stw r9,3220(r11)
	PPC_STORE_U32(ctx.r11.u32 + 3220, ctx.r9.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82242a24
	if (!ctx.cr6.eq) goto loc_82242A24;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r8,r11,3216
	ctx.r8.s64 = ctx.r11.s64 + 3216;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r10,3236(r11)
	PPC_STORE_U8(ctx.r11.u32 + 3236, ctx.r10.u8);
	// stw r5,1000(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1000, ctx.r5.u32);
	// stwx r8,r9,r29
	PPC_STORE_U32(ctx.r9.u32 + ctx.r29.u32, ctx.r8.u32);
loc_82242A24:
	// lbz r10,5157(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5157);
	// lwz r9,5140(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 5140);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82242abc
	if (ctx.cr6.eq) goto loc_82242ABC;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,5144(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 5144);
	// and r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 & ctx.r8.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x82242a8c
	if (!ctx.cr6.eq) goto loc_82242A8C;
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// addi r6,r11,5136
	ctx.r6.s64 = ctx.r11.s64 + 5136;
	// rlwinm r4,r7,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,5144(r11)
	PPC_STORE_U32(ctx.r11.u32 + 5144, ctx.r9.u32);
	// stwx r6,r4,r10
	PPC_STORE_U32(ctx.r4.u32 + ctx.r10.u32, ctx.r6.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r9,5140(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 5140);
	// stw r9,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r9.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r7,r9,1
	ctx.r7.s64 = ctx.r9.s64 + 1;
	// stw r7,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r7.u32);
loc_82242A8C:
	// lbz r10,5156(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5156);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r9,5140(r11)
	PPC_STORE_U32(ctx.r11.u32 + 5140, ctx.r9.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82242abc
	if (!ctx.cr6.eq) goto loc_82242ABC;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r8,r11,5136
	ctx.r8.s64 = ctx.r11.s64 + 5136;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r10,5156(r11)
	PPC_STORE_U8(ctx.r11.u32 + 5156, ctx.r10.u8);
	// stw r5,1000(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1000, ctx.r5.u32);
	// stwx r8,r9,r29
	PPC_STORE_U32(ctx.r9.u32 + ctx.r29.u32, ctx.r8.u32);
loc_82242ABC:
	// lis r9,-31951
	ctx.r9.s64 = -2093940736;
	// lbz r10,4677(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4677);
	// lwz r7,4660(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4660);
	// addi r8,r9,28240
	ctx.r8.s64 = ctx.r9.s64 + 28240;
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// cmplw cr6,r7,r9
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82242b64
	if (ctx.cr6.eq) goto loc_82242B64;
	// lwz r7,8192(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r6,4664(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4664);
	// and r27,r7,r6
	ctx.r27.u64 = ctx.r7.u64 & ctx.r6.u64;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// bne cr6,0x82242b38
	if (!ctx.cr6.eq) goto loc_82242B38;
	// lwz r27,8196(r10)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r7,r6
	ctx.r9.u64 = ctx.r7.u64 | ctx.r6.u64;
	// addi r7,r11,4656
	ctx.r7.s64 = ctx.r11.s64 + 4656;
	// rlwinm r6,r27,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,4664(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4664, ctx.r9.u32);
	// stwx r7,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r7.u32);
	// lwz r9,4660(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4660);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r9,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r9.u32);
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r8,r7,1
	ctx.r8.s64 = ctx.r7.s64 + 1;
	// stw r8,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r8.u32);
loc_82242B38:
	// lbz r10,4676(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4676);
	// stw r4,4660(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4660, ctx.r4.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82242b64
	if (!ctx.cr6.eq) goto loc_82242B64;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r7,r11,4656
	ctx.r7.s64 = ctx.r11.s64 + 4656;
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stb r10,4676(r11)
	PPC_STORE_U8(ctx.r11.u32 + 4676, ctx.r10.u8);
	// stw r5,1000(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1000, ctx.r5.u32);
	// stwx r7,r8,r29
	PPC_STORE_U32(ctx.r8.u32 + ctx.r29.u32, ctx.r7.u32);
loc_82242B64:
	// lbz r10,4197(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4197);
	// lwz r8,4180(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4180);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r3
	ctx.r10.u64 = ctx.r10.u64 + ctx.r3.u64;
	// cmplw cr6,r8,r9
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82242bf8
	if (ctx.cr6.eq) goto loc_82242BF8;
	// lwz r8,8192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r7,4184(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4184);
	// and r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 & ctx.r7.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x82242bcc
	if (!ctx.cr6.eq) goto loc_82242BCC;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// addi r4,r11,4176
	ctx.r4.s64 = ctx.r11.s64 + 4176;
	// rlwinm r3,r6,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,4184(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4184, ctx.r8.u32);
	// stwx r4,r3,r10
	PPC_STORE_U32(ctx.r3.u32 + ctx.r10.u32, ctx.r4.u32);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,4180(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4180);
	// add r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r8,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r8.u32);
	// lwz r8,8196(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r4,r8,1
	ctx.r4.s64 = ctx.r8.s64 + 1;
	// stw r4,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r4.u32);
loc_82242BCC:
	// lbz r10,4196(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4196);
	// stw r9,4180(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4180, ctx.r9.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82242bf8
	if (!ctx.cr6.eq) goto loc_82242BF8;
	// rlwinm r8,r5,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r7,r11,4176
	ctx.r7.s64 = ctx.r11.s64 + 4176;
	// addi r9,r5,1
	ctx.r9.s64 = ctx.r5.s64 + 1;
	// stb r10,4196(r11)
	PPC_STORE_U8(ctx.r11.u32 + 4196, ctx.r10.u8);
	// stw r9,1000(r30)
	PPC_STORE_U32(ctx.r30.u32 + 1000, ctx.r9.u32);
	// stwx r7,r8,r29
	PPC_STORE_U32(ctx.r8.u32 + ctx.r29.u32, ctx.r7.u32);
loc_82242BF8:
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,412(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 412);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// oris r6,r6,32768
	ctx.r6.u64 = ctx.r6.u64 | 2147483648;
	// bl 0x821b7020
	ctx.lr = 0x82242C10;
	sub_821B7020(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_82242C18"))) PPC_WEAK_FUNC(sub_82242C18);
PPC_FUNC_IMPL(__imp__sub_82242C18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bc0
	ctx.lr = 0x82242C20;
	sub_82CA2BC0(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// addi r19,r11,27824
	ctx.r19.s64 = ctx.r11.s64 + 27824;
	// mr r22,r4
	ctx.r22.u64 = ctx.r4.u64;
	// lwz r11,27824(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 27824);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82242e2c
	if (ctx.cr6.eq) goto loc_82242E2C;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// addi r11,r11,-32624
	ctx.r11.s64 = ctx.r11.s64 + -32624;
	// addi r7,r10,-9360
	ctx.r7.s64 = ctx.r10.s64 + -9360;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r6,r11,8200
	ctx.r6.s64 = ctx.r11.s64 + 8200;
	// stb r5,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r5.u8);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stwx r7,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, ctx.r7.u32);
	// lwz r9,8196(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// lwz r8,8192(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r10,16396(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16396);
	// rlwinm r5,r10,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r9,1
	ctx.r10.s64 = ctx.r9.s64 + 1;
	// rlwinm r9,r8,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
	// stw r9,8192(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8192, ctx.r9.u32);
	// stwx r7,r5,r6
	PPC_STORE_U32(ctx.r5.u32 + ctx.r6.u32, ctx.r7.u32);
	// lwz r10,16396(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16396);
	// lwz r9,16392(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16392);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r9,16392(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16392, ctx.r9.u32);
	// stw r10,16396(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16396, ctx.r10.u32);
	// bl 0x821c4ab0
	ctx.lr = 0x82242CA8;
	sub_821C4AB0(ctx, base);
	// lwz r4,0(r19)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	// li r25,0
	ctx.r25.s64 = 0;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// mr r21,r25
	ctx.r21.u64 = ctx.r25.u64;
	// beq cr6,0x82242e1c
	if (ctx.cr6.eq) goto loc_82242E1C;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// addi r30,r23,20
	ctx.r30.s64 = ctx.r23.s64 + 20;
	// addi r24,r11,16880
	ctx.r24.s64 = ctx.r11.s64 + 16880;
	// lis r11,-31946
	ctx.r11.s64 = -2093613056;
	// li r31,24
	ctx.r31.s64 = 24;
	// li r28,4
	ctx.r28.s64 = 4;
	// li r26,12
	ctx.r26.s64 = 12;
	// li r27,8
	ctx.r27.s64 = 8;
	// addi r20,r11,412
	ctx.r20.s64 = ctx.r11.s64 + 412;
loc_82242CE0:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r29,r30,4
	ctx.r29.s64 = ctx.r30.s64 + 4;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// divw. r8,r9,r31
	ctx.r8.s32 = ctx.r9.s32 / ctx.r31.s32;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x82242e04
	if (ctx.cr0.eq) goto loc_82242E04;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// divw r5,r10,r31
	ctx.r5.s32 = ctx.r10.s32 / ctx.r31.s32;
	// stw r25,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r25.u32);
	// stw r25,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r25.u32);
	// ld r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// ld r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x8228e688
	ctx.lr = 0x82242D1C;
	sub_8228E688(ctx, base);
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x82242720
	ctx.lr = 0x82242D28;
	sub_82242720(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x822150e0
	ctx.lr = 0x82242D34;
	sub_822150E0(ctx, base);
	// lbz r9,1802(r22)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r22.u32 + 1802);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x82242d44
	if (!ctx.cr6.eq) goto loc_82242D44;
	// bl 0x8226e048
	ctx.lr = 0x82242D44;
	sub_8226E048(ctx, base);
loc_82242D44:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// divw r8,r9,r31
	ctx.r8.s32 = ctx.r9.s32 / ctx.r31.s32;
	// rlwinm r18,r8,2,0,29
	ctx.r18.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8221e5d0
	ctx.lr = 0x82242D5C;
	sub_8221E5D0(ctx, base);
	// li r6,4
	ctx.r6.s64 = 4;
	// lwz r3,0(r20)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// mr r5,r18
	ctx.r5.u64 = ctx.r18.u64;
	// li r4,13
	ctx.r4.s64 = 13;
	// bl 0x822060a0
	ctx.lr = 0x82242D70;
	sub_822060A0(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82242e04
	if (ctx.cr6.eq) goto loc_82242E04;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// subf r10,r9,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r9.s64;
	// divw. r7,r10,r31
	ctx.r7.s32 = ctx.r10.s32 / ctx.r31.s32;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq 0x82242df8
	if (ctx.cr0.eq) goto loc_82242DF8;
	// vspltisw v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x0)));
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_82242D98:
	// add r10,r11,r9
	ctx.r10.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lvlx v13,r11,r9
	temp.u32 = ctx.r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor v12,v0,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// lvlx v11,r10,r28
	temp.u32 = ctx.r10.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,r10,r26
	temp.u32 = ctx.r10.u32 + ctx.r26.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v11,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// lvlx v9,r10,r27
	temp.u32 = ctx.r10.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v9,v10,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v13,v9,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 78), 3));
	// vpkd3d128 v12,v13,1,2,2
	ctx.fpscr.enableFlushMode();
	__builtin_debugtrap();
	// vspltw v8,v12,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// vspltw v7,v12,1
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xAA));
	// stvewx v8,r0,r9
	ea = (ctx.r9.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v8.u32[3 - ((ea & 0xF) >> 2)]);
	// stvewx v7,r9,r28
	ea = (ctx.r9.u32 + ctx.r28.u32) & ~0x3;
	PPC_STORE_U32(ea, ctx.v7.u32[3 - ((ea & 0xF) >> 2)]);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r7,0(r29)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// subf r6,r9,r7
	ctx.r6.s64 = ctx.r7.s64 - ctx.r9.s64;
	// divw r5,r6,r31
	ctx.r5.s32 = ctx.r6.s32 / ctx.r31.s32;
	// cmplw cr6,r8,r5
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r5.u32, ctx.xer);
	// blt cr6,0x82242d98
	if (ctx.cr6.lt) goto loc_82242D98;
loc_82242DF8:
	// lwz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// lwz r10,13444(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 13444);
	// stw r10,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r10.u32);
loc_82242E04:
	// lwz r11,0(r19)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r19.u32 + 0);
	// addi r21,r21,1
	ctx.r21.s64 = ctx.r21.s64 + 1;
	// addi r30,r30,16
	ctx.r30.s64 = ctx.r30.s64 + 16;
	// addi r24,r24,96
	ctx.r24.s64 = ctx.r24.s64 + 96;
	// cmplw cr6,r21,r11
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x82242ce0
	if (ctx.cr6.lt) goto loc_82242CE0;
loc_82242E1C:
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x821bc370
	ctx.lr = 0x82242E24;
	sub_821BC370(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8221f410
	ctx.lr = 0x82242E2C;
	sub_8221F410(ctx, base);
loc_82242E2C:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c10
	// ERROR 82CA2C10
	return;
}

__attribute__((alias("__imp__sub_82242E34"))) PPC_WEAK_FUNC(sub_82242E34);
PPC_FUNC_IMPL(__imp__sub_82242E34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82242E38"))) PPC_WEAK_FUNC(sub_82242E38);
PPC_FUNC_IMPL(__imp__sub_82242E38) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,4(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r11,44
	ctx.r9.s64 = ctx.r11.s64 + 44;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r8,r3
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r3.u32);
	// lwzx r3,r7,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82242E54"))) PPC_WEAK_FUNC(sub_82242E54);
PPC_FUNC_IMPL(__imp__sub_82242E54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82242E58"))) PPC_WEAK_FUNC(sub_82242E58);
PPC_FUNC_IMPL(__imp__sub_82242E58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// rlwinm r4,r3,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
loc_82242E70:
	// lfs f0,0(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// lfs f13,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// ble cr6,0x82242eb8
	if (!ctx.cr6.gt) goto loc_82242EB8;
	// addi r7,r5,4
	ctx.r7.s64 = ctx.r5.s64 + 4;
	// addi r8,r10,4
	ctx.r8.s64 = ctx.r10.s64 + 4;
	// addi r9,r3,-1
	ctx.r9.s64 = ctx.r3.s64 + -1;
loc_82242E94:
	// lfs f0,0(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addic. r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lfs f13,0(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addi r7,r7,4
	ctx.r7.s64 = ctx.r7.s64 + 4;
	// fmadds f0,f0,f13,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f12.f64));
	// stfs f0,0(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 0, temp.u32);
	// bne 0x82242e94
	if (!ctx.cr0.eq) goto loc_82242E94;
loc_82242EB8:
	// addic. r6,r6,-1
	ctx.xer.ca = ctx.r6.u32 > 0;
	ctx.r6.s64 = ctx.r6.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// bne 0x82242e70
	if (!ctx.cr0.eq) goto loc_82242E70;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82242ECC"))) PPC_WEAK_FUNC(sub_82242ECC);
PPC_FUNC_IMPL(__imp__sub_82242ECC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82242ED0"))) PPC_WEAK_FUNC(sub_82242ED0);
PPC_FUNC_IMPL(__imp__sub_82242ED0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82242ED8;
	sub_82CA2BEC(ctx, base);
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82ca74d0
	ctx.lr = 0x82242EE0;
	sub_82CA74D0(ctx, base);
	// ld r12,-4096(r1)
	ctx.r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// stwu r1,-5872(r1)
	ea = -5872 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// addi r6,r31,4
	ctx.r6.s64 = ctx.r31.s64 + 4;
	// lfs f27,8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f27.f64 = double(temp.f32);
	// addi r5,r30,4
	ctx.r5.s64 = ctx.r30.s64 + 4;
	// lfs f26,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f26.f64 = double(temp.f32);
	// fneg f0,f27
	ctx.f0.u64 = ctx.f27.u64 ^ 0x8000000000000000;
	// lfs f28,32(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	ctx.f28.f64 = double(temp.f32);
	// fneg f13,f26
	ctx.f13.u64 = ctx.f26.u64 ^ 0x8000000000000000;
	// lfs f31,36(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	ctx.f31.f64 = double(temp.f32);
	// fneg f12,f28
	ctx.f12.u64 = ctx.f28.u64 ^ 0x8000000000000000;
	// lfs f20,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f20.f64 = double(temp.f32);
	// fneg f11,f31
	ctx.f11.u64 = ctx.f31.u64 ^ 0x8000000000000000;
	// lfs f30,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f30.f64 = double(temp.f32);
	// addi r4,r1,5200
	ctx.r4.s64 = ctx.r1.s64 + 5200;
	// lfs f19,16(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	ctx.f19.f64 = double(temp.f32);
	// li r3,3
	ctx.r3.s64 = 3;
	// lfs f29,20(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	ctx.f29.f64 = double(temp.f32);
	// lfs f18,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f18.f64 = double(temp.f32);
	// lfs f10,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// stfs f20,5232(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5232, temp.u32);
	// stfs f30,5208(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5208, temp.u32);
	// stfs f0,5220(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5220, temp.u32);
	// stfs f19,5224(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5224, temp.u32);
	// stfs f29,5200(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5200, temp.u32);
	// stfs f13,5212(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5212, temp.u32);
	// stfs f12,5228(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5228, temp.u32);
	// stfs f11,5204(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5204, temp.u32);
	// stfs f18,5216(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5216, temp.u32);
	// stfs f10,0(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// stfs f27,156(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// stfs f26,196(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// stfs f28,120(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f20,320(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 320, temp.u32);
	// stfs f30,236(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 236, temp.u32);
	// stfs f19,312(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 312, temp.u32);
	// stfs f29,332(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 332, temp.u32);
	// stfs f18,80(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// bl 0x82242e58
	ctx.lr = 0x82242F90;
	sub_82242E58(ctx, base);
	// cmplwi cr6,r29,2
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 2, ctx.xer);
	// bgt cr6,0x82242f9c
	if (ctx.cr6.gt) goto loc_82242F9C;
	// b 0x8225dc2c
	goto loc_8225DC2C;
loc_82242F9C:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// fmuls f16,f28,f20
	ctx.fpscr.disableFlushMode();
	ctx.f16.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f16,692(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 692, temp.u32);
	// fmuls f21,f19,f20
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// fmuls f16,f29,f27
	ctx.f16.f64 = double(float(ctx.f29.f64 * ctx.f27.f64));
	// stfs f16,376(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 376, temp.u32);
	// fmuls f13,f29,f30
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// lfd f0,3568(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 3568);
	// fmuls f11,f28,f19
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f19.f64));
	// lfd f9,-19408(r10)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r10.u32 + -19408);
	// fsqrts f10,f0
	ctx.f10.f64 = double(float(sqrt(ctx.f0.f64)));
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lfs f0,-19020(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19020);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f21,192(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// stfd f21,5104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 5104, ctx.f21.u64);
	// lfs f8,2704(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2704);
	ctx.f8.f64 = double(temp.f32);
	// stfs f13,284(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 284, temp.u32);
	// lfs f7,3056(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 3056);
	ctx.f7.f64 = double(temp.f32);
	// lfs f12,-19024(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19024);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,380(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 380, temp.u32);
	// stfs f10,596(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 596, temp.u32);
	// stfs f8,348(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 348, temp.u32);
	// stfs f7,124(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f0,544(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// fsqrts f9,f9
	ctx.f9.f64 = double(float(sqrt(ctx.f9.f64)));
	// fmuls f4,f9,f10
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmuls f16,f26,f27
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// stfs f16,568(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 568, temp.u32);
	// fmuls f6,f31,f20
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// stfs f6,5096(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5096, temp.u32);
	// fmuls f16,f18,f18
	ctx.f16.f64 = double(float(ctx.f18.f64 * ctx.f18.f64));
	// stfs f16,132(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// fmuls f5,f31,f19
	ctx.f5.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// fmuls f16,f30,f20
	ctx.f16.f64 = double(float(ctx.f30.f64 * ctx.f20.f64));
	// stfs f16,356(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 356, temp.u32);
	// fmuls f16,f29,f19
	ctx.f16.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// stfs f16,248(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 248, temp.u32);
	// fmuls f16,f21,f0
	ctx.f16.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// fmuls f9,f28,f29
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// stfs f9,536(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 536, temp.u32);
	// fmuls f21,f27,f27
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f27.f64));
	// stfs f21,168(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// fmuls f3,f4,f18
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f18.f64));
	// fmuls f2,f4,f31
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fmuls f9,f28,f30
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// stfs f9,584(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 584, temp.u32);
	// fmuls f21,f26,f26
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f26.f64));
	// stfs f21,240(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 240, temp.u32);
	// fmuls f1,f13,f4
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f4.f64));
	// lfs f21,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f17,f11,f4
	ctx.f17.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// fmuls f10,f31,f30
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// stfs f10,864(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 864, temp.u32);
	// fmuls f24,f20,f20
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f20.f64));
	// stfs f24,288(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// fmuls f23,f30,f30
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f30.f64));
	// stfs f23,232(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 232, temp.u32);
	// fmuls f22,f19,f19
	ctx.f22.f64 = double(float(ctx.f19.f64 * ctx.f19.f64));
	// stfs f22,428(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 428, temp.u32);
	// fmuls f15,f3,f26
	ctx.f15.f64 = double(float(ctx.f3.f64 * ctx.f26.f64));
	// fmuls f8,f19,f30
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f30.f64));
	// stfs f8,268(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 268, temp.u32);
	// fmuls f7,f29,f20
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f20.f64));
	// stfs f7,208(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// fmuls f9,f31,f29
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f29.f64));
	// stfs f9,1104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1104, temp.u32);
	// fmuls f11,f31,f31
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f31.f64));
	// stfs f11,140(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// fmuls f13,f29,f29
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f29.f64));
	// stfs f13,272(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// fmuls f25,f28,f28
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f28.f64));
	// stfs f25,160(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// fneg f5,f5
	ctx.f5.u64 = ctx.f5.u64 ^ 0x8000000000000000;
	// fneg f6,f6
	ctx.f6.u64 = ctx.f6.u64 ^ 0x8000000000000000;
	// fmuls f2,f2,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// fmuls f14,f3,f27
	ctx.f14.f64 = double(float(ctx.f3.f64 * ctx.f27.f64));
	// fmuls f15,f15,f12
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f12.f64));
	// stfd f22,5064(r1)
	PPC_STORE_U64(ctx.r1.u32 + 5064, ctx.f22.u64);
	// fadds f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 + ctx.f22.f64));
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// stfs f22,524(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// lfs f22,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f14,f14,f12
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f12.f64));
	// fmsubs f12,f22,f12,f16
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f12.f64 - ctx.f16.f64));
	// stfs f12,580(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// stfd f11,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f11.u64);
	// fadds f8,f7,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// stfd f24,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f24.u64);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f12,2948(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2948);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// stfd f12,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f12.u64);
	// fmuls f12,f31,f27
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// stfd f13,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f13.u64);
	// fmuls f21,f21,f4
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// lfs f7,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f22,f26,f30
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// lfs f13,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f7,f5,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 - ctx.f7.f64));
	// lfs f5,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f5.f64 = double(temp.f32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// lfs f16,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f16.f64 = double(temp.f32);
	// fsubs f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// stfs f11,404(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f2,f2,f16
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// lfs f11,3052(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3052);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f19,f19,f27
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// stfs f6,5212(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5212, temp.u32);
	// fmuls f6,f1,f0
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f24,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f1,f17,f0,f15
	ctx.f1.f64 = double(float(ctx.f17.f64 * ctx.f0.f64 - ctx.f15.f64));
	// stfs f11,500(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fnmsubs f17,f25,f13,f24
	ctx.f17.f64 = double(float(-(ctx.f25.f64 * ctx.f13.f64 - ctx.f24.f64)));
	// lfs f16,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f13,f23,f4
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// lfs f11,760(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 760);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f9,f9,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lfs f5,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f5.f64 = double(temp.f32);
	// addi r6,r31,16
	ctx.r6.s64 = ctx.r31.s64 + 16;
	// stfs f14,444(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fmuls f14,f18,f29
	ctx.f14.f64 = double(float(ctx.f18.f64 * ctx.f29.f64));
	// stfs f11,508(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f18,f18,f30
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f30.f64));
	// stfs f21,448(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fsubs f5,f16,f5
	ctx.f5.f64 = double(float(ctx.f16.f64 - ctx.f5.f64));
	// stfs f12,516(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 516, temp.u32);
	// fmuls f16,f26,f20
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f20.f64));
	// lfs f21,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f21.f64 = double(temp.f32);
	// lfd f11,720(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfd f12,704(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// fmuls f11,f11,f4
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// lfd f24,1096(r1)
	ctx.f24.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// stfd f10,2216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 2216, ctx.f10.u64);
	// fmuls f10,f31,f26
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// stfd f29,3720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3720, ctx.f29.u64);
	// fmuls f29,f26,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// stfd f30,3712(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3712, ctx.f30.u64);
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// stfs f8,5200(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5200, temp.u32);
	// stfd f26,3680(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3680, ctx.f26.u64);
	// fmuls f26,f28,f26
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// stfd f27,3696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3696, ctx.f27.u64);
	// stfs f7,5204(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5204, temp.u32);
	// fneg f7,f21
	ctx.f7.u64 = ctx.f21.u64 ^ 0x8000000000000000;
	// stfs f2,5208(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5208, temp.u32);
	// fmuls f2,f3,f31
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f18,1148(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1148, temp.u32);
	// fmuls f18,f24,f12
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f12.f64));
	// stfs f13,696(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// lfs f8,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f8.f64 = double(temp.f32);
	// lfs f27,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f27.f64 = double(temp.f32);
	// lfd f13,1696(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// stfs f22,372(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 372, temp.u32);
	// stfs f10,632(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 632, temp.u32);
	// stfs f14,2224(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2224, temp.u32);
	// stfs f29,316(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 316, temp.u32);
	// stfs f30,416(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 416, temp.u32);
	// stfs f5,5216(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5216, temp.u32);
	// stfs f16,300(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 300, temp.u32);
	// fadds f5,f13,f24
	ctx.f5.f64 = double(float(ctx.f13.f64 + ctx.f24.f64));
	// lfs f16,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f8,f8,f4
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfs f27,404(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f16,f23,f16
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f16.f64));
	// stfs f19,344(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 344, temp.u32);
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// stfs f26,456(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 456, temp.u32);
	// addi r5,r30,16
	ctx.r5.s64 = ctx.r30.s64 + 16;
	// lfs f19,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f19.f64 = double(temp.f32);
	// addi r4,r1,5200
	ctx.r4.s64 = ctx.r1.s64 + 5200;
	// lfd f26,3680(r1)
	ctx.f26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3680);
	// li r3,5
	ctx.r3.s64 = 5;
	// lfs f15,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f19,f26,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f19.f64));
	// lfd f27,3696(r1)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3696);
	// stfs f15,216(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// fmuls f20,f27,f20
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// stfs f19,352(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 352, temp.u32);
	// lfs f15,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f14.f64 = double(temp.f32);
	// stfs f20,496(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 496, temp.u32);
	// stfs f12,540(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmsubs f12,f15,f0,f14
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f0.f64 - ctx.f14.f64));
	// lfd f10,2216(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 2216);
	// fmr f14,f30
	ctx.f14.f64 = ctx.f30.f64;
	// lfs f15,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f10,f10,f4
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// stfs f15,592(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// lfs f15,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f13,f15,f4
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f4.f64));
	// stfs f13,508(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f15,f28,f27
	ctx.f15.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// stfs f15,400(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// lfs f13,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f13.f64 = double(temp.f32);
	// lfs f19,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f5,f5,f13,f16
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 - ctx.f16.f64));
	// fmadds f20,f19,f0,f18
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 + ctx.f18.f64));
	// lfs f15,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f0,f25,f4
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// stfs f0,404(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// lfs f0,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f6,f15,f4,f6
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f4.f64 - ctx.f6.f64));
	// fmadds f9,f9,f0,f1
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f1.f64));
	// lfs f19,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f19.f64 = double(temp.f32);
	// fmr f1,f22
	ctx.f1.f64 = ctx.f22.f64;
	// lfd f22,5064(r1)
	ctx.f22.u64 = PPC_LOAD_U64(ctx.r1.u32 + 5064);
	// fmadds f12,f10,f0,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f10,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f10.f64 = double(temp.f32);
	// fneg f19,f19
	ctx.f19.u64 = ctx.f19.u64 ^ 0x8000000000000000;
	// lfs f18,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f11,f11,f18
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// lfs f18,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f18.f64 = double(temp.f32);
	// fmr f15,f29
	ctx.f15.f64 = ctx.f29.f64;
	// lfs f16,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f5,f22,f13,f5
	ctx.f5.f64 = double(float(-(ctx.f22.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// lfs f29,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f4,f20,f4
	ctx.f4.f64 = double(float(ctx.f20.f64 * ctx.f4.f64));
	// lfs f20,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f2,f2,f18
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f18.f64));
	// lfs f18,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f18.f64 = double(temp.f32);
	// fsubs f7,f7,f1
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f1.f64));
	// lfs f1,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f1.f64 = double(temp.f32);
	// fadds f10,f10,f1
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f1.f64));
	// lfs f1,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f1.f64 = double(temp.f32);
	// fadds f1,f1,f20
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f20.f64));
	// lfs f20,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f13,f20,f13,f17
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f13.f64 - ctx.f17.f64)));
	// lfs f17,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f21,f16,f18,f17
	ctx.f21.f64 = double(float(ctx.f16.f64 * ctx.f18.f64 + ctx.f17.f64));
	// lfs f18,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f3,f3,f18
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f18.f64));
	// lfs f18,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f18.f64 = double(temp.f32);
	// fsubs f29,f19,f29
	ctx.f29.f64 = double(float(ctx.f19.f64 - ctx.f29.f64));
	// lfs f19,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f8,f8,f0,f4
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 - ctx.f4.f64));
	// lfs f0,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f4,f16,f19,f18
	ctx.f4.f64 = double(float(ctx.f16.f64 * ctx.f19.f64 + ctx.f18.f64));
	// lfs f18,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f18.f64 = double(temp.f32);
	// fsubs f30,f15,f14
	ctx.f30.f64 = double(float(ctx.f15.f64 - ctx.f14.f64));
	// stfs f21,500(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fsubs f0,f0,f18
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f18.f64));
	// lfs f19,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f19.f64 = double(temp.f32);
	// lfd f21,5104(r1)
	ctx.f21.u64 = PPC_LOAD_U64(ctx.r1.u32 + 5104);
	// stfd f27,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f27.u64);
	// fsubs f18,f21,f19
	ctx.f18.f64 = double(float(ctx.f21.f64 - ctx.f19.f64));
	// lfs f27,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f27.f64 = double(temp.f32);
	// stfs f0,404(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// stfs f18,448(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// lfs f17,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f18.f64 = double(temp.f32);
	// fsubs f17,f17,f18
	ctx.f17.f64 = double(float(ctx.f17.f64 - ctx.f18.f64));
	// stfs f17,444(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// stfs f13,5248(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5248, temp.u32);
	// stfs f9,5244(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5244, temp.u32);
	// lfs f0,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f9.f64 = double(temp.f32);
	// stfd f31,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f31.u64);
	// lfs f31,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f31.f64 = double(temp.f32);
	// stfs f12,5252(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5252, temp.u32);
	// fmsubs f11,f27,f31,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f31.f64 - ctx.f11.f64));
	// stfd f28,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f28.u64);
	// lfs f12,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f12.f64 = double(temp.f32);
	// lfs f28,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f28.f64 = double(temp.f32);
	// lfs f13,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,5264(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5264, temp.u32);
	// lfs f17,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f17.f64 = double(temp.f32);
	// fsubs f31,f28,f17
	ctx.f31.f64 = double(float(ctx.f28.f64 - ctx.f17.f64));
	// stfs f7,5220(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5220, temp.u32);
	// stfs f10,5224(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5224, temp.u32);
	// lfs f13,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,5276(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5276, temp.u32);
	// lfs f13,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,5280(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5280, temp.u32);
	// lfs f13,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,5284(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5284, temp.u32);
	// fnmsubs f13,f9,f0,f8
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// stfs f2,5228(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5228, temp.u32);
	// stfs f1,5232(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5232, temp.u32);
	// stfs f30,5236(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5236, temp.u32);
	// stfs f6,5240(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5240, temp.u32);
	// stfs f29,5260(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5260, temp.u32);
	// stfs f3,5268(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5268, temp.u32);
	// stfs f4,5272(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5272, temp.u32);
	// stfs f11,5288(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5288, temp.u32);
	// stfs f31,5292(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5292, temp.u32);
	// fmadds f0,f12,f0,f13
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f5,5296(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5296, temp.u32);
	// stfs f0,5256(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5256, temp.u32);
	// bl 0x82242e58
	ctx.lr = 0x82243430;
	sub_82242E58(ctx, base);
	// lfd f29,3720(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3720);
	// lfd f30,3712(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3712);
	// cmplwi cr6,r29,3
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 3, ctx.xer);
	// lfd f31,720(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfd f28,704(r1)
	ctx.f28.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfd f27,1096(r1)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// bgt cr6,0x82243450
	if (ctx.cr6.gt) goto loc_82243450;
	// b 0x8225dc2c
	goto loc_8225DC2C;
loc_82243450:
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f0,268(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// fmuls f9,f0,f28
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfs f0,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f0.f64 = double(temp.f32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lfs f0,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f0.f64 = double(temp.f32);
	// lfs f6,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f6.f64 = double(temp.f32);
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// lfd f13,-19424(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + -19424);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// fsqrts f7,f13
	ctx.f7.f64 = double(float(sqrt(ctx.f13.f64)));
	// lfd f13,-19528(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + -19528);
	// lfd f0,3552(r9)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r9.u32 + 3552);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f6,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f6.f64 = double(temp.f32);
	// stfs f9,964(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// lfd f11,2352(r8)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r8.u32 + 2352);
	// lfs f6,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f6.f64 = double(temp.f32);
	// lfd f10,-19416(r7)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r7.u32 + -19416);
	// lfs f6,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f6.f64 = double(temp.f32);
	// lfs f12,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f12.f64 = double(temp.f32);
	// stfd f24,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f24.u64);
	// stfd f27,3680(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3680, ctx.f27.u64);
	// stfd f26,2216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 2216, ctx.f26.u64);
	// stfd f29,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f29.u64);
	// fsqrts f9,f13
	ctx.f9.f64 = double(float(sqrt(ctx.f13.f64)));
	// stfs f9,444(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fsqrts f13,f0
	ctx.f13.f64 = double(float(sqrt(ctx.f0.f64)));
	// stfs f13,328(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 328, temp.u32);
	// fsqrts f0,f11
	ctx.f0.f64 = double(float(sqrt(ctx.f11.f64)));
	// stfs f0,396(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 396, temp.u32);
	// stfd f0,5064(r1)
	PPC_STORE_U64(ctx.r1.u32 + 5064, ctx.f0.u64);
	// fmuls f6,f0,f13
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fsqrts f10,f10
	ctx.f10.f64 = double(float(sqrt(ctx.f10.f64)));
	// fmuls f8,f22,f29
	ctx.f8.f64 = double(float(ctx.f22.f64 * ctx.f29.f64));
	// stfs f8,676(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 676, temp.u32);
	// fmuls f14,f7,f13
	ctx.f14.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// stfs f14,448(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmuls f8,f24,f29
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f29.f64));
	// stfs f8,544(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// fmuls f13,f9,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f13,540(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmuls f11,f23,f29
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f29.f64));
	// stfs f11,768(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// fmuls f1,f7,f24
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f24.f64));
	// lfs f24,3196(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3196);
	ctx.f24.f64 = double(temp.f32);
	// lfs f8,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f13,f0,f26
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f26.f64));
	// fmuls f11,f12,f29
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// stfs f24,100(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f11,852(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 852, temp.u32);
	// fmuls f5,f9,f31
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// stfs f13,404(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f4,f9,f20
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f20.f64));
	// fmuls f24,f25,f29
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f26,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f26.f64 = double(temp.f32);
	// lfs f9,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f11,f21,f30
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// lfs f13,2708(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2708);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f3,f7,f22
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f22.f64));
	// fmuls f2,f7,f23
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f23.f64));
	// fmuls f15,f7,f12
	ctx.f15.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f19,f19,f31
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// fmuls f27,f7,f6
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f6.f64));
	// fmuls f8,f8,f28
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f28.f64));
	// fmuls f14,f21,f31
	ctx.f14.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// fmuls f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f29,f20,f29
	ctx.f29.f64 = double(float(ctx.f20.f64 * ctx.f29.f64));
	// stfs f1,500(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fmr f1,f12
	ctx.f1.f64 = ctx.f12.f64;
	// stfd f21,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f21.u64);
	// fmuls f18,f18,f31
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f31.f64));
	// stfs f11,508(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f11,f11,f13
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfd f13,3696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3696, ctx.f13.u64);
	// fmuls f4,f4,f31
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// stfd f23,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f23.u64);
	// fmuls f17,f17,f31
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f31.f64));
	// stfs f1,592(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// fmuls f12,f12,f30
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// stfs f11,580(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmr f11,f23
	ctx.f11.f64 = ctx.f23.f64;
	// lfs f13,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f23,f5,f25
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f25.f64));
	// stfs f8,848(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// fmuls f8,f22,f30
	ctx.f8.f64 = double(float(ctx.f22.f64 * ctx.f30.f64));
	// stfs f18,760(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// fmuls f18,f13,f28
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// lfs f21,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f9,f9,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// stfs f6,956(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// fmuls f6,f16,f31
	ctx.f6.f64 = double(float(ctx.f16.f64 * ctx.f31.f64));
	// stfs f19,972(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// fmuls f19,f15,f31
	ctx.f19.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// stfs f0,952(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// fmuls f16,f10,f26
	ctx.f16.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// stfs f8,804(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 804, temp.u32);
	// fmuls f20,f20,f30
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// stfs f14,824(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// stfs f5,860(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 860, temp.u32);
	// fmuls f5,f3,f31
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f4,976(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// fmuls f4,f2,f31
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// stfs f17,992(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// fmuls f17,f21,f28
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f28.f64));
	// lfs f8,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f8.f64 = double(temp.f32);
	// lfs f13,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f13.f64 = double(temp.f32);
	// lfs f14,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f14,f11
	ctx.f11.f64 = double(float(ctx.f14.f64 * ctx.f11.f64));
	// stfs f29,980(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// lfd f21,1096(r1)
	ctx.f21.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// lfd f29,704(r1)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// stfs f27,696(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// fmuls f29,f21,f29
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// stfs f6,212(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// fmuls f21,f8,f10
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// stfs f13,448(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// stfs f0,404(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// lfd f27,3680(r1)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3680);
	// lfs f1,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f1.f64 = double(temp.f32);
	// lfd f13,3696(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3696);
	// lfs f6,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f6.f64 = double(temp.f32);
	// lfd f0,5064(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 5064);
	// stfs f1,524(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// fmuls f1,f25,f30
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// stfs f23,728(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 728, temp.u32);
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f13,f10,f27
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f27.f64));
	// stfs f6,700(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 700, temp.u32);
	// fmuls f6,f0,f27
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfd f23,720(r1)
	ctx.f23.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// stfs f24,968(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// stfs f8,752(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// lfd f24,1696(r1)
	ctx.f24.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// lfs f8,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f8.f64 = double(temp.f32);
	// lfd f26,2216(r1)
	ctx.f26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 2216);
	// lfs f27,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f27.f64 = double(temp.f32);
	// stfs f1,800(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// fmuls f1,f23,f30
	ctx.f1.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// stfs f19,832(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 832, temp.u32);
	// fmuls f30,f24,f30
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// stfs f12,840(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 840, temp.u32);
	// fmuls f19,f8,f7
	ctx.f19.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// stfs f16,836(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 836, temp.u32);
	// fmuls f12,f10,f26
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// stfs f9,844(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 844, temp.u32);
	// fmuls f27,f0,f27
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// stfs f11,780(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 780, temp.u32);
	// fmuls f9,f0,f22
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f22.f64));
	// fmuls f26,f0,f23
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f23.f64));
	// lfs f16,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f11,f0,f24
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f24.f64));
	// lfs f0,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f8,f28
	ctx.f28.f64 = double(float(ctx.f8.f64 * ctx.f28.f64));
	// lfs f23,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f7,f0,f7
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f0,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f8,f8,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f25.f64));
	// lfs f25,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f25,f0,f16,f25
	ctx.f25.f64 = double(float(ctx.f0.f64 * ctx.f16.f64 + ctx.f25.f64));
	// stfs f0,872(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 872, temp.u32);
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// lfs f22,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f23,f23,f0
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfs f7,856(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 856, temp.u32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f16,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f16.f64 = double(temp.f32);
	// lfs f7,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f16,f16,f0
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f30,924(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 924, temp.u32);
	// stfs f19,588(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// stfs f31,960(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 960, temp.u32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f30.f64 = double(temp.f32);
	// lfs f19,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f20,996(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// fmr f20,f31
	ctx.f20.f64 = ctx.f31.f64;
	// fmuls f19,f19,f0
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// stfs f29,984(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// fmr f29,f0
	ctx.f29.f64 = ctx.f0.f64;
	// lfs f24,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f27,f31
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// stfs f17,540(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmuls f10,f24,f10
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// lfs f17,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f17.f64 = double(temp.f32);
	// stfs f21,580(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// lfs f21,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f21.f64 = double(temp.f32);
	// stfs f18,544(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// lfs f18,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f9,f9,f20
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f20.f64));
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// stfs f4,524(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// fmuls f29,f20,f29
	ctx.f29.f64 = double(float(ctx.f20.f64 * ctx.f29.f64));
	// stfs f28,508(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmr f20,f0
	ctx.f20.f64 = ctx.f0.f64;
	// lfs f28,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f28.f64 = double(temp.f32);
	// stfs f1,444(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// lfs f1,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f1.f64 = double(temp.f32);
	// stfs f25,552(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// stfs f5,592(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// lfs f25,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f25,f24,f25
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// lfs f5,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f27,f14,f27
	ctx.f27.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// stfs f2,500(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fmuls f5,f12,f5
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// stfs f3,404(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f20,f17,f20
	ctx.f20.f64 = double(float(ctx.f17.f64 * ctx.f20.f64));
	// lfs f2,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f2.f64 = double(temp.f32);
	// fmr f17,f0
	ctx.f17.f64 = ctx.f0.f64;
	// lfs f3,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f21,f21,f17
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f17.f64));
	// lfs f17,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f18,f17
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// lfs f17,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f17.f64 = double(temp.f32);
	// fmr f4,f17
	ctx.f4.f64 = ctx.f17.f64;
	// fmuls f17,f12,f17
	ctx.f17.f64 = double(float(ctx.f12.f64 * ctx.f17.f64));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f28,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f1,f1,f28
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f28.f64 = double(temp.f32);
	// stfs f0,164(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// fmuls f3,f13,f3
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// stfs f9,264(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// fmuls f2,f2,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// stfs f10,384(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// stfs f12,608(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 608, temp.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f9,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f7,476(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 476, temp.u32);
	// stfs f5,304(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// stfs f4,200(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f3,296(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
	// lfs f0,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f0.f64 = double(temp.f32);
	// lfs f7,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f4,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f3,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfs f1,144(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f2,244(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f31,224(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f0,f31
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// lfs f28,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f28.f64 = double(temp.f32);
	// stfs f30,104(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fmuls f28,f13,f28
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f30,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f29,1012(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1012, temp.u32);
	// stfs f28,644(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 644, temp.u32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// stfs f27,276(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f27,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f27,f0
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// lfs f0,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f0.f64 = double(temp.f32);
	// stfs f23,512(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 512, temp.u32);
	// fmuls f23,f6,f0
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f24,772(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 772, temp.u32);
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// lfs f24,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f24.f64 = double(temp.f32);
	// stfs f22,256(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// fmuls f24,f24,f0
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f22,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f22.f64 = double(temp.f32);
	// fmr f0,f22
	ctx.f0.f64 = ctx.f22.f64;
	// stfs f25,560(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 560, temp.u32);
	// fmuls f11,f11,f22
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f22.f64));
	// lfs f22,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f22.f64 = double(temp.f32);
	// lfs f25,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f25.f64 = double(temp.f32);
	// stfs f21,228(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// fmuls f25,f6,f25
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f25.f64));
	// lfs f21,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f21.f64 = double(temp.f32);
	// stfs f14,988(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 988, temp.u32);
	// stfs f16,472(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 472, temp.u32);
	// stfs f20,184(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f19,2064(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2064, temp.u32);
	// stfs f18,1928(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1928, temp.u32);
	// fmuls f26,f26,f0
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f0,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f0.f64 = double(temp.f32);
	// stfs f17,292(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
	// fmuls f21,f21,f0
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f19,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f19.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f20,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// stfs f7,1244(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// fmuls f20,f20,f0
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// fmuls f7,f19,f0
	ctx.f7.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// stfs f13,128(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f3,1060(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// stfs f6,1716(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1716, temp.u32);
	// fmuls f6,f15,f0
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// lfs f3,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f3.f64 = double(temp.f32);
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// stfs f30,280(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// fmr f30,f13
	ctx.f30.f64 = ctx.f13.f64;
	// stfs f10,1388(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1388, temp.u32);
	// stfs f4,1084(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// stfs f2,1540(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1540, temp.u32);
	// lfs f10,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f10.f64 = double(temp.f32);
	// lfs f15,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f15.f64 = double(temp.f32);
	// lfs f4,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f27,1040(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// fmr f27,f3
	ctx.f27.f64 = ctx.f3.f64;
	// stfs f5,1016(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f31,1836(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1836, temp.u32);
	// fmuls f31,f15,f0
	ctx.f31.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// stfs f29,112(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// fmuls f29,f10,f0
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f5,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f5.f64 = double(temp.f32);
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f5,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmr f3,f0
	ctx.f3.f64 = ctx.f0.f64;
	// stfs f1,1156(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// lfs f1,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f1,f30
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// stfs f12,108(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// lfs f12,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f30,f0,f27
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// stfs f26,1004(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// stfs f9,1648(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1648, temp.u32);
	// stfs f11,1068(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// stfs f20,1036(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// lfs f17,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f17.f64 = double(temp.f32);
	// lfs f11,2680(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2680);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f9.f64 = double(temp.f32);
	// lfs f20,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f20.f64 = double(temp.f32);
	// lfs f10,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f10.f64 = double(temp.f32);
	// lfs f27,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f3,f27,f3
	ctx.f3.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// lfs f18,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f26,f26,f12
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f12.f64));
	// stfs f23,1656(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1656, temp.u32);
	// fmuls f23,f0,f12
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfs f22,776(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// fadds f16,f17,f18
	ctx.f16.f64 = double(float(ctx.f17.f64 + ctx.f18.f64));
	// lfs f22,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f22.f64 = double(temp.f32);
	// stfs f28,1140(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// fmuls f28,f20,f11
	ctx.f28.f64 = double(float(ctx.f20.f64 * ctx.f11.f64));
	// stfs f25,1828(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1828, temp.u32);
	// fmuls f25,f10,f13
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfs f24,1048(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// stfs f21,1516(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1516, temp.u32);
	// fmuls f21,f9,f13
	ctx.f21.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f24,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f24.f64 = double(temp.f32);
	// lfs f0,500(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 500);
	ctx.f0.f64 = double(temp.f32);
	// lfs f19,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f19.f64 = double(temp.f32);
	// lfs f15,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f14.f64 = double(temp.f32);
	// stfs f17,848(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// stfs f28,500(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fmuls f9,f9,f24
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f24.f64));
	// stfd f12,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f12.u64);
	// fmuls f17,f14,f24
	ctx.f17.f64 = double(float(ctx.f14.f64 * ctx.f24.f64));
	// lfs f28,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f15,f15,f24
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f24.f64));
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f9,404(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f12,f28,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// stfs f28,448(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f28,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f28.f64 = double(temp.f32);
	// stfd f10,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f10.u64);
	// stfs f28,444(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fmuls f28,f19,f28
	ctx.f28.f64 = double(float(ctx.f19.f64 * ctx.f28.f64));
	// stfs f8,880(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// lfs f8,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f8.f64 = double(temp.f32);
	// stfs f6,896(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// lfs f6,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f6.f64 = double(temp.f32);
	// stfs f7,1028(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// fmuls f7,f17,f0
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// stfs f5,1164(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// fmuls f5,f16,f8
	ctx.f5.f64 = double(float(ctx.f16.f64 * ctx.f8.f64));
	// fmuls f6,f15,f6
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f6.f64));
	// lfs f10,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,188(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// fmuls f11,f10,f0
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f30,1076(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// fmuls f10,f22,f24
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// stfs f4,604(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// lfs f4,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f4.f64 = double(temp.f32);
	// stfs f28,508(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// lfs f28,-31056(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -31056);
	ctx.f28.f64 = double(temp.f32);
	// stfs f10,524(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// lfs f10,-19432(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19432);
	ctx.f10.f64 = double(temp.f32);
	// lfs f30,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f30.f64 = double(temp.f32);
	// stfs f11,592(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// fmuls f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// stfs f3,756(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f28,468(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 468, temp.u32);
	// stfs f21,520(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// lfs f11,-19028(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19028);
	ctx.f11.f64 = double(temp.f32);
	// lfs f28,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f28.f64 = double(temp.f32);
	// lfs f3,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f3,f3,f13,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f13.f64 + ctx.f29.f64));
	// lfs f21,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f21.f64 = double(temp.f32);
	// stfs f10,504(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 504, temp.u32);
	// fmadds f30,f21,f30,f28
	ctx.f30.f64 = double(float(ctx.f21.f64 * ctx.f30.f64 + ctx.f28.f64));
	// stfs f1,1020(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// stfs f25,1236(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// stfs f20,2060(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2060, temp.u32);
	// stfs f19,432(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// lfs f1,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f1.f64 = double(temp.f32);
	// lfs f9,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f9.f64 = double(temp.f32);
	// lfd f10,720(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// fmuls f28,f9,f24
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f24.f64));
	// lfs f20,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f10,f10,f20
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// lfs f25,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f25.f64 = double(temp.f32);
	// stfs f11,260(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
	// fmadds f11,f5,f11,f7
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 + ctx.f7.f64));
	// stfs f27,1024(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// stfs f23,912(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fmuls f23,f19,f25
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f25.f64));
	// stfs f26,828(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// stfs f2,492(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f2,f12,f13
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f31,460(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// fmuls f31,f1,f24
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f24.f64));
	// lfs f29,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f26.f64 = double(temp.f32);
	// lfs f7,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f7.f64 = double(temp.f32);
	// stfs f18,796(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 796, temp.u32);
	// stfs f14,420(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 420, temp.u32);
	// stfs f0,172(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// stfs f22,884(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 884, temp.u32);
	// lfd f12,704(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// fmadds f9,f9,f13,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f3.f64));
	// fmuls f18,f2,f12
	ctx.f18.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f10,580(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmr f12,f13
	ctx.f12.f64 = ctx.f13.f64;
	// lfs f13,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// lfs f26,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f26.f64 = double(temp.f32);
	// lfs f10,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f26,f13,f10
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f10.f64)));
	// lfs f13,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f13.f64 = double(temp.f32);
	// stfs f26,980(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// fmr f26,f13
	ctx.f26.f64 = ctx.f13.f64;
	// lfs f0,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f0.f64 = double(temp.f32);
	// lfs f5,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// stfs f9,444(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fnmsubs f11,f31,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f31,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f6,f4,f0,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 - ctx.f6.f64));
	// fmuls f31,f31,f20
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// stfs f20,544(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// lfs f20,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f27,f27,f12
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f12.f64));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// stfs f13,404(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f4,f20,f4
	ctx.f4.f64 = double(float(ctx.f20.f64 * ctx.f4.f64));
	// fmr f9,f26
	ctx.f9.f64 = ctx.f26.f64;
	// stfs f6,696(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// stfs f5,404(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// lfs f5,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f6.f64 = double(temp.f32);
	// lfs f13,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f6,f2,f6
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// stfs f4,448(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmsubs f30,f5,f13,f30
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 - ctx.f30.f64));
	// lfs f4,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f26
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// lfs f2,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f0.f64 = double(temp.f32);
	// lfs f22,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f9,f3,f9,f2
	ctx.f9.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f2.f64)));
	// stfs f31,508(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfd f12,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f12.u64);
	// stfs f30,524(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// lfs f17,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f17,f25
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// lfs f14,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f14.f64 = double(temp.f32);
	// lfs f31,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f31.f64 = double(temp.f32);
	// lfs f12,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f31,f16,f31
	ctx.f31.f64 = double(float(ctx.f16.f64 * ctx.f31.f64));
	// lfs f30,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f28,f12
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// stfs f29,500(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// lfs f26,-19032(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19032);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,-19436(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19436);
	ctx.f29.f64 = double(temp.f32);
	// stfs f8,540(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmuls f8,f14,f8
	ctx.f8.f64 = double(float(ctx.f14.f64 * ctx.f8.f64));
	// stfs f3,552(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// stfs f5,648(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 648, temp.u32);
	// fmuls f5,f30,f0
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f7,784(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// lfs f7,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f3.f64 = double(temp.f32);
	// stfs f1,868(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 868, temp.u32);
	// stfs f26,480(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 480, temp.u32);
	// stfs f21,368(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 368, temp.u32);
	// stfs f29,592(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// stfs f20,712(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 712, temp.u32);
	// stfs f2,732(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 732, temp.u32);
	// stfs f17,612(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 612, temp.u32);
	// stfs f16,620(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 620, temp.u32);
	// stfs f19,924(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 924, temp.u32);
	// stfs f14,1708(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1708, temp.u32);
	// stfs f27,340(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfs f22,616(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f21,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f22,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f22.f64 = double(temp.f32);
	// fmr f0,f26
	ctx.f0.f64 = ctx.f26.f64;
	// lfd f12,720(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// fmsubs f22,f15,f22,f21
	ctx.f22.f64 = double(float(ctx.f15.f64 * ctx.f22.f64 - ctx.f21.f64));
	// lfs f16,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f16.f64 = double(temp.f32);
	// lfs f27,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f9,f9,f24,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f24.f64 - ctx.f4.f64));
	// lfs f21,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f27,f24
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// lfs f19,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f21,f24
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f24.f64));
	// lfs f17,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f17.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f24,996(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// fmuls f17,f17,f12
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f12.f64));
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f24,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f24.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// fmuls f24,f24,f12
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f12.f64));
	// lfs f15,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f2,f18,f0
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f19,f19,f18,f16
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f18.f64 + ctx.f16.f64));
	// lfs f16,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f16.f64 = double(temp.f32);
	// stfs f2,956(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// fmuls f16,f16,f12
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f12.f64));
	// lfs f12,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f14,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f14,f14,f15
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f24,976(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// fmuls f24,f12,f2
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f2.f64));
	// lfs f29,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f29.f64 = double(temp.f32);
	// stfs f24,968(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// fnmsubs f13,f29,f13,f10
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f13.f64 - ctx.f10.f64)));
	// stfs f14,984(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// stfs f19,972(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// lfs f14,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f19.f64 = double(temp.f32);
	// lfs f24,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f24.f64 = double(temp.f32);
	// lfs f0,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f24,f14,f24,f19
	ctx.f24.f64 = double(float(ctx.f14.f64 * ctx.f24.f64 + ctx.f19.f64));
	// lfs f10,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f30,f23,f0
	ctx.f30.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// lfs f1,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f23,f10,f25
	ctx.f23.f64 = double(float(ctx.f10.f64 * ctx.f25.f64));
	// stfs f17,952(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// fmadds f31,f1,f0,f31
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f31.f64));
	// stfs f14,760(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// lfs f14,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f17.f64 = double(temp.f32);
	// lfs f0,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f28,f28,f17,f14
	ctx.f28.f64 = double(float(-(ctx.f28.f64 * ctx.f17.f64 - ctx.f14.f64)));
	// stfs f28,676(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 676, temp.u32);
	// fnmsubs f11,f8,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f24,992(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// stfs f5,840(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 840, temp.u32);
	// lfs f4,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f4.f64 = double(temp.f32);
	// lfs f8,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f8.f64 = double(temp.f32);
	// lfs f20,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f8,f4,f8,f20
	ctx.f8.f64 = double(float(-(ctx.f4.f64 * ctx.f8.f64 - ctx.f20.f64)));
	// lfs f19,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f18,f18,f15
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f15.f64));
	// lfs f17,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f19,f15
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f15.f64));
	// lfs f14,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f14.f64 = double(temp.f32);
	// lfs f28,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f28,f17,f28
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f28.f64));
	// lfs f5,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f24,f14,f24
	ctx.f24.f64 = double(float(ctx.f14.f64 * ctx.f24.f64));
	// stfs f6,696(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f6,-19036(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19036);
	ctx.f6.f64 = double(temp.f32);
	// stfs f7,832(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 832, temp.u32);
	// stfs f6,176(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f13,500(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// stfs f11,848(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// fmuls f7,f25,f29
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f11,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f11.f64 = double(temp.f32);
	// stfs f10,800(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// fmuls f10,f28,f0
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f0,-19048(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19048);
	ctx.f0.f64 = double(temp.f32);
	// stfs f9,448(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmuls f9,f17,f12
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f12.f64));
	// stfs f0,728(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 728, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f13,-19040(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19040);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lfs f12,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-19052(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19052);
	ctx.f13.f64 = double(temp.f32);
	// stfs f3,836(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 836, temp.u32);
	// fmuls f3,f0,f12
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfs f13,576(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 576, temp.u32);
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f13.f64 = double(temp.f32);
	// stfs f31,580(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmuls f31,f0,f13
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f13,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f13.f64 = double(temp.f32);
	// lfs f29,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f29.f64 = double(temp.f32);
	// lfs f0,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f5,f24,f0,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 - ctx.f5.f64));
	// lfs f0,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f0.f64 = double(temp.f32);
	// stfs f30,540(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// stfs f8,544(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// stfs f2,404(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// lfs f30,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f24.f64 = double(temp.f32);
	// stfs f27,588(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// fmadds f27,f29,f13,f0
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f13.f64 + ctx.f0.f64));
	// fmr f0,f26
	ctx.f0.f64 = ctx.f26.f64;
	// lfs f13,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f13.f64 = double(temp.f32);
	// lfs f28,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,444(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fmr f26,f22
	ctx.f26.f64 = ctx.f22.f64;
	// stfs f4,824(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// fmuls f4,f21,f12
	ctx.f4.f64 = double(float(ctx.f21.f64 * ctx.f12.f64));
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// stfs f23,524(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// fmuls f2,f19,f12
	ctx.f2.f64 = double(float(ctx.f19.f64 * ctx.f12.f64));
	// lfs f19,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f19.f64 = double(temp.f32);
	// stfs f20,768(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// lfs f20,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f20.f64 = double(temp.f32);
	// stfs f22,508(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fnmsubs f28,f0,f13,f28
	ctx.f28.f64 = double(float(-(ctx.f0.f64 * ctx.f13.f64 - ctx.f28.f64)));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f13,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f13.f64 = double(temp.f32);
	// fmr f0,f23
	ctx.f0.f64 = ctx.f23.f64;
	// lfs f23,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f23.f64 = double(temp.f32);
	// stfs f1,964(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// lfs f8,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f8.f64 = double(temp.f32);
	// lfs f1,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f6,f8,f17
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f17.f64));
	// lfs f14,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f1,f1,f15
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f15.f64));
	// stfs f21,540(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmadds f26,f0,f13,f26
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f26.f64));
	// lfs f13,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f23,f13,f25,f23
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f25.f64 - ctx.f23.f64));
	// lfs f13,-19044(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19044);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f19,f16,f13,f19
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f13.f64 - ctx.f19.f64));
	// lfs f16,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f16.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f20,f20,f12,f16
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f12.f64 + ctx.f16.f64));
	// fmuls f22,f18,f0
	ctx.f22.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// lfs f16,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f16.f64 = double(temp.f32);
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f24,f24,f30
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// fmuls f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// lfs f12,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f12.f64 = double(temp.f32);
	// lfs f16,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f16.f64 = double(temp.f32);
	// lfs f21,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f31,f12,f0,f31
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f31.f64));
	// stfs f29,580(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmsubs f21,f21,f13,f14
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f13.f64 - ctx.f14.f64));
	// lfs f13,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f16,f15,f16
	ctx.f16.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f29,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f29.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// fmuls f29,f13,f29
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// stfs f29,508(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// lfs f29,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f29.f64 = double(temp.f32);
	// lfs f12,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f12,f12,f29
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// stfs f12,500(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// lfs f14,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f14,f14,f0
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// stfs f29,524(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// fmuls f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f29,-19440(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19440);
	ctx.f29.f64 = double(temp.f32);
	// stfs f1,696(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f0,404(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// lfs f1,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f1.f64 = double(temp.f32);
	// stfs f8,484(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmsubs f8,f5,f25,f7
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f25.f64 - ctx.f7.f64));
	// fmadds f11,f3,f1,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f1.f64 + ctx.f11.f64));
	// lfs f5,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f5,-19056(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19056);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f10,f2,f13,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f10.f64));
	// stfs f5,960(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 960, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f12,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f3,f13,f12
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f2,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f2.f64 = double(temp.f32);
	// stfs f17,444(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// lfs f17,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f17,f17,f25
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// stfs f29,544(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// fmadds f11,f11,f25,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f25.f64 + ctx.f8.f64));
	// lfs f8,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f8.f64 = double(temp.f32);
	// stfs f17,448(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// lfs f5,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f5,f12
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// lfs f12,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f2,f2,f12
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f12,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f8,f8,f12
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// lfs f12,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f10,f9,f12,f10
	ctx.f10.f64 = double(float(-(ctx.f9.f64 * ctx.f12.f64 - ctx.f10.f64)));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f12.f64 = double(temp.f32);
	// lfs f29,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f31,f12,f0,f31
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f31.f64));
	// lfs f17,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f17.f64 = double(temp.f32);
	// lfs f7,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f7.f64 = double(temp.f32);
	// lfs f9,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f7,f16,f0,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f1,3140(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3140);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f9,f29,f9,f17
	ctx.f9.f64 = double(float(-(ctx.f29.f64 * ctx.f9.f64 - ctx.f17.f64)));
	// stfs f28,5204(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5204, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f30,452(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f12.f64 = double(temp.f32);
	// lfs f30,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f29,f30,f0,f29
	ctx.f29.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f29.f64)));
	// stfs f1,424(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 424, temp.u32);
	// fnmsubs f4,f4,f12,f28
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f12.f64 - ctx.f28.f64)));
	// lfs f1,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f1,f15
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f15.f64));
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// stfs f27,5200(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5200, temp.u32);
	// stfs f26,5208(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5208, temp.u32);
	// stfs f24,5212(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5212, temp.u32);
	// stfs f23,5216(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5216, temp.u32);
	// fmadds f27,f0,f12,f20
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f20.f64));
	// lfs f0,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f26,f18,f0
	ctx.f26.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// fnmsubs f24,f14,f12,f21
	ctx.f24.f64 = double(float(-(ctx.f14.f64 * ctx.f12.f64 - ctx.f21.f64)));
	// lfs f28,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f28.f64 = double(temp.f32);
	// lfs f0,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f28,f22,f28,f19
	ctx.f28.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f19.f64)));
	// lfs f12,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f12.f64 = double(temp.f32);
	// lfs f23,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f2,f12,f0,f2
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f2.f64));
	// lfs f20,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f23,f0
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// lfs f17,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f0,f20,f0,f17
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f17.f64));
	// lfs f20,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f7,f7,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f20.f64));
	// lfs f20,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f13,f13,f20
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f20.f64));
	// lfs f20,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f10,f6,f20,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f20.f64 + ctx.f10.f64));
	// lfs f6,-19064(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19064);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,432(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// lfs f18,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f15,f18
	ctx.f18.f64 = double(float(ctx.f15.f64 * ctx.f18.f64));
	// lfs f16,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f1,f1,f16
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// lfs f19,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f8,f3,f14,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f14.f64 - ctx.f8.f64));
	// lfs f16,-19060(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19060);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f12,f19,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f12.f64));
	// lfs f21,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f21.f64 = double(temp.f32);
	// lfs f3,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f20,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f20.f64 = double(temp.f32);
	// stfs f9,5220(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5220, temp.u32);
	// fmr f9,f16
	ctx.f9.f64 = ctx.f16.f64;
	// fnmsubs f6,f3,f20,f31
	ctx.f6.f64 = double(float(-(ctx.f3.f64 * ctx.f20.f64 - ctx.f31.f64)));
	// stfs f4,5228(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5228, temp.u32);
	// lfs f4,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f4.f64 = double(temp.f32);
	// lfs f17,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f11,f22,f4,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f4.f64 - ctx.f11.f64)));
	// stfs f28,5236(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5236, temp.u32);
	// fmuls f17,f15,f17
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f17.f64));
	// lfs f28,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f28.f64 = double(temp.f32);
	// lfs f4,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f4.f64 = double(temp.f32);
	// fadds f4,f28,f4
	ctx.f4.f64 = double(float(ctx.f28.f64 + ctx.f4.f64));
	// lfs f28,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f28.f64 = double(temp.f32);
	// stfs f27,5232(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5232, temp.u32);
	// lfs f27,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f10,f21,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f21.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f9,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f9.f64 = double(temp.f32);
	// lfs f31,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f9,f5,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmsubs f7,f18,f31,f7
	ctx.f7.f64 = double(float(ctx.f18.f64 * ctx.f31.f64 - ctx.f7.f64));
	// stfs f29,5224(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5224, temp.u32);
	// fmr f31,f14
	ctx.f31.f64 = ctx.f14.f64;
	// lfs f3,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f29,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f0,f0,f3
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// stfs f26,5240(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5240, temp.u32);
	// fmuls f29,f25,f29
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f27,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f17,f5
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f5.f64));
	// stfs f24,5244(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5244, temp.u32);
	// stfs f16,676(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 676, temp.u32);
	// fmadds f13,f13,f31,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f31.f64 + ctx.f2.f64));
	// lfs f2,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f8,f28,f2,f8
	ctx.f8.f64 = double(float(-(ctx.f28.f64 * ctx.f2.f64 - ctx.f8.f64)));
	// lfs f28,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f10,f1,f28,f10
	ctx.f10.f64 = double(float(-(ctx.f1.f64 * ctx.f28.f64 - ctx.f10.f64)));
	// lfs f31,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f31,f31,f19
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// lfs f1,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f2,f15,f2
	ctx.f2.f64 = double(float(ctx.f15.f64 * ctx.f2.f64));
	// lfs f28,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f26,f1,f28
	ctx.f26.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f20,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f20.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f22,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f22,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f18,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f18.f64 = double(temp.f32);
	// lfs f20,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f20.f64 = double(temp.f32);
	// lfs f17,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f6,f18,f20,f6
	ctx.f6.f64 = double(float(-(ctx.f18.f64 * ctx.f20.f64 - ctx.f6.f64)));
	// fmsubs f0,f0,f17,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f17.f64 - ctx.f9.f64));
	// lfs f9,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f9.f64 = double(temp.f32);
	// lfs f18,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f11,f29,f9,f11
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f9.f64 + ctx.f11.f64));
	// lfs f20,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// lfs f18,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f18.f64 = double(temp.f32);
	// lfs f9,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f29,f25,f18
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// fnmsubs f9,f5,f9,f7
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f9.f64 - ctx.f7.f64)));
	// lfs f18,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f18.f64 = double(temp.f32);
	// lfs f7,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f4,f7,f18
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f18.f64));
	// lfs f5,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f5,f4,f13
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f4.f64 - ctx.f13.f64)));
	// lfs f18,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f18.f64 = double(temp.f32);
	// lfs f4,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f4.f64 = double(temp.f32);
	// lfs f17,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f17.f64 = double(temp.f32);
	// lfs f5,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f12,f12,f17
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f17.f64));
	// fmadds f5,f5,f4,f18
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 + ctx.f18.f64));
	// lfs f16,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f16.f64 = double(temp.f32);
	// stfs f3,984(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// fmuls f3,f22,f16
	ctx.f3.f64 = double(float(ctx.f22.f64 * ctx.f16.f64));
	// lfs f4,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f4.f64 = double(temp.f32);
	// lfs f18,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f8,f4,f18,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f18.f64 + ctx.f8.f64));
	// lfs f4,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f18,f31,f17
	ctx.f18.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// lfs f17,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f10,f2,f4,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f10.f64));
	// lfs f24,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f2,f17,f27
	ctx.f2.f64 = double(float(ctx.f17.f64 * ctx.f27.f64));
	// fmuls f21,f24,f27
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// stfs f24,968(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// lfs f24,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f9,f7,f15,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f15.f64 + ctx.f9.f64));
	// fmsubs f12,f5,f19,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f19.f64 - ctx.f12.f64));
	// lfs f5,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f5,f30,f5,f3
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 - ctx.f3.f64));
	// lfs f3,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f3.f64 = double(temp.f32);
	// fmr f30,f3
	ctx.f30.f64 = ctx.f3.f64;
	// lfs f22,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f3,f20,f3,f0
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f0,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f0.f64 = double(temp.f32);
	// stfs f1,952(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// lfs f4,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f4.f64 = double(temp.f32);
	// lfs f17,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmuls f2,f2,f24
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f24.f64));
	// lfs f24,-19068(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19068);
	ctx.f24.f64 = double(temp.f32);
	// stfs f24,92(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f22,f21,f22
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// lfs f24,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f17,f17,f25
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f6,f24,f21,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f21.f64 - ctx.f6.f64)));
	// lfs f24,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f11,f29,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f30,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f24,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f8,f30,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f7,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f7.f64 = double(temp.f32);
	// stfs f31,972(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// fmuls f31,f1,f28
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f29,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f7,f28
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// lfs f30,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f24.f64 = double(temp.f32);
	// lfs f16,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f30,f29
	ctx.f21.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f29,980(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// fmuls f29,f16,f29
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// lfs f14,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f20,f24,f27
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// stfs f29,444(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fmuls f29,f14,f27
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// stfs f29,448(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f29,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f29.f64 = double(temp.f32);
	// stfs f4,404(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f4,f29,f28
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// stfs f4,508(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// lfs f4,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f4.f64 = double(temp.f32);
	// stfs f23,500(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fnmsubs f10,f18,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f18.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f23,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f23,f15,f23
	ctx.f23.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// lfs f4,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f4.f64 = double(temp.f32);
	// stfs f15,848(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// fmuls f0,f0,f4
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f15,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f15.f64 = double(temp.f32);
	// fmr f4,f15
	ctx.f4.f64 = ctx.f15.f64;
	// lfs f18,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f26,f26,f15
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f15.f64));
	// lfs f15,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f18,f17,f18
	ctx.f18.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// lfs f17,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f2,f2,f17,f22
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f17.f64 - ctx.f22.f64));
	// stfs f12,5268(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5268, temp.u32);
	// lfs f12,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f12.f64 = double(temp.f32);
	// lfs f17,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f17.f64 = double(temp.f32);
	// stfs f11,5256(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5256, temp.u32);
	// fmr f11,f12
	ctx.f11.f64 = ctx.f12.f64;
	// stfs f23,580(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmuls f31,f31,f17
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// lfs f23,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f23.f64 = double(temp.f32);
	// stfs f10,524(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// fmuls f4,f15,f4
	ctx.f4.f64 = double(float(ctx.f15.f64 * ctx.f4.f64));
	// lfs f10,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f10.f64 = double(temp.f32);
	// lfs f17,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f17.f64 = double(temp.f32);
	// stfs f13,5264(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5264, temp.u32);
	// fmadds f13,f21,f23,f26
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f23.f64 + ctx.f26.f64));
	// fmadds f10,f10,f17,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f17.f64 + ctx.f5.f64));
	// stfs f24,956(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// lfs f15,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f15.f64 = double(temp.f32);
	// lfs f24,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f7,f7,f15
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f15.f64));
	// stfs f19,540(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fnmsubs f11,f20,f11,f2
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// stfs f3,5252(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5252, temp.u32);
	// stfs f9,5260(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5260, temp.u32);
	// stfs f8,5272(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5272, temp.u32);
	// fmadds f0,f0,f12,f4
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f4.f64));
	// lfs f19,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f19.f64 = double(temp.f32);
	// fmr f12,f23
	ctx.f12.f64 = ctx.f23.f64;
	// lfs f22,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f22.f64 = double(temp.f32);
	// lfs f9,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f15,f19,f22
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f22.f64));
	// lfs f8,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f10,f10,f25,f18
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f25.f64 - ctx.f18.f64));
	// lfs f5,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f8,f22
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f22.f64));
	// lfs f3,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f2.f64 = double(temp.f32);
	// stfs f6,5248(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5248, temp.u32);
	// fmuls f6,f9,f27
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// stfs f30,588(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// stfs f1,796(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 796, temp.u32);
	// fadds f1,f3,f5
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f5.f64));
	// stfs f29,824(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// fmuls f29,f2,f28
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// fnmsubs f13,f24,f12,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f12.f64 - ctx.f13.f64)));
	// lfs f30,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f30.f64 = double(temp.f32);
	// lfs f26,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f26.f64 = double(temp.f32);
	// lfs f12,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f12.f64 = double(temp.f32);
	// stfs f16,996(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// stfs f14,976(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// stfs f19,964(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// lfs f24,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f12.f64));
	// fmr f26,f24
	ctx.f26.f64 = ctx.f24.f64;
	// lfs f19,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f7,f7,f24
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f24.f64));
	// lfs f23,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f23.f64 = double(temp.f32);
	// fmr f24,f17
	ctx.f24.f64 = ctx.f17.f64;
	// lfs f21,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f11,f19,f23,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f23.f64 - ctx.f11.f64)));
	// lfs f16,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f16.f64 = double(temp.f32);
	// lfs f20,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f10,f0,f25,f10
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f25.f64 + ctx.f10.f64));
	// lfs f23,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f13,f16,f20,f13
	ctx.f13.f64 = double(float(ctx.f16.f64 * ctx.f20.f64 + ctx.f13.f64));
	// stfs f21,444(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fmuls f18,f23,f28
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f28.f64));
	// stfs f18,404(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f1,f1,f28
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f0,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f31,f15,f26,f31
	ctx.f31.f64 = double(float(ctx.f15.f64 * ctx.f26.f64 + ctx.f31.f64));
	// lfs f26,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f21,f26
	ctx.f26.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f21,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f24,f30,f24
	ctx.f24.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// stfs f28,448(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmuls f12,f12,f21
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f21.f64));
	// lfs f21,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f11,f6,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f6,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f7,f4,f21,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f21.f64 - ctx.f7.f64));
	// lfs f4,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f4.f64 = double(temp.f32);
	// stfs f6,768(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// fmuls f21,f4,f22
	ctx.f21.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// stfs f4,856(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 856, temp.u32);
	// lfs f4,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f4.f64 = double(temp.f32);
	// lfs f20,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f31,f29,f18,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f18.f64 + ctx.f31.f64));
	// lfs f29,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f29.f64 = double(temp.f32);
	// lfs f18,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f29,f29,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// fmadds f0,f6,f0,f24
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f24.f64));
	// lfs f28,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f6,f18,f27
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f27.f64));
	// stfs f18,992(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// lfs f18,1708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1708);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f13,f28,f4,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f4.f64 - ctx.f13.f64)));
	// lfs f16,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f16.f64 = double(temp.f32);
	// stfs f6,1708(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1708, temp.u32);
	// fadds f14,f16,f20
	ctx.f14.f64 = double(float(ctx.f16.f64 + ctx.f20.f64));
	// lfs f28,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f24.f64 = double(temp.f32);
	// lfs f6,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f6.f64 = double(temp.f32);
	// stfs f9,924(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 924, temp.u32);
	// fnmsubs f6,f6,f24,f28
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f24.f64 - ctx.f28.f64)));
	// stfs f5,524(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// lfs f5,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f9.f64 = double(temp.f32);
	// stfs f14,500(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fmuls f9,f9,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// lfs f19,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,2060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2060);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f15,f17,f27
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f27.f64));
	// lfs f24,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f24.f64 = double(temp.f32);
	// lfs f14,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f18,f24,f18
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f18.f64));
	// lfs f4,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f14,f19,f14
	ctx.f14.f64 = double(float(ctx.f19.f64 * ctx.f14.f64));
	// stfs f8,760(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// fnmsubs f10,f26,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f5,2064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2064);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f8,f28,f25
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// stfs f3,540(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// stfs f2,800(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// stfs f30,508(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// stfs f23,580(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// stfs f19,2060(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2060, temp.u32);
	// stfs f17,852(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 852, temp.u32);
	// stfs f20,872(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 872, temp.u32);
	// stfs f16,804(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 804, temp.u32);
	// lfs f2,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f5,f27
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// fmadds f12,f12,f2,f14
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f2.f64 + ctx.f14.f64));
	// lfs f28,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f28.f64 = double(temp.f32);
	// lfs f2,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f15,f28,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f28.f64 - ctx.f11.f64)));
	// fmadds f7,f1,f2,f7
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f2.f64 + ctx.f7.f64));
	// lfs f28,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f13,f1,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f28,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f28.f64 = double(temp.f32);
	// lfs f2,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f28,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// lfs f28,1836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1836);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f1.f64 = double(temp.f32);
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f1,f21,f1,f31
	ctx.f1.f64 = double(float(ctx.f21.f64 * ctx.f1.f64 + ctx.f31.f64));
	// fnmsubs f26,f28,f26,f0
	ctx.f26.f64 = double(float(-(ctx.f28.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// lfs f4,1928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1928);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f4,f22
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// lfs f31,1828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1828);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f31,f0
	ctx.f21.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f22,1708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1708);
	ctx.f22.f64 = double(temp.f32);
	// lfs f23,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f23.f64 = double(temp.f32);
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f0,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f6,f18,f0,f6
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f6.f64));
	// lfs f22,1716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1716);
	ctx.f22.f64 = double(temp.f32);
	// lfs f20,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f20.f64 = double(temp.f32);
	// lfs f0,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f12,f20,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f20.f64 + ctx.f13.f64));
	// lfs f19,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f11,f3,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// fmuls f22,f22,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f10,f8,f19,f10
	ctx.f10.f64 = double(float(-(ctx.f8.f64 * ctx.f19.f64 - ctx.f10.f64)));
	// lfs f8,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f12,f0,f12,f2
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 - ctx.f2.f64));
	// lfs f18,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f8,f30,f8,f7
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f8.f64 + ctx.f7.f64));
	// fmsubs f2,f26,f27,f23
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 - ctx.f23.f64));
	// lfs f26,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f26.f64 = double(temp.f32);
	// lfs f19,1388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1388);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f26,f18,f26
	ctx.f26.f64 = double(float(ctx.f18.f64 * ctx.f26.f64));
	// lfs f7,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f17,f19,f27
	ctx.f17.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// lfs f0,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f7,f29,f7,f1
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f7.f64 - ctx.f1.f64)));
	// lfs f30,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f30.f64 = double(temp.f32);
	// lfs f18,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f29,f30,f0
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f28,1716(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1716, temp.u32);
	// fmuls f18,f18,f0
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// stfs f19,1388(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1388, temp.u32);
	// lfs f3,1516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1516);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,1540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f23,f3,f25
	ctx.f23.f64 = double(float(ctx.f3.f64 * ctx.f25.f64));
	// lfs f15,1648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1648);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f0,f1,f14
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f14.f64));
	// lfs f28,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f16,f14,f16
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// lfs f19,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f9,f28,f21
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f28.f64 + ctx.f21.f64));
	// stfs f30,1648(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1648, temp.u32);
	// fmuls f30,f15,f27
	ctx.f30.f64 = double(float(ctx.f15.f64 * ctx.f27.f64));
	// stfs f5,1540(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1540, temp.u32);
	// fmuls f5,f22,f19
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// stfs f4,1516(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1516, temp.u32);
	// stfs f6,5276(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5276, temp.u32);
	// stfs f10,5280(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5280, temp.u32);
	// stfs f11,5288(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5288, temp.u32);
	// stfs f13,5284(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5284, temp.u32);
	// stfs f8,5292(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5292, temp.u32);
	// lfs f4,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f4.f64 = double(temp.f32);
	// stfs f0,1036(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// fmr f0,f28
	ctx.f0.f64 = ctx.f28.f64;
	// lfs f13,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f29,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// lfs f11,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f11.f64 = double(temp.f32);
	// fmr f29,f28
	ctx.f29.f64 = ctx.f28.f64;
	// stfs f12,5296(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5296, temp.u32);
	// fmuls f11,f26,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// stfs f7,5300(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5300, temp.u32);
	// fmuls f12,f9,f20
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f20.f64));
	// lfs f7,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f7.f64 = double(temp.f32);
	// stfs f3,500(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fadds f3,f4,f7
	ctx.f3.f64 = double(float(ctx.f4.f64 + ctx.f7.f64));
	// lfs f28,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// lfs f9,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f26,f18,f26
	ctx.f26.f64 = double(float(ctx.f18.f64 * ctx.f26.f64));
	// stfs f31,1836(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1836, temp.u32);
	// fmuls f6,f9,f25
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f25.f64));
	// lfs f31,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f13,f23,f0,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f18,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f2,f17,f29,f2
	ctx.f2.f64 = double(float(ctx.f17.f64 * ctx.f29.f64 + ctx.f2.f64));
	// lfs f29,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f28,f29
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f16,f28
	ctx.f28.f64 = double(float(ctx.f16.f64 * ctx.f28.f64));
	// lfs f16,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f16.f64 = double(temp.f32);
	// stfs f15,1236(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// fmuls f15,f18,f16
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f17,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f3,f3,f14
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// stfs f3,1068(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// stfs f1,1828(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1828, temp.u32);
	// lfs f21,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f0,f21,f0,f11
	ctx.f0.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 - ctx.f11.f64));
	// lfs f21,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f12,f5,f21,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f21.f64 - ctx.f12.f64));
	// lfs f5,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f5.f64 = double(temp.f32);
	// stfs f16,1140(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// fmuls f19,f5,f27
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// lfs f21,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f16.f64 = double(temp.f32);
	// stfs f15,1040(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// fadds f3,f16,f17
	ctx.f3.f64 = double(float(ctx.f16.f64 + ctx.f17.f64));
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f15.f64 = double(temp.f32);
	// lfs f1,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f1.f64 = double(temp.f32);
	// lfs f23,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f23.f64 = double(temp.f32);
	// lfs f11,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f23,f23,f22
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// stfs f3,1016(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// fmuls f11,f1,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// lfs f3,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f3.f64 = double(temp.f32);
	// stfs f25,1048(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// fmuls f25,f15,f25
	ctx.f25.f64 = double(float(ctx.f15.f64 * ctx.f25.f64));
	// fnmsubs f3,f30,f3,f2
	ctx.f3.f64 = double(float(-(ctx.f30.f64 * ctx.f3.f64 - ctx.f2.f64)));
	// lfs f30,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f30.f64 = double(temp.f32);
	// stfs f25,1004(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// fmsubs f13,f6,f30,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f30.f64 - ctx.f13.f64));
	// lfs f25,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f25.f64 = double(temp.f32);
	// lfs f2,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// lfs f6,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f2,f25,f2,f21
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f2.f64 + ctx.f21.f64));
	// lfs f25,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f28,f6,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// lfs f6,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f30,f29,f25,f26
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f25.f64 + ctx.f26.f64));
	// lfs f10,1656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1656);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f11,f23,f6,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f6.f64 - ctx.f11.f64));
	// lfs f22,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f8,f10,f20
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// lfs f29,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f22,f31,f22
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f22.f64));
	// lfs f28,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f14,f29
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// lfs f6,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f28,f28,f24
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// lfs f26,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f6,f6,f26,f22
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64 - ctx.f22.f64));
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// stfs f9,448(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fnmsubs f9,f19,f26,f3
	ctx.f9.f64 = double(float(-(ctx.f19.f64 * ctx.f26.f64 - ctx.f3.f64)));
	// fmr f3,f25
	ctx.f3.f64 = ctx.f25.f64;
	// stfs f1,444(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// lfs f1,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f8,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// stfs f5,1708(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1708, temp.u32);
	// stfs f31,1928(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1928, temp.u32);
	// lfs f31,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// stfs f7,1244(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// fmuls f7,f2,f27
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f27.f64));
	// fmuls f5,f5,f27
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// lfs f8,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f8.f64 = double(temp.f32);
	// lfs f23,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f13,f1,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f3,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f0,f31,f3,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f3,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f11,f8,f3,f11
	ctx.f11.f64 = double(float(-(ctx.f8.f64 * ctx.f3.f64 - ctx.f11.f64)));
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// stfs f10,404(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// lfs f10,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f10.f64 = double(temp.f32);
	// lfs f2,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f9,f7,f23,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 + ctx.f9.f64));
	// lfs f23,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f5,f5,f22
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// lfs f26,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f2,f10,f2,f30
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 + ctx.f30.f64));
	// lfs f7,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f7.f64 = double(temp.f32);
	// lfs f22,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f22.f64 = double(temp.f32);
	// lfs f31,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f25,f31,f14
	ctx.f25.f64 = double(float(ctx.f31.f64 * ctx.f14.f64));
	// fnmsubs f11,f26,f23,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f23.f64 - ctx.f11.f64)));
	// lfs f3,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f0,f1,f7,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// lfs f1,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f6,f22,f1,f6
	ctx.f6.f64 = double(float(-(ctx.f22.f64 * ctx.f1.f64 - ctx.f6.f64)));
	// lfs f1,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f29,f3,f30
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfs f21,1024(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// fmuls f7,f28,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// fnmsubs f9,f5,f1,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f1.f64 - ctx.f9.f64)));
	// lfs f5,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f13,f2,f30,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f30.f64 + ctx.f13.f64));
	// lfs f21,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f21.f64 = double(temp.f32);
	// lfs f2,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f2,f21
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f23,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f11,f5,f1,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f11.f64));
	// lfs f28,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f28.f64 = double(temp.f32);
	// fmr f5,f21
	ctx.f5.f64 = ctx.f21.f64;
	// stfs f4,1656(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1656, temp.u32);
	// fnmsubs f12,f28,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// lfs f23,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f23.f64 = double(temp.f32);
	// lfs f28,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f0,f25,f23,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f23.f64 + ctx.f0.f64));
	// lfs f23,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f23.f64 = double(temp.f32);
	// lfs f4,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f7,f7,f23
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f23.f64));
	// fnmsubs f13,f29,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f29,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f20.f64));
	// lfs f1,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f1.f64 = double(temp.f32);
	// lfs f25,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// lfs f21,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f5,f28,f5,f2
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 + ctx.f2.f64));
	// stfs f18,1156(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// stfs f16,1036(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// stfs f17,1164(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// stfs f15,2064(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2064, temp.u32);
	// stfs f11,5320(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5320, temp.u32);
	// fnmsubs f6,f25,f21,f6
	ctx.f6.f64 = double(float(-(ctx.f25.f64 * ctx.f21.f64 - ctx.f6.f64)));
	// lfs f11,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f7,f5,f24,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f24.f64 - ctx.f7.f64));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f8,756(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// fmuls f8,f11,f24
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f24.f64));
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// stfs f13,5312(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5312, temp.u32);
	// stfs f0,5316(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5316, temp.u32);
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// lfs f21,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f21.f64 = double(temp.f32);
	// lfs f13,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f13.f64 = double(temp.f32);
	// stfs f9,5304(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5304, temp.u32);
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f10,1084(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// fmuls f10,f21,f14
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f14.f64));
	// lfs f20,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f20.f64 = double(temp.f32);
	// stfs f2,1028(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// fmadds f12,f4,f20,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f2,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f2.f64 = double(temp.f32);
	// stfs f31,1076(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// lfs f5,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// lfs f31,2060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2060);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f31,f31,f4
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// stfs f3,828(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// fmuls f8,f8,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// lfs f13,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f13.f64 = double(temp.f32);
	// lfs f3,1648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1648);
	ctx.f3.f64 = double(temp.f32);
	// stfs f29,912(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fmuls f3,f3,f13
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// lfs f29,-19072(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19072);
	ctx.f29.f64 = double(temp.f32);
	// stfs f28,1060(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// stfs f29,552(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,1388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1388);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f10,f10,f29
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// fmsubs f9,f28,f0,f9
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 - ctx.f9.f64));
	// lfs f0,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f13,f28,f13
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// stfs f22,1020(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// fmuls f22,f29,f0
	ctx.f22.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f21,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f28,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f8,f5,f21,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f21.f64 + ctx.f8.f64));
	// lfs f31,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// lfs f19,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f19.f64 = double(temp.f32);
	// fadds f5,f19,f31
	ctx.f5.f64 = double(float(ctx.f19.f64 + ctx.f31.f64));
	// lfs f20,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f20.f64 = double(temp.f32);
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// lfs f31,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f9,f20,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f20.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// lfs f21,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f6,f23,f31,f6
	ctx.f6.f64 = double(float(ctx.f23.f64 * ctx.f31.f64 + ctx.f6.f64));
	// fnmsubs f12,f1,f21,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f21.f64 - ctx.f12.f64)));
	// stfs f25,880(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// lfs f2,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f2.f64 = double(temp.f32);
	// lfs f25,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f25,f4
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// lfs f20,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f28,f28,f4
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// lfs f31,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f23,f20,f24
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f24.f64));
	// lfs f1,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f19,f31,f30
	ctx.f19.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// lfs f21,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f1,f1,f4
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// stfs f26,776(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// fmuls f26,f2,f30
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// lfs f18,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f13,f21,f22
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f21.f64 + ctx.f22.f64));
	// lfs f17,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f17,f18
	ctx.f21.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// lfs f17,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f25,f25,f17,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f17.f64 - ctx.f0.f64));
	// lfs f22,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f3,f26,f22,f3
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f22.f64 - ctx.f3.f64));
	// fmsubs f10,f10,f17,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f17.f64 - ctx.f8.f64));
	// lfs f0,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f0.f64 = double(temp.f32);
	// lfs f22,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f22.f64 = double(temp.f32);
	// lfs f8,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f22,f0,f22
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f22.f64));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fadds f8,f5,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 + ctx.f8.f64));
	// lfs f17,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f9,f17,f0,f9
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f0,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f0,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f0.f64 = double(temp.f32);
	// stfs f7,5324(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5324, temp.u32);
	// fmr f15,f0
	ctx.f15.f64 = ctx.f0.f64;
	// fmadds f7,f1,f0,f25
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f25.f64));
	// lfs f25,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f25.f64 = double(temp.f32);
	// lfs f17,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f17.f64 = double(temp.f32);
	// lfs f5,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f5.f64 = double(temp.f32);
	// stfs f12,5308(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5308, temp.u32);
	// fnmsubs f12,f19,f0,f3
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// fmadds f13,f5,f17,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f26,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f26.f64 = double(temp.f32);
	// fsubs f8,f8,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f25.f64));
	// lfs f19,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f23,f0,f10
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f23,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f26,f4
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// stfs f6,5328(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5328, temp.u32);
	// lfs f6,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f6.f64 = double(temp.f32);
	// fsubs f9,f9,f23
	ctx.f9.f64 = double(float(ctx.f9.f64 - ctx.f23.f64));
	// fmr f23,f19
	ctx.f23.f64 = ctx.f19.f64;
	// stfs f11,616(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmadds f7,f21,f19,f7
	ctx.f7.f64 = double(float(ctx.f21.f64 * ctx.f19.f64 + ctx.f7.f64));
	// lfs f21,1540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f6,f6,f30
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// lfs f11,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f11.f64 = double(temp.f32);
	// lfs f3,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f1,f11,f30
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f30.f64));
	// lfs f17,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f13,f13,f30,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f30.f64 + ctx.f12.f64));
	// lfs f25,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f25.f64 = double(temp.f32);
	// fsubs f8,f8,f21
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f21.f64));
	// fmr f21,f19
	ctx.f21.f64 = ctx.f19.f64;
	// lfs f10,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f3,f18
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f18.f64));
	// stfs f2,604(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// fmuls f17,f17,f4
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// stfs f25,520(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f16,f25,f14
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// lfs f12,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f12.f64 = double(temp.f32);
	// fmsubs f28,f26,f19,f28
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f19.f64 - ctx.f28.f64));
	// lfs f19,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f19.f64 = double(temp.f32);
	// stfs f31,492(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f10,f6,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f26,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f12,f12,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// lfs f31,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f31,f31,f26,f22
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 - ctx.f22.f64));
	// lfs f2,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f2.f64 = double(temp.f32);
	// lfs f6,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f13,f1,f15,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f15.f64 - ctx.f13.f64)));
	// lfs f25,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f2,f2,f14
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// lfs f26,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f7,f3,f23,f7
	ctx.f7.f64 = double(float(-(ctx.f3.f64 * ctx.f23.f64 - ctx.f7.f64)));
	// fmuls f22,f17,f21
	ctx.f22.f64 = double(float(ctx.f17.f64 * ctx.f21.f64));
	// lfs f1,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f1.f64 = double(temp.f32);
	// lfs f21,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f6,f6,f27
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f27.f64));
	// lfs f17,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f17.f64 = double(temp.f32);
	// fmr f3,f15
	ctx.f3.f64 = ctx.f15.f64;
	// fmuls f23,f19,f24
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f24.f64));
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// fnmsubs f0,f16,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// fnmsubs f12,f12,f3,f7
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f3.f64 - ctx.f7.f64)));
	// lfs f26,1516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1516);
	ctx.f26.f64 = double(temp.f32);
	// fmr f7,f3
	ctx.f7.f64 = ctx.f3.f64;
	// lfs f16,1716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1716);
	ctx.f16.f64 = double(temp.f32);
	// fsubs f8,f8,f21
	ctx.f8.f64 = double(float(ctx.f8.f64 - ctx.f21.f64));
	// lfs f3,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f2,f2,f17,f28
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f17.f64 - ctx.f28.f64)));
	// stfs f8,5344(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5344, temp.u32);
	// fmuls f26,f26,f14
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// lfs f21,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f8,f23,f3,f0
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f0,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f9,f9,f16
	ctx.f9.f64 = double(float(ctx.f9.f64 - ctx.f16.f64));
	// lfs f3,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f0,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// stfs f12,5340(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5340, temp.u32);
	// fnmsubs f13,f10,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// lfs f10,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f1,f31,f14,f22
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f14.f64 - ctx.f22.f64));
	// lfs f16,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f22,f21,f4
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// lfs f21,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f21.f64 = double(temp.f32);
	// lfs f31,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f12,f10,f7
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f16,f27
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f27.f64));
	// fmuls f28,f25,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f7,f26,f21,f2
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f21.f64 + ctx.f2.f64));
	// lfs f23,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fadds f9,f9,f31
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f31.f64));
	// lfs f25,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f25.f64 = double(temp.f32);
	// lfs f17,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f21,f0
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// lfs f16,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f23,f23,f18
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f18.f64));
	// lfs f2,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f2.f64 = double(temp.f32);
	// fmr f0,f25
	ctx.f0.f64 = ctx.f25.f64;
	// lfs f31,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f26,f20,f2
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f2.f64));
	// stfs f13,5336(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5336, temp.u32);
	// fmsubs f3,f31,f25,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f25.f64 - ctx.f3.f64));
	// lfs f15,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f13,f17,f16
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// fmuls f6,f6,f15
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// lfs f17,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f10,f10,f2
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// stfs f10,340(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f25,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f15,f17,f18
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// lfs f10,1656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1656);
	ctx.f10.f64 = double(temp.f32);
	// addi r6,r31,36
	ctx.r6.s64 = ctx.r31.s64 + 36;
	// lfs f31,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f31.f64 = double(temp.f32);
	// fadds f10,f10,f25
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f25.f64));
	// fnmsubs f8,f28,f31,f8
	ctx.f8.f64 = double(float(-(ctx.f28.f64 * ctx.f31.f64 - ctx.f8.f64)));
	// lfs f25,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f25,f23,f25
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// lfs f28,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f26,f26,f0
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f20,1836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1836);
	ctx.f20.f64 = double(temp.f32);
	// addi r5,r30,36
	ctx.r5.s64 = ctx.r30.s64 + 36;
	// lfs f31,1928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1928);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f13,f13,f16,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f16.f64 + ctx.f12.f64));
	// lfs f23,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f31,f31,f28
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f28.f64));
	// fmadds f3,f20,f0,f3
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f3.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,1828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1828);
	ctx.f12.f64 = double(temp.f32);
	// fmsubs f6,f27,f23,f6
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f23.f64 - ctx.f6.f64));
	// lfs f28,2064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2064);
	ctx.f28.f64 = double(temp.f32);
	// addi r4,r1,5200
	ctx.r4.s64 = ctx.r1.s64 + 5200;
	// lfs f16,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f16.f64 = double(temp.f32);
	// li r3,7
	ctx.r3.s64 = 7;
	// lfs f27,1708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1708);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f12,f12,f18
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f18.f64));
	// fmuls f17,f17,f14
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// fmsubs f28,f28,f0,f21
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 - ctx.f21.f64));
	// fmsubs f29,f29,f0,f11
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 - ctx.f11.f64));
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// fadds f9,f9,f27
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f27.f64));
	// fmuls f14,f12,f0
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f12,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f15,f0,f26
	ctx.f0.f64 = double(float(ctx.f15.f64 * ctx.f0.f64 + ctx.f26.f64));
	// lfs f26,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f12,f12,f26,f28
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f26.f64 - ctx.f28.f64)));
	// lfs f23,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f23.f64 = double(temp.f32);
	// stfs f9,5360(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5360, temp.u32);
	// fmuls f23,f23,f4
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// lfs f11,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f4,f3,f4,f25
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f4.f64 + ctx.f25.f64));
	// fmadds f9,f13,f24,f8
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f24.f64 + ctx.f8.f64));
	// lfs f13,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f1,f22,f16,f1
	ctx.f1.f64 = double(float(ctx.f22.f64 * ctx.f16.f64 + ctx.f1.f64));
	// lfs f3,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f22,f11,f2
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// lfs f11,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f11.f64 = double(temp.f32);
	// lfs f27,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f10,f10,f18
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f18.f64));
	// fmuls f27,f27,f11
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f11.f64));
	// lfs f11,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f11.f64 = double(temp.f32);
	// lfs f20,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f3,f3,f13,f0
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// lfs f16,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f16.f64 = double(temp.f32);
	// lfs f0,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f31,f20,f16,f31
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 + ctx.f31.f64));
	// lfs f21,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f21.f64 = double(temp.f32);
	// stfs f1,5356(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5356, temp.u32);
	// fmadds f1,f0,f11,f12
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f12.f64));
	// stfs f6,5352(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5352, temp.u32);
	// fnmsubs f6,f5,f11,f29
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f11.f64 - ctx.f29.f64)));
	// fmr f12,f16
	ctx.f12.f64 = ctx.f16.f64;
	// stfs f7,5348(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5348, temp.u32);
	// fmuls f21,f21,f30
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// lfs f11,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f7,f19,f2
	ctx.f7.f64 = double(float(ctx.f19.f64 * ctx.f2.f64));
	// lfs f26,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f26.f64 = double(temp.f32);
	// lfs f20,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f4,f23,f13,f4
	ctx.f4.f64 = double(float(-(ctx.f23.f64 * ctx.f13.f64 - ctx.f4.f64)));
	// lfs f28,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f8,f20,f18
	ctx.f8.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// fnmsubs f3,f22,f13,f3
	ctx.f3.f64 = double(float(-(ctx.f22.f64 * ctx.f13.f64 - ctx.f3.f64)));
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f31,f30
	ctx.f5.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// lfs f29,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f10,f10,f0,f14
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 - ctx.f14.f64));
	// lfs f23,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f29,f29,f18
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f18.f64));
	// fmadds f6,f26,f11,f6
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f11.f64 + ctx.f6.f64));
	// lfs f26,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f28,f28,f12,f27
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f12.f64 - ctx.f27.f64));
	// lfs f27,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f21,f12
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f12.f64));
	// lfs f24,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f27,f27,f25,f26
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f25.f64 - ctx.f26.f64));
	// lfs f26,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f23,f30
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// lfs f23,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f24,f18
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f18.f64));
	// lfs f22,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f13,f7,f13,f3
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f3.f64));
	// fnmsubs f11,f23,f11,f1
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f11.f64 - ctx.f1.f64)));
	// stfs f11,5368(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5368, temp.u32);
	// fmadds f9,f17,f26,f9
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f26.f64 + ctx.f9.f64));
	// stfs f9,5332(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5332, temp.u32);
	// fnmsubs f9,f8,f0,f4
	ctx.f9.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// stfs f9,5364(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5364, temp.u32);
	// lfs f9,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f9,f9,f22,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f22.f64 + ctx.f6.f64));
	// stfs f9,5392(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5392, temp.u32);
	// fmsubs f11,f28,f30,f31
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 - ctx.f31.f64));
	// stfs f11,5376(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5376, temp.u32);
	// fmadds f11,f29,f0,f10
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 + ctx.f10.f64));
	// stfs f11,5372(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5372, temp.u32);
	// fmuls f11,f27,f2
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// stfs f11,5380(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5380, temp.u32);
	// fmsubs f12,f25,f12,f5
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f12.f64 - ctx.f5.f64));
	// stfs f12,5384(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5384, temp.u32);
	// fnmsubs f0,f24,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f0,5388(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5388, temp.u32);
	// bl 0x82242e58
	ctx.lr = 0x82244EEC;
	sub_82242E58(ctx, base);
	// cmplwi cr6,r29,4
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 4, ctx.xer);
	// bgt cr6,0x82244ef8
	if (ctx.cr6.gt) goto loc_82244EF8;
	// b 0x8225dc2c
	goto loc_8225DC2C;
loc_82244EF8:
	// lfs f9,288(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f22,f9,f3
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// lfs f6,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f11.f64 = double(temp.f32);
	// lfs f2,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f21,f6,f11
	ctx.f21.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// fmuls f23,f9,f2
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f10,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f24,f8,f2
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// fmuls f25,f5,f10
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// lfs f13,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f20,f8,f3
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// lfs f0,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f19,f0,f13
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f26,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f18,f1,f0
	ctx.f18.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmuls f31,f22,f10
	ctx.f31.f64 = double(float(ctx.f22.f64 * ctx.f10.f64));
	// stfs f31,340(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfd f26,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f26.u64);
	// fmuls f17,f13,f10
	ctx.f17.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f27,f21,f2
	ctx.f27.f64 = double(float(ctx.f21.f64 * ctx.f2.f64));
	// stfs f27,452(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f7,f23,f11
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f11.f64));
	// stfs f7,460(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f4,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f14,f24,f11
	ctx.f14.f64 = double(float(ctx.f24.f64 * ctx.f11.f64));
	// lfs f30,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f16,f25,f3
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f3.f64));
	// stfs f14,1836(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1836, temp.u32);
	// fmuls f15,f13,f25
	ctx.f15.f64 = double(float(ctx.f13.f64 * ctx.f25.f64));
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f14,f30,f4
	ctx.f14.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// stfs f25,520(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f25,f20,f10
	ctx.f25.f64 = double(float(ctx.f20.f64 * ctx.f10.f64));
	// stfd f30,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f30.u64);
	// lfs f27,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f28.f64 = double(temp.f32);
	// stfs f20,484(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f20,f29,f5
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// stfs f24,492(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f24,f13,f24
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f24.f64));
	// stfd f10,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f10.u64);
	// stfd f27,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f27.u64);
	// fmuls f27,f28,f6
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// stfs f16,1716(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1716, temp.u32);
	// fmuls f16,f13,f2
	ctx.f16.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lfs f12,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f12.f64 = double(temp.f32);
	// stfs f25,540(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmuls f25,f6,f0
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f26,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f26.f64 = double(temp.f32);
	// stfs f26,1516(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1516, temp.u32);
	// fmuls f26,f30,f0
	ctx.f26.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f30,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f30.f64 = double(temp.f32);
	// lfs f10,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f10.f64 = double(temp.f32);
	// stfs f30,444(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// fmuls f30,f9,f0
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f23,1076(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// fmuls f23,f5,f0
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f10,696(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// fmuls f10,f13,f8
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// stfs f20,848(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// fmuls f20,f6,f4
	ctx.f20.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// stfs f15,1236(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// fmuls f15,f9,f4
	ctx.f15.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// stfs f24,1156(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// fmuls f24,f8,f4
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfs f17,1244(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// fmuls f17,f5,f4
	ctx.f17.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f12,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f12.f64 = double(temp.f32);
	// lfs f7,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f7.f64 = double(temp.f32);
	// lfs f31,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f31.f64 = double(temp.f32);
	// stfs f27,676(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 676, temp.u32);
	// stfs f26,1656(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1656, temp.u32);
	// stfs f16,1648(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1648, temp.u32);
	// stfs f14,1388(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1388, temp.u32);
	// stfs f30,1828(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1828, temp.u32);
	// fmuls f30,f12,f28
	ctx.f30.f64 = double(float(ctx.f12.f64 * ctx.f28.f64));
	// lfd f27,1096(r1)
	ctx.f27.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// fmuls f28,f7,f28
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// stfs f28,1540(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1540, temp.u32);
	// fmuls f28,f12,f29
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// stfs f10,552(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// fmuls f29,f7,f29
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f29.f64));
	// stfs f29,952(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// fmuls f10,f13,f11
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// stfs f10,340(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmuls f29,f27,f0
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfs f29,984(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// fmuls f29,f19,f1
	ctx.f29.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// lfd f10,720(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// stfs f29,2060(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2060, temp.u32);
	// fmuls f29,f1,f10
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f25,980(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// fmuls f25,f18,f2
	ctx.f25.f64 = double(float(ctx.f18.f64 * ctx.f2.f64));
	// stfs f29,956(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// fmuls f29,f12,f3
	ctx.f29.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// stfs f25,976(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// fmuls f25,f12,f2
	ctx.f25.f64 = double(float(ctx.f12.f64 * ctx.f2.f64));
	// stfs f29,1048(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// fmuls f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fmuls f29,f1,f3
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// stfs f30,2064(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2064, temp.u32);
	// fmuls f1,f1,f11
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f1,996(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// lfd f30,1696(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// fmuls f1,f24,f0
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// stfs f1,1708(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1708, temp.u32);
	// fmuls f1,f7,f30
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// stfs f1,404(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// fmuls f1,f31,f7
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// stfs f1,580(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmuls f1,f20,f0
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// stfs f1,964(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// fmuls f1,f17,f0
	ctx.f1.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// stfs f1,968(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// fmuls f1,f12,f30
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// stfs f1,992(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// fmuls f1,f31,f12
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f12.f64));
	// stfs f25,1004(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// fmuls f25,f12,f4
	ctx.f25.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// stfs f1,576(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 576, temp.u32);
	// fmuls f1,f15,f0
	ctx.f1.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// stfs f28,500(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fmuls f14,f13,f3
	ctx.f14.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// stfs f25,1084(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// fmuls f25,f13,f21
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// lfd f26,704(r1)
	ctx.f26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// fmuls f16,f13,f22
	ctx.f16.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// stfs f1,868(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 868, temp.u32);
	// fmuls f28,f18,f10
	ctx.f28.f64 = double(float(ctx.f18.f64 * ctx.f10.f64));
	// fmuls f13,f13,f9
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// stfs f23,972(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// fmuls f1,f8,f0
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f2,448(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// stfs f28,592(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// fmuls f23,f31,f4
	ctx.f23.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// stfs f13,452(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f2,f0,f4
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// stfs f1,768(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// fmuls f28,f26,f0
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fmuls f30,f31,f0
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f4,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f4.f64 = double(temp.f32);
	// lfs f13,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f31,f31,f6
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// stfs f19,828(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// fmuls f1,f7,f0
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f24,844(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 844, temp.u32);
	// fmuls f3,f5,f3
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// stfs f20,1928(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1928, temp.u32);
	// stfs f17,796(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 796, temp.u32);
	// stfs f15,960(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 960, temp.u32);
	// stfs f28,524(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// stfs f29,544(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// stfs f25,872(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 872, temp.u32);
	// stfs f16,588(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// stfs f14,432(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// stfs f23,824(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// stfs f30,840(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 840, temp.u32);
	// fmuls f30,f9,f4
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// stfs f30,604(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// fmr f30,f4
	ctx.f30.f64 = ctx.f4.f64;
	// stfs f2,228(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// fmuls f17,f6,f13
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// fmuls f14,f12,f4
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// lfs f26,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f16,f12,f10
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f25,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f27,f12,f13
	ctx.f27.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f25,852(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 852, temp.u32);
	// fmuls f24,f9,f13
	ctx.f24.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f25,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f20,f8,f13
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f25,504(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 504, temp.u32);
	// fmuls f2,f5,f13
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f12,616(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f28,f6,f4
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// stfs f28,784(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// fmuls f12,f13,f26
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// stfs f12,276(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// lfs f28,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f28.f64 = double(temp.f32);
	// fmr f12,f11
	ctx.f12.f64 = ctx.f11.f64;
	// fmuls f30,f28,f30
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f26,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f26.f64 = double(temp.f32);
	// lfs f28,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f9,f11
	ctx.f29.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// lfs f26,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f26.f64 = double(temp.f32);
	// lfs f28,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f6,f10
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// lfs f26,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f26.f64 = double(temp.f32);
	// lfs f6,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f6.f64 = double(temp.f32);
	// stfs f31,836(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 836, temp.u32);
	// fmuls f31,f9,f10
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f26,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f9,f25,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// lfs f6,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f8,f4
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfs f14,452(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f4,f5,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f14,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f14.f64 = double(temp.f32);
	// lfs f23,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f23,f14,f23
	ctx.f23.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// lfs f25,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f25.f64 = double(temp.f32);
	// stfs f6,492(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// lfs f26,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f8,f10
	ctx.f26.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// lfs f6,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f8,f8,f11
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// stfs f29,484(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f29,f25,f12
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f12.f64));
	// stfs f14,752(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// fmuls f14,f3,f12
	ctx.f14.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// stfs f22,896(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// fmuls f22,f22,f12
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f12.f64));
	// lfs f19,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f19.f64 = double(temp.f32);
	// lfs f10,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f10,f7,f13
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lfs f6,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f19,f21,f19
	ctx.f19.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// stfd f12,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f12.u64);
	// fmuls f7,f7,f11
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f11.f64));
	// lfs f15,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f11,f5,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// lfs f12,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f12.f64 = double(temp.f32);
	// lfs f6,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f15,f12,f15
	ctx.f15.f64 = double(float(ctx.f12.f64 * ctx.f15.f64));
	// lfs f6,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f6.f64 = double(temp.f32);
	// lfs f6,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f6.f64 = double(temp.f32);
	// stfs f9,800(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// stfs f25,1060(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// stfs f29,804(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 804, temp.u32);
	// stfs f23,860(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 860, temp.u32);
	// stfs f19,772(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 772, temp.u32);
	// stfs f3,1024(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// stfs f10,340(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmuls f5,f12,f13
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f7,460(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// fmuls f3,f31,f13
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f13.f64));
	// stfs f11,520(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// lfs f6,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f11.f64 = double(temp.f32);
	// lfs f29,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f25,f6,f11
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// stfs f22,700(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 700, temp.u32);
	// fmuls f29,f6,f29
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// lfs f23,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f24,f0
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// stfs f24,1028(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// fmuls f24,f20,f0
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// stfd f6,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f6.u64);
	// fmuls f6,f27,f0
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfs f25,856(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 856, temp.u32);
	// fmuls f25,f1,f11
	ctx.f25.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f29,780(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 780, temp.u32);
	// fmuls f29,f23,f13
	ctx.f29.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// stfs f15,644(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 644, temp.u32);
	// fmuls f15,f26,f13
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// stfs f20,912(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fmuls f20,f28,f13
	ctx.f20.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// stfs f14,732(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 732, temp.u32);
	// lfs f19,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f19.f64 = double(temp.f32);
	// lfs f14,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f14.f64 = double(temp.f32);
	// lfs f9,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f7.f64 = double(temp.f32);
	// stfs f30,924(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 924, temp.u32);
	// fmuls f30,f2,f0
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f23,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f23.f64 = double(temp.f32);
	// stfs f17,880(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// fmuls f17,f17,f0
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// stfs f6,560(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 560, temp.u32);
	// stfs f25,608(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 608, temp.u32);
	// stfs f29,512(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 512, temp.u32);
	// stfs f22,480(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 480, temp.u32);
	// stfs f15,468(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 468, temp.u32);
	// stfs f26,1288(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1288, temp.u32);
	// stfs f20,224(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// lfs f26,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f26.f64 = double(temp.f32);
	// lfs f20,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f20.f64 = double(temp.f32);
	// lfs f6,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f6.f64 = double(temp.f32);
	// lfs f25,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f25.f64 = double(temp.f32);
	// lfs f29,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f29.f64 = double(temp.f32);
	// lfs f10,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f10.f64 = double(temp.f32);
	// lfs f22,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f15.f64 = double(temp.f32);
	// stfs f5,712(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 712, temp.u32);
	// fmuls f5,f23,f7
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f7.f64));
	// stfs f31,1280(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1280, temp.u32);
	// fmuls f31,f19,f14
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f14.f64));
	// stfs f2,776(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// fmuls f2,f1,f13
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// stfs f24,648(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 648, temp.u32);
	// fmuls f24,f1,f9
	ctx.f24.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// stfs f28,756(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// fmuls f19,f22,f0
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfs f3,884(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 884, temp.u32);
	// fmuls f3,f16,f13
	ctx.f3.f64 = double(float(ctx.f16.f64 * ctx.f13.f64));
	// stfs f30,760(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// fmuls f30,f18,f10
	ctx.f30.f64 = double(float(ctx.f18.f64 * ctx.f10.f64));
	// stfs f23,1068(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// fmuls f14,f15,f13
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f13.f64));
	// stfs f1,508(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f23,f26,f13
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// stfs f17,292(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
	// fmuls f1,f20,f13
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// stfs f6,1020(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// fmuls f17,f21,f13
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f13.f64));
	// stfs f27,1216(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f25,484(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f27,f8,f13
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f29,616(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f25,f25,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f28,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// stfs f5,384(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// lfs f5,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,520(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfs f5,340(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmuls f5,f13,f9
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// stfs f8,1492(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1492, temp.u32);
	// fmuls f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// stfs f13,1012(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1012, temp.u32);
	// lfs f13,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f13.f64 = double(temp.f32);
	// stfs f30,472(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 472, temp.u32);
	// stfs f1,620(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 620, temp.u32);
	// fmuls f1,f13,f7
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
	// stfs f31,368(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 368, temp.u32);
	// fmuls f31,f28,f9
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f9.f64));
	// stfs f20,832(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 832, temp.u32);
	// stfs f6,420(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 420, temp.u32);
	// lfs f28,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f28.f64 = double(temp.f32);
	// lfd f6,704(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// stfs f19,612(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 612, temp.u32);
	// fmuls f6,f28,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// lfs f28,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f28.f64 = double(temp.f32);
	// stfs f3,296(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
	// fmuls f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f14,476(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 476, temp.u32);
	// lfs f14,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f14.f64 = double(temp.f32);
	// stfs f31,144(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// lfs f31,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f31.f64 = double(temp.f32);
	// stfs f29,164(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// fmuls f31,f31,f11
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f11.f64));
	// stfs f3,1724(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1724, temp.u32);
	// fmuls f3,f18,f11
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f11.f64));
	// lfs f8,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f8.f64 = double(temp.f32);
	// stfs f12,340(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfs f8,128(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lfs f8,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f30,f13,f8
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f13,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f20,f13,f7
	ctx.f20.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
	// lfs f7,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// fmr f19,f7
	ctx.f19.f64 = ctx.f7.f64;
	// stfs f23,304(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f25,200(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// fmuls f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f25,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f25.f64 = double(temp.f32);
	// fmr f28,f14
	ctx.f28.f64 = ctx.f14.f64;
	// stfs f22,1200(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// fmuls f29,f13,f11
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f11,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f11.f64 = double(temp.f32);
	// stfs f27,104(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fmuls f27,f11,f25
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f25.f64));
	// stfs f24,264(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// fmuls f14,f14,f21
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f21.f64));
	// lfs f24,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f24.f64 = double(temp.f32);
	// stfs f15,1340(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1340, temp.u32);
	// stfs f17,256(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// lfs f17,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f9,f15,f9
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f9.f64));
	// lfs f15,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f15.f64 = double(temp.f32);
	// lfd f12,720(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// stfs f2,280(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// fmuls f2,f18,f12
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// lfs f23,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f11,f23
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f23.f64));
	// lfs f11,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f24,f24,f11
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f11.f64));
	// stfs f23,1164(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// fmuls f19,f19,f11
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f11.f64));
	// lfs f23,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f11,f17,f11
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f11.f64));
	// lfs f17,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f17.f64 = double(temp.f32);
	// stfs f21,1036(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// fmuls f28,f28,f23
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// stfs f23,1040(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// fmuls f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// stfs f25,1016(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// fmuls f25,f13,f8
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f21,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f23.f64 = double(temp.f32);
	// lfs f15,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f15.f64 = double(temp.f32);
	// stfs f4,340(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmr f4,f23
	ctx.f4.f64 = ctx.f23.f64;
	// fmuls f21,f21,f8
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// stfs f0,1936(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1936, temp.u32);
	// fmuls f8,f23,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// stfs f8,896(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// fmuls f8,f13,f12
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// stfs f8,604(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// lfs f8,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f12,f15,f12
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f12.f64));
	// lfs f8,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f8.f64 = double(temp.f32);
	// stfs f11,2144(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2144, temp.u32);
	// lfs f0,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f11.f64 = double(temp.f32);
	// stfs f9,1820(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1820, temp.u32);
	// fmuls f11,f0,f11
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// fmuls f8,f4,f10
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// lfs f4,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f4,f13,f10
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f13,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f0,f13
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f7,2072(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2072, temp.u32);
	// lfs f13,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f0.f64 = double(temp.f32);
	// lfs f7,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f7.f64 = double(temp.f32);
	// stfs f27,1412(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1412, temp.u32);
	// fmr f27,f15
	ctx.f27.f64 = ctx.f15.f64;
	// stfs f6,1396(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1396, temp.u32);
	// fmuls f6,f0,f13
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// fmuls f7,f7,f13
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// stfs f21,752(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// fmr f21,f15
	ctx.f21.f64 = ctx.f15.f64;
	// stfs f5,988(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 988, temp.u32);
	// fmuls f5,f0,f13
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f3,1692(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1692, temp.u32);
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// lfs f3,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f31,108(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// fmuls f3,f0,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// stfs f28,1292(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1292, temp.u32);
	// stfs f2,1812(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1812, temp.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f28,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// stfs f24,1260(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1260, temp.u32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// stfs f19,1640(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1640, temp.u32);
	// fmr f19,f15
	ctx.f19.f64 = ctx.f15.f64;
	// lfs f24,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f10,f21,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f10.f64));
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// stfs f22,1564(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1564, temp.u32);
	// stfs f17,1508(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1508, temp.u32);
	// fmr f17,f0
	ctx.f17.f64 = ctx.f0.f64;
	// lfs f22,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f22.f64 = double(temp.f32);
	// lfs f24,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f24,f22,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// lfs f21,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f21.f64 = double(temp.f32);
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f21,f22
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// lfs f21,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f19,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f19.f64 = double(temp.f32);
	// lfs f15,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f15.f64 = double(temp.f32);
	// lfs f23,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f23.f64 = double(temp.f32);
	// stfs f14,1268(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1268, temp.u32);
	// fmuls f23,f23,f29
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f29.f64));
	// fmuls f19,f17,f19
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// lfs f17,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f17,f15,f17
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f17.f64));
	// lfs f14,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f15.f64 = double(temp.f32);
	// stfs f8,1140(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// fmr f8,f0
	ctx.f8.f64 = ctx.f0.f64;
	// stfs f4,784(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f14,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f14.f64 = double(temp.f32);
	// lfs f4,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f4.f64 = double(temp.f32);
	// stfs f3,1224(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// lfs f3,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f3.f64 = double(temp.f32);
	// stfs f10,492(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// lfs f10,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f10.f64 = double(temp.f32);
	// stfs f2,1728(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1728, temp.u32);
	// lfs f2,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f2.f64 = double(temp.f32);
	// stfs f12,460(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// fmuls f8,f8,f14
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f14.f64));
	// lfs f14,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// stfs f4,1620(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1620, temp.u32);
	// lfs f4,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f4.f64 = double(temp.f32);
	// lfs f14,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f3,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f10,f10,f14
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// lfs f14,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f3,f2,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// fmr f2,f14
	ctx.f2.f64 = ctx.f14.f64;
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfs f23,1308(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1308, temp.u32);
	// stfs f13,1704(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1704, temp.u32);
	// fmuls f23,f0,f14
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f12,1944(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1944, temp.u32);
	// lfs f0,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f12.f64 = double(temp.f32);
	// stfs f31,1636(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1636, temp.u32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f31,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f31.f64 = double(temp.f32);
	// stfs f11,1668(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1668, temp.u32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f10,1420(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1420, temp.u32);
	// fmuls f2,f31,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f10.f64 = double(temp.f32);
	// stfs f29,1532(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1532, temp.u32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f29,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// stfs f9,1316(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1316, temp.u32);
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f29,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// lfs f13,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f13.f64 = double(temp.f32);
	// stfs f28,1404(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1404, temp.u32);
	// lfs f28,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// stfs f7,1908(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1908, temp.u32);
	// fmuls f7,f13,f0
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f8,1232(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// fmuls f8,f18,f0
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// stfs f27,1332(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1332, temp.u32);
	// fmuls f29,f28,f29
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// lfs f0,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f13.f64 = double(temp.f32);
	// lfs f27,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f0,f27
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// stfs f24,1740(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1740, temp.u32);
	// stfs f6,1192(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// stfs f5,1324(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1324, temp.u32);
	// fmuls f5,f0,f13
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f6,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f0,f0,f28
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// lfs f24,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f13,f6,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f28,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f24,f6,f24
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f24.f64));
	// stfs f22,1612(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1612, temp.u32);
	// stfs f21,1372(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1372, temp.u32);
	// stfs f19,1356(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1356, temp.u32);
	// stfs f17,1380(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1380, temp.u32);
	// stfs f15,1208(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// stfs f12,1468(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1468, temp.u32);
	// stfs f0,1444(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1444, temp.u32);
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,1460(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1460, temp.u32);
	// fmuls f11,f0,f0
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f0,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f0.f64 = double(temp.f32);
	// stfs f4,1364(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1364, temp.u32);
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// stfs f31,1556(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1556, temp.u32);
	// lfs f31,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f31.f64 = double(temp.f32);
	// lfs f19,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f19.f64 = double(temp.f32);
	// lfs f15,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f19,f30,f19
	ctx.f19.f64 = double(float(ctx.f30.f64 * ctx.f19.f64));
	// lfs f14,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f14.f64 = double(temp.f32);
	// stfs f30,1616(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1616, temp.u32);
	// fmuls f14,f15,f14
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// lfs f30,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f30.f64 = double(temp.f32);
	// stfs f15,828(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// lfs f15,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f4,f4,f31
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// lfs f31,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// fmr f30,f15
	ctx.f30.f64 = ctx.f15.f64;
	// lfs f21,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f21.f64 = double(temp.f32);
	// stfs f6,452(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f28,f28,f22
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f22.f64));
	// lfs f6,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f6.f64 = double(temp.f32);
	// fmr f22,f21
	ctx.f22.f64 = ctx.f21.f64;
	// stfs f9,1240(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// fmr f9,f6
	ctx.f9.f64 = ctx.f6.f64;
	// stfs f7,1428(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1428, temp.u32);
	// fmr f7,f6
	ctx.f7.f64 = ctx.f6.f64;
	// stfs f29,1256(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// fmuls f21,f1,f21
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// lfs f29,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f29.f64 = double(temp.f32);
	// stfs f28,1188(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// stfs f3,1628(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1628, temp.u32);
	// lfs f28,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f29,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// lfs f3,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f29.f64 = double(temp.f32);
	// stfs f13,1484(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1484, temp.u32);
	// fmuls f13,f3,f6
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// lfs f17,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f29,f28,f29
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// stfs f18,776(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// fmuls f22,f17,f22
	ctx.f22.f64 = double(float(ctx.f17.f64 * ctx.f22.f64));
	// stfs f10,1676(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1676, temp.u32);
	// stfs f8,1436(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1436, temp.u32);
	// stfs f27,1580(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1580, temp.u32);
	// stfs f5,1452(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1452, temp.u32);
	// stfs f2,1548(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1548, temp.u32);
	// stfs f1,1632(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1632, temp.u32);
	// lfs f18,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f18.f64 = double(temp.f32);
	// lfs f10,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f18,f18,f15
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f15.f64));
	// lfs f8,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f10,f25,f10
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// lfs f2,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f20,f8
	ctx.f8.f64 = double(float(ctx.f20.f64 * ctx.f8.f64));
	// lfs f6,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f9,f2,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// lfs f1,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f7,f1,f7
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f6,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f28,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f16,f27
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f27.f64));
	// stfs f23,1524(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1524, temp.u32);
	// stfs f24,1596(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1596, temp.u32);
	// fmr f24,f28
	ctx.f24.f64 = ctx.f28.f64;
	// lfs f23,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f23.f64 = double(temp.f32);
	// stfs f21,1328(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1328, temp.u32);
	// stfs f17,1360(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1360, temp.u32);
	// stfs f22,1432(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1432, temp.u32);
	// stfs f19,1544(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1544, temp.u32);
	// stfs f14,1588(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1588, temp.u32);
	// fmuls f28,f28,f24
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// lfs f22,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f22.f64 = double(temp.f32);
	// fmr f24,f0
	ctx.f24.f64 = ctx.f0.f64;
	// stfs f27,1568(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1568, temp.u32);
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f23,f26,f23
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f21,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f21.f64 = double(temp.f32);
	// stfs f25,1212(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// fmr f25,f27
	ctx.f25.f64 = ctx.f27.f64;
	// lfs f19,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f19,f0
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f16,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f0,f17
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f17.f64));
	// lfs f15,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f16,f0
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f14,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f14.f64 = double(temp.f32);
	// stfs f13,1116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// fmuls f24,f24,f22
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// lfs f22,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f21,f22
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// lfs f21,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f0,f21
	ctx.f21.f64 = double(float(ctx.f0.f64 * ctx.f21.f64));
	// lfs f13,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f0,f0,f15
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f15.f64));
	// lfs f15,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f15.f64 = double(temp.f32);
	// stfs f0,1204(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// fmuls f14,f15,f14
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// lfs f0,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// stfs f10,1272(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1272, temp.u32);
	// stfs f5,1312(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1312, temp.u32);
	// stfs f4,2092(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2092, temp.u32);
	// stfs f31,1816(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1816, temp.u32);
	// stfs f29,1368(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1368, temp.u32);
	// stfs f23,1108(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// stfs f15,756(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// stfs f20,1624(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1624, temp.u32);
	// stfs f8,1320(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1320, temp.u32);
	// stfs f7,1336(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1336, temp.u32);
	// stfs f30,1132(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// stfs f28,1592(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1592, temp.u32);
	// stfs f12,1576(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1576, temp.u32);
	// stfs f26,1348(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1348, temp.u32);
	// stfs f24,1600(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1600, temp.u32);
	// stfs f22,1056(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// lfs f15,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f15.f64 = double(temp.f32);
	// lfs f12,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f10,f7,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f8,f5
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f31,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f28,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f24,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f26,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// lfs f23,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f24,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// lfs f22,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f22.f64 = double(temp.f32);
	// lfs f20,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// stfs f9,1124(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// fmuls f9,f0,f13
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f11,1584(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1584, temp.u32);
	// fmuls f11,f12,f15
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f15.f64));
	// stfs f19,1000(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// fmuls f13,f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// stfs f21,892(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// fmuls f23,f20,f23
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f19.f64 = double(temp.f32);
	// stfs f17,1180(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// stfs f16,1064(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// stfs f14,1112(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// fmr f16,f19
	ctx.f16.f64 = ctx.f19.f64;
	// lfs f15,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f14.f64 = double(temp.f32);
	// stfs f11,1080(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// lfs f11,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// stfs f1,340(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfs f2,452(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// stfs f10,1220(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// fmr f10,f11
	ctx.f10.f64 = ctx.f11.f64;
	// lfs f17,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f17.f64 = double(temp.f32);
	// stfs f12,784(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// fmuls f21,f17,f21
	ctx.f21.f64 = double(float(ctx.f17.f64 * ctx.f21.f64));
	// stfs f17,616(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f19,f19,f16
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// lfs f12,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f12.f64 = double(temp.f32);
	// fmr f16,f15
	ctx.f16.f64 = ctx.f15.f64;
	// lfs f17,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f17.f64 = double(temp.f32);
	// stfs f26,520(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f26,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f17,f0,f17
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f17.f64));
	// stfs f8,492(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// stfs f9,900(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// fmr f9,f10
	ctx.f9.f64 = ctx.f10.f64;
	// stfs f18,460(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f2,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f2.f64 = double(temp.f32);
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// stfs f30,484(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f18,f3,f18
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f18.f64));
	// fmuls f16,f15,f16
	ctx.f16.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f30,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f30.f64 = double(temp.f32);
	// fmr f15,f14
	ctx.f15.f64 = ctx.f14.f64;
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// stfs f7,1172(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// fmuls f1,f6,f1
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// stfs f2,1296(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1296, temp.u32);
	// stfs f3,112(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lfs f3,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f3.f64 = double(temp.f32);
	// lfs f7,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f11,f3,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f13,944(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// fmuls f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// stfs f12,948(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// lfs f13,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f14,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f14,f0,f14
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f5,1120(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// fmuls f0,f0,f26
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f26.f64));
	// stfs f0,1176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// fmr f26,f8
	ctx.f26.f64 = ctx.f8.f64;
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f8,f30,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// stfs f8,740(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// stfs f4,876(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f28,1660(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1660, temp.u32);
	// stfs f31,1184(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// stfs f29,1088(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// stfs f24,1300(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1300, temp.u32);
	// stfs f27,928(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// stfs f22,1008(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// fmuls f26,f2,f26
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// lfs f2,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f2.f64 = double(temp.f32);
	// stfs f25,888(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// fmuls f10,f2,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f20,1500(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1500, temp.u32);
	// stfs f23,920(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// stfs f21,668(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// stfs f19,932(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f16,1044(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// stfs f15,1052(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// stfs f14,940(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// stfs f17,1136(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// stfs f30,1476(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1476, temp.u32);
	// stfs f26,736(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// stfs f18,816(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// stfs f6,1352(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1352, temp.u32);
	// stfs f1,744(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// lfs f16,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f16.f64 = double(temp.f32);
	// fmr f14,f16
	ctx.f14.f64 = ctx.f16.f64;
	// lfs f30,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f24,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f24.f64 = double(temp.f32);
	// lfs f28,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f16,f12,f16
	ctx.f16.f64 = double(float(ctx.f12.f64 * ctx.f16.f64));
	// lfs f31,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f28,f24
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// lfs f22,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f29,f13,f29
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// lfs f23,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// lfs f24,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f24.f64 = double(temp.f32);
	// stfs f13,752(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// fmuls f24,f23,f24
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f24.f64));
	// lfs f15,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f15.f64 = double(temp.f32);
	// fmr f13,f14
	ctx.f13.f64 = ctx.f14.f64;
	// lfs f19,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// fmr f18,f20
	ctx.f18.f64 = ctx.f20.f64;
	// stfs f23,484(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f23,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// stfs f19,452(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmr f19,f23
	ctx.f19.f64 = ctx.f23.f64;
	// lfs f25,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f8,f25,f8
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// stfs f25,460(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f25,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f25.f64 = double(temp.f32);
	// lfs f5,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f13,f25,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f5,f4
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f17,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f15,f18
	ctx.f18.f64 = double(float(ctx.f15.f64 * ctx.f18.f64));
	// lfs f6,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f20,f17,f20
	ctx.f20.f64 = double(float(ctx.f17.f64 * ctx.f20.f64));
	// lfs f27,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f6,f1,f6
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// stfs f5,604(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// fmuls f27,f26,f27
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// stfs f13,340(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfs f3,1068(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// lfs f5,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f14,f5,f14
	ctx.f14.f64 = double(float(ctx.f5.f64 * ctx.f14.f64));
	// lfs f3,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f23,f13,f23
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f23.f64));
	// stfs f11,904(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// fmuls f19,f3,f19
	ctx.f19.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// stfs f2,1684(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1684, temp.u32);
	// stfs f10,936(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// stfs f7,1572(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1572, temp.u32);
	// stfs f9,808(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// stfs f8,748(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// stfs f6,764(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f1,360(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f4,652(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f31,792(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// stfs f30,600(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfs f29,680(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// stfs f0,640(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// stfs f26,1832(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1832, temp.u32);
	// stfs f27,788(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// stfs f28,688(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// stfs f24,532(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// stfs f22,488(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// stfs f21,684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// stfs f17,1652(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1652, temp.u32);
	// stfs f20,656(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// stfs f15,756(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// stfs f18,664(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// stfs f12,1276(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1276, temp.u32);
	// lfs f0,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// stfs f0,556(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// lfs f12,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,1284(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1284, temp.u32);
	// fmuls f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f5,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// fmr f5,f0
	ctx.f5.f64 = ctx.f0.f64;
	// lfs f4,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// lfs f24,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f24.f64 = double(temp.f32);
	// lfs f9,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f9.f64 = double(temp.f32);
	// lfs f1,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f24,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// lfs f28,1648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1648);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f1,f0,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// lfs f26,1388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1388);
	ctx.f26.f64 = double(temp.f32);
	// stfs f24,880(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// fmuls f20,f26,f28
	ctx.f20.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// lfs f24,1828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1828);
	ctx.f24.f64 = double(temp.f32);
	// lfs f31,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f4,f2,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f2,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// lfs f2,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f2.f64 = double(temp.f32);
	// stfs f20,616(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f20,f24,f28
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f28.f64));
	// lfs f17,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f2,f31,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f20,896(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// lfs f31,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,1656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1656);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,2064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2064);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f27,f29,f31
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// stfs f13,776(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// lfs f13,1928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1928);
	ctx.f13.f64 = double(temp.f32);
	// stfs f28,520(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f28,f20,f17
	ctx.f28.f64 = double(float(ctx.f20.f64 * ctx.f17.f64));
	// stfs f23,564(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// lfs f23,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,1540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	ctx.f21.f64 = double(temp.f32);
	// stfs f31,828(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f25,1604(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1604, temp.u32);
	// lfs f25,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f25.f64 = double(temp.f32);
	// lfs f22,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f22.f64 = double(temp.f32);
	// stfs f28,452(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// stfs f16,628(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// fmuls f16,f21,f17
	ctx.f16.f64 = double(float(ctx.f21.f64 * ctx.f17.f64));
	// lfs f28,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f28.f64 = double(temp.f32);
	// stfs f31,492(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f11,f11,f28
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// stfs f27,460(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// stfs f14,672(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// stfs f17,340(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// stfs f3,1164(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// lfs f10,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f10,f25,f10
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// lfs f7,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f23,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// lfs f30,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f7,f22,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// lfs f18,1516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1516);
	ctx.f18.f64 = double(temp.f32);
	// lfs f15,1716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1716);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,1836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1836);
	ctx.f14.f64 = double(temp.f32);
	// lfs f31,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// lfs f17,2060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2060);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f27,f14,f27
	ctx.f27.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// lfs f3,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f3.f64 = double(temp.f32);
	// stfs f12,912(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fadds f12,f15,f18
	ctx.f12.f64 = double(float(ctx.f15.f64 + ctx.f18.f64));
	// fmuls f28,f3,f17
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// stfs f10,572(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f10,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f10,f16,f10
	ctx.f10.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// stfs f7,436(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// stfs f6,392(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// lfs f7,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f12,f12,f7,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f7.f64 - ctx.f27.f64));
	// fmsubs f11,f31,f6,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f6.f64 - ctx.f11.f64));
	// stfs f1,408(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f0,364(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f13,204(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f1,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,1708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1708);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f7.f64 = double(temp.f32);
	// stfs f30,252(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f9,528(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fadds f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f4,412(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmuls f4,f1,f7
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// stfs f2,388(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// lfs f30,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f30.f64 = double(temp.f32);
	// lfs f2,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f2.f64 = double(temp.f32);
	// stfs f29,1252(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// fmuls f2,f30,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// lfs f27,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f30.f64 = double(temp.f32);
	// fmsubs f10,f27,f29,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 - ctx.f10.f64));
	// stfs f25,1304(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1304, temp.u32);
	// fmuls f30,f28,f30
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// stfs f24,1076(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// lfs f25,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f9,f9,f25
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f25.f64));
	// lfs f31,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f31.f64 = double(temp.f32);
	// stfs f19,464(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fmuls f19,f24,f25
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// stfs f5,336(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f31,f17,f31
	ctx.f31.f64 = double(float(ctx.f17.f64 * ctx.f31.f64));
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f27.f64 = double(temp.f32);
	// lfs f5,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f27,f29,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 + ctx.f11.f64));
	// stfs f8,440(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fnmsubs f12,f5,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f8,3124(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3124);
	ctx.f8.f64 = double(temp.f32);
	// stfs f19,452(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// stfs f17,340(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f16,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f17.f64 = double(temp.f32);
	// stfs f23,1712(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1712, temp.u32);
	// fmadds f19,f16,f17,f19
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f17.f64 + ctx.f19.f64));
	// stfs f18,1084(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// stfs f20,1540(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1540, temp.u32);
	// stfs f8,244(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
	// lfs f8,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f8.f64 = double(temp.f32);
	// lfs f28,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f6,f8,f25
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f25.f64));
	// lfs f29,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f27.f64 = double(temp.f32);
	// lfs f23,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f23.f64 = double(temp.f32);
	// lfs f20,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f4,f4,f23
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// lfs f18,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f18.f64 = double(temp.f32);
	// stfs f22,1344(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1344, temp.u32);
	// stfs f26,1748(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1748, temp.u32);
	// fmuls f26,f29,f28
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// stfs f21,1656(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1656, temp.u32);
	// fmuls f21,f27,f1
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// stfs f15,1140(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// fadds f15,f18,f20
	ctx.f15.f64 = double(float(ctx.f18.f64 + ctx.f20.f64));
	// stfs f14,1024(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// stfs f16,604(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// lfs f22,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f22.f64 = double(temp.f32);
	// lfs f14,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f16.f64 = double(temp.f32);
	// stfs f14,520(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f14,f14,f17
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f17.f64));
	// stfs f14,484(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// lfs f14,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f2,f2,f14
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// lfs f14,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f14.f64 = double(temp.f32);
	// stfs f19,460(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// fmuls f31,f31,f14
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f14.f64));
	// lfs f19,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f19.f64 = double(temp.f32);
	// lfs f14,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f30,f19
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f19.f64));
	// fmr f19,f14
	ctx.f19.f64 = ctx.f14.f64;
	// stfs f17,492(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmadds f10,f6,f14,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f14.f64 + ctx.f10.f64));
	// lfs f6,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f12,f22,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// lfs f17,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f17,f16,f17
	ctx.f17.f64 = double(float(ctx.f16.f64 * ctx.f17.f64));
	// lfs f16,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f1,f16
	ctx.f16.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// stfs f1,1648(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1648, temp.u32);
	// stfs f0,592(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// lfs f1,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f0.f64 = double(temp.f32);
	// stfs f7,1800(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1800, temp.u32);
	// fmuls f7,f0,f25
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// lfs f14,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f9,f9,f19
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f19.f64));
	// stfs f29,1196(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// fmr f19,f6
	ctx.f19.f64 = ctx.f6.f64;
	// lfs f6,2736(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2736);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,184(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// fnmsubs f10,f26,f1,f10
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f1.f64 - ctx.f10.f64)));
	// lfs f6,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f6,f17,f6,f2
	ctx.f6.f64 = double(float(ctx.f17.f64 * ctx.f6.f64 - ctx.f2.f64));
	// stfs f8,1928(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1928, temp.u32);
	// stfs f5,1016(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// lfs f5,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f8.f64 = double(temp.f32);
	// stfs f3,1608(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1608, temp.u32);
	// fmuls f3,f5,f8
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f8.f64));
	// stfs f27,1228(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// lfs f27,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f27.f64 = double(temp.f32);
	// fmr f29,f19
	ctx.f29.f64 = ctx.f19.f64;
	// lfs f1,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f11,f14,f19,f11
	ctx.f11.f64 = double(float(-(ctx.f14.f64 * ctx.f19.f64 - ctx.f11.f64)));
	// stfs f18,1060(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// fnmsubs f10,f7,f27,f10
	ctx.f10.f64 = double(float(-(ctx.f7.f64 * ctx.f27.f64 - ctx.f10.f64)));
	// lfs f27,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f26.f64 = double(temp.f32);
	// lfs f19,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f19.f64 = double(temp.f32);
	// lfs f7,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f19,f5,f19
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f19.f64));
	// lfs f18,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f7,f16,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 * ctx.f7.f64));
	// stfs f13,508(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f13,f21,f23
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// stfs f28,980(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// fmuls f3,f3,f23
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// stfs f24,2064(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2064, temp.u32);
	// fmadds f12,f15,f29,f12
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f29.f64 + ctx.f12.f64));
	// lfs f29,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f24,f29,f5
	ctx.f24.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// fmuls f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f2,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f2,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f2.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f2,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f1.f64 = double(temp.f32);
	// stfs f22,1036(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// fmuls f28,f1,f2
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// stfs f20,1028(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// fmuls f20,f26,f25
	ctx.f20.f64 = double(float(ctx.f26.f64 * ctx.f25.f64));
	// lfs f22,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f6,f6,f27,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f27.f64 + ctx.f4.f64));
	// lfs f4,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f4,f31,f4,f30
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f4.f64 - ctx.f30.f64));
	// lfs f31,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f9,f31,f18,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f18.f64 - ctx.f9.f64));
	// lfs f16,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f24,f24,f23
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f23.f64));
	// fmadds f11,f16,f17,f11
	ctx.f11.f64 = double(float(ctx.f16.f64 * ctx.f17.f64 + ctx.f11.f64));
	// lfs f17,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f10,f28,f17,f10
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f17.f64 + ctx.f10.f64));
	// lfs f17,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f17.f64 = double(temp.f32);
	// stfs f0,1716(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1716, temp.u32);
	// fmuls f7,f7,f17
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f17.f64));
	// lfs f17,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f17.f64 = double(temp.f32);
	// fadds f30,f21,f22
	ctx.f30.f64 = double(float(ctx.f21.f64 + ctx.f22.f64));
	// lfs f0,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f15.f64 = double(temp.f32);
	// lfs f28,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f15
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f15.f64));
	// lfs f31,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f31.f64 = double(temp.f32);
	// lfs f18,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f12,f31,f18,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f18.f64 - ctx.f12.f64)));
	// fnmsubs f11,f0,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f0.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// lfs f18,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f18.f64 = double(temp.f32);
	// fmr f0,f17
	ctx.f0.f64 = ctx.f17.f64;
	// stfs f26,544(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// fnmsubs f6,f3,f16,f6
	ctx.f6.f64 = double(float(-(ctx.f3.f64 * ctx.f16.f64 - ctx.f6.f64)));
	// lfs f16,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f18,f16
	ctx.f14.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f3,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f19,f3
	ctx.f3.f64 = double(float(ctx.f19.f64 * ctx.f3.f64));
	// lfs f26,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f26.f64 = double(temp.f32);
	// stfs f29,1072(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// fmuls f30,f30,f25
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f29,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f29.f64 = double(temp.f32);
	// stfs f23,1004(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// stfs f27,616(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// lfs f23,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f23.f64 = double(temp.f32);
	// stfs f6,5212(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5212, temp.u32);
	// fmadds f11,f28,f0,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f28,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f28.f64 = double(temp.f32);
	// fadds f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 + ctx.f26.f64));
	// lfs f26,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f29,f0
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f6,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f26,f23,f26
	ctx.f26.f64 = double(float(ctx.f23.f64 * ctx.f26.f64));
	// stfs f15,340(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f15,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f4,f13,f15,f4
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f15.f64 + ctx.f4.f64));
	// lfs f19,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f9,f20,f19,f9
	ctx.f9.f64 = double(float(-(ctx.f20.f64 * ctx.f19.f64 - ctx.f9.f64)));
	// lfs f13,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// lfs f20,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f20.f64 = double(temp.f32);
	// stfs f12,5200(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5200, temp.u32);
	// fmuls f12,f13,f20
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f20.f64));
	// stfs f22,2060(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2060, temp.u32);
	// fnmsubs f11,f14,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f14.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f6,f0
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f7,f3,f0,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 - ctx.f7.f64));
	// stfs f1,968(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// fmr f3,f15
	ctx.f3.f64 = ctx.f15.f64;
	// stfs f31,1156(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// stfs f8,1732(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1732, temp.u32);
	// stfs f5,1388(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1388, temp.u32);
	// stfs f10,5208(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5208, temp.u32);
	// lfs f10,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f13.f64 = double(temp.f32);
	// lfs f8,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f10,f1
	ctx.f1.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// lfs f22,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f4,f24,f3,f4
	ctx.f4.f64 = double(float(-(ctx.f24.f64 * ctx.f3.f64 - ctx.f4.f64)));
	// lfs f0,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f21,448(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfs f2,896(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// fmuls f2,f8,f5
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f21,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f21.f64 = double(temp.f32);
	// fmr f3,f20
	ctx.f3.f64 = ctx.f20.f64;
	// stfs f18,136(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f29,752(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// fmuls f3,f21,f3
	ctx.f3.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// lfs f29,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f9,f30,f29,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 + ctx.f9.f64));
	// lfs f29,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f29.f64 = double(temp.f32);
	// stfs f5,492(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f13,f29,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// lfs f5,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f1,f29
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f29.f64));
	// fmuls f30,f5,f30
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// lfs f29,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f29.f64 = double(temp.f32);
	// stfs f11,5204(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5204, temp.u32);
	// fmadds f29,f28,f29,f26
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 + ctx.f26.f64));
	// lfs f11,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f11.f64 = double(temp.f32);
	// stfs f5,1020(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// stfs f9,484(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// lfs f9,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f15,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f5,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f11,f30,f11,f1
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f11.f64 - ctx.f1.f64));
	// stfs f12,220(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmr f1,f28
	ctx.f1.f64 = ctx.f28.f64;
	// lfs f21,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f12,f15,f28,f23
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f28.f64 - ctx.f23.f64));
	// lfs f20,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f5,f5,f9
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f17,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f17.f64 = double(temp.f32);
	// fadds f19,f20,f21
	ctx.f19.f64 = double(float(ctx.f20.f64 + ctx.f21.f64));
	// lfs f24,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f14,f17,f16
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// stfs f14,460(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// fmadds f0,f24,f0,f27
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f27.f64));
	// lfs f27,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f27.f64 = double(temp.f32);
	// lfs f14,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f14.f64 = double(temp.f32);
	// stfs f7,5216(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5216, temp.u32);
	// lfs f28,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f28.f64 = double(temp.f32);
	// lfs f7,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f7.f64 = double(temp.f32);
	// stfs f9,452(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f9,f27,f14
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f14.f64));
	// fmsubs f13,f13,f1,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f1.f64 - ctx.f31.f64));
	// stfs f9,340(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmsubs f5,f5,f28,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f28.f64 - ctx.f2.f64));
	// stfs f3,308(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// stfs f8,1040(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// fnmsubs f12,f7,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// stfs f6,404(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// stfs f24,1644(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1644, temp.u32);
	// lfs f18,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f18.f64 = double(temp.f32);
	// lfs f9,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f26,f18,f9
	ctx.f26.f64 = double(float(ctx.f18.f64 * ctx.f9.f64));
	// lfs f8,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f6.f64 = double(temp.f32);
	// lfs f24,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f24.f64 = double(temp.f32);
	// lfs f2,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f30,f14,f24
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f24.f64));
	// lfs f1,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f9,f2
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f28,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f28.f64 = double(temp.f32);
	// stfs f22,324(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f22,f28,f25
	ctx.f22.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// stfs f4,5220(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5220, temp.u32);
	// fmuls f4,f8,f6
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// stfs f10,152(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f10,f3,f16
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f16.f64));
	// stfs f20,444(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// stfs f21,848(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// stfs f27,1516(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1516, temp.u32);
	// fadds f27,f19,f1
	ctx.f27.f64 = double(float(ctx.f19.f64 + ctx.f1.f64));
	// lfs f23,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f20.f64 = double(temp.f32);
	// stfs f18,1128(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// stfs f17,1264(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1264, temp.u32);
	// stfs f15,1836(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1836, temp.u32);
	// fmadds f11,f0,f8,f11
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f11.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmr f17,f21
	ctx.f17.f64 = ctx.f21.f64;
	// stfs f8,756(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// fmadds f13,f29,f21,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f21.f64 + ctx.f13.f64));
	// lfs f21,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f5,f21,f0,f5
	ctx.f5.f64 = double(float(-(ctx.f21.f64 * ctx.f0.f64 - ctx.f5.f64)));
	// lfs f21,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f19,f8,f20
	ctx.f19.f64 = double(float(ctx.f8.f64 * ctx.f20.f64));
	// stfs f25,784(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// stfs f19,520(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmuls f8,f21,f25
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f25.f64));
	// lfs f19,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f19.f64 = double(temp.f32);
	// lfs f25,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f25.f64 = double(temp.f32);
	// stfs f30,452(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f25,f25,f19
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// lfs f30,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f30.f64 = double(temp.f32);
	// stfs f19,828(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// fnmsubs f11,f26,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// lfs f17,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f17.f64 = double(temp.f32);
	// lfs f26,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f13,f27,f0,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f13.f64));
	// fmuls f26,f26,f17
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f17.f64));
	// lfs f17,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f17.f64 = double(temp.f32);
	// lfs f19,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f30,f17
	ctx.f17.f64 = double(float(ctx.f30.f64 * ctx.f17.f64));
	// stfs f6,340(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f6,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f6.f64 = double(temp.f32);
	// stfs f30,912(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fnmsubs f0,f6,f0,f5
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f5.f64)));
	// lfs f27,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f30.f64 = double(temp.f32);
	// fadds f5,f27,f30
	ctx.f5.f64 = double(float(ctx.f27.f64 + ctx.f30.f64));
	// lfs f27,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f27.f64 = double(temp.f32);
	// lfs f6,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f6,f22,f6,f27
	ctx.f6.f64 = double(float(-(ctx.f22.f64 * ctx.f6.f64 - ctx.f27.f64)));
	// lfs f27,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f30,f27
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// stfs f30,460(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f30,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f30.f64 = double(temp.f32);
	// lfs f27,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f27.f64 = double(temp.f32);
	// lfs f18,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f27,f30,f27
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// lfs f29,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f12,f23,f18,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f18.f64 + ctx.f12.f64));
	// stfs f30,776(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// fmuls f15,f9,f29
	ctx.f15.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f30,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f30.f64 = double(temp.f32);
	// stfs f9,1236(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// fmr f9,f30
	ctx.f9.f64 = ctx.f30.f64;
	// lfs f18,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f11,f31,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f22,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f18,f22
	ctx.f22.f64 = double(float(ctx.f18.f64 * ctx.f22.f64));
	// stfs f22,484(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// stfs f16,1048(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// lfs f16,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f22.f64 = double(temp.f32);
	// stfs f20,604(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// fnmsubs f12,f16,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f16.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// lfs f20,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f20,f19,f20
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lfs f31,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f9,f25,f9,f0
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64 + ctx.f0.f64));
	// stfs f2,148(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmadds f13,f22,f31,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f31.f64 + ctx.f13.f64));
	// stfs f1,696(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// fmr f0,f30
	ctx.f0.f64 = ctx.f30.f64;
	// stfs f29,180(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f19,880(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// stfs f7,1828(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1828, temp.u32);
	// stfs f3,860(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 860, temp.u32);
	// stfs f28,1708(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1708, temp.u32);
	// stfs f14,1244(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// stfs f24,524(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// stfs f23,972(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// stfs f21,952(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// stfs f18,856(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 856, temp.u32);
	// stfs f16,964(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// fmr f3,f31
	ctx.f3.f64 = ctx.f31.f64;
	// lfs f2,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f2.f64 = double(temp.f32);
	// fmr f31,f30
	ctx.f31.f64 = ctx.f30.f64;
	// lfs f1,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f12,f2,f1,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f12.f64));
	// lfs f30,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f7,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f1,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f11,f15,f0,f11
	ctx.f11.f64 = double(float(ctx.f15.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fmuls f1,f1,f30
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f28.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f24,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f9,f20,f3,f9
	ctx.f9.f64 = double(float(-(ctx.f20.f64 * ctx.f3.f64 - ctx.f9.f64)));
	// lfs f18,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f18.f64 = double(temp.f32);
	// fmr f30,f31
	ctx.f30.f64 = ctx.f31.f64;
	// stfs f7,540(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmr f3,f29
	ctx.f3.f64 = ctx.f29.f64;
	// lfs f21,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f13,f26,f29,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f13.f64));
	// lfs f29,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f4,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// lfs f4,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f31.f64 = double(temp.f32);
	// fadds f26,f24,f23
	ctx.f26.f64 = double(float(ctx.f24.f64 + ctx.f23.f64));
	// fmuls f5,f5,f31
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// lfs f23,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f25,f31
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f31.f64));
	// lfs f24,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f16,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f16.f64 = double(temp.f32);
	// lfs f7,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f20,f31,f21
	ctx.f20.f64 = double(float(ctx.f31.f64 * ctx.f21.f64));
	// stfs f22,992(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// fmadds f10,f10,f30,f9
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f30.f64 + ctx.f9.f64));
	// lfs f9,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f4,f3,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f3.f64 + ctx.f12.f64));
	// lfs f3,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f19,f28,f0,f13
	ctx.f19.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f13,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f30,f17,f0,f11
	ctx.f30.f64 = double(float(-(ctx.f17.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f11,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f3,f3,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f9,996(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// fmadds f8,f8,f11,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f11.f64 + ctx.f6.f64));
	// lfs f11,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f6,f27,f23
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f23.f64));
	// lfs f27,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f23,f11,f31
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// lfs f11,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f11.f64 = double(temp.f32);
	// lfs f17,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f5,f5,f27
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// stfs f8,5224(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5224, temp.u32);
	// fmuls f27,f11,f27
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// fmadds f10,f13,f29,f10
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f29.f64 + ctx.f10.f64));
	// lfs f13,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f24,f9,f24,f12
	ctx.f24.f64 = double(float(-(ctx.f9.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f9,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f9.f64 = double(temp.f32);
	// lfs f29,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f17,f17,f16
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// fmadds f30,f13,f0,f30
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f30.f64));
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f29,f18,f29
	ctx.f29.f64 = double(float(ctx.f18.f64 * ctx.f29.f64));
	// lfs f18,2848(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2848);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f22,f3,f13
	ctx.f22.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f2,984(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// fmadds f8,f26,f25,f1
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f25.f64 + ctx.f1.f64));
	// stfs f4,500(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fnmsubs f7,f7,f12,f19
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f12.f64 - ctx.f19.f64)));
	// stfs f24,5232(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5232, temp.u32);
	// stfs f28,956(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// fnmsubs f10,f9,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// stfs f30,5228(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5228, temp.u32);
	// stfs f21,116(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f9,f11,f31
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f3,712(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 712, temp.u32);
	// lfs f11,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f17,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f12.f64));
	// lfs f3,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f2,f3,f11,f27
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f11.f64 + ctx.f27.f64));
	// lfs f28,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f29,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// fmuls f28,f22,f27
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f27.f64));
	// lfs f24,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f24.f64 = double(temp.f32);
	// lfs f21,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f11,f20,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f11.f64));
	// fmuls f19,f21,f24
	ctx.f19.f64 = double(float(ctx.f21.f64 * ctx.f24.f64));
	// lfs f15,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f15.f64 = double(temp.f32);
	// lfs f0,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f1,f13
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f22,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f25,492(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f22,f0,f22
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f22.f64));
	// fmr f25,f15
	ctx.f25.f64 = ctx.f15.f64;
	// lfs f4,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f4.f64 = double(temp.f32);
	// stfs f6,484(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f26,f4,f24
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// fmadds f8,f8,f15,f28
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f15.f64 + ctx.f28.f64));
	// lfs f15,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f15.f64 = double(temp.f32);
	// lfs f27,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f13,f23,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// lfs f6,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f27,f27,f0
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// fmuls f6,f15,f6
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f6.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f16,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f19,f19,f15
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f15.f64));
	// lfs f20,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f0,f2,f0,f29
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f29.f64));
	// lfs f15,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f15.f64 = double(temp.f32);
	// fmsubs f12,f20,f16,f12
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 - ctx.f12.f64));
	// lfs f16,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f22,f22,f31
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// lfs f29,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f30,f25
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// fmsubs f8,f5,f15,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f15.f64 - ctx.f8.f64));
	// stfs f22,452(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmadds f1,f29,f16,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f16.f64 + ctx.f1.f64));
	// lfs f17,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f17.f64 = double(temp.f32);
	// fmr f5,f15
	ctx.f5.f64 = ctx.f15.f64;
	// lfs f2,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f2.f64 = double(temp.f32);
	// lfs f22,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f2,f26,f2
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f14,f17,f24
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// lfs f26,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f22,f9,f22
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// stfs f22,340(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f22,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f0,f0,f31,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f31.f64 - ctx.f11.f64));
	// fnmsubs f12,f26,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f26.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// lfs f22,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f22.f64 = double(temp.f32);
	// lfs f11,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f27,f27,f31
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// fmuls f11,f11,f22
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f22.f64));
	// lfs f28,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f5,f1,f5,f30
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f30.f64));
	// lfs f22,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f22.f64 = double(temp.f32);
	// lfs f1,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f30,f28,f16
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f15,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f1,f22,f1
	ctx.f1.f64 = double(float(ctx.f22.f64 * ctx.f1.f64));
	// stfs f10,460(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// fmuls f10,f25,f28
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// stfs f18,552(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// fmsubs f2,f14,f15,f2
	ctx.f2.f64 = double(float(ctx.f14.f64 * ctx.f15.f64 - ctx.f2.f64));
	// stfs f3,612(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 612, temp.u32);
	// stfs f4,800(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// stfs f21,960(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 960, temp.u32);
	// stfs f23,504(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 504, temp.u32);
	// fmr f4,f18
	ctx.f4.f64 = ctx.f18.f64;
	// stfs f29,472(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 472, temp.u32);
	// lfs f29,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// fnmsubs f0,f27,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f27.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f13,f13,f29,f8
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f29.f64 - ctx.f8.f64)));
	// stfs f20,796(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 796, temp.u32);
	// lfs f20,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f22,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f20,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f20.f64 = double(temp.f32);
	// stfs f9,844(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 844, temp.u32);
	// lfs f9,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f9,f24
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f24.f64));
	// lfs f8,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f10,f10,f4,f19
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f4.f64 + ctx.f19.f64));
	// lfs f4,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f4.f64 = double(temp.f32);
	// fmr f19,f18
	ctx.f19.f64 = ctx.f18.f64;
	// stfs f26,852(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 852, temp.u32);
	// fnmsubs f12,f6,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f6,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f7,f8,f6,f7
	ctx.f7.f64 = double(float(-(ctx.f8.f64 * ctx.f6.f64 - ctx.f7.f64)));
	// lfs f6,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f6.f64 = double(temp.f32);
	// lfs f26,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f26.f64 = double(temp.f32);
	// lfs f4,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f4.f64 = double(temp.f32);
	// lfs f27,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f31,f4
	ctx.f29.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// lfs f23,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f22,f31
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// stfs f28,784(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// fmuls f21,f23,f24
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f24.f64));
	// stfs f25,580(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmuls f25,f27,f24
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// lfs f28,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f2,f30,f19,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f19.f64 + ctx.f2.f64));
	// lfs f19,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f19,f20,f0
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f20,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f13,f19,f20,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f20.f64 - ctx.f13.f64)));
	// lfs f30,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f30.f64 = double(temp.f32);
	// fmr f20,f15
	ctx.f20.f64 = ctx.f15.f64;
	// lfs f19,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f12,f6,f30,f12
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f30.f64 - ctx.f12.f64)));
	// lfs f18,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// stfs f17,872(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 872, temp.u32);
	// stfs f16,520(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// lfs f17,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f5,f5,f31,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f31.f64 + ctx.f0.f64));
	// lfs f0,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f0,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f11,f11,f0,f13
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f13,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f10,f3,f20,f10
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f20.f64 - ctx.f10.f64)));
	// lfs f20,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f3,f19,f31
	ctx.f3.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f7,f20,f18,f7
	ctx.f7.f64 = double(float(-(ctx.f20.f64 * ctx.f18.f64 - ctx.f7.f64)));
	// lfs f18,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f19,f16,f19,f17
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f19.f64 + ctx.f17.f64));
	// lfs f0,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f12,f1,f14,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f14.f64 - ctx.f12.f64)));
	// lfs f17,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f30,f18,f13,f30
	ctx.f30.f64 = double(float(ctx.f18.f64 * ctx.f13.f64 - ctx.f30.f64));
	// lfs f16,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f16.f64 = double(temp.f32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f15,f17,f24
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// lfs f13,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// stfs f9,768(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// fnmsubs f2,f25,f0,f2
	ctx.f2.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// stfs f8,1076(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// fmuls f9,f16,f24
	ctx.f9.f64 = double(float(ctx.f16.f64 * ctx.f24.f64));
	// stfs f4,512(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 512, temp.u32);
	// fmadds f10,f21,f0,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 + ctx.f10.f64));
	// fmuls f8,f13,f3
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// stfs f7,5240(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5240, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f12,f7,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f13,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f13.f64 = double(temp.f32);
	// lfs f29,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f5,f28,f13,f5
	ctx.f5.f64 = double(float(-(ctx.f28.f64 * ctx.f13.f64 - ctx.f5.f64)));
	// lfs f4,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f29,f29,f31
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// stfs f3,868(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 868, temp.u32);
	// fmadds f11,f22,f13,f11
	ctx.f11.f64 = double(float(ctx.f22.f64 * ctx.f13.f64 + ctx.f11.f64));
	// lfs f3,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f24,f4
	ctx.f4.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// lfs f28,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f28.f64 = double(temp.f32);
	// stfs f6,576(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 576, temp.u32);
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// lfs f6,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f6.f64 = double(temp.f32);
	// stfs f27,832(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 832, temp.u32);
	// stfs f26,760(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// lfs f13,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f12,f6,f0,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f27,-19076(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19076);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f26.f64 = double(temp.f32);
	// stfs f27,104(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fmuls f27,f26,f13
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// stfs f18,840(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 840, temp.u32);
	// fmuls f13,f13,f29
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// lfs f14,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfs f16,836(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 836, temp.u32);
	// fmuls f18,f18,f14
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f14.f64));
	// lfs f22,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f16.f64 = double(temp.f32);
	// lfs f25,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f4,f4,f16
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// stfs f14,752(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// fmuls f21,f25,f22
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f22.f64));
	// stfs f29,604(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// stfs f10,460(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f14,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f14.f64 = double(temp.f32);
	// lfs f29,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f29.f64 = double(temp.f32);
	// lfs f10,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f29,f14,f29
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// lfs f26,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f10,f14,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f10.f64));
	// stfs f20,976(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// fnmsubs f0,f26,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f26.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// stfs f22,896(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// stfs f23,480(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 480, temp.u32);
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f12,f22,f16,f27
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f16.f64 + ctx.f27.f64));
	// lfs f14,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f23,f23,f20
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f20.f64));
	// stfs f19,5236(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5236, temp.u32);
	// fmuls f14,f24,f14
	ctx.f14.f64 = double(float(ctx.f24.f64 * ctx.f14.f64));
	// stfs f9,484(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// stfs f17,824(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// stfs f8,492(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// lfs f19,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f24,f19
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// lfs f17,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f17.f64 = double(temp.f32);
	// lfs f22,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f20,f17,f20
	ctx.f20.f64 = double(float(ctx.f17.f64 * ctx.f20.f64));
	// lfs f16,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f22,f24,f22
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// lfs f8,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f16,f16,f8
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f8.f64));
	// stfs f2,340(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmuls f9,f24,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// stfs f14,616(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// lfs f27,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f27.f64 = double(temp.f32);
	// lfs f2,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f14.f64 = double(temp.f32);
	// stfs f15,452(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fnmsubs f0,f23,f2,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f2.f64 - ctx.f0.f64)));
	// lfs f15,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f15.f64 = double(temp.f32);
	// fmsubs f4,f19,f14,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f14.f64 - ctx.f4.f64));
	// stfs f8,588(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// fmsubs f3,f20,f15,f3
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f15.f64 - ctx.f3.f64));
	// lfs f8,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f8.f64 = double(temp.f32);
	// stfs f6,432(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// lfs f6,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f6.f64 = double(temp.f32);
	// lfs f23,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f13,f13,f23
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f23.f64));
	// lfs f2,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f2,f18,f2,f21
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f2.f64 + ctx.f21.f64));
	// stfs f28,924(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 924, temp.u32);
	// lfs f28,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f28.f64 = double(temp.f32);
	// lfs f21,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f0,f10,f8,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f8,f22,f6,f4
	ctx.f8.f64 = double(float(-(ctx.f22.f64 * ctx.f6.f64 - ctx.f4.f64)));
	// lfs f6,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f6,f1,f6,f5
	ctx.f6.f64 = double(float(-(ctx.f1.f64 * ctx.f6.f64 - ctx.f5.f64)));
	// lfs f1,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f10,f16,f10,f3
	ctx.f10.f64 = double(float(-(ctx.f16.f64 * ctx.f10.f64 - ctx.f3.f64)));
	// lfs f3,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f3,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// lfs f5,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f5.f64 = double(temp.f32);
	// fmr f1,f14
	ctx.f1.f64 = ctx.f14.f64;
	// lfs f23,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f23.f64 = double(temp.f32);
	// lfs f4,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f4.f64 = double(temp.f32);
	// stfs f7,884(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 884, temp.u32);
	// fmuls f7,f21,f23
	ctx.f7.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// lfs f20,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// stfs f27,608(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 608, temp.u32);
	// fmuls f20,f27,f20
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// fmadds f0,f5,f28,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f28,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f27.f64 = double(temp.f32);
	// stfs f17,620(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 620, temp.u32);
	// lfs f19,1692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1692);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f13,f12,f1,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f1.f64 - ctx.f13.f64));
	// lfs f12,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f9,f12,f8
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 + ctx.f8.f64));
	// lfs f9,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f10,f2,f1,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f10.f64));
	// fmuls f2,f30,f9
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f9.f64));
	// lfs f30,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f30.f64 = double(temp.f32);
	// lfs f8,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f15,f19,f17
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// lfs f1,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f8,f28,f8,f30
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f8.f64 + ctx.f30.f64));
	// fmr f30,f1
	ctx.f30.f64 = ctx.f1.f64;
	// lfs f28,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f1,f27,f1,f28
	ctx.f1.f64 = double(float(-(ctx.f27.f64 * ctx.f1.f64 - ctx.f28.f64)));
	// lfs f27,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f27.f64 = double(temp.f32);
	// stfs f11,5244(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5244, temp.u32);
	// fmuls f11,f29,f31
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// stfs f26,804(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 804, temp.u32);
	// fmuls f7,f7,f31
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// lfs f26,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f24
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// lfs f23,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f28,f28,f4
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// lfs f21,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f18,f23,f21
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// fnmsubs f6,f27,f30,f6
	ctx.f6.f64 = double(float(-(ctx.f27.f64 * ctx.f30.f64 - ctx.f6.f64)));
	// lfs f30,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f30.f64 = double(temp.f32);
	// lfs f27,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f16,f24,f16
	ctx.f16.f64 = double(float(ctx.f24.f64 * ctx.f16.f64));
	// stfs f25,644(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 644, temp.u32);
	// fmuls f25,f30,f26
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// stfs f15,452(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// fmuls f22,f27,f26
	ctx.f22.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// lfs f14,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f15.f64 = double(temp.f32);
	// stfs f26,340(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f14,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f14.f64 = double(temp.f32);
	// stfs f25,1692(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1692, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f26,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f26.f64 = double(temp.f32);
	// lis r10,-32252
	ctx.r10.s64 = -2113667072;
	// lfs f25,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f26,f14
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// fnmsubs f13,f11,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f14,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f14.f64 = double(temp.f32);
	// lfs f11,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f3,f3,f14,f0
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// lfs f25,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f25,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f25.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// lfs f0,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f10,f20,f0,f10
	ctx.f10.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f10.f64));
	// fmr f0,f11
	ctx.f0.f64 = ctx.f11.f64;
	// stfs f9,1140(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// fmuls f9,f15,f31
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// stfs f21,1028(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// lfs f25,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f11,f11,f29
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f29.f64));
	// stfs f30,772(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 772, temp.u32);
	// fmuls f30,f25,f29
	ctx.f30.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f25,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f25.f64 = double(temp.f32);
	// stfs f27,1024(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// stfs f29,776(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// lfs f27,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f12,f16,f25,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f25.f64 + ctx.f12.f64));
	// fmuls f29,f29,f27
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f27.f64));
	// stfs f19,1060(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// fmuls f20,f0,f4
	ctx.f20.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f7,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f27,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f27.f64 = double(temp.f32);
	// lfs f19,1652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1652);
	ctx.f19.f64 = double(temp.f32);
	// stfs f17,700(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 700, temp.u32);
	// lfs f17,1476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1476);
	ctx.f17.f64 = double(temp.f32);
	// lfs f25,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f15,f17,f31
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f31.f64));
	// stfs f1,5256(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5256, temp.u32);
	// fmadds f22,f25,f27,f22
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f27.f64 + ctx.f22.f64));
	// stfs f23,1016(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// stfs f8,5248(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5248, temp.u32);
	// stfs f6,5260(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5260, temp.u32);
	// fmuls f29,f29,f31
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// stfs f28,520(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmadds f9,f9,f0,f13
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f13,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f21,f24,f13
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// lfs f8,1396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1396);
	ctx.f8.f64 = double(temp.f32);
	// fmr f13,f14
	ctx.f13.f64 = ctx.f14.f64;
	// lfs f7,1412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1412);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,1564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1564);
	ctx.f6.f64 = double(temp.f32);
	// lfs f1,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f1.f64 = double(temp.f32);
	// lfs f28,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f28.f64 = double(temp.f32);
	// lfs f23,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f1,f1,f28
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f16,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f23,f24,f23
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f23.f64));
	// stfs f5,1048(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// fmuls f5,f7,f8
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// stfs f2,5252(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5252, temp.u32);
	// fmuls f2,f6,f8
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// stfs f30,492(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// stfs f27,1396(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1396, temp.u32);
	// fmadds f3,f26,f13,f3
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f13.f64 + ctx.f3.f64));
	// lfs f13,-19080(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19080);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f26,f19,f27
	ctx.f26.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// stfs f26,460(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f26,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f12,f18,f13,f12
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f13.f64 - ctx.f12.f64)));
	// stfs f15,1652(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1652, temp.u32);
	// fmuls f27,f26,f16
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f14,1280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1280);
	ctx.f14.f64 = double(temp.f32);
	// lfs f30,1288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1288);
	ctx.f30.f64 = double(temp.f32);
	// lfs f15,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f18.f64 = double(temp.f32);
	// stfs f20,1476(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1476, temp.u32);
	// stfs f13,224(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// fnmsubs f0,f1,f0,f9
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f20,1692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1692);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f5,f18
	ctx.f18.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// fnmsubs f10,f20,f15,f10
	ctx.f10.f64 = double(float(-(ctx.f20.f64 * ctx.f15.f64 - ctx.f10.f64)));
	// lfs f15,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f15.f64 = double(temp.f32);
	// stfs f10,1692(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1692, temp.u32);
	// fmuls f10,f14,f31
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f31.f64));
	// lfs f20,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f20,f15
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// stfs f10,1280(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1280, temp.u32);
	// lfs f15,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f15.f64 = double(temp.f32);
	// lfs f10,1656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1656);
	ctx.f10.f64 = double(temp.f32);
	// stfs f31,1564(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1564, temp.u32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// fmuls f10,f10,f15
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f15.f64));
	// lfs f13,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f13.f64 = double(temp.f32);
	// stfs f31,1412(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1412, temp.u32);
	// fmadds f13,f11,f13,f12
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64 + ctx.f12.f64));
	// lfs f31,-16948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -16948);
	ctx.f31.f64 = double(temp.f32);
	// stfs f8,880(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// fmr f8,f31
	ctx.f8.f64 = ctx.f31.f64;
	// lfs f11,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f11.f64 = double(temp.f32);
	// stfs f24,756(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// stfs f7,1164(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// lfs f24,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f24.f64 = double(temp.f32);
	// lfs f9,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f9.f64 = double(temp.f32);
	// stfs f18,1288(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1288, temp.u32);
	// fmuls f9,f2,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// fmsubs f11,f20,f11,f10
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f11.f64 - ctx.f10.f64));
	// lfs f15,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f15.f64 = double(temp.f32);
	// lfs f7,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f7.f64 = double(temp.f32);
	// lfs f18,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f7,f24,f7,f13
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f7.f64 + ctx.f13.f64));
	// lfs f10,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f10.f64 = double(temp.f32);
	// fadds f18,f15,f18
	ctx.f18.f64 = double(float(ctx.f15.f64 + ctx.f18.f64));
	// lfs f13,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f10,f29,f10,f0
	ctx.f10.f64 = double(float(-(ctx.f29.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f24,1360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1360);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// stfs f9,484(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// stfs f4,732(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 732, temp.u32);
	// stfs f6,1036(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// stfs f5,1244(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// stfs f31,164(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// stfs f2,1020(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// lfs f9,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f9.f64 = double(temp.f32);
	// lfs f1,1648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1648);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,2072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2072);
	ctx.f5.f64 = double(temp.f32);
	// lfs f15,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f15.f64 = double(temp.f32);
	// lfs f12,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f12.f64 = double(temp.f32);
	// lfs f6,1388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1388);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f12,f15,f12
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f12.f64));
	// lfs f4,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,1820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1820);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,1296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1296);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,1352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1352);
	ctx.f27.f64 = double(temp.f32);
	// stfs f21,616(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f21,f24,f13
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// stfs f1,604(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// fmuls f1,f1,f9
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// stfs f23,828(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// fadds f23,f29,f27
	ctx.f23.f64 = double(float(ctx.f29.f64 + ctx.f27.f64));
	// stfs f28,1084(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// fmuls f28,f26,f2
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// stfs f22,752(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// stfs f25,1004(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// fmuls f25,f5,f9
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// stfs f3,784(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// fmuls f3,f6,f4
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// stfs f30,1040(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// fmuls f30,f5,f31
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// lfs f22,1540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	ctx.f22.f64 = double(temp.f32);
	// lfs f13,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f13.f64 = double(temp.f32);
	// lfs f20,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f20.f64 = double(temp.f32);
	// stfs f19,780(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 780, temp.u32);
	// stfs f17,1236(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// stfs f16,1068(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// stfs f14,912(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fnmsubs f11,f22,f13,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// lfs f22,1288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1288);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f8,f1,f20,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f20.f64 - ctx.f8.f64));
	// lfs f1,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f10,f22,f1,f10
	ctx.f10.f64 = double(float(-(ctx.f22.f64 * ctx.f1.f64 - ctx.f10.f64)));
	// lfs f22,1280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1280);
	ctx.f22.f64 = double(temp.f32);
	// lfs f1,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f1.f64 = double(temp.f32);
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f22,f1
	ctx.f1.f64 = double(float(ctx.f22.f64 * ctx.f1.f64));
	// fmadds f12,f18,f13,f12
	ctx.f12.f64 = double(float(ctx.f18.f64 * ctx.f13.f64 + ctx.f12.f64));
	// lfs f22,1476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1476);
	ctx.f22.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f7,f22,f0,f7
	ctx.f7.f64 = double(float(-(ctx.f22.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// lfs f19,1652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1652);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,1616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1616);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f18,f19,f13
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// lfs f16,1564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1564);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f23,f13,f21
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f13.f64 + ctx.f21.f64));
	// stfs f5,1616(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1616, temp.u32);
	// fmuls f15,f22,f16
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f16.f64));
	// lfs f23,1640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1640);
	ctx.f23.f64 = double(temp.f32);
	// lfs f5,1396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1396);
	ctx.f5.f64 = double(temp.f32);
	// lfs f14,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f30,f5,f23,f30
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f23.f64 + ctx.f30.f64));
	// lfs f21,1412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1412);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,1632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1632);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f21,f14
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f14.f64));
	// stfs f15,1632(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1632, temp.u32);
	// stfs f18,1640(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1640, temp.u32);
	// stfs f5,1352(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1352, temp.u32);
	// lfs f14,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f13,f13,f16
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f16.f64));
	// lfs f15,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f28,f28,f14
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f14.f64));
	// lfs f18,1692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1692);
	ctx.f18.f64 = double(temp.f32);
	// lfs f5,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f5,f15,f5,f18
	ctx.f5.f64 = double(float(-(ctx.f15.f64 * ctx.f5.f64 - ctx.f18.f64)));
	// lfs f14,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f18.f64 = double(temp.f32);
	// lfs f20,1624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1624);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f11,f14,f18,f11
	ctx.f11.f64 = double(float(-(ctx.f14.f64 * ctx.f18.f64 - ctx.f11.f64)));
	// lfs f14,1260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1260);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f20,f16
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// stfs f15,1624(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1624, temp.u32);
	// stfs f1,1260(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1260, temp.u32);
	// lfs f15,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f8,f3,f15,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f15.f64 + ctx.f8.f64));
	// lfs f15,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f10,f15,f18,f10
	ctx.f10.f64 = double(float(-(ctx.f15.f64 * ctx.f18.f64 - ctx.f10.f64)));
	// lfs f18,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f18.f64 = double(temp.f32);
	// lfs f3,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f15,f17,f16
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// fmadds f3,f3,f18,f12
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f18.f64 + ctx.f12.f64));
	// lfs f12,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f6,f12
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f12.f64));
	// lfs f12,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f12,f0,f7
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f7.f64));
	// stfs f9,1360(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1360, temp.u32);
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f18,f14,f4
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f4.f64));
	// stfs f2,1540(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1540, temp.u32);
	// fmuls f7,f4,f23
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// stfs f29,484(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// stfs f27,1156(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// lfs f2,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// lfs f27,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f27.f64 = double(temp.f32);
	// stfs f26,648(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 648, temp.u32);
	// stfs f24,1296(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1296, temp.u32);
	// stfs f19,1388(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1388, temp.u32);
	// stfs f22,896(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// stfs f20,2072(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2072, temp.u32);
	// lfs f12,1260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1260);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f10,f12,f9,f10
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 + ctx.f10.f64));
	// lfs f12,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f12.f64 = double(temp.f32);
	// lfs f9,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f0,f12,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f9,1936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1936);
	ctx.f9.f64 = double(temp.f32);
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f29,f31,f9
	ctx.f29.f64 = double(float(ctx.f31.f64 * ctx.f9.f64));
	// stfs f17,1820(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1820, temp.u32);
	// fmadds f30,f30,f12,f28
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f12.f64 + ctx.f28.f64));
	// lfs f26,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f28,f6,f27
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f27.f64));
	// lfs f27,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f27,f27,f26,f0
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f26.f64 + ctx.f0.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f8,f25,f12,f8
	ctx.f8.f64 = double(float(-(ctx.f25.f64 * ctx.f12.f64 - ctx.f8.f64)));
	// lfs f25,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f10,f21,f2,f10
	ctx.f10.f64 = double(float(-(ctx.f21.f64 * ctx.f2.f64 - ctx.f10.f64)));
	// lfs f21,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f21.f64 = double(temp.f32);
	// fmr f12,f2
	ctx.f12.f64 = ctx.f2.f64;
	// lfs f2,2144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2144);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f26,f25,f21,f11
	ctx.f26.f64 = double(float(-(ctx.f25.f64 * ctx.f21.f64 - ctx.f11.f64)));
	// lfs f25,1724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1724);
	ctx.f25.f64 = double(temp.f32);
	// lfs f11,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f11.f64 = double(temp.f32);
	// lfs f21,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f5,f21,f0,f5
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 + ctx.f5.f64));
	// lfs f24,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f30,f29,f0,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 - ctx.f30.f64));
	// lfs f17,1632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1632);
	ctx.f17.f64 = double(temp.f32);
	// fmr f29,f0
	ctx.f29.f64 = ctx.f0.f64;
	// lfs f21,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f28,f14,f0,f28
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f0.f64 + ctx.f28.f64));
	// lfs f14,1624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1624);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f8,f18,f0,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f8.f64));
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f22,f24,f2
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// stfs f7,1212(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// fmuls f18,f14,f18
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f18.f64));
	// lfs f14,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f14.f64 = double(temp.f32);
	// lfs f7,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f15,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f7.f64));
	// lfs f15,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f15.f64 = double(temp.f32);
	// lfs f19,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f19.f64 = double(temp.f32);
	// stfs f8,1172(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// fmuls f8,f16,f19
	ctx.f8.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// lfs f20,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f29,f25,f29,f11
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f11,1640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1640);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f0,f11,f12,f10
	ctx.f0.f64 = double(float(-(ctx.f11.f64 * ctx.f12.f64 - ctx.f10.f64)));
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f17,f17,f10
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f10.f64));
	// lfs f10,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f10.f64 = double(temp.f32);
	// fadds f14,f10,f14
	ctx.f14.f64 = double(float(ctx.f10.f64 + ctx.f14.f64));
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f1,f1,f10,f22
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64 + ctx.f22.f64));
	// lfs f11,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f22,f15,f10
	ctx.f22.f64 = double(float(ctx.f15.f64 * ctx.f10.f64));
	// stfs f29,5264(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5264, temp.u32);
	// stfs f4,1724(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1724, temp.u32);
	// fnmsubs f27,f20,f11,f27
	ctx.f27.f64 = double(float(-(ctx.f20.f64 * ctx.f11.f64 - ctx.f27.f64)));
	// lfs f29,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f20,f21,f16
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// lfs f4,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f4.f64 = double(temp.f32);
	// stfs f6,492(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// fmuls f4,f4,f29
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// lfs f10,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f12,f13,f12,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f0.f64));
	// lfs f0,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f3,f3,f0,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 + ctx.f26.f64));
	// lfs f0,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f15,f13,f0
	ctx.f15.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f0,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,1008(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f6,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f26,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f26.f64 = double(temp.f32);
	// stfs f6,1008(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// fmuls f26,f26,f10
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// stfs f5,5268(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5268, temp.u32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f27,5272(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5272, temp.u32);
	// fmadds f5,f22,f0,f15
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f15.f64));
	// lfs f27,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f29.f64 = double(temp.f32);
	// stfs f25,1624(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1624, temp.u32);
	// stfs f21,828(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// stfs f23,468(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 468, temp.u32);
	// stfs f19,1640(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1640, temp.u32);
	// fmadds f3,f14,f13,f3
	ctx.f3.f64 = double(float(ctx.f14.f64 * ctx.f13.f64 + ctx.f3.f64));
	// lfs f14,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f13,f13,f29,f5
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f29.f64 - ctx.f5.f64)));
	// lfs f5,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f27,f26,f27
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// lfs f26,1268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1268);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f11,f17,f11,f12
	ctx.f11.f64 = double(float(-(ctx.f17.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// lfs f17,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f5,f17,f5
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f5.f64));
	// lfs f12,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f12.f64 = double(temp.f32);
	// lfs f29,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f25,f26,f0
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f29,f12
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// lfs f17,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f12,f14,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f12.f64));
	// lfs f14,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f21,f16
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// lfs f15,1276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1276);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f13,f4,f17,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f17.f64 - ctx.f13.f64)));
	// lfs f4,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f6,f6,f0,f27
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f27.f64));
	// lfs f17,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f0,f18,f0,f11
	ctx.f0.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f27,1360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1360);
	ctx.f27.f64 = double(temp.f32);
	// fadds f4,f14,f4
	ctx.f4.f64 = double(float(ctx.f14.f64 + ctx.f4.f64));
	// lfs f14,1352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1352);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f20,f20,f17
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f17.f64));
	// stfs f20,1268(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1268, temp.u32);
	// fmadds f30,f28,f14,f30
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f14.f64 + ctx.f30.f64));
	// stfs f30,1276(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1276, temp.u32);
	// lfs f23,1492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f9,f27,f9
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f9.f64));
	// lfs f20,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f23,f16
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f16.f64));
	// lfs f17,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f17.f64 = double(temp.f32);
	// lfs f30,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f30.f64 = double(temp.f32);
	// stfs f9,1492(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1492, temp.u32);
	// fnmsubs f30,f30,f17,f20
	ctx.f30.f64 = double(float(-(ctx.f30.f64 * ctx.f17.f64 - ctx.f20.f64)));
	// lfs f18,1604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1604);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f9,f31,f24
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f24.f64));
	// lfs f20,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f8,f6,f16,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f16.f64 - ctx.f8.f64));
	// stfs f9,1604(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1604, temp.u32);
	// fnmsubs f7,f7,f20,f0
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f6,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f6.f64 = double(temp.f32);
	// lfs f9,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f9.f64 = double(temp.f32);
	// fmr f0,f6
	ctx.f0.f64 = ctx.f6.f64;
	// fmuls f9,f21,f9
	ctx.f9.f64 = double(float(ctx.f21.f64 * ctx.f9.f64));
	// lfs f21,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f10,f21,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f10.f64));
	// lfs f21,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f21.f64 = double(temp.f32);
	// lfs f11,1516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1516);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f1,f21
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// lfs f21,1716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1716);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f17,f11,f16
	ctx.f17.f64 = double(float(ctx.f11.f64 * ctx.f16.f64));
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f6,f22,f6
	ctx.f6.f64 = double(float(ctx.f22.f64 * ctx.f6.f64));
	// fnmsubs f3,f21,f28,f3
	ctx.f3.f64 = double(float(-(ctx.f21.f64 * ctx.f28.f64 - ctx.f3.f64)));
	// stfs f17,1172(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// lfs f19,1616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1616);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f15,f16
	ctx.f21.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f28,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f2,f19,f2
	ctx.f2.f64 = double(float(ctx.f19.f64 * ctx.f2.f64));
	// lfs f20,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f28,f16,f28
	ctx.f28.f64 = double(float(ctx.f16.f64 * ctx.f28.f64));
	// lfs f17,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f13,f5,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f22,1212(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// fmadds f5,f20,f17,f29
	ctx.f5.f64 = double(float(ctx.f20.f64 * ctx.f17.f64 + ctx.f29.f64));
	// lfs f22,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f22.f64 = double(temp.f32);
	// stfs f23,1936(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1936, temp.u32);
	// fmuls f23,f16,f18
	ctx.f23.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// stfs f22,1360(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1360, temp.u32);
	// stfs f26,1616(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1616, temp.u32);
	// stfs f27,2144(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2144, temp.u32);
	// stfs f19,1516(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1516, temp.u32);
	// stfs f31,1008(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// stfs f24,460(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f0,1284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1284);
	ctx.f0.f64 = double(temp.f32);
	// stfs f11,1632(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1632, temp.u32);
	// fmuls f11,f16,f0
	ctx.f11.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,1500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1500);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,1292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1292);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f27,f31,f0
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmadds f25,f29,f0,f25
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 + ctx.f25.f64));
	// lfs f0,1300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1300);
	ctx.f0.f64 = double(temp.f32);
	// lfs f19,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f19.f64 = double(temp.f32);
	// stfs f18,1396(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1396, temp.u32);
	// fmuls f18,f0,f16
	ctx.f18.f64 = double(float(ctx.f0.f64 * ctx.f16.f64));
	// fmadds f10,f4,f19,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f19.f64 + ctx.f10.f64));
	// lfs f0,1836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1836);
	ctx.f0.f64 = double(temp.f32);
	// lfs f4,1828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1828);
	ctx.f4.f64 = double(temp.f32);
	// fadds f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f0.f64));
	// lfs f19,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f19.f64 = double(temp.f32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f12,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// fnmsubs f12,f9,f0,f8
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f23,f0
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// fmsubs f9,f2,f9,f1
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f9.f64 - ctx.f1.f64));
	// lfs f23,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f23.f64 = double(temp.f32);
	// lfs f17,1276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1276);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f11,f0
	ctx.f19.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f1,1604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1604);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f1,f23,f17
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f23.f64 + ctx.f17.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,1928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1928);
	ctx.f2.f64 = double(temp.f32);
	// lfs f23,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f23.f64 = double(temp.f32);
	// lfs f17,1492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f3,f2,f23,f3
	ctx.f3.f64 = double(float(-(ctx.f2.f64 * ctx.f23.f64 - ctx.f3.f64)));
	// stfs f14,1260(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1260, temp.u32);
	// fmadds f30,f17,f0,f30
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f0.f64 + ctx.f30.f64));
	// lfs f14,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f14.f64 = double(temp.f32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f14,f18,f14
	ctx.f14.f64 = double(float(ctx.f18.f64 * ctx.f14.f64));
	// lfs f23,1268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1268);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f7,f23,f0,f7
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f26,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,1660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1660);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f22,f26,f31
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f31.f64));
	// stfs f14,1284(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1284, temp.u32);
	// fmuls f20,f16,f24
	ctx.f20.f64 = double(float(ctx.f16.f64 * ctx.f24.f64));
	// lfs f14,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f4,f4,f26
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// fmuls f17,f21,f0
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// stfs f15,452(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// lfs f0,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f23,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// stfs f19,1292(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1292, temp.u32);
	// fmuls f23,f15,f23
	ctx.f23.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// stfs f25,1500(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1500, temp.u32);
	// lfs f19,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f19.f64 = double(temp.f32);
	// lfs f25,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f13,f5,f19,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f19.f64 + ctx.f13.f64));
	// lfs f0,2060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2060);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f27,f27,f25
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lfs f15,2064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2064);
	ctx.f15.f64 = double(temp.f32);
	// lfs f25,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f25.f64 = double(temp.f32);
	// fadds f15,f0,f15
	ctx.f15.f64 = double(float(ctx.f0.f64 + ctx.f15.f64));
	// lfs f19,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f19.f64 = double(temp.f32);
	// lfs f2,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f10,f25,f19,f10
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f19.f64 + ctx.f10.f64));
	// lfs f0,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f0.f64 = double(temp.f32);
	// stfs f14,1660(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1660, temp.u32);
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// lfs f5,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f5.f64 = double(temp.f32);
	// lfs f14,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f5,f22,f5
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f20,f20,f14
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f14.f64));
	// lfs f19,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f12,f6,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// stfs f0,1300(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1300, temp.u32);
	// lfs f0,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fmuls f6,f19,f0
	ctx.f6.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// stfs f9,5288(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5288, temp.u32);
	// lfs f9,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f9.f64 = double(temp.f32);
	// stfs f7,5276(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5276, temp.u32);
	// fmuls f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f3,5280(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5280, temp.u32);
	// lfs f3,-19084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19084);
	ctx.f3.f64 = double(temp.f32);
	// stfs f31,340(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f31,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f31.f64 = double(temp.f32);
	// stfs f3,256(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// lfs f9,1708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1708);
	ctx.f9.f64 = double(temp.f32);
	// stfs f30,5284(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5284, temp.u32);
	// fmuls f7,f6,f26
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfs f30,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f30.f64 = double(temp.f32);
	// fmr f6,f25
	ctx.f6.f64 = ctx.f25.f64;
	// stfs f1,5292(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5292, temp.u32);
	// fmuls f31,f0,f31
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// stfs f2,1716(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1716, temp.u32);
	// fmr f0,f25
	ctx.f0.f64 = ctx.f25.f64;
	// lfs f1,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f2.f64 = double(temp.f32);
	// fadds f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// stfs f11,776(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// lfs f11,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f11.f64 = double(temp.f32);
	// lfs f1,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f18,f1
	ctx.f1.f64 = double(float(ctx.f18.f64 * ctx.f1.f64));
	// lfs f14,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f14.f64 = double(temp.f32);
	// stfs f29,1648(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1648, temp.u32);
	// lfs f29,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f17,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// lfs f6,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f13,f9,f6,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 + ctx.f13.f64));
	// lfs f9,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f9.f64 = double(temp.f32);
	// fmr f6,f25
	ctx.f6.f64 = ctx.f25.f64;
	// lfs f25,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f9,f5,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f5,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// stfs f24,1564(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1564, temp.u32);
	// stfs f22,1656(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1656, temp.u32);
	// lfs f3,1740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1740);
	ctx.f3.f64 = double(temp.f32);
	// lfs f24,1748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1748);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,1812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1812);
	ctx.f22.f64 = double(temp.f32);
	// stfs f21,756(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// fmadds f12,f23,f0,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f30,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f0,f25
	ctx.f25.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// lfs f0,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f0.f64 = double(temp.f32);
	// fmr f19,f0
	ctx.f19.f64 = ctx.f0.f64;
	// lfs f30,1296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1296);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f6,f11,f6
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f6.f64));
	// lfs f23,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// fmuls f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f1,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f14,f1
	ctx.f1.f64 = double(float(ctx.f14.f64 * ctx.f1.f64));
	// lfs f14,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f14.f64 = double(temp.f32);
	// fmsubs f9,f4,f29,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f29.f64 - ctx.f9.f64));
	// lfs f4,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f2,f2,f14,f31
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64 + ctx.f31.f64));
	// lfs f31,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f29,f4,f3
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// fmadds f13,f15,f31,f13
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f31,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f23,f24
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f24.f64));
	// fmadds f6,f30,f19,f6
	ctx.f6.f64 = double(float(ctx.f30.f64 * ctx.f19.f64 + ctx.f6.f64));
	// lfs f30,1508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1508);
	ctx.f30.f64 = double(temp.f32);
	// stfs f20,1508(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1508, temp.u32);
	// fmuls f17,f30,f22
	ctx.f17.f64 = double(float(ctx.f30.f64 * ctx.f22.f64));
	// lfs f20,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f10,f10,f20,f27
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f20.f64 + ctx.f27.f64));
	// lfs f19,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f19.f64 = double(temp.f32);
	// fmr f27,f14
	ctx.f27.f64 = ctx.f14.f64;
	// fnmsubs f12,f28,f27,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f27.f64 - ctx.f12.f64)));
	// lfs f27,1300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1300);
	ctx.f27.f64 = double(temp.f32);
	// fmr f28,f20
	ctx.f28.f64 = ctx.f20.f64;
	// fmadds f9,f27,f28,f9
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f28.f64 + ctx.f9.f64));
	// stfs f3,1296(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1296, temp.u32);
	// fmr f28,f14
	ctx.f28.f64 = ctx.f14.f64;
	// lfs f3,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f6,f6,f16
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f16.f64));
	// fmadds f12,f8,f3,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f3.f64 + ctx.f12.f64));
	// stfs f30,616(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// lfs f30,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f25,f25,f16
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f16.f64));
	// lfs f8,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// fnmsubs f13,f30,f8,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f8.f64 - ctx.f13.f64)));
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// stfs f18,1172(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// fnmsubs f9,f7,f8,f9
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f9.f64)));
	// lfs f18,1292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1292);
	ctx.f18.f64 = double(temp.f32);
	// lfs f8,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f7.f64 = double(temp.f32);
	// stfs f24,1412(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1412, temp.u32);
	// lfs f24,1308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1308);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f18,f3,f12
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f3.f64 - ctx.f12.f64)));
	// lfs f18,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f18.f64 = double(temp.f32);
	// stfs f1,1308(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1308, temp.u32);
	// lfs f14,1612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1612);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f13,f8,f7,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f13.f64));
	// lfs f8,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f8,f18,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f8.f64));
	// lfs f18,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f1,f18,f16
	ctx.f1.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f3,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f3.f64 = double(temp.f32);
	// stfs f1,1612(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1612, temp.u32);
	// fmsubs f0,f6,f3,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f3.f64 - ctx.f0.f64));
	// lfs f7,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f9,f2,f26,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f26.f64 + ctx.f9.f64));
	// lfs f1,1500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1500);
	ctx.f1.f64 = double(temp.f32);
	// stfs f11,1692(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1692, temp.u32);
	// fmsubs f11,f29,f28,f5
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f28.f64 - ctx.f5.f64));
	// lfs f27,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f12,f1,f7,f12
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f28,1660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1660);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f5,f19,f27
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// lfs f6,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f6,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f1,1316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1316);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f27,f28,f30
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// stfs f19,1288(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1288, temp.u32);
	// fmuls f19,f28,f24
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// stfs f28,1316(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1316, temp.u32);
	// fmuls f28,f4,f14
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// lfs f7,1908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1908);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f8,f16
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f16.f64));
	// lfs f6,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f6.f64 = double(temp.f32);
	// stfs f28,1192(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// fmuls f28,f4,f7
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// lfs f3,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f3.f64 = double(temp.f32);
	// stfs f28,1908(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1908, temp.u32);
	// fmr f28,f3
	ctx.f28.f64 = ctx.f3.f64;
	// stfs f22,1268(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1268, temp.u32);
	// fnmsubs f11,f21,f3,f11
	ctx.f11.f64 = double(float(-(ctx.f21.f64 * ctx.f3.f64 - ctx.f11.f64)));
	// lfs f20,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f20,f22
	ctx.f22.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// lfs f29,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f20.f64 = double(temp.f32);
	// lfs f3,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f15,f20,f2
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f2.f64));
	// stfs f23,1604(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1604, temp.u32);
	// fmuls f23,f29,f16
	ctx.f23.f64 = double(float(ctx.f29.f64 * ctx.f16.f64));
	// lfs f21,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f3,f3,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f26.f64));
	// fmadds f0,f25,f28,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f28,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// stfs f28,1748(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1748, temp.u32);
	// fnmsubs f9,f17,f25,f9
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f25.f64 - ctx.f9.f64)));
	// lfs f28,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f22,f22,f16
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f16.f64));
	// fmuls f21,f21,f26
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// stfs f29,1016(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// stfs f21,1300(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1300, temp.u32);
	// fmuls f27,f27,f1
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f21,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f21.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f29,1284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1284);
	ctx.f29.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// fnmsubs f12,f29,f21,f12
	ctx.f12.f64 = double(float(-(ctx.f29.f64 * ctx.f21.f64 - ctx.f12.f64)));
	// stfs f27,1812(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1812, temp.u32);
	// fmr f27,f28
	ctx.f27.f64 = ctx.f28.f64;
	// stfs f7,1284(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1284, temp.u32);
	// fmr f7,f21
	ctx.f7.f64 = ctx.f21.f64;
	// stfs f30,1652(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1652, temp.u32);
	// fmuls f28,f23,f28
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f28.f64));
	// stfs f28,1740(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1740, temp.u32);
	// fmuls f28,f25,f6
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// stfs f6,1476(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1476, temp.u32);
	// fmr f6,f21
	ctx.f6.f64 = ctx.f21.f64;
	// stfs f20,520(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// stfs f18,1024(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// lfs f18,1612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1612);
	ctx.f18.f64 = double(temp.f32);
	// lfs f20,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// stfs f28,1660(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1660, temp.u32);
	// fmadds f12,f10,f16,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f16.f64 + ctx.f12.f64));
	// stfs f14,784(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// fmr f10,f21
	ctx.f10.f64 = ctx.f21.f64;
	// stfs f19,604(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// fmuls f30,f19,f27
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// lfs f27,1508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1508);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f11,f31,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f7,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f9,f15,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f15.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// lfs f23,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f23.f64 = double(temp.f32);
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f13,f23,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f23.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f14,1308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1308);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f19.f64 = double(temp.f32);
	// lfs f29,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f19,f18
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// fnmsubs f0,f5,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f5.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f10,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f12,f27,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f27,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f11,f3,f6,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f11.f64));
	// stfs f2,1492(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1492, temp.u32);
	// stfs f1,1352(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1352, temp.u32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f17,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f17.f64 = double(temp.f32);
	// lfs f5,1820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1820);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f17,f17,f26
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f26.f64));
	// lfs f2,1324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1324);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// lfs f1,1704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1704);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f4,f2
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f10,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f28,f25,f1
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f1.f64));
	// lfs f6,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f10,f10,f26
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// fnmsubs f0,f22,f7,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// lfs f22,1316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1316);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f14,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f14.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f3,1332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	ctx.f3.f64 = double(temp.f32);
	// lfs f23,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfs f15,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f21,f22,f3
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f3.f64));
	// lfs f7,1620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1620);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f23,f23,f26
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f26.f64));
	// lfs f18,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f15,f15,f26
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f26.f64));
	// stfs f24,1280(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1280, temp.u32);
	// stfs f20,1508(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1508, temp.u32);
	// fmuls f20,f18,f7
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f7.f64));
	// lfs f27,1524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,1340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1340);
	ctx.f24.f64 = double(temp.f32);
	// lfs f14,1724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1724);
	ctx.f14.f64 = double(temp.f32);
	// stfs f22,1316(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1316, temp.u32);
	// stfs f30,1324(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1324, temp.u32);
	// fmuls f30,f14,f27
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// stfs f14,1620(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1620, temp.u32);
	// stfs f21,1332(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1332, temp.u32);
	// lfs f22,2144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2144);
	ctx.f22.f64 = double(temp.f32);
	// lfs f14,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f9,f17,f14,f9
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f14.f64 - ctx.f9.f64)));
	// fmuls f21,f21,f22
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// stfs f22,1704(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1704, temp.u32);
	// lfs f14,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f14.f64 = double(temp.f32);
	// lfs f22,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f24,f22
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// fmuls f17,f17,f14
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// stfs f22,1340(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1340, temp.u32);
	// stfs f17,1524(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1524, temp.u32);
	// lfs f22,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f11,f17,f22,f11
	ctx.f11.f64 = double(float(-(ctx.f17.f64 * ctx.f22.f64 - ctx.f11.f64)));
	// lfs f14,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f0,f8,f14,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f14.f64 + ctx.f0.f64));
	// stfs f13,5296(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5296, temp.u32);
	// lfs f13,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f23,f13,f9
	ctx.f13.f64 = double(float(-(ctx.f23.f64 * ctx.f13.f64 - ctx.f9.f64)));
	// stfs f3,1276(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1276, temp.u32);
	// stfs f1,1212(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// lfs f3,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,1908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1908);
	ctx.f1.f64 = double(temp.f32);
	// stfs f2,1820(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1820, temp.u32);
	// lfs f2,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// lfs f8,-19088(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19088);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f11,f1,f3,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f3.f64 - ctx.f11.f64)));
	// lfs f3,1812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1812);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f3,f3,f2,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f0.f64));
	// stfs f8,676(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 676, temp.u32);
	// stfs f6,1308(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1308, temp.u32);
	// lfs f8,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f2.f64 = double(temp.f32);
	// lfs f9,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f13,f15,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f15.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f6,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f9,f9,f8
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f6,f20,f6,f30
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f6.f64 + ctx.f30.f64));
	// stfs f24,1724(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1724, temp.u32);
	// fmuls f5,f5,f22
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// lfs f0,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f0.f64 = double(temp.f32);
	// lfs f20,1340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1340);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f1,f0,f26
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f26.f64));
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,1348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f21,f8
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// lfs f24,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f24.f64 = double(temp.f32);
	// stfs f4,2144(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2144, temp.u32);
	// fmadds f24,f2,f24,f20
	ctx.f24.f64 = double(float(ctx.f2.f64 * ctx.f24.f64 + ctx.f20.f64));
	// stfs f31,1192(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// stfs f29,1292(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1292, temp.u32);
	// stfs f10,1612(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1612, temp.u32);
	// stfs f25,1008(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// lfs f22,-19092(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19092);
	ctx.f22.f64 = double(temp.f32);
	// lfs f31,1728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1728);
	ctx.f31.f64 = double(temp.f32);
	// lfs f10,1944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1944);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f30,f18,f31
	ctx.f30.f64 = double(float(ctx.f18.f64 * ctx.f31.f64));
	// lfs f4,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f4.f64 = double(temp.f32);
	// lfs f29,1532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f4,f10,f4
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// lfs f25,1356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,1620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1620);
	ctx.f23.f64 = double(temp.f32);
	// stfs f12,5300(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5300, temp.u32);
	// fmuls f12,f19,f16
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// stfs f27,1020(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// fmuls f27,f26,f29
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// stfs f22,560(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 560, temp.u32);
	// fmuls f22,f23,f25
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// stfs f28,1500(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1500, temp.u32);
	// fadds f28,f31,f7
	ctx.f28.f64 = double(float(ctx.f31.f64 + ctx.f7.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f21,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,1524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f19,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f9,f19,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f19.f64 + ctx.f4.f64));
	// lfs f4,1748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1748);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f8,f8,f20,f6
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f20.f64 - ctx.f6.f64)));
	// lfs f20,1740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1740);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f11,f4,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f19,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f0,f20,f0,f3
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f3.f64));
	// lfs f3,1364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	ctx.f3.f64 = double(temp.f32);
	// stfs f5,1364(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1364, temp.u32);
	// fmuls f19,f19,f26
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f26.f64));
	// lfs f5,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f5.f64 = double(temp.f32);
	// lfs f20,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f5,f27,f5
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// lfs f4,1668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1668);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f24,f24,f20,f22
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f20.f64 - ctx.f22.f64));
	// stfs f19,1668(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1668, temp.u32);
	// stfs f5,1356(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1356, temp.u32);
	// lfs f19,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f21,f10,f19,f21
	ctx.f21.f64 = double(float(ctx.f10.f64 * ctx.f19.f64 - ctx.f21.f64));
	// lfs f5,1300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1300);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f11,f5,f22,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f22.f64 + ctx.f11.f64));
	// fnmsubs f13,f1,f6,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f6.f64 - ctx.f13.f64)));
	// lfs f22,1324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1324);
	ctx.f22.f64 = double(temp.f32);
	// lfs f19,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f19.f64 = double(temp.f32);
	// lfs f6,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f0,f22,f19,f0
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f19.f64 + ctx.f0.f64));
	// fmuls f19,f20,f6
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f6.f64));
	// lfs f1,1332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	ctx.f1.f64 = double(temp.f32);
	// lfs f17,1704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1704);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f1,f1,f4
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// stfs f4,1524(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1524, temp.u32);
	// fmuls f15,f17,f3
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// stfs f13,5304(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5304, temp.u32);
	// stfs f31,752(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// lfs f4,1660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1660);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f31.f64 = double(temp.f32);
	// lfs f13,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f13.f64 = double(temp.f32);
	// stfs f3,912(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fnmsubs f13,f4,f13,f11
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// lfs f5,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f3,f28,f17,f19
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f17.f64 + ctx.f19.f64));
	// lfs f22,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f22.f64 = double(temp.f32);
	// fmr f28,f31
	ctx.f28.f64 = ctx.f31.f64;
	// stfs f25,1532(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1532, temp.u32);
	// lfs f14,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f9,f5,f22,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f22.f64 - ctx.f9.f64)));
	// lfs f25,2072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2072);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f25,f25,f26
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f11,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f8,f30,f27,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f8.f64));
	// lfs f4,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f4.f64 = double(temp.f32);
	// lfs f22,1508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1508);
	ctx.f22.f64 = double(temp.f32);
	// stfs f1,1200(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// fnmsubs f13,f11,f4,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f4.f64 - ctx.f13.f64)));
	// stfs f7,2072(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2072, temp.u32);
	// lfs f1,1372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,1316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1316);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f0,f22,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// stfs f14,1348(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1348, temp.u32);
	// fmuls f14,f14,f23
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// stfs f2,1908(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1908, temp.u32);
	// fmuls f5,f26,f1
	ctx.f5.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// stfs f29,1332(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1332, temp.u32);
	// fmr f22,f31
	ctx.f22.f64 = ctx.f31.f64;
	// stfs f6,1324(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1324, temp.u32);
	// fmuls f6,f7,f10
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f2,1612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1612);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,1308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1308);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,1292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1292);
	ctx.f28.f64 = double(temp.f32);
	// lfs f11,1500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1500);
	ctx.f11.f64 = double(temp.f32);
	// lfs f4,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f4.f64 = double(temp.f32);
	// lfs f19,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f19.f64 = double(temp.f32);
	// stfs f16,1340(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1340, temp.u32);
	// stfs f18,880(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// fnmsubs f0,f28,f22,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f22.f64 - ctx.f0.f64)));
	// lfs f16,1380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1380);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f2,f31,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f31.f64 + ctx.f13.f64));
	// stfs f6,1380(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1380, temp.u32);
	// lfs f18,1628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1628);
	ctx.f18.f64 = double(temp.f32);
	// lfs f6,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f6.f64 = double(temp.f32);
	// lfs f31,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f6,f14,f6,f24
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f6.f64 + ctx.f24.f64));
	// stfs f5,1628(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1628, temp.u32);
	// fmuls f5,f17,f16
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// stfs f26,1620(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1620, temp.u32);
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f8,f15,f28,f8
	ctx.f8.f64 = double(float(-(ctx.f15.f64 * ctx.f28.f64 - ctx.f8.f64)));
	// fmuls f26,f26,f18
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f18.f64));
	// lfs f28,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f0,f12,f19,f0
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f19.f64 - ctx.f0.f64)));
	// lfs f12,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f12.f64 = double(temp.f32);
	// fmr f19,f12
	ctx.f19.f64 = ctx.f12.f64;
	// lfs f14,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f12,f12,f31
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfs f12,1372(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1372, temp.u32);
	// fnmsubs f13,f30,f29,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// lfs f29,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f29.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// lfs f30,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f12,f25,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f12.f64));
	// fnmsubs f30,f30,f29,f21
	ctx.f30.f64 = double(float(-(ctx.f30.f64 * ctx.f29.f64 - ctx.f21.f64)));
	// lfs f25,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f25.f64 = double(temp.f32);
	// lfs f29,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f9,f25,f29,f9
	ctx.f9.f64 = double(float(-(ctx.f25.f64 * ctx.f29.f64 - ctx.f9.f64)));
	// lfs f25,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f6,f5,f25,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f25.f64 + ctx.f6.f64));
	// lfs f5,1364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f8,f26,f29,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f8.f64));
	// lfs f29,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f0,f29,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// stfs f0,5308(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5308, temp.u32);
	// fmadds f13,f5,f27,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f27.f64 + ctx.f13.f64));
	// lfs f0,1668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1668);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f19,f19,f14
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f14.f64));
	// stfs f19,1728(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1728, temp.u32);
	// fmr f19,f28
	ctx.f19.f64 = ctx.f28.f64;
	// lfs f2,1548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	ctx.f2.f64 = double(temp.f32);
	// lfs f15,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f24,f23,f2
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f2.f64));
	// stfs f7,1208(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// lfs f7,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f7.f64 = double(temp.f32);
	// stfs f24,1548(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1548, temp.u32);
	// fmuls f24,f20,f15
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// stfs f20,1216(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// lfs f20,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f20.f64 = double(temp.f32);
	// stfs f3,1224(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// fmuls f3,f7,f23
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f23.f64));
	// fnmsubs f13,f11,f4,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f4.f64 - ctx.f13.f64)));
	// stfs f3,1944(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1944, temp.u32);
	// stfs f7,1308(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1308, temp.u32);
	// lfs f3,1604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1604);
	ctx.f3.f64 = double(temp.f32);
	// lfs f7,1260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1260);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f21,f3,f20
	ctx.f21.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// lfs f5,1532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	ctx.f5.f64 = double(temp.f32);
	// stfs f10,1084(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// fnmsubs f8,f7,f5,f8
	ctx.f8.f64 = double(float(-(ctx.f7.f64 * ctx.f5.f64 - ctx.f8.f64)));
	// lfs f29,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f29.f64 = double(temp.f32);
	// lfs f11,1556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,1936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1936);
	ctx.f10.f64 = double(temp.f32);
	// stfs f1,1704(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1704, temp.u32);
	// fmadds f0,f0,f22,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f22.f64 + ctx.f13.f64));
	// lfs f13,1356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	ctx.f13.f64 = double(temp.f32);
	// stfs f18,1076(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// stfs f2,1612(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1612, temp.u32);
	// stfs f16,1292(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1292, temp.u32);
	// stfs f31,1060(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// stfs f15,1192(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// stfs f14,1028(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// stfs f17,1604(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1604, temp.u32);
	// stfs f20,1364(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1364, temp.u32);
	// stfs f3,1660(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1660, temp.u32);
	// fnmsubs f13,f13,f19,f0
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f19.f64 - ctx.f0.f64)));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f3,f29,f0,f30
	ctx.f3.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// lfs f30,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f10,f30
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// lfs f0,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f0,f2
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// lfs f0,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f6,f24,f0,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// lfs f0,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,1636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1636);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,1348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f26,f23,f28,f0
	ctx.f26.f64 = double(float(ctx.f23.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f16,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f31,f1,f7
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// stfs f31,1216(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// lfs f14,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f14.f64 = double(temp.f32);
	// lfs f31,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fnmsubs f9,f14,f31,f9
	ctx.f9.f64 = double(float(-(ctx.f14.f64 * ctx.f31.f64 - ctx.f9.f64)));
	// stfs f30,1224(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// lfs f30,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f31.f64 = double(temp.f32);
	// lfs f15,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f3,f30,f31,f3
	ctx.f3.f64 = double(float(-(ctx.f30.f64 * ctx.f31.f64 - ctx.f3.f64)));
	// lfs f14,1276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1276);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f13,f12,f15,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f15.f64 - ctx.f13.f64)));
	// lfs f31,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f31.f64 = double(temp.f32);
	// lfs f15,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f31,f14,f31
	ctx.f31.f64 = double(float(ctx.f14.f64 * ctx.f31.f64));
	// stfs f14,1208(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// lfs f4,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f4.f64 = double(temp.f32);
	// lfs f25,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// lfs f14,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f6,f16,f25,f6
	ctx.f6.f64 = double(float(-(ctx.f16.f64 * ctx.f25.f64 - ctx.f6.f64)));
	// lfs f30,1624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1624);
	ctx.f30.f64 = double(temp.f32);
	// stfs f31,1232(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// fmuls f30,f30,f14
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f22,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f22.f64 = double(temp.f32);
	// lfs f27,1284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1284);
	ctx.f27.f64 = double(temp.f32);
	// lfs f31,1492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
	ctx.f31.f64 = double(temp.f32);
	// stfs f30,1636(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1636, temp.u32);
	// fmuls f30,f31,f27
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// stfs f23,1240(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// lfs f29,1404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1404);
	ctx.f29.f64 = double(temp.f32);
	// lfs f14,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f14.f64 = double(temp.f32);
	// lfs f23,1548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	ctx.f23.f64 = double(temp.f32);
	// stfs f30,1404(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1404, temp.u32);
	// fnmsubs f8,f23,f14,f8
	ctx.f8.f64 = double(float(-(ctx.f23.f64 * ctx.f14.f64 - ctx.f8.f64)));
	// lfs f24,1640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1640);
	ctx.f24.f64 = double(temp.f32);
	// lfs f30,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f24,f30
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// lfs f23,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f23.f64 = double(temp.f32);
	// stfs f30,1556(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1556, temp.u32);
	// fmuls f23,f23,f14
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f14.f64));
	// lfs f17,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f17.f64 = double(temp.f32);
	// lfs f30,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f4,f22,f30,f4
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f30.f64 + ctx.f4.f64));
	// lfs f18,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f2,f14,f0,f2
	ctx.f2.f64 = double(float(ctx.f14.f64 * ctx.f0.f64 + ctx.f2.f64));
	// fmuls f18,f18,f17
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// stfs f7,1200(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// stfs f11,1812(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1812, temp.u32);
	// lfs f11,1628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1628);
	ctx.f11.f64 = double(temp.f32);
	// lfs f20,1632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1632);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f29,f19
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// lfs f12,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f17,f15,f17
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f17.f64));
	// lfs f7,1380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1380);
	ctx.f7.f64 = double(temp.f32);
	// lfs f30,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f12,f7,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f20,f30
	ctx.f30.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// stfs f10,1748(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1748, temp.u32);
	// fmuls f14,f11,f0
	ctx.f14.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f29,1628(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1628, temp.u32);
	// stfs f11,1300(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1300, temp.u32);
	// lfs f0,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,1372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f0
	ctx.f10.f64 = ctx.f0.f64;
	// fmadds f11,f11,f0,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f8.f64));
	// lfs f8,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f13,f21,f8,f13
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f8.f64 + ctx.f13.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f8,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f12,f17,f0,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f0.f64 - ctx.f12.f64));
	// stfs f7,1356(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1356, temp.u32);
	// fmuls f8,f15,f8
	ctx.f8.f64 = double(float(ctx.f15.f64 * ctx.f8.f64));
	// lfs f7,1420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1420);
	ctx.f7.f64 = double(temp.f32);
	// stfs f1,1532(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1532, temp.u32);
	// lfs f1,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f1.f64 = double(temp.f32);
	// stfs f5,1348(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1348, temp.u32);
	// fmadds f5,f7,f0,f19
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + ctx.f19.f64));
	// fnmsubs f0,f1,f0,f9
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f29,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f29.f64 = double(temp.f32);
	// lfs f9,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f9,f29,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// lfs f29,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f6,f26,f29,f6
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f6.f64));
	// stfs f27,1284(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1284, temp.u32);
	// lfs f27,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f27.f64 = double(temp.f32);
	// lfs f1,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f8,f27,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f27.f64 + ctx.f12.f64));
	// lfs f29,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f3,f18,f1,f3
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f1.f64 + ctx.f3.f64));
	// fmr f8,f27
	ctx.f8.f64 = ctx.f27.f64;
	// lfs f1,1728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1728);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f13,f14,f29,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f29.f64 + ctx.f13.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f1,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// stfs f20,1548(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1548, temp.u32);
	// lfs f21,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,1636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1636);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f0,f9,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f9.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f29,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f6,f5,f29,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f29.f64 + ctx.f6.f64));
	// lfs f5,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f5.f64 = double(temp.f32);
	// lfs f26,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f5,f21,f5,f20
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f5.f64 + ctx.f20.f64));
	// lfs f21,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,1556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f8,f26,f8,f3
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// lfs f3,1944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1944);
	ctx.f3.f64 = double(temp.f32);
	// lfs f20,1404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1404);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f23,f18,f21,f23
	ctx.f23.f64 = double(float(ctx.f18.f64 * ctx.f21.f64 + ctx.f23.f64));
	// lfs f26,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f11,f3,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// stfs f16,1276(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1276, temp.u32);
	// fmuls f26,f20,f26
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f26.f64));
	// lfs f19,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f18.f64 = double(temp.f32);
	// lfs f21,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// stfs f22,1508(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1508, temp.u32);
	// fmadds f2,f2,f18,f16
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f18.f64 + ctx.f16.f64));
	// stfs f25,1316(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1316, temp.u32);
	// stfs f24,1380(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1380, temp.u32);
	// lfs f1,1676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1676);
	ctx.f1.f64 = double(temp.f32);
	// lfs f27,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,1268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1268);
	ctx.f25.f64 = double(temp.f32);
	// lfs f9,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f4,f4,f25
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f25.f64));
	// lfs f10,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f10.f64 = double(temp.f32);
	// lfs f3,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f10,f10,f27
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f27.f64));
	// lfs f24,1620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1620);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,1616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1616);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f3,f3,f24
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f24.f64));
	// lfs f20,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f15,f22
	ctx.f22.f64 = double(float(ctx.f15.f64 * ctx.f22.f64));
	// stfs f28,1668(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1668, temp.u32);
	// fmuls f28,f31,f1
	ctx.f28.f64 = double(float(ctx.f31.f64 * ctx.f1.f64));
	// lfs f17,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f20,f9
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f9.f64));
	// lfs f19,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f14.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f9,1740(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1740, temp.u32);
	// fmr f9,f14
	ctx.f9.f64 = ctx.f14.f64;
	// stfs f13,5312(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5312, temp.u32);
	// fmadds f30,f17,f19,f30
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f19.f64 + ctx.f30.f64));
	// stfs f12,5324(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5324, temp.u32);
	// fmadds f12,f5,f24,f4
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f24.f64 + ctx.f4.f64));
	// stfs f0,5328(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5328, temp.u32);
	// fmuls f18,f18,f24
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f24.f64));
	// lfs f0,1340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1340);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f5,1428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1428);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f13,f3,f13,f26
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f13.f64 + ctx.f26.f64));
	// lfs f4,-19096(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19096);
	ctx.f4.f64 = double(temp.f32);
	// stfs f6,5316(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5316, temp.u32);
	// fmuls f6,f2,f0
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f4,592(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// fmuls f4,f24,f5
	ctx.f4.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// stfs f8,5320(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5320, temp.u32);
	// fmuls f9,f22,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// lfs f8,1360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1360);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f3,f24,f8
	ctx.f3.f64 = double(float(ctx.f24.f64 * ctx.f8.f64));
	// stfs f27,1260(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1260, temp.u32);
	// stfs f17,1240(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// lfs f17,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f17.f64 = double(temp.f32);
	// lfs f27,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f27.f64 = double(temp.f32);
	// lfs f16,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f11,f27,f17,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f17.f64 + ctx.f11.f64));
	// lfs f27,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// lfs f2,1524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	ctx.f2.f64 = double(temp.f32);
	// fsubs f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// lfs f14,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f14.f64 = double(temp.f32);
	// fmsubs f9,f9,f27,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f27.f64 - ctx.f6.f64));
	// stfs f28,1232(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// lfs f19,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f28,f15,f2
	ctx.f28.f64 = double(float(ctx.f15.f64 * ctx.f2.f64));
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// lfs f6,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f19,f19,f24
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f24.f64));
	// stfs f2,1636(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1636, temp.u32);
	// fmuls f6,f6,f24
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f24.f64));
	// lfs f14,1436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1436);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f12,f3,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// lfs f2,1644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1644);
	ctx.f2.f64 = double(temp.f32);
	// stfs f24,1436(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1436, temp.u32);
	// fmuls f24,f2,f14
	ctx.f24.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// stfs f1,1640(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1640, temp.u32);
	// fmuls f1,f21,f0
	ctx.f1.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// lfs f3,1572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1572);
	ctx.f3.f64 = double(temp.f32);
	// stfs f24,1676(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1676, temp.u32);
	// stfs f29,1500(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1500, temp.u32);
	// stfs f1,1572(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1572, temp.u32);
	// lfs f24,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f24.f64 = double(temp.f32);
	// lfs f29,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f29.f64 = double(temp.f32);
	// lfs f1,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// stfs f25,1172(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// fmadds f1,f1,f24,f23
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f24.f64 + ctx.f23.f64));
	// lfs f26,1352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1352);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f25.f64 = double(temp.f32);
	// stfs f30,1644(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1644, temp.u32);
	// fmuls f21,f25,f26
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f17,1296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1296);
	ctx.f17.f64 = double(temp.f32);
	// lfs f30,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// stfs f0,1420(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1420, temp.u32);
	// fmuls f30,f20,f30
	ctx.f30.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// lfs f27,1288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1288);
	ctx.f27.f64 = double(temp.f32);
	// lfs f0,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f0.f64 = double(temp.f32);
	// lfs f24,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f27,f24
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// stfs f26,1224(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// fmuls f26,f31,f17
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// stfs f31,1428(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1428, temp.u32);
	// fmuls f31,f0,f7
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f20,1332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f11,f10,f23,f11
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f23.f64 - ctx.f11.f64)));
	// lfs f10,1444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1444);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f28,f28,f20
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// stfs f28,1444(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1444, temp.u32);
	// lfs f23,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f23.f64 = double(temp.f32);
	// lfs f28,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f13,f18,f23,f13
	ctx.f13.f64 = double(float(-(ctx.f18.f64 * ctx.f23.f64 - ctx.f13.f64)));
	// fnmsubs f9,f16,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f16.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// lfs f28,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f28.f64 = double(temp.f32);
	// stfs f10,1556(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1556, temp.u32);
	// fmuls f28,f21,f28
	ctx.f28.f64 = double(float(ctx.f21.f64 * ctx.f28.f64));
	// stfs f0,1492(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1492, temp.u32);
	// fmuls f10,f3,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f0,f31,f0,f11
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f21,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f21.f64 = double(temp.f32);
	// stfs f7,1372(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1372, temp.u32);
	// fmuls f21,f8,f21
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f21.f64));
	// lfs f7,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f7.f64 = double(temp.f32);
	// stfs f6,1404(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1404, temp.u32);
	// lfs f6,1644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1644);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f13,f19,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f7,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f7.f64 = double(temp.f32);
	// lfs f11,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f11,f6,f11,f9
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f11.f64 + ctx.f9.f64));
	// stfs f3,1048(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// fmadds f10,f10,f7,f28
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f7.f64 + ctx.f28.f64));
	// lfs f7,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f7.f64 = double(temp.f32);
	// lfs f31,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f31.f64 = double(temp.f32);
	// lfs f9,1460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1460);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,1280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1280);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f0,f7,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f23,1580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1580);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f3,f3,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f31,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f18,f15,f23
	ctx.f18.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// lfs f7,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f7.f64 = double(temp.f32);
	// stfs f15,1216(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// fmadds f7,f31,f7,f21
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f21.f64));
	// stfs f8,1208(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// lfs f28,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f28.f64 = double(temp.f32);
	// lfs f15,1452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1452);
	ctx.f15.f64 = double(temp.f32);
	// fmr f31,f28
	ctx.f31.f64 = ctx.f28.f64;
	// lfs f8,1712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1712);
	ctx.f8.f64 = double(temp.f32);
	// stfs f22,1068(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// fmuls f22,f8,f15
	ctx.f22.f64 = double(float(ctx.f8.f64 * ctx.f15.f64));
	// stfs f27,1004(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// lfs f27,1436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1436);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f13,f1,f27,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f27.f64 + ctx.f13.f64));
	// lfs f1,1572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1572);
	ctx.f1.f64 = double(temp.f32);
	// lfs f16,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f11,f1,f28,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f28.f64 - ctx.f11.f64)));
	// stfs f12,1452(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1452, temp.u32);
	// fmuls f16,f16,f29
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// lfs f1,1468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1468);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f12,f18,f25
	ctx.f12.f64 = double(float(ctx.f18.f64 * ctx.f25.f64));
	// stfs f29,1616(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1616, temp.u32);
	// stfs f5,1200(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// lfs f6,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f10,f22,f31,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f31.f64 + ctx.f10.f64));
	// lfs f31,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,1588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1588);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f0,f31,f1,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f0.f64));
	// lfs f29,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f6
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f6.f64));
	// lfs f22,1428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1428);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,1676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1676);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,1476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1476);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f21,f22,f28,f21
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f28.f64 + ctx.f21.f64));
	// lfs f18,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f18.f64 = double(temp.f32);
	// stfs f26,1712(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1712, temp.u32);
	// stfs f17,1936(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1936, temp.u32);
	// stfs f4,1580(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1580, temp.u32);
	// fmuls f4,f6,f20
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f20.f64));
	// stfs f2,1728(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1728, temp.u32);
	// fmuls f2,f6,f5
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// stfs f14,1624(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1624, temp.u32);
	// fadds f14,f18,f19
	ctx.f14.f64 = double(float(ctx.f18.f64 + ctx.f19.f64));
	// lfs f26,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f26.f64 = double(temp.f32);
	// lfs f17,1652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1652);
	ctx.f17.f64 = double(temp.f32);
	// lfs f31,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f31,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// stfs f31,1676(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1676, temp.u32);
	// stfs f21,1428(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1428, temp.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f21,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f21.f64 = double(temp.f32);
	// lfs f31,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f21,f31
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// stfs f31,1468(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1468, temp.u32);
	// stfs f16,1256(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// lfs f16,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f16.f64 = double(temp.f32);
	// lfs f31,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f29,f29,f16
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f16.f64));
	// fmuls f31,f6,f31
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f6,1644(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1644, temp.u32);
	// lfs f16,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f16.f64 = double(temp.f32);
	// lfs f6,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f13,f6,f16,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f16.f64 + ctx.f13.f64));
	// stfs f29,1116(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// stfs f12,1460(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1460, temp.u32);
	// lfs f12,1420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1420);
	ctx.f12.f64 = double(temp.f32);
	// lfs f29,1636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1636);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// stfs f0,5332(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5332, temp.u32);
	// lfs f0,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f0.f64 = double(temp.f32);
	// stfs f29,1420(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1420, temp.u32);
	// fmuls f29,f17,f29
	ctx.f29.f64 = double(float(ctx.f17.f64 * ctx.f29.f64));
	// stfs f29,1572(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1572, temp.u32);
	// lfs f29,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f11,f24,f29,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f29.f64 + ctx.f11.f64));
	// stfs f15,1588(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1588, temp.u32);
	// fnmsubs f13,f30,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f15,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f15.f64 = double(temp.f32);
	// fmr f0,f16
	ctx.f0.f64 = ctx.f16.f64;
	// lfs f6,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f6.f64 = double(temp.f32);
	// stfs f8,1240(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// fmuls f6,f2,f6
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// fmuls f8,f31,f14
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f14.f64));
	// lfs f2,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f10,f3,f2,f10
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f2.f64 - ctx.f10.f64)));
	// stfs f9,1268(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1268, temp.u32);
	// lfs f9,1712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1712);
	ctx.f9.f64 = double(temp.f32);
	// stfs f7,1436(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1436, temp.u32);
	// stfs f27,1620(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1620, temp.u32);
	// fmr f27,f31
	ctx.f27.f64 = ctx.f31.f64;
	// lfs f7,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f7.f64 = double(temp.f32);
	// lfs f21,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f7,f15,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f7.f64));
	// stfs f5,1360(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1360, temp.u32);
	// fmadds f13,f9,f0,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f5,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f21,f26,f21
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f21.f64));
	// lfs f3,1444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1444);
	ctx.f3.f64 = double(temp.f32);
	// fmr f9,f31
	ctx.f9.f64 = ctx.f31.f64;
	// fnmsubs f11,f3,f5,f11
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// stfs f26,1476(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1476, temp.u32);
	// lfs f26,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// stfs f28,1632(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1632, temp.u32);
	// fmuls f4,f4,f29
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// fmsubs f6,f26,f27,f6
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 - ctx.f6.f64));
	// lfs f27,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f27.f64 = double(temp.f32);
	// stfs f12,1944(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1944, temp.u32);
	// fmuls f12,f15,f23
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// stfs f1,1008(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// fmr f28,f16
	ctx.f28.f64 = ctx.f16.f64;
	// lfs f29,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f29.f64 = double(temp.f32);
	// lfs f5,1452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1452);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f9,f7,f9,f21
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + ctx.f21.f64));
	// lfs f3,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,1580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1580);
	ctx.f2.f64 = double(temp.f32);
	// fmr f7,f16
	ctx.f7.f64 = ctx.f16.f64;
	// lfs f1,1404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1404);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f11,f27,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f30,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f30.f64 = double(temp.f32);
	// stfs f22,1212(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// stfs f25,1636(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1636, temp.u32);
	// stfs f20,1524(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1524, temp.u32);
	// stfs f18,1332(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1332, temp.u32);
	// stfs f19,1340(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1340, temp.u32);
	// stfs f17,1232(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// fnmsubs f13,f5,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f14,1460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1460);
	ctx.f14.f64 = double(temp.f32);
	// fmr f0,f3
	ctx.f0.f64 = ctx.f3.f64;
	// lfs f27,1468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1468);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f11,f14,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f14.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f24,1684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1684);
	ctx.f24.f64 = double(temp.f32);
	// lfs f19,1556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f10,f27,f7,f10
	ctx.f10.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f10.f64)));
	// lfs f27,1596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1596);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f19,f24,f19
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// stfs f19,1596(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1596, temp.u32);
	// fmr f7,f31
	ctx.f7.f64 = ctx.f31.f64;
	// lfs f19,1644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1644);
	ctx.f19.f64 = double(temp.f32);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f25,1564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1564);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f9,f9,f19,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f19.f64 + ctx.f6.f64));
	// lfs f22,1588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1588);
	ctx.f22.f64 = double(temp.f32);
	// lfs f26,1484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1484);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f21,f22,f25
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// lfs f14,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f13,f2,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f2,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f2,1436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1436);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,1272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1272);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f0,f2,f0,f11
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f6,1428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1428);
	ctx.f6.f64 = double(temp.f32);
	// stfs f12,1188(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// fmuls f12,f14,f26
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f26.f64));
	// lfs f20,1264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1264);
	ctx.f20.f64 = double(temp.f32);
	// stfs f0,5340(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5340, temp.u32);
	// stfs f21,1684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1684, temp.u32);
	// fmuls f21,f19,f30
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f30.f64));
	// lfs f5,1412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1412);
	ctx.f5.f64 = double(temp.f32);
	// stfs f12,1264(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1264, temp.u32);
	// fmuls f15,f5,f27
	ctx.f15.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// fnmsubs f13,f1,f31,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f31.f64 - ctx.f13.f64)));
	// lfs f12,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f12.f64 = double(temp.f32);
	// stfs f21,1272(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1272, temp.u32);
	// fmuls f12,f29,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// lfs f21,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f21.f64 = double(temp.f32);
	// fmr f29,f28
	ctx.f29.f64 = ctx.f28.f64;
	// lfs f18,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f11,f21,f26
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// stfs f15,1252(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// fmr f15,f28
	ctx.f15.f64 = ctx.f28.f64;
	// lfs f3,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f17,f20,f18
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// stfs f17,1116(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// fmuls f17,f3,f18
	ctx.f17.f64 = double(float(ctx.f3.f64 * ctx.f18.f64));
	// stfs f17,1256(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// lfs f17,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f17.f64 = double(temp.f32);
	// stfs f22,1124(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// fmuls f22,f19,f17
	ctx.f22.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// fmadds f0,f6,f28,f13
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f28.f64 + ctx.f13.f64));
	// lfs f13,1676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1676);
	ctx.f13.f64 = double(temp.f32);
	// stfs f22,1484(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1484, temp.u32);
	// lfs f16,1396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1396);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,1364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f29,f16,f29
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// lfs f2,1572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1572);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f22,f27,f22
	ctx.f22.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// stfs f23,1580(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1580, temp.u32);
	// fmadds f10,f2,f15,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f15.f64 + ctx.f10.f64));
	// stfs f25,1364(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1364, temp.u32);
	// stfs f24,1564(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1564, temp.u32);
	// stfs f20,1412(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1412, temp.u32);
	// stfs f27,1444(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1444, temp.u32);
	// fmadds f0,f13,f7,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + ctx.f0.f64));
	// stfs f0,5336(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5336, temp.u32);
	// lfs f0,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,1040(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// fmadds f13,f8,f0,f4
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f4.f64));
	// stfs f18,1556(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1556, temp.u32);
	// stfs f3,1652(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1652, temp.u32);
	// stfs f16,1436(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1436, temp.u32);
	// stfs f30,1352(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1352, temp.u32);
	// stfs f14,1572(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1572, temp.u32);
	// stfs f17,1296(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1296, temp.u32);
	// stfs f26,1712(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1712, temp.u32);
	// stfs f21,1404(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1404, temp.u32);
	// fmr f6,f28
	ctx.f6.f64 = ctx.f28.f64;
	// lfs f8,1704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1704);
	ctx.f8.f64 = double(temp.f32);
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// lfs f7,1692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1692);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f19,f8
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// lfs f27,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f7,f19
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f19.f64));
	// lfs f16,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f16.f64 = double(temp.f32);
	// lfs f30,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f30.f64 = double(temp.f32);
	// lfs f20,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f20.f64 = double(temp.f32);
	// lfs f26,1320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1320);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,1328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1328);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f19,f26
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f26.f64));
	// lfs f3,1548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f21,f19,f24
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f24.f64));
	// lfs f1,1420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1420);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f12,f6,f10
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f6.f64 - ctx.f10.f64)));
	// lfs f6,1304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1304);
	ctx.f6.f64 = double(temp.f32);
	// fmr f10,f28
	ctx.f10.f64 = ctx.f28.f64;
	// lfs f17,1628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1628);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f9,f22,f4,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f4.f64 + ctx.f9.f64));
	// lfs f22,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f8,f8,f27
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// lfs f27,1336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1336);
	ctx.f27.f64 = double(temp.f32);
	// lfs f4,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f15,f16,f22
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f22.f64));
	// stfs f15,1252(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// fmuls f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// fmuls f18,f19,f27
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f27.f64));
	// lfs f4,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f15,f20,f19
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// stfs f15,1328(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1328, temp.u32);
	// lfs f15,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f0,f4,f0,f29
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f29.f64));
	// stfs f23,1336(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1336, temp.u32);
	// fmuls f5,f5,f15
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f15.f64));
	// lfs f29,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f3,f1
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// fnmsubs f13,f30,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f10,1312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1312);
	ctx.f10.f64 = double(temp.f32);
	// stfs f18,1312(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1312, temp.u32);
	// fmuls f25,f19,f10
	ctx.f25.f64 = double(float(ctx.f19.f64 * ctx.f10.f64));
	// lfs f18,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f23,f16,f18
	ctx.f23.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// stfs f5,1124(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// stfs f23,1304(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1304, temp.u32);
	// lfs f23,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f23.f64 = double(temp.f32);
	// lfs f5,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f12,f5,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// lfs f15,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f15.f64 = double(temp.f32);
	// lfs f23,1272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1272);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f9,f23,f15,f9
	ctx.f9.f64 = double(float(-(ctx.f23.f64 * ctx.f15.f64 - ctx.f9.f64)));
	// lfs f15,1264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1264);
	ctx.f15.f64 = double(temp.f32);
	// lfs f5,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f13,f15,f5,f13
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f5.f64 + ctx.f13.f64));
	// lfs f23,1344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1344);
	ctx.f23.f64 = double(temp.f32);
	// lfs f5,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f6,f23,f5,f6
	ctx.f6.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 + ctx.f6.f64));
	// lfs f15,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f15.f64 = double(temp.f32);
	// lfs f5,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f5.f64 = double(temp.f32);
	// lfs f30,1380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1380);
	ctx.f30.f64 = double(temp.f32);
	// fadds f5,f15,f5
	ctx.f5.f64 = double(float(ctx.f15.f64 + ctx.f5.f64));
	// lfs f14,1668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1668);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f1,f30
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f2,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f2.f64 = double(temp.f32);
	// lfs f23,2860(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2860);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f28,f2,f29
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f29.f64));
	// lfs f15,2676(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2676);
	ctx.f15.f64 = double(temp.f32);
	// stfs f21,1320(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1320, temp.u32);
	// fadds f21,f17,f14
	ctx.f21.f64 = double(float(ctx.f17.f64 + ctx.f14.f64));
	// stfs f0,1264(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1264, temp.u32);
	// stfs f4,1548(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1548, temp.u32);
	// stfs f31,1468(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1468, temp.u32);
	// stfs f3,1452(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1452, temp.u32);
	// stfs f26,1628(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1628, temp.u32);
	// stfs f23,432(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// stfs f24,1380(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1380, temp.u32);
	// stfs f15,480(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 480, temp.u32);
	// stfs f7,1396(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1396, temp.u32);
	// stfs f30,1588(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1588, temp.u32);
	// stfs f27,1668(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1668, temp.u32);
	// lfs f7,1684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1684);
	ctx.f7.f64 = double(temp.f32);
	// lfs f0,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f12,f7,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// stfs f8,1188(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,1484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1484);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f9,f7,f8,f9
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f9.f64)));
	// stfs f10,1704(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1704, temp.u32);
	// lfs f10,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f10,f1,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// lfs f4,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f8,f0,f16
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f16.f64));
	// lfs f30,1596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1596);
	ctx.f30.f64 = double(temp.f32);
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f11,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fmadds f12,f30,f4,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f30,1336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1336);
	ctx.f30.f64 = double(temp.f32);
	// fmr f31,f0
	ctx.f31.f64 = ctx.f0.f64;
	// stfs f28,1460(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1460, temp.u32);
	// stfs f29,1272(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1272, temp.u32);
	// fmuls f7,f6,f19
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f19.f64));
	// fnmsubs f9,f30,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f29,1800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1800);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,1328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1328);
	ctx.f28.f64 = double(temp.f32);
	// lfs f15,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f15.f64 = double(temp.f32);
	// lfs f4,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f4.f64 = double(temp.f32);
	// lfs f23,1832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1832);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f4,f28,f4
	ctx.f4.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// lfs f27,1320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1320);
	ctx.f27.f64 = double(temp.f32);
	// stfs f17,1224(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// fmuls f17,f29,f23
	ctx.f17.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// stfs f25,1344(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1344, temp.u32);
	// fnmsubs f12,f15,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f15.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f25,1816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1816);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f0,f27,f0,f13
	ctx.f0.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f1,1372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	ctx.f1.f64 = double(temp.f32);
	// lfs f28,1532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	ctx.f28.f64 = double(temp.f32);
	// lfs f15,1312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1312);
	ctx.f15.f64 = double(temp.f32);
	// stfs f17,1684(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1684, temp.u32);
	// fmadds f9,f15,f31,f9
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f31.f64 + ctx.f9.f64));
	// stfs f20,1676(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1676, temp.u32);
	// fmuls f20,f29,f25
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// lfs f11,2092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2092);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,1732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1732);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f24,f28,f11
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f27,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f3,f21,f11
	ctx.f3.f64 = double(float(ctx.f21.f64 * ctx.f11.f64));
	// lfs f17,1908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1908);
	ctx.f17.f64 = double(temp.f32);
	// stfs f14,1428(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1428, temp.u32);
	// fadds f14,f28,f1
	ctx.f14.f64 = double(float(ctx.f28.f64 + ctx.f1.f64));
	// stfs f2,1644(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1644, temp.u32);
	// fmuls f17,f17,f30
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f30.f64));
	// stfs f22,1280(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1280, temp.u32);
	// fmuls f22,f30,f27
	ctx.f22.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// stfs f20,1328(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1328, temp.u32);
	// stfs f1,2092(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2092, temp.u32);
	// lfs f20,1304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1304);
	ctx.f20.f64 = double(temp.f32);
	// lfs f6,1356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f2.f64 = double(temp.f32);
	// lfs f13,1324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1324);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f2,f6,f2
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// lfs f31,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f26,f30,f13
	ctx.f26.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// lfs f15,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f31,f20,f31
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f31.f64));
	// lfs f1,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f1.f64 = double(temp.f32);
	// stfs f18,1288(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1288, temp.u32);
	// fmuls f15,f1,f15
	ctx.f15.f64 = double(float(ctx.f1.f64 * ctx.f15.f64));
	// stfs f22,1336(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1336, temp.u32);
	// stfs f24,1800(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1800, temp.u32);
	// stfs f17,1304(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1304, temp.u32);
	// lfs f21,1348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f18.f64 = double(temp.f32);
	// lfs f22,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f22.f64 = double(temp.f32);
	// lfs f24,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f24.f64 = double(temp.f32);
	// lfs f17,1316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1316);
	ctx.f17.f64 = double(temp.f32);
	// stfs f20,1816(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1816, temp.u32);
	// lfs f20,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f20.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// fnmsubs f0,f20,f22,f0
	ctx.f0.f64 = double(float(-(ctx.f20.f64 * ctx.f22.f64 - ctx.f0.f64)));
	// stfs f24,1312(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1312, temp.u32);
	// fmuls f20,f29,f24
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// stfs f20,1252(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// fmuls f24,f11,f21
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f21.f64));
	// stfs f24,1484(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1484, temp.u32);
	// lfs f20,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f30,f17
	ctx.f17.f64 = double(float(ctx.f30.f64 * ctx.f17.f64));
	// lfs f24,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f24.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f22,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f20,f24
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f24.f64));
	// lfs f20,1344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1344);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f3,f3,f22
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f22.f64));
	// stfs f11,1344(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1344, temp.u32);
	// lfs f22,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f22.f64 = double(temp.f32);
	// lfs f11,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f12,f11,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// stfs f28,1420(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1420, temp.u32);
	// lfs f28,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f28.f64 = double(temp.f32);
	// stfs f10,1132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// fmadds f1,f1,f28,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f28.f64 + ctx.f31.f64));
	// stfs f13,1372(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1372, temp.u32);
	// lfs f10,1264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1264);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,1272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1272);
	ctx.f13.f64 = double(temp.f32);
	// lfs f31,2824(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2824);
	ctx.f31.f64 = double(temp.f32);
	// stfs f31,544(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// fmr f31,f22
	ctx.f31.f64 = ctx.f22.f64;
	// stfs f4,1196(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// fmr f4,f22
	ctx.f4.f64 = ctx.f22.f64;
	// stfs f17,1596(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1596, temp.u32);
	// fmadds f12,f10,f13,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f12.f64));
	// lfs f17,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f2,f2,f17
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f17.f64));
	// lfs f17,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f17.f64 = double(temp.f32);
	// stfs f29,1320(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1320, temp.u32);
	// fmuls f29,f29,f18
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f18.f64));
	// fnmsubs f11,f20,f17,f9
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f17.f64 - ctx.f9.f64)));
	// lfs f9,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,1468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1468);
	ctx.f10.f64 = double(temp.f32);
	// stfs f29,1116(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// lfs f29,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f29.f64 = double(temp.f32);
	// stfs f27,1188(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// fnmsubs f31,f29,f31,f0
	ctx.f31.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f0.f64)));
	// lfs f27,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f27.f64 = double(temp.f32);
	// lfs f0,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f5,f14,f27,f5
	ctx.f5.f64 = double(float(ctx.f14.f64 * ctx.f27.f64 + ctx.f5.f64));
	// fmadds f12,f10,f9,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 + ctx.f12.f64));
	// stfs f25,1124(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f0,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f0.f64 = double(temp.f32);
	// lfs f25,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f3,f26,f0,f3
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 - ctx.f3.f64));
	// fmadds f2,f1,f25,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f25.f64 + ctx.f2.f64));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f1.f64 = double(temp.f32);
	// fmr f10,f9
	ctx.f10.f64 = ctx.f9.f64;
	// stfs f6,1732(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1732, temp.u32);
	// fmuls f1,f0,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// lfs f6,1588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1588);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,1612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1612);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f11,f7,f0,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f27,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f16,f9
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f9.f64));
	// fnmsubs f12,f6,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f26,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// lfs f6,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f27,f15,f27
	ctx.f27.f64 = double(float(ctx.f15.f64 * ctx.f27.f64));
	// lfs f28,1460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1460);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f5,f5,f16
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// lfs f4,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f24,f26
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// stfs f19,1832(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1832, temp.u32);
	// fmr f7,f22
	ctx.f7.f64 = ctx.f22.f64;
	// stfs f23,1256(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// fnmsubs f6,f6,f0,f31
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f31.f64)));
	// stfs f21,1208(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// stfs f18,1216(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// lfs f0,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f12,f28,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f21,1816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1816);
	ctx.f21.f64 = double(temp.f32);
	// fmr f22,f19
	ctx.f22.f64 = ctx.f19.f64;
	// fmuls f21,f21,f0
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// lfs f0,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f8,f8,f0,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 - ctx.f2.f64));
	// lfs f2,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f29,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f2.f64));
	// lfs f31,1368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1368);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f28.f64 = double(temp.f32);
	// fmsubs f5,f5,f0,f27
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 - ctx.f27.f64));
	// lfs f29,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f23,f16,f31
	ctx.f23.f64 = double(float(ctx.f16.f64 * ctx.f31.f64));
	// lfs f18,1800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1800);
	ctx.f18.f64 = double(temp.f32);
	// lfs f10,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f3,f18,f29,f3
	ctx.f3.f64 = double(float(-(ctx.f18.f64 * ctx.f29.f64 - ctx.f3.f64)));
	// fmuls f10,f10,f28
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// lfs f28,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f28.f64 = double(temp.f32);
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f27,f1,f0,f26
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 - ctx.f26.f64));
	// fmuls f29,f28,f29
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// lfs f28,1732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1732);
	ctx.f28.f64 = double(temp.f32);
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f18,f28,f0
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f25,1432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1432);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f0,f4,f7,f12
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f7.f64 + ctx.f12.f64));
	// stfs f0,5344(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5344, temp.u32);
	// lfs f0,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f20,f22,f25
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// fmr f7,f17
	ctx.f7.f64 = ctx.f17.f64;
	// stfs f9,1832(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1832, temp.u32);
	// fnmsubs f8,f23,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f9,1544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1544);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f0,f21,f0,f5
	ctx.f0.f64 = double(float(-(ctx.f21.f64 * ctx.f0.f64 - ctx.f5.f64)));
	// lfs f21,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f21.f64 = double(temp.f32);
	// lfs f5,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f22,f9
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// stfs f31,1356(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1356, temp.u32);
	// fnmsubs f5,f29,f5,f27
	ctx.f5.f64 = double(float(-(ctx.f29.f64 * ctx.f5.f64 - ctx.f27.f64)));
	// lfs f31,1336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1336);
	ctx.f31.f64 = double(temp.f32);
	// stfs f24,1272(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1272, temp.u32);
	// lfs f24,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f24.f64 = double(temp.f32);
	// lfs f26,-19688(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19688);
	ctx.f26.f64 = double(temp.f32);
	// stfs f13,1432(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1432, temp.u32);
	// fnmsubs f7,f20,f7,f6
	ctx.f7.f64 = double(float(-(ctx.f20.f64 * ctx.f7.f64 - ctx.f6.f64)));
	// lfs f20,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f8,f2,f21,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f21.f64 + ctx.f8.f64));
	// lfs f2,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f21,f18,f20,f0
	ctx.f21.f64 = double(float(-(ctx.f18.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f6,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f6.f64 = double(temp.f32);
	// fmr f0,f2
	ctx.f0.f64 = ctx.f2.f64;
	// lfs f20,1308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1308);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f6,f31,f6,f3
	ctx.f6.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f3.f64)));
	// stfs f26,476(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 476, temp.u32);
	// stfs f1,1348(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1348, temp.u32);
	// lfs f19,1724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1724);
	ctx.f19.f64 = double(temp.f32);
	// lfs f13,2092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2092);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f19,f30
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f30.f64));
	// lfs f12,1344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1344);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,1568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1568);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmadds f7,f4,f24,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f24.f64 + ctx.f7.f64));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f10,f20,f4,f10
	ctx.f10.f64 = double(float(ctx.f20.f64 * ctx.f4.f64 + ctx.f10.f64));
	// lfs f3,1576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1576);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,1584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1584);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f27,f16,f1
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f1.f64));
	// fmadds f5,f2,f0,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f5.f64));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,1600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1600);
	ctx.f2.f64 = double(temp.f32);
	// lfs f26,1592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1592);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f24,f0,f2
	ctx.f24.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// lfs f4,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f4.f64 = double(temp.f32);
	// fadds f23,f26,f31
	ctx.f23.f64 = double(float(ctx.f26.f64 + ctx.f31.f64));
	// lfs f18,1328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1328);
	ctx.f18.f64 = double(temp.f32);
	// stfs f25,1732(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1732, temp.u32);
	// fmuls f25,f16,f3
	ctx.f25.f64 = double(float(ctx.f16.f64 * ctx.f3.f64));
	// lfs f0,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f6,f18,f4,f6
	ctx.f6.f64 = double(float(-(ctx.f18.f64 * ctx.f4.f64 - ctx.f6.f64)));
	// lfs f18,1608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1608);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f8,f27,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// stfs f30,1584(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1584, temp.u32);
	// fadds f11,f11,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f7.f64));
	// stfs f19,1608(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1608, temp.u32);
	// fnmsubs f25,f25,f0,f21
	ctx.f25.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f21.f64)));
	// lfs f19,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f10,f10,f16
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f16.f64));
	// lfs f30,1712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1712);
	ctx.f30.f64 = double(temp.f32);
	// lfs f7,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f30,f19,f30
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f30.f64));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// lfs f27,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f0,f0,f7
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f14,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f19.f64 = double(temp.f32);
	// stfs f7,1108(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// stfs f0,892(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// stfs f30,1228(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// lfs f30,1684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1684);
	ctx.f30.f64 = double(temp.f32);
	// lfs f7,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f30,f7,f6
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f7.f64 + ctx.f6.f64));
	// lfs f17,1320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1320);
	ctx.f17.f64 = double(temp.f32);
	// lfs f4,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f15,f18,f17
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// stfs f2,1204(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// fmadds f5,f23,f4,f5
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f4.f64 + ctx.f5.f64));
	// stfs f9,1336(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1336, temp.u32);
	// fmuls f4,f16,f27
	ctx.f4.f64 = double(float(ctx.f16.f64 * ctx.f27.f64));
	// stfs f13,1368(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1368, temp.u32);
	// stfs f15,1592(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1592, temp.u32);
	// lfs f15,1312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1312);
	ctx.f15.f64 = double(temp.f32);
	// lfs f9,1304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1304);
	ctx.f9.f64 = double(temp.f32);
	// lfs f2,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f2.f64 = double(temp.f32);
	// lfs f13,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f2,f24,f2
	ctx.f2.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// lfs f23,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f13,f9,f13,f7
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f7.f64));
	// stfs f12,1600(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1600, temp.u32);
	// fmuls f12,f14,f15
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// stfs f26,1344(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1344, temp.u32);
	// stfs f29,2092(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2092, temp.u32);
	// stfs f5,5360(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5360, temp.u32);
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// lfs f26,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f26.f64 = double(temp.f32);
	// lfs f5,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// stfs f3,1192(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// fmuls f29,f26,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// stfs f11,5348(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5348, temp.u32);
	// fmuls f11,f0,f19
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f19.f64));
	// lfs f9,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f0,f5
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// lfs f24,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f12,f12,f9,f2
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f9.f64 - ctx.f2.f64));
	// stfs f27,1532(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1532, temp.u32);
	// fmadds f13,f24,f9,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f9.f64 + ctx.f13.f64));
	// lfs f21,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f21.f64 = double(temp.f32);
	// lfs f0,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f22,f21
	ctx.f6.f64 = double(float(ctx.f22.f64 * ctx.f21.f64));
	// lfs f30,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f30.f64 = double(temp.f32);
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// stfs f28,1816(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1816, temp.u32);
	// fmuls f28,f17,f30
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f30.f64));
	// stfs f1,1324(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1324, temp.u32);
	// fmuls f26,f26,f17
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f17.f64));
	// stfs f31,1328(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1328, temp.u32);
	// fmr f31,f0
	ctx.f31.f64 = ctx.f0.f64;
	// lfs f7,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f7.f64 = double(temp.f32);
	// lfs f1,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f1.f64 = double(temp.f32);
	// lfs f24,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f24.f64 = double(temp.f32);
	// lfs f9,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f9.f64 = double(temp.f32);
	// lfs f2,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f2.f64 = double(temp.f32);
	// stfs f20,1800(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1800, temp.u32);
	// stfs f18,1132(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// stfs f16,1576(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1576, temp.u32);
	// stfs f15,1196(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// stfs f21,1568(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1568, temp.u32);
	// fmadds f10,f10,f9,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 + ctx.f8.f64));
	// lfs f15,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f8,f4,f2,f25
	ctx.f8.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f25.f64)));
	// lfs f25,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f25.f64 = double(temp.f32);
	// lfs f21,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f15,f15,f5
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f5.f64));
	// lfs f18,1484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1484);
	ctx.f18.f64 = double(temp.f32);
	// fmr f9,f1
	ctx.f9.f64 = ctx.f1.f64;
	// stfs f5,1072(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// fmadds f13,f18,f7,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f7.f64 + ctx.f13.f64));
	// stfs f15,1080(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// lfs f15,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f15.f64 = double(temp.f32);
	// lfs f5,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f15,f5
	ctx.f5.f64 = double(float(ctx.f15.f64 * ctx.f5.f64));
	// lfs f15,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f6,f6,f15
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// lfs f15,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f12,f28,f15,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f15.f64 - ctx.f12.f64)));
	// lfs f28,1596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1596);
	ctx.f28.f64 = double(temp.f32);
	// lfs f20,1508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1508);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f16,f21,f20
	ctx.f16.f64 = double(float(ctx.f21.f64 * ctx.f20.f64));
	// stfs f16,1000(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// lfs f16,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f13,f28,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f18,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f18,f16
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// stfs f16,892(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// lfs f16,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f16.f64 = double(temp.f32);
	// stfs f10,5356(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5356, temp.u32);
	// fmuls f16,f25,f16
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f16.f64));
	// lfs f10,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f10.f64 = double(temp.f32);
	// stfs f16,1064(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// lfs f16,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f16,f19
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// stfs f16,1056(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// lfs f16,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f16.f64 = double(temp.f32);
	// lfs f4,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f11,f11,f16,f29
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f16.f64 - ctx.f29.f64));
	// fnmsubs f13,f10,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f2,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f2.f64 = double(temp.f32);
	// lfs f7,1812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1812);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// lfs f16,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f23,f14,f2
	ctx.f23.f64 = double(float(ctx.f14.f64 * ctx.f2.f64));
	// stfs f8,5364(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5364, temp.u32);
	// fmuls f21,f21,f7
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f7.f64));
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// lfs f8,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f10,f0,f16
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f16.f64));
	// stfs f4,1180(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// lfs f4,1608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1608);
	ctx.f4.f64 = double(temp.f32);
	// stfs f2,1544(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1544, temp.u32);
	// fmuls f2,f0,f8
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfs f7,1108(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// fmuls f7,f0,f24
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f24.f64));
	// fmadds f0,f4,f1,f13
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f1.f64 + ctx.f13.f64));
	// lfs f13,1600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1600);
	ctx.f13.f64 = double(temp.f32);
	// lfs f29,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f26,f28
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// lfs f27,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f26.f64 = double(temp.f32);
	// lfs f4,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f4.f64 = double(temp.f32);
	// stfs f30,1240(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// stfs f14,1712(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1712, temp.u32);
	// stfs f25,1200(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// stfs f17,1460(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1460, temp.u32);
	// fnmsubs f0,f13,f31,f0
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f31.f64 - ctx.f0.f64)));
	// lfs f13,1592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1592);
	ctx.f13.f64 = double(temp.f32);
	// stfs f20,1228(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// fmadds f0,f13,f9,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f9.f64 + ctx.f0.f64));
	// stfs f0,5352(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5352, temp.u32);
	// fmr f0,f15
	ctx.f0.f64 = ctx.f15.f64;
	// lfs f9,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f9,f5,f9,f29
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f29.f64));
	// fmuls f13,f27,f26
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// fmr f5,f15
	ctx.f5.f64 = ctx.f15.f64;
	// fnmsubs f11,f3,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// fmadds f0,f18,f0,f12
	ctx.f0.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f12,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f12,f23,f12,f11
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f12.f64 - ctx.f11.f64)));
	// lfs f23,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f23.f64 = double(temp.f32);
	// lfs f20,1452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1452);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f5,f23,f5,f0
	ctx.f5.f64 = double(float(-(ctx.f23.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// fmadds f9,f6,f4,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f4.f64 + ctx.f9.f64));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f20,f22
	ctx.f4.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// lfs f20,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f0,f16
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f16.f64));
	// lfs f31,1748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1748);
	ctx.f31.f64 = double(temp.f32);
	// fmr f0,f20
	ctx.f0.f64 = ctx.f20.f64;
	// lfs f15,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f26,f31,f22
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f22.f64));
	// lfs f1,1636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1636);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f20,f20,f15
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// lfs f15,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f14,f22,f15
	ctx.f14.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// stfs f14,1112(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// lfs f14,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f22,f1
	ctx.f30.f64 = double(float(ctx.f22.f64 * ctx.f1.f64));
	// lfs f11,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f11.f64 = double(temp.f32);
	// lfs f23,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f23.f64 = double(temp.f32);
	// lfs f6,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f0,f19
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f19.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f26,f26,f0
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// stfs f26,1080(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// lfs f26,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f3,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// stfs f22,1128(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// stfs f22,1204(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f22,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f22,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// lfs f22,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f5,f22,f23,f5
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f23.f64 + ctx.f5.f64));
	// lfs f22,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f22.f64 = double(temp.f32);
	// lfs f11,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f9,f22,f6,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f6.f64 + ctx.f9.f64));
	// fmr f23,f6
	ctx.f23.f64 = ctx.f6.f64;
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// fmr f6,f11
	ctx.f6.f64 = ctx.f11.f64;
	// lfs f25,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f25.f64 = double(temp.f32);
	// lfs f18,1584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1584);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f11,f11,f16
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f16.f64));
	// fmuls f25,f25,f18
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// lfs f0,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f30,f22
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f22.f64));
	// stfs f27,1000(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// stfs f3,1252(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// stfs f15,1304(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1304, temp.u32);
	// stfs f1,1056(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// stfs f20,1120(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// stfs f19,1220(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// fmadds f0,f21,f0,f9
	ctx.f0.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 + ctx.f9.f64));
	// fmadds f13,f13,f23,f29
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f23.f64 + ctx.f29.f64));
	// lfs f29,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f29.f64 = double(temp.f32);
	// fmr f16,f6
	ctx.f16.f64 = ctx.f6.f64;
	// lfs f23,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// fmr f6,f22
	ctx.f6.f64 = ctx.f22.f64;
	// lfs f22,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f29,f29,f24
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// lfs f24,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f24.f64 = double(temp.f32);
	// stfs f29,1072(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// fmuls f23,f26,f23
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f29,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f10,f10,f29,f5
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f29.f64 - ctx.f5.f64)));
	// fmuls f22,f16,f22
	ctx.f22.f64 = double(float(ctx.f16.f64 * ctx.f22.f64));
	// lfs f16,1136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1136);
	ctx.f16.f64 = double(temp.f32);
	// stfs f16,1064(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// fmuls f24,f24,f16
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f16.f64));
	// fmuls f6,f25,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// lfs f25,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f12,f28,f25,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f25.f64 + ctx.f12.f64));
	// lfs f16,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f16.f64 = double(temp.f32);
	// lfs f28,1580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1580);
	ctx.f28.f64 = double(temp.f32);
	// stfs f17,1136(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// fmr f9,f29
	ctx.f9.f64 = ctx.f29.f64;
	// lfs f3,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f3.f64 = double(temp.f32);
	// lfs f27,1444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1444);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f16,f28
	ctx.f5.f64 = double(float(ctx.f16.f64 * ctx.f28.f64));
	// lfs f21,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f19.f64 = double(temp.f32);
	// lfs f25,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f25.f64 = double(temp.f32);
	// lfs f20,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f20.f64 = double(temp.f32);
	// lfs f1,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f1.f64 = double(temp.f32);
	// stfs f14,1584(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1584, temp.u32);
	// lfs f14,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f12,f2,f9,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f9.f64 + ctx.f12.f64));
	// lfs f9,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f10,f7,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f7.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f7,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f0,f7,f3,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f9,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f9.f64 = double(temp.f32);
	// fmr f7,f29
	ctx.f7.f64 = ctx.f29.f64;
	// lfs f2,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f2.f64 = double(temp.f32);
	// fadds f29,f1,f2
	ctx.f29.f64 = double(float(ctx.f1.f64 + ctx.f2.f64));
	// lfs f3,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f3.f64 = double(temp.f32);
	// stfs f18,1232(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// lfs f18,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f5,f3,f18,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f18.f64 + ctx.f5.f64));
	// stfs f31,1468(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1468, temp.u32);
	// lfs f31,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,892(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// fadds f26,f28,f31
	ctx.f26.f64 = double(float(ctx.f28.f64 + ctx.f31.f64));
	// fmsubs f13,f13,f21,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f21.f64 - ctx.f0.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f12,f9,f7,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f9,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f9,f9,f27
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// lfs f7,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f7.f64 = double(temp.f32);
	// fmr f27,f17
	ctx.f27.f64 = ctx.f17.f64;
	// fmuls f24,f24,f0
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// fmuls f17,f19,f0
	ctx.f17.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// fmuls f7,f7,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// fmadds f0,f11,f20,f12
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f20,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f15,f15,f20
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// stfs f20,876(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// fmadds f10,f27,f25,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f25.f64 + ctx.f10.f64));
	// lfs f25,1728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1728);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f23,f25,f14,f23
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f14.f64 + ctx.f23.f64));
	// lfs f14,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,1644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1644);
	ctx.f20.f64 = double(temp.f32);
	// lfs f12,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f20,f20,f14
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f14.f64));
	// stfs f5,932(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// fadds f26,f26,f12
	ctx.f26.f64 = double(float(ctx.f26.f64 + ctx.f12.f64));
	// lfs f5,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f5.f64 = double(temp.f32);
	// lfs f14,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f14,f14,f5
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f5.f64));
	// stfs f15,948(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// lfs f15,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f15.f64 = double(temp.f32);
	// stfs f14,1044(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// fmuls f29,f29,f15
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// lfs f14,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f9,f9,f14
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f14.f64));
	// lfs f14,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f7,f7,f15
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f15.f64));
	// fmr f15,f14
	ctx.f15.f64 = ctx.f14.f64;
	// stfs f5,1052(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// lfs f27,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f27.f64 = double(temp.f32);
	// lfs f5,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f5,f27
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// stfs f5,1088(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// lfs f5,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f5.f64 = double(temp.f32);
	// stfs f7,900(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// lfs f11,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f11.f64 = double(temp.f32);
	// lfs f7,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f11,f11,f27
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// fnmsubs f13,f30,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f14,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f10,f14,f5,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f5.f64 + ctx.f10.f64));
	// lfs f5,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f5.f64 = double(temp.f32);
	// lfs f7,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f7.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fnmsubs f7,f5,f7,f0
	ctx.f7.f64 = double(float(-(ctx.f5.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// lfs f30,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f10,f8,f30,f10
	ctx.f10.f64 = double(float(-(ctx.f8.f64 * ctx.f30.f64 - ctx.f10.f64)));
	// fmr f8,f0
	ctx.f8.f64 = ctx.f0.f64;
	// stfs f12,1592(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1592, temp.u32);
	// lfs f12,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f12.f64 = double(temp.f32);
	// fmsubs f11,f11,f0,f24
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f24.f64));
	// lfs f5,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f5.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f3,1176(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// fmadds f3,f20,f0,f17
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f17.f64));
	// lfs f0,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f0.f64 = double(temp.f32);
	// stfs f2,1124(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f2,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f12,f6,f12,f7
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f7.f64));
	// lfs f6,-19100(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19100);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,944(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// fnmsubs f10,f22,f2,f10
	ctx.f10.f64 = double(float(-(ctx.f22.f64 * ctx.f2.f64 - ctx.f10.f64)));
	// lfs f6,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f13,f6,f8,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 + ctx.f13.f64));
	// lfs f6,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f6.f64 = double(temp.f32);
	// stfs f1,1320(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1320, temp.u32);
	// fmuls f6,f0,f6
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// lfs f1,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f9,f3,f21,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f21.f64 - ctx.f9.f64));
	// fmadds f30,f0,f0,f15
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f0.f64 + ctx.f15.f64));
	// stfs f28,1600(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1600, temp.u32);
	// lfs f28,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f12,f2,f1,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f12.f64));
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f1,f0,f27
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f8,f23,f7,f29
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 + ctx.f29.f64));
	// lfs f7,-19104(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19104);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f13,f4,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f7,940(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// fmr f0,f28
	ctx.f0.f64 = ctx.f28.f64;
	// stfs f31,1608(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1608, temp.u32);
	// fadds f7,f26,f5
	ctx.f7.f64 = double(float(ctx.f26.f64 + ctx.f5.f64));
	// lfs f4,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f11,f4,f28,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f28.f64 - ctx.f11.f64)));
	// lfs f4,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f4.f64 = double(temp.f32);
	// lfs f23,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f23.f64 = double(temp.f32);
	// lfs f3,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f3.f64 = double(temp.f32);
	// fadds f12,f10,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// lfs f10,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f10.f64 = double(temp.f32);
	// lfs f26,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f26.f64 = double(temp.f32);
	// lfs f28,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f10,f23,f10,f13
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f10.f64 + ctx.f13.f64));
	// lfs f13,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f8,f31,f0,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 + ctx.f8.f64));
	// lfs f0,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f0,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f0,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f7,f7,f26
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// lfs f2,1436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1436);
	ctx.f2.f64 = double(temp.f32);
	// lfs f26,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f24,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f24.f64 = double(temp.f32);
	// fadds f28,f0,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 + ctx.f28.f64));
	// stfs f25,1580(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1580, temp.u32);
	// fmuls f1,f1,f13
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f0,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f21,f2
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f2.f64));
	// lfs f23,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f21,f26
	ctx.f25.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f24,f24,f27
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// stfs f16,1188(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// stfs f19,1264(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1264, temp.u32);
	// stfs f18,1684(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1684, temp.u32);
	// fmuls f29,f29,f23
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// lfs f23,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f23.f64 = double(temp.f32);
	// lfs f20,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f7,f30,f0,f7
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f0.f64 + ctx.f7.f64));
	// fmadds f11,f20,f23,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f23,1944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1944);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f9,f8,f21,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f21.f64 + ctx.f9.f64));
	// stfs f5,1064(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// fmuls f5,f4,f23
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// fmuls f4,f3,f0
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfs f12,5368(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5368, temp.u32);
	// fmuls f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// lfs f12,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f30,f13,f12
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f8,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f13,f13,f8
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// lfs f8,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f8.f64 = double(temp.f32);
	// stfs f2,1120(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// lfs f2,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f8,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// fnmsubs f0,f25,f0,f9
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// stfs f26,1204(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// lfs f9,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f5,f5,f2,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f4.f64));
	// lfs f2,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f9,f7,f9,f1
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 - ctx.f1.f64));
	// lfs f7,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f7.f64 = double(temp.f32);
	// lfs f25,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f2,f28,f2,f30
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f2.f64 + ctx.f30.f64));
	// lfs f28,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f28.f64 = double(temp.f32);
	// fmr f26,f28
	ctx.f26.f64 = ctx.f28.f64;
	// lfs f30,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f13,f13,f23
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f23.f64));
	// lfs f4,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f4.f64 = double(temp.f32);
	// stfs f31,1312(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1312, temp.u32);
	// fmuls f31,f21,f4
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// fnmsubs f11,f25,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f3,1576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1576);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f30,f29,f30,f0
	ctx.f30.f64 = double(float(-(ctx.f29.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// lfs f29,2144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2144);
	ctx.f29.f64 = double(temp.f32);
	// lfs f0,1340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1340);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f18,f29,f0
	ctx.f18.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f8,1432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1432);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// lfs f20,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f9,f6,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f6.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// stfs f3,736(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// fmr f0,f26
	ctx.f0.f64 = ctx.f26.f64;
	// lfs f3,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f13,f13,f26,f5
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f26.f64 - ctx.f5.f64)));
	// lfs f26,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f26.f64 = double(temp.f32);
	// lfs f19,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f11,f26,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f7,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f7.f64 = double(temp.f32);
	// lfs f26,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmuls f27,f26,f27
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// lfs f26,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f12,f3,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// lfs f3,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f26,f3
	ctx.f3.f64 = double(float(ctx.f26.f64 * ctx.f3.f64));
	// lfs f1,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f1.f64 = double(temp.f32);
	// lfs f6,1620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1620);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f25,f8,f1
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// lfs f17,1524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f28,f6,f20
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f20.f64));
	// fnmsubs f31,f31,f0,f30
	ctx.f31.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// lfs f15,1332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	ctx.f15.f64 = double(temp.f32);
	// lfs f30,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f16,f21,f19
	ctx.f16.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// lfs f26,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f14,f29,f15
	ctx.f14.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// stfs f27,668(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// fmuls f30,f17,f30
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f30.f64));
	// lfs f5,1740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1740);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f26,f26,f23
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f27,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f27.f64 = double(temp.f32);
	// stfs f26,920(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// fmadds f9,f2,f0,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f26,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f2,f27,f26
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// lfs f0,1300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1300);
	ctx.f0.f64 = double(temp.f32);
	// lfs f27,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f0,f27
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f0,1820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1820);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f0,928(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f0,f24,f0,f11
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f26,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f5,f26
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// stfs f26,888(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f13,f25,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// stfs f8,1136(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// stfs f4,1056(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// stfs f1,1112(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// lfs f1,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f24,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f24.f64 = double(temp.f32);
	// stfs f28,944(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// fmadds f8,f22,f8,f0
	ctx.f8.f64 = double(float(ctx.f22.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f4,f16,f0,f31
	ctx.f4.f64 = double(float(-(ctx.f16.f64 * ctx.f0.f64 - ctx.f31.f64)));
	// lfs f31,1500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1500);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f12,f12,f0,f9
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f3,f0,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f9,1292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1292);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f14,f0
	ctx.f3.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f9,f31,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f9.f64));
	// lfs f16,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f16.f64 = double(temp.f32);
	// lfs f25,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f28.f64 = double(temp.f32);
	// stfs f18,948(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// fmsubs f2,f2,f24,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f24.f64 - ctx.f1.f64));
	// lfs f18,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f28,f25,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// fnmsubs f8,f7,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f7,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f7.f64 = double(temp.f32);
	// lfs f24,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f7,f23,f7
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f7.f64));
	// lfs f26,1660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1660);
	ctx.f26.f64 = double(temp.f32);
	// fadds f10,f10,f4
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f4.f64));
	// lfs f22,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f22.f64 = double(temp.f32);
	// lfs f11,1572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1572);
	ctx.f11.f64 = double(temp.f32);
	// stfs f23,740(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// fmuls f11,f26,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// stfs f21,1484(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1484, temp.u32);
	// fmuls f9,f9,f22
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// stfs f15,1088(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// stfs f20,1432(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1432, temp.u32);
	// lfs f30,1604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1604);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f0,f16,f0,f8
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f8,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f8.f64 = double(temp.f32);
	// lfs f25,1428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1428);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f23,f6,f8
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f21,1404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1404);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f30,f25
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f4,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f26,f21
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f21.f64));
	// lfs f1,1284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1284);
	ctx.f1.f64 = double(temp.f32);
	// lfs f20,1568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1568);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f1,f5,f1
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// lfs f15,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f15.f64 = double(temp.f32);
	// stfs f19,1576(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1576, temp.u32);
	// fmuls f19,f29,f24
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// stfs f17,1128(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// fmuls f17,f6,f4
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// stfs f24,1052(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// fmuls f14,f15,f20
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// stfs f23,1044(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// lfs f22,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,1276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1276);
	ctx.f16.f64 = double(temp.f32);
	// lfs f24,1544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1544);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f13,f28,f23,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f23.f64 - ctx.f13.f64)));
	// lfs f28,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f28.f64 = double(temp.f32);
	// lfs f23,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f2,f18,f28,f2
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f28.f64 + ctx.f2.f64));
	// fmuls f7,f7,f23
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f23.f64));
	// lfs f28,1368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1368);
	ctx.f28.f64 = double(temp.f32);
	// lfs f23,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f16,f24
	ctx.f24.f64 = double(float(ctx.f16.f64 * ctx.f24.f64));
	// stfs f27,744(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmadds f9,f25,f23,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f23.f64 + ctx.f9.f64));
	// stfs f28,668(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// fmuls f28,f30,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// stfs f30,816(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// fadds f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// lfs f25,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f25.f64 = double(temp.f32);
	// lfs f30,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f30,f25
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// stfs f0,5376(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5376, temp.u32);
	// lfs f25,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f25.f64 = double(temp.f32);
	// lfs f12,2072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2072);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f0.f64 = double(temp.f32);
	// stfs f8,1544(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1544, temp.u32);
	// fmuls f8,f16,f0
	ctx.f8.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// stfs f11,736(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// fadds f11,f25,f12
	ctx.f11.f64 = double(float(ctx.f25.f64 + ctx.f12.f64));
	// stfs f5,1568(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1568, temp.u32);
	// stfs f10,5372(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5372, temp.u32);
	// lfs f10,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f13,f7,f10,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f13.f64));
	// lfs f27,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f9,f24,f5,f9
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f5.f64 - ctx.f9.f64)));
	// stfs f4,1596(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1596, temp.u32);
	// fmsubs f4,f2,f6,f3
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 - ctx.f3.f64));
	// fmuls f27,f22,f27
	ctx.f27.f64 = double(float(ctx.f22.f64 * ctx.f27.f64));
	// lfs f3,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,1676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1676);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f5,f5,f3,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64 + ctx.f30.f64));
	// stfs f26,940(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// stfs f29,1452(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1452, temp.u32);
	// fmadds f11,f11,f31,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64 + ctx.f8.f64));
	// lfs f29,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f29.f64 = double(temp.f32);
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// lfs f26,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f26.f64 = double(temp.f32);
	// stfs f1,932(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// lfs f30,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f30.f64 = double(temp.f32);
	// lfs f1,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f4,f26,f29,f4
	ctx.f4.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f4.f64));
	// stfs f18,1368(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1368, temp.u32);
	// fnmsubs f13,f27,f8,f13
	ctx.f13.f64 = double(float(-(ctx.f27.f64 * ctx.f8.f64 - ctx.f13.f64)));
	// stfs f21,1588(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1588, temp.u32);
	// fmadds f9,f28,f30,f9
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 + ctx.f9.f64));
	// stfs f22,1220(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// fmuls f1,f1,f30
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f10,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,1556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	ctx.f7.f64 = double(temp.f32);
	// lfs f2,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f7,f14,f7
	ctx.f7.f64 = double(float(ctx.f14.f64 * ctx.f7.f64));
	// lfs f3,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f10,f2
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// lfs f8,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f3,f3,f6
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// lfs f26,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f28,f6,f8
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f25,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// lfs f24,1420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1420);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f6,f25
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f25.f64));
	// lfs f22,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f18.f64 = double(temp.f32);
	// stfs f19,876(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// fmuls f18,f22,f18
	ctx.f18.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// stfs f20,1228(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// fmuls f20,f24,f21
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f30,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f27.f64 = double(temp.f32);
	// lfs f19,1492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
	ctx.f19.f64 = double(temp.f32);
	// stfs f15,1184(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// stfs f17,900(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// stfs f18,744(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmuls f12,f19,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f12.f64));
	// lfs f18,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f2,f27,f18,f2
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f18.f64 + ctx.f2.f64));
	// stfs f23,808(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// lfs f18,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f18.f64 = double(temp.f32);
	// lfs f23,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f11,f21,f18,f11
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f18.f64 + ctx.f11.f64));
	// fmuls f3,f3,f23
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// stfs f18,904(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// lfs f23,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f1,f23,f18,f1
	ctx.f1.f64 = double(float(ctx.f23.f64 * ctx.f18.f64 + ctx.f1.f64));
	// stfs f3,748(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// lfs f18,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f18.f64 = double(temp.f32);
	// lfs f3,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f4,f18,f3,f4
	ctx.f4.f64 = double(float(-(ctx.f18.f64 * ctx.f3.f64 - ctx.f4.f64)));
	// lfs f3,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f9,f16,f3,f9
	ctx.f9.f64 = double(float(-(ctx.f16.f64 * ctx.f3.f64 - ctx.f9.f64)));
	// lfs f15,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f15.f64 = double(temp.f32);
	// stfs f12,740(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// fmr f12,f15
	ctx.f12.f64 = ctx.f15.f64;
	// lfs f23,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f15,f30,f15
	ctx.f15.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// fmadds f13,f7,f23,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 + ctx.f13.f64));
	// lfs f7,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f7.f64 = double(temp.f32);
	// stfs f10,1116(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// fmuls f23,f22,f7
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// stfs f0,1444(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1444, temp.u32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// lfs f14,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f17.f64 = double(temp.f32);
	// stfs f29,888(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// fmuls f17,f17,f14
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// fnmsubs f10,f21,f7,f9
	ctx.f10.f64 = double(float(-(ctx.f21.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// lfs f9,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f9,f9,f0,f4
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f29,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// stfs f22,936(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// fmr f29,f0
	ctx.f29.f64 = ctx.f0.f64;
	// lfs f22,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f22.f64 = double(temp.f32);
	// stfs f8,1000(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// lfs f8,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f8.f64 = double(temp.f32);
	// lfs f18,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f18.f64 = double(temp.f32);
	// lfs f3,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f3,f18
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f18.f64));
	// stfs f19,1072(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// lfs f19,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f7,f22,f18
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// fnmsubs f9,f8,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// stfs f30,1080(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// fmadds f11,f11,f19,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f19.f64 + ctx.f10.f64));
	// lfs f4,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f17,f29,f13
	ctx.f13.f64 = double(float(-(ctx.f17.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// lfs f30,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f30.f64 = double(temp.f32);
	// fmr f22,f0
	ctx.f22.f64 = ctx.f0.f64;
	// lfs f18,1656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1656);
	ctx.f18.f64 = double(temp.f32);
	// fmr f10,f0
	ctx.f10.f64 = ctx.f0.f64;
	// lfs f17,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f17.f64 = double(temp.f32);
	// stfs f14,1132(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// fmuls f18,f18,f17
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// stfs f26,1180(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// stfs f27,1196(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// stfs f25,1256(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// stfs f24,920(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// fnmsubs f9,f4,f30,f9
	ctx.f9.f64 = double(float(-(ctx.f4.f64 * ctx.f30.f64 - ctx.f9.f64)));
	// lfs f29,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f13,f5,f0,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// lfs f5,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f5.f64 = double(temp.f32);
	// fmr f14,f0
	ctx.f14.f64 = ctx.f0.f64;
	// lfs f26,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f5,f21,f5
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// lfs f25,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f24.f64 = double(temp.f32);
	// lfs f8,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f8.f64 = double(temp.f32);
	// lfs f19,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f19.f64 = double(temp.f32);
	// lfs f4,1268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1268);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f15,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f15.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f30,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f9,f29,f27,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f27.f64 + ctx.f9.f64));
	// lfs f17,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f4,f30,f4,f23
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 + ctx.f23.f64));
	// lfs f30,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f30.f64 = double(temp.f32);
	// fmr f23,f10
	ctx.f23.f64 = ctx.f10.f64;
	// lfs f10,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f17,f17,f14
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// lfs f15,1364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f14,f6,f10
	ctx.f14.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// stfs f14,816(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// fmadds f11,f1,f16,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f16.f64 + ctx.f11.f64));
	// lfs f1,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f1.f64 = double(temp.f32);
	// fmr f27,f19
	ctx.f27.f64 = ctx.f19.f64;
	// lfs f14,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f15,f14
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// lfs f29,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f29.f64 = double(temp.f32);
	// lfs f14,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f13,f12,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// lfs f12,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f12.f64 = double(temp.f32);
	// stfs f10,1108(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// fnmsubs f10,f26,f0,f9
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// stfs f13,5380(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5380, temp.u32);
	// fmsubs f4,f29,f14,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f14.f64 - ctx.f4.f64));
	// lfs f9,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f9.f64 = double(temp.f32);
	// fmr f29,f19
	ctx.f29.f64 = ctx.f19.f64;
	// stfs f21,684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// lfs f30,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f11,f20,f1,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f1.f64 + ctx.f11.f64));
	// stfs f11,5388(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5388, temp.u32);
	// lfs f11,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f7,f7,f27
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// lfs f21,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f12,f12,f30,f17
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f30.f64 + ctx.f17.f64));
	// fmuls f21,f31,f21
	ctx.f21.f64 = double(float(ctx.f31.f64 * ctx.f21.f64));
	// stfs f31,928(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// lfs f31,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f25,f0,f10
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f30,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f4,f30,f31,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f31.f64 + ctx.f4.f64));
	// lfs f31,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f31.f64 = double(temp.f32);
	// lfs f10,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f29,f18,f29
	ctx.f29.f64 = double(float(ctx.f18.f64 * ctx.f29.f64));
	// lfs f27,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f27.f64 = double(temp.f32);
	// lfs f1,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f1.f64 = double(temp.f32);
	// stfs f21,668(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// fmuls f1,f27,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// lfs f27,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f5,f5,f21
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f21.f64));
	// lfs f30,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f25,f6,f27
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f27.f64));
	// lfs f26,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f13,f24,f0,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f0,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f0.f64 = double(temp.f32);
	// stfs f16,736(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// fnmsubs f13,f8,f22,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f22.f64 - ctx.f13.f64)));
	// lfs f8,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f11,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f11,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f0,f28,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f13,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f2,f6,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f6.f64 + ctx.f0.f64));
	// lfs f2,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f8,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// fmr f2,f19
	ctx.f2.f64 = ctx.f19.f64;
	// fnmsubs f0,f13,f19,f0
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f19.f64 - ctx.f0.f64)));
	// lfs f13,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f13.f64 = double(temp.f32);
	// fmsubs f7,f15,f2,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f2.f64 - ctx.f7.f64));
	// fmuls f2,f10,f31
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// lfs f31,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f31.f64 = double(temp.f32);
	// fmr f28,f31
	ctx.f28.f64 = ctx.f31.f64;
	// fmadds f0,f13,f23,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f23.f64 + ctx.f0.f64));
	// stfs f0,5384(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5384, temp.u32);
	// lfs f0,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f9,f0,f9
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// lfs f13,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmsubs f9,f3,f31,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f31.f64 - ctx.f9.f64));
	// lfs f3,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f3.f64 = double(temp.f32);
	// lfs f17,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f7,f7,f6,f29
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 - ctx.f29.f64));
	// fmr f16,f17
	ctx.f16.f64 = ctx.f17.f64;
	// lfs f18,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f18.f64 = double(temp.f32);
	// lfs f20,1832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1832);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f8,f8,f28,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f28.f64 - ctx.f2.f64));
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// stfs f20,656(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// lfs f23,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f6,f26
	ctx.f24.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfs f20,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f20.f64 = double(temp.f32);
	// lfs f2,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f20,f20,f23
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// lfs f28,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f28.f64 = double(temp.f32);
	// lfs f19,1260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1260);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f17,f19
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// stfs f11,688(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fmadds f4,f22,f30,f4
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f30.f64 + ctx.f4.f64));
	// stfs f1,680(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// fmuls f16,f16,f3
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f3.f64));
	// lfs f1,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// lfs f11,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f11.f64 = double(temp.f32);
	// stfs f20,748(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// fnmsubs f11,f11,f1,f7
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f7.f64)));
	// lfs f29,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f20.f64 = double(temp.f32);
	// stfs f16,684(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// fnmsubs f9,f29,f20,f9
	ctx.f9.f64 = double(float(-(ctx.f29.f64 * ctx.f20.f64 - ctx.f9.f64)));
	// stfs f19,764(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// lfs f31,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f31.f64 = double(temp.f32);
	// lfs f14,1640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1640);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,1624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1624);
	ctx.f19.f64 = double(temp.f32);
	// lfs f16,1632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1632);
	ctx.f16.f64 = double(temp.f32);
	// stfs f19,808(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// fadds f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 + ctx.f14.f64));
	// fmuls f19,f31,f19
	ctx.f19.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// lfs f17,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f12,f12,f17,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f17.f64 + ctx.f11.f64));
	// fmuls f15,f17,f15
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// stfs f15,788(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// lfs f14,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f4,f18,f20,f4
	ctx.f4.f64 = double(float(ctx.f18.f64 * ctx.f20.f64 + ctx.f4.f64));
	// stfs f10,944(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f0,740(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// lfs f30,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f30.f64 = double(temp.f32);
	// lfs f7,1816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1816);
	ctx.f7.f64 = double(temp.f32);
	// lfs f10,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f10.f64 = double(temp.f32);
	// lfs f1,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f0.f64 = double(temp.f32);
	// stfs f24,664(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// fmadds f11,f0,f10,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 + ctx.f9.f64));
	// lfs f22,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f22.f64 = double(temp.f32);
	// lfs f24,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f24.f64 = double(temp.f32);
	// lfs f20,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,1936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1936);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f8,f28,f20,f8
	ctx.f8.f64 = double(float(-(ctx.f28.f64 * ctx.f20.f64 - ctx.f8.f64)));
	// lfs f14,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f21,f21,f22
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// stfs f13,672(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// fmuls f13,f7,f1
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f1.f64));
	// stfs f25,652(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// fmuls f14,f30,f14
	ctx.f14.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// stfs f3,892(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// fmuls f25,f24,f2
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// lfs f10,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f20
	ctx.f9.f64 = ctx.f20.f64;
	// lfs f3,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f7,f16,f22,f19
	ctx.f7.f64 = double(float(ctx.f16.f64 * ctx.f22.f64 + ctx.f19.f64));
	// stfs f27,900(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// stfs f26,1052(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// stfs f31,1184(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// stfs f23,1088(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// stfs f30,932(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f29,948(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// stfs f2,816(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// stfs f28,744(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmr f31,f20
	ctx.f31.f64 = ctx.f20.f64;
	// lfs f2,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f5,f5,f3,f4
	ctx.f5.f64 = double(float(-(ctx.f5.f64 * ctx.f3.f64 - ctx.f4.f64)));
	// lfs f1,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f0,f0,f2,f8
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64 + ctx.f8.f64));
	// lfs f2,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f8,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f11,f10,f9,f11
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f9.f64 - ctx.f11.f64)));
	// lfs f9,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f9.f64 = double(temp.f32);
	// fmr f1,f20
	ctx.f1.f64 = ctx.f20.f64;
	// lfs f4,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f24,f8
	ctx.f3.f64 = double(float(ctx.f24.f64 * ctx.f8.f64));
	// lfs f30,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f30.f64 = double(temp.f32);
	// lfs f19,1648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1648);
	ctx.f19.f64 = double(temp.f32);
	// lfs f16,1136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1136);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f13,f25,f31,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 - ctx.f13.f64));
	// lfs f31,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f21,f31,f12
	ctx.f12.f64 = double(float(ctx.f21.f64 * ctx.f31.f64 + ctx.f12.f64));
	// lfs f31,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f9,f31,f9,f5
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f9.f64 - ctx.f5.f64)));
	// lfs f21,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f21.f64 = double(temp.f32);
	// fmr f5,f20
	ctx.f5.f64 = ctx.f20.f64;
	// lfs f20,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f20.f64 = double(temp.f32);
	// fmr f31,f30
	ctx.f31.f64 = ctx.f30.f64;
	// stfs f20,600(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// fnmsubs f11,f4,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f1,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f21,f24,f21
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f29,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f0,f15,f30,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// lfs f30,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f24,f24,f19
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// lfs f19,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f16,f19
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// lfs f16,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f1,f1,f20
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f20.f64));
	// lfs f20,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f20.f64 = double(temp.f32);
	// stfs f18,628(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// fmuls f27,f6,f29
	ctx.f27.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// lfs f28,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f13,f3,f5,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f13.f64));
	// lfs f3,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f3,f30,f3,f14
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64 + ctx.f14.f64));
	// lfs f14,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f14.f64 = double(temp.f32);
	// stfs f14,640(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// fmuls f16,f14,f16
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// lfs f14,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f11,f2,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f2.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// fmuls f31,f20,f14
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f14.f64));
	// lfs f20,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f20.f64 = double(temp.f32);
	// lfs f2,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f25,f6,f28
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f28.f64));
	// fmuls f2,f2,f20
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f20.f64));
	// lfs f5,1800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1800);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f5,f18
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// lfs f18,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f9,f18,f14,f9
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f14.f64 + ctx.f9.f64));
	// lfs f18,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f18.f64 = double(temp.f32);
	// lfs f26,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f26.f64 = double(temp.f32);
	// lfs f14,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f6,f26
	ctx.f15.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfs f20,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f20.f64 = double(temp.f32);
	// stfs f22,688(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fnmsubs f22,f23,f18,f0
	ctx.f22.f64 = double(float(-(ctx.f23.f64 * ctx.f18.f64 - ctx.f0.f64)));
	// stfs f13,5396(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5396, temp.u32);
	// fmadds f12,f14,f20,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f0,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f0.f64 = double(temp.f32);
	// stfs f30,940(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// fmuls f13,f2,f6
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// stfs f8,668(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// stfs f17,680(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// stfs f29,876(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f28,1044(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// stfs f10,652(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f4,936(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// stfs f26,920(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// stfs f23,904(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// stfs f11,5392(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5392, temp.u32);
	// lfs f11,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f11,f0,f11,f9
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f11.f64 + ctx.f9.f64));
	// lfs f2,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f12,f2,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f23,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f23.f64 = double(temp.f32);
	// fmr f9,f10
	ctx.f9.f64 = ctx.f10.f64;
	// lfs f8,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f8.f64 = double(temp.f32);
	// fmr f17,f8
	ctx.f17.f64 = ctx.f8.f64;
	// lfs f2,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f2.f64 = double(temp.f32);
	// lfs f18,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f18.f64 = double(temp.f32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// lfs f29,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f29.f64 = double(temp.f32);
	// fmr f4,f8
	ctx.f4.f64 = ctx.f8.f64;
	// fmuls f28,f21,f29
	ctx.f28.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// lfs f21,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f20.f64 = double(temp.f32);
	// fmr f0,f8
	ctx.f0.f64 = ctx.f8.f64;
	// fnmsubs f11,f23,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f23,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f23.f64 = double(temp.f32);
	// stfs f25,528(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmuls f20,f21,f20
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f20.f64));
	// lfs f25,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f18,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f18,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f18.f64 = double(temp.f32);
	// stfs f29,532(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// fnmsubs f22,f18,f17,f22
	ctx.f22.f64 = double(float(-(ctx.f18.f64 * ctx.f17.f64 - ctx.f22.f64)));
	// lfs f17,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f17.f64 = double(temp.f32);
	// stfs f19,440(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmuls f29,f17,f6
	ctx.f29.f64 = double(float(ctx.f17.f64 * ctx.f6.f64));
	// lfs f14,1616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1616);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f19.f64 = double(temp.f32);
	// stfs f21,792(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// fmuls f19,f14,f19
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// fnmsubs f11,f25,f9,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f9.f64 - ctx.f11.f64)));
	// lfs f21,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f21.f64 = double(temp.f32);
	// stfs f29,488(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmuls f29,f21,f6
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// lfs f9,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f7,f30,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f30.f64 + ctx.f12.f64));
	// stfs f19,564(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// lfs f19,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f19.f64 = double(temp.f32);
	// stfs f29,436(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f29,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f14,f29
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// stfs f29,556(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// lfs f29,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f11,f19,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// stfs f9,684(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// fmuls f30,f30,f29
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// lfs f29,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f9,f29
	ctx.f29.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f9,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f9.f64 = double(temp.f32);
	// lfs f2,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f12,f3,f6,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f12.f64));
	// fmuls f2,f23,f2
	ctx.f2.f64 = double(float(ctx.f23.f64 * ctx.f2.f64));
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// stfs f30,572(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// fmr f19,f25
	ctx.f19.f64 = ctx.f25.f64;
	// lfs f26,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f26.f64 = double(temp.f32);
	// lfs f10,1548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,1388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1388);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f11,f9,f4,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f4.f64 + ctx.f11.f64));
	// lfs f30,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f30.f64 = double(temp.f32);
	// stfs f18,672(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// stfs f22,5400(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5400, temp.u32);
	// stfs f23,788(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// fnmsubs f12,f27,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// fmadds f7,f20,f7,f2
	ctx.f7.f64 = double(float(ctx.f20.f64 * ctx.f7.f64 + ctx.f2.f64));
	// lfs f2,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f20.f64 = double(temp.f32);
	// stfs f17,764(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f21,656(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// stfs f14,664(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// fnmsubs f11,f5,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// fmadds f11,f1,f0,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fmr f0,f2
	ctx.f0.f64 = ctx.f2.f64;
	// fmuls f9,f24,f0
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// stfs f11,5404(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5404, temp.u32);
	// fmuls f11,f26,f0
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fmr f0,f20
	ctx.f0.f64 = ctx.f20.f64;
	// lfs f27,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f27.f64 = double(temp.f32);
	// lfs f1,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f1.f64 = double(temp.f32);
	// lfs f23,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f25,f25,f23
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// lfs f21,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f18.f64 = double(temp.f32);
	// lfs f22,1380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1380);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f5,f31,f0
	ctx.f5.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f31,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f10,f10,f0,f16
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f16.f64));
	// lfs f0,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f15,f0
	ctx.f4.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// fmuls f3,f28,f0
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmuls f1,f1,f28
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f28,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f25,f6
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// fmadds f10,f10,f2,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 + ctx.f5.f64));
	// lfs f5,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f13,f13,f20,f4
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f20.f64 - ctx.f4.f64));
	// lfs f2,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f9,f9,f19,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f19.f64 - ctx.f3.f64));
	// lfs f3,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f0,f2,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// lfs f2,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f2.f64 = double(temp.f32);
	// lfs f19,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f4,f30,f6
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f6.f64));
	// lfs f20,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f2,f22
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f22.f64));
	// fmadds f13,f7,f6,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f13.f64));
	// lfs f7,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f5,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// lfs f5,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// fnmsubs f9,f27,f31,f9
	ctx.f9.f64 = double(float(-(ctx.f27.f64 * ctx.f31.f64 - ctx.f9.f64)));
	// lfs f31,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f26,f31,f6
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// lfs f27,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f13,f19,f21,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f21.f64 - ctx.f13.f64)));
	// lfs f21,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f7,f7,f2
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f2.f64));
	// lfs f19,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f5,f5,f28
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f28.f64));
	// lfs f28,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f23
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f23,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f11,f11,f20,f9
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f20.f64 - ctx.f9.f64)));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// lfs f20,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f23,f21,f23
	ctx.f23.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f12,f20,f9,f12
	ctx.f12.f64 = double(float(-(ctx.f20.f64 * ctx.f9.f64 - ctx.f12.f64)));
	// lfs f9,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f1,f1,f21
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// lfs f20,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f19,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f29,f18,f19,f29
	ctx.f29.f64 = double(float(ctx.f18.f64 * ctx.f19.f64 + ctx.f29.f64));
	// lfs f19,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f7,f9,f5
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64 + ctx.f5.f64));
	// lfs f7,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f5,f27,f20
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// lfs f20,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f7,f7,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f20.f64));
	// lfs f18,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f20.f64));
	// lfs f20,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f19,f20
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lfs f19,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f26,f26,f18
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f18.f64));
	// lfs f18,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f11,f10,f2,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 + ctx.f11.f64));
	// lfs f10,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f0,f0,f10,f13
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f13,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f19,f19,f18
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// lfs f18,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f13,f18,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f13.f64));
	// lfs f17,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f10,f28,f2
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f18,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f18.f64 = double(temp.f32);
	// lfs f28,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f28.f64 = double(temp.f32);
	// fmsubs f9,f22,f17,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f17.f64 - ctx.f9.f64));
	// fmadds f28,f28,f18,f23
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f18.f64 + ctx.f23.f64));
	// lfs f23,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f1,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// fmadds f5,f21,f22,f5
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f22.f64 + ctx.f5.f64));
	// stfs f30,440(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// lfs f1,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f1.f64 = double(temp.f32);
	// stfs f31,436(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// fmuls f3,f3,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// fmadds f30,f29,f6,f0
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f6.f64 + ctx.f0.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f31.f64 = double(temp.f32);
	// fmr f1,f0
	ctx.f1.f64 = ctx.f0.f64;
	// lfs f29,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f29,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// stfs f27,464(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fnmsubs f10,f10,f0,f9
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f31,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f27,f13,f6
	ctx.f27.f64 = double(float(ctx.f13.f64 * ctx.f6.f64));
	// lfs f13,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f28,f28,f2
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f22,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f12,f5,f6,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f12.f64));
	// lfs f5,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f13,f5
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// lfs f13,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f13.f64 = double(temp.f32);
	// lfs f21,1668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1668);
	ctx.f21.f64 = double(temp.f32);
	// lfs f29,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f4,f4,f1,f30
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f30.f64)));
	// lfs f23,1628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1628);
	ctx.f23.f64 = double(temp.f32);
	// fmr f9,f29
	ctx.f9.f64 = ctx.f29.f64;
	// lfs f30,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f11,f13,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f13.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f31,f2,f23
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// lfs f23,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f23.f64 = double(temp.f32);
	// fmr f18,f29
	ctx.f18.f64 = ctx.f29.f64;
	// lfs f16,1360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1360);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f10,f28,f0,f10
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f28,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f22
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f22.f64));
	// lfs f14,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f22,f2,f21
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// fmr f21,f0
	ctx.f21.f64 = ctx.f0.f64;
	// fmuls f23,f23,f13
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// fmadds f4,f26,f0,f4
	ctx.f4.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f4.f64));
	// lfs f26,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f26.f64 = double(temp.f32);
	// fmr f0,f17
	ctx.f0.f64 = ctx.f17.f64;
	// lfs f17,1704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1704);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f11,f8,f29,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f8,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f8.f64 = double(temp.f32);
	// fmr f29,f13
	ctx.f29.f64 = ctx.f13.f64;
	// fmuls f5,f5,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// lfs f30,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f17,f2,f17
	ctx.f17.f64 = double(float(ctx.f2.f64 * ctx.f17.f64));
	// fmuls f16,f2,f16
	ctx.f16.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// fnmsubs f4,f25,f1,f4
	ctx.f4.f64 = double(float(-(ctx.f25.f64 * ctx.f1.f64 - ctx.f4.f64)));
	// lfs f1,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f10,f31,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f31,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f11,f7,f9,f11
	ctx.f11.f64 = double(float(-(ctx.f7.f64 * ctx.f9.f64 - ctx.f11.f64)));
	// lfs f9,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f29,f29,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// lfs f26,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f26.f64 = double(temp.f32);
	// lfs f7,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// fmadds f9,f1,f9,f23
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 + ctx.f23.f64));
	// lfs f31,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f31.f64 = double(temp.f32);
	// lfs f1,1540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f15,f26,f30
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// fmuls f7,f28,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// fnmsubs f11,f20,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f8,572(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// fmr f28,f0
	ctx.f28.f64 = ctx.f0.f64;
	// lfs f20,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f4,f27,f21,f4
	ctx.f4.f64 = double(float(-(ctx.f27.f64 * ctx.f21.f64 - ctx.f4.f64)));
	// stfs f4,5416(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5416, temp.u32);
	// lfs f4,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f30,f30,f1
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f23,1732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1732);
	ctx.f23.f64 = double(temp.f32);
	// stfs f12,5408(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5408, temp.u32);
	// fmuls f12,f2,f23
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// lfs f25,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f23.f64 = double(temp.f32);
	// lfs f27,1516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1516);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f11,f19,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f10,f22,f28,f10
	ctx.f10.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f10.f64)));
	// lfs f22,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f8,f0,f31,f8
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f31.f64 + ctx.f8.f64));
	// lfs f31,1352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1352);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f0,f0,f14,f5
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f14.f64 + ctx.f5.f64));
	// lfs f5,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f9,f9,f22,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f22.f64 - ctx.f7.f64));
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// lfs f28,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f20
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f20.f64));
	// fmuls f28,f25,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f22,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f22.f64 = double(temp.f32);
	// lfs f19,1476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1476);
	ctx.f19.f64 = double(temp.f32);
	// lfs f14,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f11,f3,f18,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f18.f64 + ctx.f11.f64));
	// stfs f11,5412(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5412, temp.u32);
	// lfs f11,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f10,f17,f7,f10
	ctx.f10.f64 = double(float(ctx.f17.f64 * ctx.f7.f64 + ctx.f10.f64));
	// fmadds f3,f4,f11,f15
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f11.f64 + ctx.f15.f64));
	// lfs f18,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f18.f64 = double(temp.f32);
	// fmr f7,f20
	ctx.f7.f64 = ctx.f20.f64;
	// lfs f20,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f0,f29,f5,f0
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// lfs f29,1296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1296);
	ctx.f29.f64 = double(temp.f32);
	// fadds f31,f29,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 + ctx.f31.f64));
	// lfs f5,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,2092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2092);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f25,f27,f5
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// fmuls f29,f29,f23
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// lfs f23,1288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1288);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f22,f23
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f17,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f9,f8,f18,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f18.f64 + ctx.f9.f64));
	// lfs f18,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f18.f64 = double(temp.f32);
	// lfs f8,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f10,f16,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f16.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// lfs f8,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// lfs f15,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f7,f3,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// lfs f3,1280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1280);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f13,f13,f20,f0
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// fadds f31,f31,f19
	ctx.f31.f64 = double(float(ctx.f31.f64 + ctx.f19.f64));
	// lfs f19,1272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1272);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f19,f17
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// lfs f17,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f29,f29,f18
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f18.f64));
	// lfs f16,1336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1336);
	ctx.f16.f64 = double(temp.f32);
	// fmsubs f8,f21,f8,f30
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f8.f64 - ctx.f30.f64));
	// lfs f30,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f20,f3,f5
	ctx.f20.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// fmuls f21,f30,f3
	ctx.f21.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// fmuls f16,f2,f16
	ctx.f16.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// fmsubs f7,f25,f18,f7
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f18.f64 - ctx.f7.f64));
	// lfs f18,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f18.f64 = double(temp.f32);
	// lfs f25,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f18,f18,f17
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// lfs f17,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f13,f25,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// lfs f25,1344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1344);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f0,f14,f0
	ctx.f0.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f15,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f31,f31,f2
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// fnmsubs f9,f28,f15,f9
	ctx.f9.f64 = double(float(-(ctx.f28.f64 * ctx.f15.f64 - ctx.f9.f64)));
	// lfs f28,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f28.f64 = double(temp.f32);
	// lfs f15,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f28,f14,f28
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f28.f64));
	// lfs f14,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f12,f12,f15,f10
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f15.f64 - ctx.f10.f64)));
	// fmuls f15,f26,f14
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// stfs f26,360(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// lfs f26,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f27,f30
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f10,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f29,f19,f26,f29
	ctx.f29.f64 = double(float(ctx.f19.f64 * ctx.f26.f64 - ctx.f29.f64));
	// fnmsubs f10,f20,f10,f7
	ctx.f10.f64 = double(float(-(ctx.f20.f64 * ctx.f10.f64 - ctx.f7.f64)));
	// lfs f20,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f20.f64 = double(temp.f32);
	// lfs f26,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f11,f22,f11
	ctx.f11.f64 = double(float(ctx.f22.f64 * ctx.f11.f64));
	// lfs f7,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f13,f26,f20,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f20.f64 + ctx.f13.f64));
	// lfs f19,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f7,f7,f25
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f25.f64));
	// stfs f25,564(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fnmsubs f8,f21,f19,f8
	ctx.f8.f64 = double(float(-(ctx.f21.f64 * ctx.f19.f64 - ctx.f8.f64)));
	// lfs f25,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f25.f64 = double(temp.f32);
	// fmr f21,f20
	ctx.f21.f64 = ctx.f20.f64;
	// fnmsubs f9,f18,f25,f9
	ctx.f9.f64 = double(float(-(ctx.f18.f64 * ctx.f25.f64 - ctx.f9.f64)));
	// stfs f3,488(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmr f20,f19
	ctx.f20.f64 = ctx.f19.f64;
	// lfs f3,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f3.f64 = double(temp.f32);
	// lfs f26,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f1,f14,f1,f27
	ctx.f1.f64 = double(float(ctx.f14.f64 * ctx.f1.f64 + ctx.f27.f64));
	// lfs f19,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f13,f3,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f3.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// lfs f3,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f3.f64 = double(temp.f32);
	// fmr f26,f25
	ctx.f26.f64 = ctx.f25.f64;
	// fnmsubs f12,f16,f3,f12
	ctx.f12.f64 = double(float(-(ctx.f16.f64 * ctx.f3.f64 - ctx.f12.f64)));
	// lfs f3,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f10,f15,f27,f10
	ctx.f10.f64 = double(float(ctx.f15.f64 * ctx.f27.f64 + ctx.f10.f64));
	// lfs f15,1584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1584);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f3,f3,f21,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f21.f64 + ctx.f28.f64));
	// lfs f28,1328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1328);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f0,f0,f20,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f20.f64 + ctx.f9.f64));
	// lfs f9,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f20,f19,f18
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// lfs f21,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f21.f64 = double(temp.f32);
	// fmr f19,f27
	ctx.f19.f64 = ctx.f27.f64;
	// lfs f27,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f9,f7,f9,f29
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f9.f64 - ctx.f29.f64)));
	// lfs f18,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f25,f23,f4
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// lfs f7,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f13,f17,f26,f13
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f26.f64 + ctx.f13.f64));
	// lfs f26,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f12,f31,f27,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f27.f64 + ctx.f12.f64));
	// lfs f31,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f29,f26,f28
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// lfs f26,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f26.f64 = double(temp.f32);
	// lfs f17,1608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1608);
	ctx.f17.f64 = double(temp.f32);
	// fadds f3,f3,f7
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f7.f64));
	// fnmsubs f11,f11,f19,f8
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f19.f64 - ctx.f8.f64)));
	// lfs f8,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// fmr f27,f8
	ctx.f27.f64 = ctx.f8.f64;
	// lfs f19,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f8,f26,f8,f0
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f13,f21
	ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f21.f64));
	// fmadds f12,f20,f18,f12
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f18.f64 + ctx.f12.f64));
	// lfs f20,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f9,f29,f19,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f19.f64 + ctx.f9.f64));
	// lfs f29,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f29.f64 = double(temp.f32);
	// lfs f18,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f16,f6,f29
	ctx.f16.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// fmadds f11,f1,f0,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f1,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f10,f25,f27,f10
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f27.f64 + ctx.f10.f64));
	// lfs f27,1600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1600);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,1592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1592);
	ctx.f25.f64 = double(temp.f32);
	// fadds f19,f27,f17
	ctx.f19.f64 = double(float(ctx.f27.f64 + ctx.f17.f64));
	// lfs f0,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// stfs f23,412(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmadds f8,f3,f1,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f1.f64 + ctx.f8.f64));
	// fmuls f23,f25,f0
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// stfs f22,336(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f1,f20,f18
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// stfs f13,5424(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5424, temp.u32);
	// fmuls f22,f2,f15
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f3,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f3.f64 = double(temp.f32);
	// lfs f13,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f13.f64 = double(temp.f32);
	// stfs f10,5428(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5428, temp.u32);
	// fmuls f13,f3,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f11,5436(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5436, temp.u32);
	// fmuls f3,f16,f0
	ctx.f3.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f10,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f10.f64 = double(temp.f32);
	// stfs f7,628(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// lfs f11,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f11.f64 = double(temp.f32);
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f9,5432(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5432, temp.u32);
	// fmsubs f9,f19,f0,f23
	ctx.f9.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 - ctx.f23.f64));
	// stfs f12,5420(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5420, temp.u32);
	// fmuls f12,f1,f2
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// fmuls f10,f22,f7
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// stfs f31,532(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// lfs f31,1312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1312);
	ctx.f31.f64 = double(temp.f32);
	// lfs f1,1320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1320);
	ctx.f1.f64 = double(temp.f32);
	// fadds f1,f31,f1
	ctx.f1.f64 = double(float(ctx.f31.f64 + ctx.f1.f64));
	// stfs f29,888(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// lfs f29,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f31,f29,f31,f13
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f7,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f9,f9,f6,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 - ctx.f3.f64));
	// fmuls f7,f2,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// stfs f28,556(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// fmsubs f0,f12,f0,f10
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f10.f64));
	// lfs f13,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f13.f64 = double(temp.f32);
	// fmr f3,f29
	ctx.f3.f64 = ctx.f29.f64;
	// lfs f28,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f13
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// lfs f13,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f13.f64 = double(temp.f32);
	// stfs f21,528(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// lfs f10,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f10.f64 = double(temp.f32);
	// lfs f21,1304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1304);
	ctx.f21.f64 = double(temp.f32);
	// lfs f12,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f31,f31,f6
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// stfs f27,808(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// fmadds f11,f1,f29,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f29.f64 + ctx.f11.f64));
	// fmuls f27,f13,f5
	ctx.f27.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// stfs f26,408(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// lfs f26,1716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1716);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f2,f21
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// fnmsubs f0,f7,f3,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f3.f64 - ctx.f0.f64)));
	// lfs f3,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f3.f64 = double(temp.f32);
	// stfs f25,736(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// fmuls f23,f13,f26
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// lfs f10,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f24,f3
	ctx.f3.f64 = double(float(ctx.f24.f64 * ctx.f3.f64));
	// lfs f25,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f1,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f20,f25,f6
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// lfs f7,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f21,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// stfs f17,748(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// fmuls f21,f21,f26
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f22,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f28,f28,f2
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f19,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f17.f64 = double(temp.f32);
	// lfs f24,1576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1576);
	ctx.f24.f64 = double(temp.f32);
	// lfs f16,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f9,f31,f15,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f15.f64 + ctx.f9.f64));
	// lfs f15,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f11,f11,f2,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + ctx.f0.f64));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fadds f24,f24,f17
	ctx.f24.f64 = double(float(ctx.f24.f64 + ctx.f17.f64));
	// lfs f17,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f19,f18
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// lfs f18,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f10,f16,f10
	ctx.f10.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// lfs f16,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f18,f17
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// stfs f22,928(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// fmuls f27,f27,f16
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// stfs f6,308(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fnmsubs f12,f12,f15,f8
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f15.f64 - ctx.f8.f64)));
	// lfs f31,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f14,f22,f6
	ctx.f14.f64 = double(float(ctx.f22.f64 * ctx.f6.f64));
	// lfs f6,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f31,f6,f31
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f25,1176(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// fnmsubs f9,f23,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f23,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f11,f29,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f23,364(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmr f29,f0
	ctx.f29.f64 = ctx.f0.f64;
	// stfs f18,388(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f22,f23,f4
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// lfs f23,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f23.f64 = double(temp.f32);
	// fmr f25,f0
	ctx.f25.f64 = ctx.f0.f64;
	// stfs f5,360(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// lfs f18,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f5,f24,f2
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// fmadds f27,f17,f23,f27
	ctx.f27.f64 = double(float(ctx.f17.f64 * ctx.f23.f64 + ctx.f27.f64));
	// lfs f23,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f23.f64 = double(temp.f32);
	// lfs f16,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f16.f64 = double(temp.f32);
	// fmr f15,f0
	ctx.f15.f64 = ctx.f0.f64;
	// lfs f24,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f8,f18,f16
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f17,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f6,f6,f24
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f24.f64));
	// fnmsubs f9,f20,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f20.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f20,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f20.f64 = double(temp.f32);
	// fmr f18,f0
	ctx.f18.f64 = ctx.f0.f64;
	// lfs f24,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f11,f1,f29,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f1,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f23,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f1,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f1,f20
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f20.f64));
	// lfs f20,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f26,f17,f26
	ctx.f26.f64 = double(float(ctx.f17.f64 * ctx.f26.f64));
	// lfs f17,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f31,f31,f20,f27
	ctx.f31.f64 = double(float(-(ctx.f31.f64 * ctx.f20.f64 - ctx.f27.f64)));
	// lfs f27,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f17,f27
	ctx.f27.f64 = double(float(ctx.f17.f64 * ctx.f27.f64));
	// lfs f17,1652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1652);
	ctx.f17.f64 = double(temp.f32);
	// stfs f4,324(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f4,f17,f4
	ctx.f4.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// lfs f20,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f9,f13,f25,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f25.f64 + ctx.f9.f64));
	// lfs f13,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f13.f64 = double(temp.f32);
	// lfs f25,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f28,f24,f11
	ctx.f11.f64 = double(float(-(ctx.f28.f64 * ctx.f24.f64 - ctx.f11.f64)));
	// lfs f28,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f28.f64 = double(temp.f32);
	// stfs f4,220(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fnmsubs f12,f8,f20,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f20.f64 - ctx.f12.f64)));
	// lfs f16,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f8,f13,f28
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// lfs f29,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f29.f64 = double(temp.f32);
	// fmr f24,f20
	ctx.f24.f64 = ctx.f20.f64;
	// lfs f4,1264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1264);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f6,f6,f25,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f25.f64 + ctx.f31.f64));
	// stfs f30,252(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmr f30,f16
	ctx.f30.f64 = ctx.f16.f64;
	// lfs f20,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f29,f2,f29
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f29.f64));
	// lfs f13,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f1,f2
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// lfs f31,1684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1684);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f2,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// fnmsubs f9,f7,f18,f9
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f18.f64 - ctx.f9.f64)));
	// fmadds f11,f3,f16,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f16.f64 + ctx.f11.f64));
	// lfs f7,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f12,f27,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// stfs f12,5440(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5440, temp.u32);
	// fmadds f12,f21,f15,f9
	ctx.f12.f64 = double(float(ctx.f21.f64 * ctx.f15.f64 + ctx.f9.f64));
	// lfs f18,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f6,f22,f25,f6
	ctx.f6.f64 = double(float(-(ctx.f22.f64 * ctx.f25.f64 - ctx.f6.f64)));
	// lfs f3,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f3.f64 = double(temp.f32);
	// lfs f24,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f18,f18,f13
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f13.f64));
	// lfs f27,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f20,f31
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f31.f64));
	// fadds f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 + ctx.f24.f64));
	// lfs f16,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f21.f64 = double(temp.f32);
	// stfs f20,640(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// fnmsubs f11,f19,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f30,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f30.f64 = double(temp.f32);
	// lfs f20,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f12,f10,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f10,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f10.f64 = double(temp.f32);
	// lfs f19,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f19.f64 = double(temp.f32);
	// stfs f23,600(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfs f17,392(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// lfs f23,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f23.f64 = double(temp.f32);
	// lfs f17,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f11,f5,f0,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f5,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f12,f14,f0,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f0.f64 + ctx.f12.f64));
	// fnmsubs f11,f29,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f29,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f29,f2
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f2.f64));
	// fnmsubs f12,f26,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f26.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f0,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f26,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f9,f0,f16
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f16.f64));
	// lfs f0,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f5,f0,f5,f18
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f5.f64 + ctx.f18.f64));
	// lfs f0,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f11,f1,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f1,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f1,f30
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// fmr f30,f25
	ctx.f30.f64 = ctx.f25.f64;
	// lfs f25,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f12,f8,f3,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f3.f64 + ctx.f12.f64));
	// lfs f8,1568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1568);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f7,f10,f28
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// lfs f3,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfs f28,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f24,f28,f3
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// fmuls f9,f9,f2
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// fmuls f5,f5,f21
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f21.f64));
	// lfs f21,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f6,f26,f30,f6
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f30.f64 + ctx.f6.f64));
	// lfs f26,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f13,f27,f26,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f26.f64 + ctx.f13.f64));
	// lfs f27,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f26,f27,f25
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f25,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f30,f22,f30
	ctx.f30.f64 = double(float(ctx.f22.f64 * ctx.f30.f64));
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f0,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f25,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f27,f23
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f23.f64));
	// fmuls f9,f9,f18
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f18.f64));
	// fnmsubs f6,f24,f20,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f20.f64 - ctx.f6.f64)));
	// lfs f24,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f13,f21,f19,f13
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f19.f64 + ctx.f13.f64));
	// lfs f20,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f10,f26,f17,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f17.f64 - ctx.f10.f64));
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmsubs f7,f23,f20,f7
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f20.f64 - ctx.f7.f64));
	// lfs f0,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f26,f27,f24
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// lfs f24,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f27,f21
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f21.f64));
	// lfs f21,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f21.f64 = double(temp.f32);
	// fmr f20,f18
	ctx.f20.f64 = ctx.f18.f64;
	// lfs f18,1564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1564);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f24,f24,f21
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f17,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f17.f64 = double(temp.f32);
	// fmr f21,f19
	ctx.f21.f64 = ctx.f19.f64;
	// fmsubs f5,f1,f19,f5
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f19.f64 - ctx.f5.f64));
	// lfs f1,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f19,f0,f2
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// fmuls f13,f13,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// fadds f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 + ctx.f0.f64));
	// lfs f1,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f10,f26,f1,f10
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f1.f64 - ctx.f10.f64)));
	// fnmsubs f7,f23,f1,f7
	ctx.f7.f64 = double(float(-(ctx.f23.f64 * ctx.f1.f64 - ctx.f7.f64)));
	// lfs f1,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f1.f64 = double(temp.f32);
	// fmsubs f9,f29,f20,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f20.f64 - ctx.f9.f64));
	// lfs f20,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f20.f64 = double(temp.f32);
	// lfs f29,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f26,f27,f17
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f17.f64));
	// fnmsubs f6,f30,f21,f6
	ctx.f6.f64 = double(float(-(ctx.f30.f64 * ctx.f21.f64 - ctx.f6.f64)));
	// lfs f30,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f5,f25,f20,f5
	ctx.f5.f64 = double(float(-(ctx.f25.f64 * ctx.f20.f64 - ctx.f5.f64)));
	// lfs f20,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f18,f3
	ctx.f21.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// lfs f18,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f30,f29,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// lfs f25,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f29,f27,f18
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f18.f64));
	// lfs f18,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f0,f0,f2
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// lfs f23,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f1,f19,f1
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// lfs f17,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f17.f64 = double(temp.f32);
	// fadds f19,f23,f25
	ctx.f19.f64 = double(float(ctx.f23.f64 + ctx.f25.f64));
	// fmadds f13,f13,f20,f9
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f20.f64 + ctx.f9.f64));
	// lfs f20,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f9,f24,f18,f5
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f18.f64 - ctx.f5.f64)));
	// lfs f18,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f18.f64 = double(temp.f32);
	// lfs f5,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f6,f21,f5,f6
	ctx.f6.f64 = double(float(ctx.f21.f64 * ctx.f5.f64 + ctx.f6.f64));
	// lfs f5,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f7,f29,f18,f7
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f18.f64 - ctx.f7.f64)));
	// lfs f18,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f18.f64 = double(temp.f32);
	// fmr f29,f16
	ctx.f29.f64 = ctx.f16.f64;
	// lfs f24,1412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1412);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f10,f26,f5,f10
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f5.f64 - ctx.f10.f64)));
	// lfs f5,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f26,f27,f20
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// lfs f20,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f5,f5,f20,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f20.f64 - ctx.f30.f64));
	// lfs f20,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f0,f18
	ctx.f18.f64 = double(float(ctx.f0.f64 * ctx.f18.f64));
	// lfs f0,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f19,f19,f17
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// lfs f17,1432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1432);
	ctx.f17.f64 = double(temp.f32);
	// lfs f21,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f24,f24,f21
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f30,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// fmuls f29,f29,f20
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f20.f64));
	// lfs f20,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f20,f0
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f1,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f9,f17,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f1,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f1.f64 = double(temp.f32);
	// fmr f0,f15
	ctx.f0.f64 = ctx.f15.f64;
	// fnmsubs f12,f31,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f1,1484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1484);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f11,f4,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f4,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f26,f4,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f4.f64 + ctx.f10.f64));
	// lfs f4,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f6,f24,f0,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// fmr f0,f4
	ctx.f0.f64 = ctx.f4.f64;
	// lfs f26,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f7,f30,f4,f7
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 + ctx.f7.f64));
	// lfs f30,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f30.f64 = double(temp.f32);
	// lfs f4,1544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1544);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fadds f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 + ctx.f4.f64));
	// lfs f30,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f24.f64 = double(temp.f32);
	// stfs f12,5448(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5448, temp.u32);
	// stfs f7,5472(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5472, temp.u32);
	// stfs f11,5444(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5444, temp.u32);
	// lfs f11,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f11.f64 = double(temp.f32);
	// lfs f7,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f31,f19,f0
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f19,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f13,f18,f0,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f0,1368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1368);
	ctx.f0.f64 = double(temp.f32);
	// fadds f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 + ctx.f0.f64));
	// lfs f0,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f26,f26,f0
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f24,f0,f24
	ctx.f24.f64 = double(float(ctx.f0.f64 * ctx.f24.f64));
	// fmadds f9,f29,f18,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f18.f64 + ctx.f9.f64));
	// lfs f0,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f20,f19,f0,f20
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 - ctx.f20.f64));
	// fmr f0,f29
	ctx.f0.f64 = ctx.f29.f64;
	// lfs f19,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f12,f19,f29
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f29.f64));
	// lfs f29,1596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1596);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f11,f7
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// stfs f5,5468(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5468, temp.u32);
	// stfs f6,5452(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5452, temp.u32);
	// lfs f5,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f6.f64 = double(temp.f32);
	// lfs f19,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f6,f5,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// lfs f5,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// lfs f18,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f9,f29,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f29,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f31,f20,f29,f31
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f29.f64 + ctx.f31.f64));
	// lfs f29,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f20,f19,f3
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f3.f64));
	// lfs f19,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f19.f64 = double(temp.f32);
	// fadds f29,f29,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 + ctx.f28.f64));
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f19,f28
	ctx.f28.f64 = double(float(ctx.f19.f64 * ctx.f28.f64));
	// lfs f19,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f7,f7,f19
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f19.f64));
	// lfs f19,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f12,f18,f0,f12
	ctx.f12.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 - ctx.f12.f64));
	// lfs f18,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f6,f6,f18
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f18.f64));
	// lfs f18,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f18,f16
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f15,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f9,f4,f0,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f19,f0
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f14,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f16,f15,f16
	ctx.f16.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f15,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f15.f64 = double(temp.f32);
	// stfs f10,5464(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5464, temp.u32);
	// fmsubs f28,f14,f15,f28
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f15.f64 - ctx.f28.f64));
	// fmsubs f7,f20,f0,f7
	ctx.f7.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 - ctx.f7.f64));
	// lfs f10,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f10.f64 = double(temp.f32);
	// lfs f19,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f10,f10,f3
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f3.f64));
	// lfs f20,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f19,f2
	ctx.f17.f64 = double(float(ctx.f19.f64 * ctx.f2.f64));
	// lfs f15,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f29,f29,f21
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f21.f64));
	// fnmsubs f12,f20,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f20.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f14,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f9,f15,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f15.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f20,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f6,f31,f2,f6
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f2.f64 - ctx.f6.f64));
	// fmadds f20,f20,f14,f18
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f14.f64 + ctx.f18.f64));
	// lfs f18,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f18.f64 = double(temp.f32);
	// stfs f25,148(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f18,f17,f18
	ctx.f18.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// lfs f25,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f7,f29,f0,f7
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 + ctx.f7.f64));
	// stfs f1,136(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fadds f14,f19,f25
	ctx.f14.f64 = double(float(ctx.f19.f64 + ctx.f25.f64));
	// lfs f1,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f25,f2,f25
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// lfs f17,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f4,f1,f17,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f17.f64 + ctx.f4.f64));
	// lfs f1,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f0,f1,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f31,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f31.f64 = double(temp.f32);
	// lfs f15,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f31,f15,f31
	ctx.f31.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// lfs f12,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f17,f12,f28
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f12.f64 + ctx.f28.f64));
	// lfs f28,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f9,f30,f28,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f28.f64 + ctx.f9.f64));
	// fnmsubs f6,f5,f1,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f1.f64 - ctx.f6.f64)));
	// lfs f1,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f30,f14,f21
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f21.f64));
	// stfs f19,252(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fadds f5,f1,f5
	ctx.f5.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// lfs f19,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f19.f64 = double(temp.f32);
	// lfs f1,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f1,f1,f28,f0
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// lfs f29,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f7,f31,f19,f7
	ctx.f7.f64 = double(float(-(ctx.f31.f64 * ctx.f19.f64 - ctx.f7.f64)));
	// lfs f0,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f0.f64 = double(temp.f32);
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f29,f16,f29
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// lfs f31,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f9,f26,f19,f9
	ctx.f9.f64 = double(float(-(ctx.f26.f64 * ctx.f19.f64 - ctx.f9.f64)));
	// fmuls f31,f0,f31
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// lfs f26,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f0,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f0.f64 = double(temp.f32);
	// fadds f26,f0,f26
	ctx.f26.f64 = double(float(ctx.f0.f64 + ctx.f26.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f17,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f19,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f19,f17
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// lfs f17,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f0,f10,f0,f7
	ctx.f0.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// lfs f16,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f16.f64 = double(temp.f32);
	// lfs f10,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f17,f16
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// fmadds f10,f4,f10,f29
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f10.f64 + ctx.f29.f64));
	// lfs f17,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f17.f64 = double(temp.f32);
	// lfs f4,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f17,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// fnmsubs f6,f18,f29,f6
	ctx.f6.f64 = double(float(-(ctx.f18.f64 * ctx.f29.f64 - ctx.f6.f64)));
	// lfs f29,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f29.f64 = double(temp.f32);
	// lfs f4,1396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1396);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f4,f4,f29
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f1,f20,f29,f1
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f29.f64 + ctx.f1.f64));
	// lfs f17,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f17.f64 = double(temp.f32);
	// lfs f20,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f20.f64 = double(temp.f32);
	// lfs f29,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f13,f24,f20,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f20.f64 - ctx.f13.f64)));
	// lfs f18,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f29,f17,f29
	ctx.f29.f64 = double(float(ctx.f17.f64 * ctx.f29.f64));
	// lfs f17,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f18,f22
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f22.f64));
	// lfs f24,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f7,f7,f2
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f2.f64));
	// lfs f20,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f9,f17,f24,f9
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f24.f64 - ctx.f9.f64)));
	// lfs f24,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f25,f24
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// lfs f24,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f26,f26,f20,f19
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f20.f64 + ctx.f19.f64));
	// lfs f19,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f10,f10,f2,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f2.f64 + ctx.f6.f64));
	// lfs f6,1468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1468);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f12,f4,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f24,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f6,f6,f19,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f19.f64 + ctx.f31.f64));
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f30,f29,f24,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f24.f64 - ctx.f30.f64));
	// lfs f4,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f24,f18,f19,f0
	ctx.f24.f64 = double(float(-(ctx.f18.f64 * ctx.f19.f64 - ctx.f0.f64)));
	// stfs f9,5456(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5456, temp.u32);
	// fmr f0,f19
	ctx.f0.f64 = ctx.f19.f64;
	// lfs f29,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f5,f5,f4,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 + ctx.f1.f64));
	// lfs f9,1588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1588);
	ctx.f9.f64 = double(temp.f32);
	// stfs f13,5460(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5460, temp.u32);
	// fmuls f13,f15,f29
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f29.f64));
	// lfs f29,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f31.f64 = double(temp.f32);
	// lfs f4,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f4.f64 = double(temp.f32);
	// stfs f26,152(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f4,f31,f4
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// lfs f26,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f26.f64 = double(temp.f32);
	// lfs f31,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f31.f64 = double(temp.f32);
	// stfs f21,180(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f31,f31,f21
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f21.f64));
	// fmadds f12,f6,f0,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f12.f64));
	// stfs f25,204(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fnmsubs f5,f28,f0,f5
	ctx.f5.f64 = double(float(-(ctx.f28.f64 * ctx.f0.f64 - ctx.f5.f64)));
	// lfs f25,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f0,f29
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// lfs f29,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f28,f0,f27
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f21,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f11,f11,f29
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f29.f64));
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f26,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// lfs f26,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f26.f64 = double(temp.f32);
	// lfs f18,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f18.f64 = double(temp.f32);
	// lfs f19,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f18,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f17,f17,f0
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// lfs f14,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f6,f25,f26,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f26.f64 + ctx.f6.f64));
	// lfs f25,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f28,f28,f21
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f21.f64));
	// lfs f21,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f11,f11,f18
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// stfs f21,220(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f25,f25,f21
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f21.f64));
	// lfs f26,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f26.f64 = double(temp.f32);
	// lfs f21,1712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1712);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f13,f13,f14,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f14.f64 + ctx.f30.f64));
	// lfs f15,1460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1460);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f30,f21,f26
	ctx.f30.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f20,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f15,f26,f15
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f15.f64));
	// lfs f1,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f20,f20,f3
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f3.f64));
	// lfs f18,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f16,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f18,f22,f18
	ctx.f18.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// fmsubs f6,f6,f0,f29
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 - ctx.f29.f64));
	// lfs f26,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f16,f16,f27
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f27.f64));
	// lfs f21,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f11,f19,f0,f11
	ctx.f11.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 - ctx.f11.f64));
	// fmadds f23,f3,f23,f18
	ctx.f23.f64 = double(float(ctx.f3.f64 * ctx.f23.f64 + ctx.f18.f64));
	// lfs f19,1372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f10,f7,f26,f10
	ctx.f10.f64 = double(float(-(ctx.f7.f64 * ctx.f26.f64 - ctx.f10.f64)));
	// lfs f26,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f1,f1,f0,f24
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f24.f64));
	// lfs f24,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f24.f64 = double(temp.f32);
	// lfs f7,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f24,f20,f24
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f24.f64));
	// fmuls f7,f7,f26
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f8,f19,f8
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// lfs f26,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f26.f64 = double(temp.f32);
	// lfs f18,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f26,f20,f26,f17
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f26.f64 + ctx.f17.f64));
	// fmsubs f6,f6,f27,f28
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f27.f64 - ctx.f28.f64));
	// lfs f28,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f17,f15,f18
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f18.f64));
	// lfs f20,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f20.f64 = double(temp.f32);
	// fmr f15,f14
	ctx.f15.f64 = ctx.f14.f64;
	// stfs f10,116(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmadds f11,f23,f28,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f28.f64 + ctx.f11.f64));
	// lfs f23,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f29,f29,f21
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f21.f64));
	// lfs f21,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f5,f4,f20,f5
	ctx.f5.f64 = double(float(-(ctx.f4.f64 * ctx.f20.f64 - ctx.f5.f64)));
	// lfs f4,1580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1580);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f30,f30,f18
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// lfs f19,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f16,f21
	ctx.f21.f64 = double(float(ctx.f16.f64 * ctx.f21.f64));
	// lfs f16,1452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1452);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f8,f8,f23
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// lfs f23,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f10,f22,f4
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// lfs f4,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f25,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f25,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f23,f17,f23
	ctx.f23.f64 = double(float(ctx.f17.f64 * ctx.f23.f64));
	// lfs f17,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f16,f19
	ctx.f20.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f11,f7,f15,f11
	ctx.f11.f64 = double(float(-(ctx.f7.f64 * ctx.f15.f64 - ctx.f11.f64)));
	// lfs f15,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f15.f64 = double(temp.f32);
	// fmr f7,f17
	ctx.f7.f64 = ctx.f17.f64;
	// fmadds f0,f4,f0,f9
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f9.f64));
	// fmuls f9,f25,f19
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// lfs f25,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f1,f31,f28,f1
	ctx.f1.f64 = double(float(-(ctx.f31.f64 * ctx.f28.f64 - ctx.f1.f64)));
	// lfs f28,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f28.f64 = double(temp.f32);
	// fmr f31,f19
	ctx.f31.f64 = ctx.f19.f64;
	// lfs f19,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f6,f21,f17,f6
	ctx.f6.f64 = double(float(-(ctx.f21.f64 * ctx.f17.f64 - ctx.f6.f64)));
	// lfs f17,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f19,f28
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f28.f64));
	// lfs f21,1532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f28,f17,f28
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f28.f64));
	// lfs f17,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f12,f29,f15,f12
	ctx.f12.f64 = double(float(-(ctx.f29.f64 * ctx.f15.f64 - ctx.f12.f64)));
	// lfs f29,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f26,f26,f27
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// fmuls f7,f30,f7
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f7.f64));
	// lfs f30,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f0,f0,f30,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f30.f64 + ctx.f13.f64));
	// lfs f13,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f17,f13,f9
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f13.f64 + ctx.f9.f64));
	// lfs f30,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f30.f64 = double(temp.f32);
	// lfs f9,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f21,f2,f21
	ctx.f21.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f17,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f5,f25,f31,f5
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 + ctx.f5.f64));
	// lfs f25,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f9,f9,f17,f30
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f17.f64 - ctx.f30.f64)));
	// lfs f31,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f31.f64 = double(temp.f32);
	// fmr f30,f15
	ctx.f30.f64 = ctx.f15.f64;
	// fadds f31,f25,f31
	ctx.f31.f64 = double(float(ctx.f25.f64 + ctx.f31.f64));
	// lfs f25,1356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f25,f2,f25
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// fmuls f19,f19,f18
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// fnmsubs f1,f24,f30,f1
	ctx.f1.f64 = double(float(-(ctx.f24.f64 * ctx.f30.f64 - ctx.f1.f64)));
	// lfs f24,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f31,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// fmr f30,f14
	ctx.f30.f64 = ctx.f14.f64;
	// fmuls f28,f28,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// lfs f18,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f18,f13,f18,f0
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f18.f64 + ctx.f0.f64));
	// lfs f0,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f0.f64 = double(temp.f32);
	// fmr f13,f15
	ctx.f13.f64 = ctx.f15.f64;
	// lfs f17,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f6,f20,f0,f6
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f6.f64));
	// lfs f0,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f5,f29,f30,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f30.f64 + ctx.f5.f64));
	// stfs f12,5488(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5488, temp.u32);
	// fmuls f29,f24,f27
	ctx.f29.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// lfs f12,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f30,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f30.f64 = double(temp.f32);
	// stfs f1,5484(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5484, temp.u32);
	// fmuls f30,f30,f3
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// stfs f5,5480(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5480, temp.u32);
	// fmr f1,f15
	ctx.f1.f64 = ctx.f15.f64;
	// lfs f5,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f25,f25,f17,f23
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f17.f64 - ctx.f23.f64));
	// stfs f9,5476(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5476, temp.u32);
	// fmuls f9,f3,f4
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// lfs f24,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f24.f64 = double(temp.f32);
	// addi r6,r31,64
	ctx.r6.s64 = ctx.r31.s64 + 64;
	// fnmsubs f13,f10,f13,f11
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// lfs f4,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f10,f0,f27
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f0,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f27,f0,f27
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f27.f64));
	// lfs f0,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f0.f64 = double(temp.f32);
	// fmr f11,f17
	ctx.f11.f64 = ctx.f17.f64;
	// lfs f23,1348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	ctx.f23.f64 = double(temp.f32);
	// fadds f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// lfs f0,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f0,f5
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// lfs f0,3128(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3128);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,468(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 468, temp.u32);
	// fmadds f8,f26,f24,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f24.f64 + ctx.f8.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// addi r5,r30,64
	ctx.r5.s64 = ctx.r30.s64 + 64;
	// fmadds f6,f29,f0,f6
	ctx.f6.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 + ctx.f6.f64));
	// lfs f24,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f25,f19,f0,f25
	ctx.f25.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f25.f64)));
	// lfs f0,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f1,f30,f1,f13
	ctx.f1.f64 = double(float(-(ctx.f30.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// lfs f29,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f29.f64 = double(temp.f32);
	// addi r4,r1,5200
	ctx.r4.s64 = ctx.r1.s64 + 5200;
	// lfs f26,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f26.f64 = double(temp.f32);
	// li r3,9
	ctx.r3.s64 = 9;
	// fmsubs f11,f21,f11,f7
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f11.f64 - ctx.f7.f64));
	// lfs f7,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f7.f64 = double(temp.f32);
	// fadds f12,f12,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f4.f64));
	// lfs f4,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f7,f22,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// lfs f30,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f22,f13,f0
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f21,1444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1444);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f4,f13,f4
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f4.f64));
	// lfs f13,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f13.f64 = double(temp.f32);
	// fadds f26,f26,f24
	ctx.f26.f64 = double(float(ctx.f26.f64 + ctx.f24.f64));
	// lfs f24,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f5,f5,f29
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f29.f64));
	// lfs f29,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f10,f10,f13,f6
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f13.f64 - ctx.f6.f64)));
	// lfs f6,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f24,f23,f24
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f24.f64));
	// lfs f23,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f28,f28,f13,f11
	ctx.f28.f64 = double(float(-(ctx.f28.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// lfs f11,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f8,f31,f6,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f6.f64 - ctx.f8.f64));
	// fnmsubs f7,f7,f0,f1
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f1.f64)));
	// lfs f1,1324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1324);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f23,f23,f11
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f11.f64));
	// lfs f11,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f11.f64 = double(temp.f32);
	// fadds f6,f12,f30
	ctx.f6.f64 = double(float(ctx.f12.f64 + ctx.f30.f64));
	// lfs f30,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f30.f64 = double(temp.f32);
	// fmr f31,f0
	ctx.f31.f64 = ctx.f0.f64;
	// fmuls f20,f16,f21
	ctx.f20.f64 = double(float(ctx.f16.f64 * ctx.f21.f64));
	// lfs f19,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f26,f26,f11
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// fmuls f24,f24,f31
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f31.f64));
	// lfs f17,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f10,f27,f13,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f13.f64 + ctx.f10.f64));
	// lfs f12,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f27,f19,f21
	ctx.f27.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f11,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f9,f9,f0,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f7,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f3,f3,f17
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// lfs f13,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f6,f6,f12,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f5.f64));
	// lfs f31,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fmuls f2,f2,f30
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// lfs f30,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f8,f20,f11,f8
	ctx.f8.f64 = double(float(-(ctx.f20.f64 * ctx.f11.f64 - ctx.f8.f64)));
	// stfs f8,5496(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5496, temp.u32);
	// fnmsubs f5,f23,f0,f18
	ctx.f5.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f18.f64)));
	// stfs f5,5492(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5492, temp.u32);
	// fmsubs f12,f26,f12,f24
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f12.f64 - ctx.f24.f64));
	// stfs f12,5504(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5504, temp.u32);
	// fnmsubs f12,f27,f11,f10
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// stfs f12,5512(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5512, temp.u32);
	// lfs f11,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f12,f3,f0,f9
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// stfs f12,5516(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5516, temp.u32);
	// fmadds f12,f4,f0,f6
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f6.f64));
	// fmadds f8,f1,f13,f25
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f13.f64 + ctx.f25.f64));
	// stfs f8,5500(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5500, temp.u32);
	// fmadds f13,f2,f13,f28
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f28.f64));
	// stfs f13,5508(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5508, temp.u32);
	// fmuls f13,f31,f30
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// fnmsubs f12,f7,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// fmadds f12,f29,f0,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 + ctx.f12.f64));
	// fnmsubs f12,f22,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// fnmsubs f12,f11,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// fnmsubs f0,f13,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// stfs f0,5520(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5520, temp.u32);
	// bl 0x82242e58
	ctx.lr = 0x8224A834;
	sub_82242E58(ctx, base);
	// cmplwi cr6,r29,5
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 5, ctx.xer);
	// bgt cr6,0x8224a840
	if (ctx.cr6.gt) goto loc_8224A840;
	// b 0x8225dc2c
	goto loc_8225DC2C;
loc_8224A840:
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f2,328(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f2.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f12,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f28,f31,f31
	ctx.f28.f64 = double(float(ctx.f31.f64 * ctx.f31.f64));
	// fmuls f1,f12,f12
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// lfs f11,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f3,f30,f10
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f0,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f0.f64 = double(temp.f32);
	// lfd f6,-19448(r11)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r11.u32 + -19448);
	// fmuls f27,f30,f30
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f30.f64));
	// fsqrts f6,f6
	ctx.f6.f64 = double(float(sqrt(ctx.f6.f64)));
	// lfd f5,-19456(r10)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r10.u32 + -19456);
	// lfs f8,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f8.f64 = double(temp.f32);
	// stfs f1,792(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// lfs f9,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f9.f64 = double(temp.f32);
	// stfs f28,688(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// lfs f13,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f13.f64 = double(temp.f32);
	// lfs f13,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f13.f64 = double(temp.f32);
	// stfs f27,640(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// lfs f13,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f13.f64 = double(temp.f32);
	// stfs f6,1120(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// stfd f10,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f10.u64);
	// lfs f13,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f2,f6,f2
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// stfs f2,252(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fsqrts f5,f5
	ctx.f5.f64 = double(float(sqrt(ctx.f5.f64)));
	// stfs f5,360(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmuls f29,f11,f11
	ctx.f29.f64 = double(float(ctx.f11.f64 * ctx.f11.f64));
	// stfs f29,680(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// fmuls f4,f31,f0
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// stfs f4,336(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f26,f12,f8
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fmuls f2,f5,f2
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// stfs f2,1620(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1620, temp.u32);
	// fmuls f2,f28,f10
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// stfs f2,1704(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1704, temp.u32);
	// fmuls f1,f1,f10
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f1,1508(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1508, temp.u32);
	// fmuls f2,f11,f3
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// stfs f2,1276(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1276, temp.u32);
	// fmuls f1,f3,f31
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f1,1316(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1316, temp.u32);
	// fmuls f2,f27,f10
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f10.f64));
	// stfs f2,1212(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// fmuls f25,f11,f9
	ctx.f25.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// fmuls f1,f29,f10
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f10.f64));
	// stfs f1,1332(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1332, temp.u32);
	// fmuls f24,f4,f8
	ctx.f24.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// lfs f8,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f28,f30,f0
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// fmuls f2,f31,f10
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// fmuls f29,f26,f0
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f3,f12
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// stfs f1,1604(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1604, temp.u32);
	// fmuls f20,f5,f6
	ctx.f20.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// lfs f5,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f17,f11,f10
	ctx.f17.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f17,220(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f1,f12,f10
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// lfs f6,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f27,f25,f10
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// lfs f17,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f4,f8
	ctx.f21.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// lfs f4,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f23,f3,f9
	ctx.f23.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// fmuls f22,f3,f8
	ctx.f22.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// fmuls f30,f30,f8
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// fmuls f19,f11,f8
	ctx.f19.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// fmuls f18,f12,f8
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fmuls f16,f2,f12
	ctx.f16.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// fmuls f15,f24,f9
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// fmuls f14,f29,f9
	ctx.f14.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// fmuls f10,f5,f28
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f28.f64));
	// stfs f1,148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f4,f25,f4
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// stfs f19,152(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f17,f17,f26
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f26.f64));
	// stfs f4,116(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f4,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f4.f64 = double(temp.f32);
	// stfs f18,180(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfd f5,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f5.u64);
	// fmuls f5,f1,f11
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f30,136(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f1,f24,f0
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// stfs f2,204(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfd f4,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f4.u64);
	// fmuls f30,f27,f0
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfs f15,1564(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1564, temp.u32);
	// fmuls f15,f11,f6
	ctx.f15.f64 = double(float(ctx.f11.f64 * ctx.f6.f64));
	// stfs f15,900(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// lfs f19,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f18.f64 = double(temp.f32);
	// stfs f28,940(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// fmuls f28,f22,f6
	ctx.f28.f64 = double(float(ctx.f22.f64 * ctx.f6.f64));
	// stfs f5,756(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// fmuls f18,f18,f25
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f25.f64));
	// stfs f2,1084(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// fmuls f2,f19,f0
	ctx.f2.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// stfs f22,1468(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1468, temp.u32);
	// fmuls f19,f13,f3
	ctx.f19.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// stfs f16,1724(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1724, temp.u32);
	// fmuls f16,f23,f0
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// stfs f14,828(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// fmuls f14,f31,f8
	ctx.f14.f64 = double(float(ctx.f31.f64 * ctx.f8.f64));
	// stfs f10,616(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f10,f21,f9
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f9.f64));
	// lfs f15,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f15.f64 = double(temp.f32);
	// lfs f22,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f5,f13,f15
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f15.f64));
	// lfs f4,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f4,1352(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1352, temp.u32);
	// stfs f18,1908(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1908, temp.u32);
	// stfs f28,1660(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1660, temp.u32);
	// stfs f2,152(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfs f18,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f18.f64 = double(temp.f32);
	// lfs f28,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f28.f64 = double(temp.f32);
	// lfd f4,720(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfd f5,704(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f2,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f13,f5
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// stfs f24,556(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// fmuls f24,f31,f0
	ctx.f24.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// stfs f1,1524(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1524, temp.u32);
	// fmuls f1,f11,f0
	ctx.f1.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f27,1064(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// fmuls f27,f29,f0
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// stfs f30,1340(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1340, temp.u32);
	// fmuls f30,f12,f0
	ctx.f30.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f23,1072(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// fmuls f23,f12,f9
	ctx.f23.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
	// stfs f17,1192(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// fmuls f17,f7,f3
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// stfs f16,1324(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1324, temp.u32);
	// fmuls f16,f18,f0
	ctx.f16.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// stfs f25,412(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmuls f25,f20,f4
	ctx.f25.f64 = double(float(ctx.f20.f64 * ctx.f4.f64));
	// stfs f26,148(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f26,f22,f0
	ctx.f26.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfs f14,572(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// fmuls f14,f28,f0
	ctx.f14.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// stfs f10,1612(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1612, temp.u32);
	// fmuls f10,f13,f2
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// stfs f24,736(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// stfs f1,180(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f29,628(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// stfs f27,1284(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1284, temp.u32);
	// stfs f30,764(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f23,532(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// stfs f19,1640(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1640, temp.u32);
	// stfs f3,392(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f17,1936(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1936, temp.u32);
	// stfs f20,136(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f25,1288(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1288, temp.u32);
	// fmr f25,f7
	ctx.f25.f64 = ctx.f7.f64;
	// stfs f22,748(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fmuls f29,f7,f7
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f7.f64));
	// lfs f3,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f1.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f3,340(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// fmuls f3,f7,f0
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f28,672(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// fmuls f30,f1,f7
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// fmuls f28,f7,f8
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// lfd f7,-19472(r11)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r11.u32 + -19472);
	// fsqrts f23,f7
	ctx.f23.f64 = double(float(sqrt(ctx.f7.f64)));
	// stfs f2,888(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// stfs f5,452(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f26,776(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// lfd f5,-19488(r10)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r10.u32 + -19488);
	// stfs f21,564(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// lfs f19,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f19.f64 = double(temp.f32);
	// stfs f18,668(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// lfs f18,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f18.f64 = double(temp.f32);
	// stfs f16,1280(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1280, temp.u32);
	// stfd f11,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f11.u64);
	// lfs f16,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f16.f64 = double(temp.f32);
	// lfs f11,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f11.f64 = double(temp.f32);
	// stfs f14,460(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// lfs f14,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f14.f64 = double(temp.f32);
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f11,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f11.f64 = double(temp.f32);
	// stfs f10,1024(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// stfs f1,600(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfd f8,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f8.u64);
	// lfd f10,1096(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// stfs f11,136(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfd f7,-19544(r9)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r9.u32 + -19544);
	// stfs f30,1016(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// stfs f15,920(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// stfs f23,652(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// fmr f22,f25
	ctx.f22.f64 = ctx.f25.f64;
	// fmr f17,f25
	ctx.f17.f64 = ctx.f25.f64;
	// fmuls f26,f13,f0
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f26,744(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmuls f2,f25,f2
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// stfs f2,1076(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// fmr f27,f13
	ctx.f27.f64 = ctx.f13.f64;
	// fmuls f21,f21,f0
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// stfs f21,880(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// fsqrts f5,f5
	ctx.f5.f64 = double(float(sqrt(ctx.f5.f64)));
	// fmuls f2,f22,f15
	ctx.f2.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// stfs f2,1028(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// fmuls f21,f29,f0
	ctx.f21.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f0,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f24,f13,f8
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// lfs f8,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f2,f20,f19
	ctx.f2.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// stfs f2,1060(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// fmuls f2,f18,f0
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// stfs f2,1020(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// fmuls f0,f17,f0
	ctx.f0.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// stfs f0,1656(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1656, temp.u32);
	// fmuls f0,f23,f16
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f16.f64));
	// stfs f0,1616(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1616, temp.u32);
	// fmuls f1,f1,f27
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// stfs f1,912(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// fmuls f0,f26,f14
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// stfs f0,1632(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1632, temp.u32);
	// fmr f11,f18
	ctx.f11.f64 = ctx.f18.f64;
	// lfs f11,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f0,f28,f8
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// lfs f11,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f5,f10
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// stfs f0,1624(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1624, temp.u32);
	// fsqrts f7,f7
	ctx.f7.f64 = double(float(sqrt(ctx.f7.f64)));
	// fmuls f30,f13,f13
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f13.f64));
	// fmuls f2,f31,f9
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f9.f64));
	// fmuls f27,f5,f9
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// fmuls f4,f24,f4
	ctx.f4.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// fmuls f10,f13,f10
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// fmuls f18,f23,f31
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f31.f64));
	// lfs f15,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f25,f7,f3
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// lfd f11,720(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f31,408(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// fmuls f19,f13,f9
	ctx.f19.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// lfs f9,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f31,f21,f7
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f7.f64));
	// stfs f7,204(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f22,f7,f30
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// lfs f0,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f24,f6
	ctx.f6.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// lfd f8,704(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// stfs f0,528(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmuls f15,f23,f15
	ctx.f15.f64 = double(float(ctx.f23.f64 * ctx.f15.f64));
	// stfs f15,464(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fmuls f14,f23,f11
	ctx.f14.f64 = double(float(ctx.f23.f64 * ctx.f11.f64));
	// stfs f20,388(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f17,f13,f12
	ctx.f17.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// stfs f2,364(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f23,f23,f12
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f12.f64));
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f0,f8
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfs f13,488(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// lfs f13,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f26,f12
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f12.f64));
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f29,f1
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// stfs f21,252(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f11,f10,f13
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// stfs f31,220(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f26,f5,f16
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// stfs f14,436(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// fmuls f14,f25,f0
	ctx.f14.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// stfs f29,308(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// stfs f23,440(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f23,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f7,f4,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// lfs f15,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f22,f22,f9
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// lfs f20,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f15,f28,f15
	ctx.f15.f64 = double(float(ctx.f28.f64 * ctx.f15.f64));
	// lfs f2,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f20,f3,f20
	ctx.f20.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// lfs f21,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f2,f24,f2
	ctx.f2.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// lfs f31,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f24,f21
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// stfs f25,152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f31,f3,f31
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f27,f27,f30
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// stfs f14,148(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f12,136(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f29,324(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f11,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f11.f64 = double(temp.f32);
	// lfs f25,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f25,f3,f25
	ctx.f25.f64 = double(float(ctx.f3.f64 * ctx.f25.f64));
	// lfs f12,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f12.f64 = double(temp.f32);
	// lfs f29,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f29.f64 = double(temp.f32);
	// stfs f8,1360(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1360, temp.u32);
	// fmuls f8,f19,f13
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// stfs f15,1296(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1296, temp.u32);
	// fmuls f15,f14,f3
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// stfs f20,484(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f20,f6,f9
	ctx.f20.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// stfs f2,1004(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// fmuls f2,f11,f24
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f24.f64));
	// stfs f21,1048(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// fmuls f21,f28,f12
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// stfs f31,1648(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1648, temp.u32);
	// fmuls f31,f0,f23
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f23.f64));
	// stfs f16,180(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f16,f11,f28
	ctx.f16.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// stfs f5,664(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// fmuls f5,f18,f3
	ctx.f5.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// stfs f10,1184(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,1652(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1652, temp.u32);
	// lfs f10,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,604(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// lfs f10,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,1040(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// lfs f10,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,4708(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4708, temp.u32);
	// lfs f10,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,752(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// stfs f1,784(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// lfs f10,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f10.f64 = double(temp.f32);
	// lfs f1,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f1.f64 = double(temp.f32);
	// stfs f26,1476(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1476, temp.u32);
	// fmuls f26,f17,f9
	ctx.f26.f64 = double(float(ctx.f17.f64 * ctx.f9.f64));
	// stfs f10,1412(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1412, temp.u32);
	// stfs f1,1584(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1584, temp.u32);
	// lfs f1,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f1.f64 = double(temp.f32);
	// lfs f10,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f10.f64 = double(temp.f32);
	// stfs f4,1568(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1568, temp.u32);
	// fmuls f4,f0,f29
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// stfs f30,1576(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1576, temp.u32);
	// fmuls f30,f1,f13
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// stfs f31,2072(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2072, temp.u32);
	// fmuls f31,f1,f0
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f5,1236(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// fmuls f5,f24,f12
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f12.f64));
	// stfs f26,1716(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1716, temp.u32);
	// fmuls f26,f11,f10
	ctx.f26.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f2,492(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// stfs f28,936(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// lfs f13,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f12.f64 = double(temp.f32);
	// lfs f2,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f28.f64 = double(temp.f32);
	// lfs f10,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f10.f64 = double(temp.f32);
	// stfs f7,520(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// stfs f23,684(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// fmuls f23,f2,f12
	ctx.f23.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// stfs f18,1008(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// fmuls f18,f13,f10
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// stfs f25,1388(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1388, temp.u32);
	// fmuls f25,f0,f12
	ctx.f25.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfs f27,1396(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1396, temp.u32);
	// fmuls f27,f28,f12
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// stfs f8,1692(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1692, temp.u32);
	// lfs f7,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f7.f64 = double(temp.f32);
	// lfs f8,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f8.f64 = double(temp.f32);
	// lfs f11,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f12.f64 = double(temp.f32);
	// stfs f7,740(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// stfs f17,1368(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1368, temp.u32);
	// fmuls f17,f13,f8
	ctx.f17.f64 = double(float(ctx.f13.f64 * ctx.f8.f64));
	// stfs f15,1140(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// stfs f16,1068(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// stfs f18,388(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// lfs f7,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f7.f64 = double(temp.f32);
	// lfs f16,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f14,f15,f9
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f9.f64));
	// stfs f22,1540(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1540, temp.u32);
	// fmuls f22,f0,f13
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f20,1516(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1516, temp.u32);
	// fmuls f20,f12,f13
	ctx.f20.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f21,1820(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1820, temp.u32);
	// fmuls f21,f11,f13
	ctx.f21.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f7,1860(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1860, temp.u32);
	// stfs f19,904(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// stfs f6,1544(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1544, temp.u32);
	// stfs f17,360(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f23,336(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f23,f18,f3
	ctx.f23.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// lfs f7,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f7.f64 = double(temp.f32);
	// lfs f19,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f19.f64 = double(temp.f32);
	// lfs f6,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f6.f64 = double(temp.f32);
	// lfs f13,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f13.f64 = double(temp.f32);
	// lfs f17,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f17.f64 = double(temp.f32);
	// stfs f1,324(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f1,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f1.f64 = double(temp.f32);
	// lfs f1,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f1.f64 = double(temp.f32);
	// lfs f1,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f1.f64 = double(temp.f32);
	// stfs f31,116(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f1,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f31.f64 = double(temp.f32);
	// lfs f1,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f31,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f9.f64));
	// lfs f1,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f1.f64 = double(temp.f32);
	// stfs f31,308(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f31,f1,f10
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f30,136(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f30,f28,f7
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// stfs f31,364(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f31,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f31.f64 = double(temp.f32);
	// stfs f31,148(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f31,f0,f11
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfs f25,152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f30,1164(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// stfs f21,252(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f21,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f21.f64 = double(temp.f32);
	// stfs f22,220(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f22,f21,f3
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// stfs f23,976(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// fmuls f23,f0,f10
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f21,1172(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// fmuls f21,f12,f10
	ctx.f21.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// stfs f5,552(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// fmuls f5,f11,f10
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f10,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f10.f64 = double(temp.f32);
	// stfs f28,928(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// lfs f30,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f30.f64 = double(temp.f32);
	// stfs f30,2060(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2060, temp.u32);
	// stfs f15,1432(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1432, temp.u32);
	// fmuls f15,f16,f8
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f8.f64));
	// stfs f20,204(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f20,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f20,f3
	ctx.f25.f64 = double(float(ctx.f20.f64 * ctx.f3.f64));
	// stfs f18,1268(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1268, temp.u32);
	// lfs f30,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f18,f12,f6
	ctx.f18.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// stfs f30,508(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// fmuls f6,f0,f6
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// stfs f24,808(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// fmuls f24,f19,f8
	ctx.f24.f64 = double(float(ctx.f19.f64 * ctx.f8.f64));
	// lfs f30,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f10,f30,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f30,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f30.f64 = double(temp.f32);
	// fmr f28,f30
	ctx.f28.f64 = ctx.f30.f64;
	// stfs f25,760(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// stfs f29,788(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// fmuls f25,f17,f8
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f8.f64));
	// stfs f4,1836(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1836, temp.u32);
	// fmuls f8,f11,f7
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// stfs f14,1828(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1828, temp.u32);
	// fmuls f29,f12,f11
	ctx.f29.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f27,1156(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// fmuls f4,f12,f0
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// fmuls f14,f19,f7
	ctx.f14.f64 = double(float(ctx.f19.f64 * ctx.f7.f64));
	// stfs f26,1244(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// lfs f0,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f0.f64 = double(temp.f32);
	// lfs f26,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f11,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f11.f64 = double(temp.f32);
	// lfs f7,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f7.f64 = double(temp.f32);
	// lfs f27,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f22,996(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// fmuls f22,f1,f13
	ctx.f22.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// stfs f3,656(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// fmuls f3,f2,f13
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f20,1492(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1492, temp.u32);
	// fmuls f28,f26,f28
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// stfs f9,848(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// fmuls f9,f15,f11
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f11.f64));
	// stfs f1,816(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// fmuls f27,f26,f27
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// lfs f13,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f12.f64 = double(temp.f32);
	// lfs f20,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f20.f64 = double(temp.f32);
	// lfs f1,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f1.f64 = double(temp.f32);
	// stfs f8,532(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// fmuls f1,f20,f1
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f1.f64));
	// lfs f8,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f8.f64 = double(temp.f32);
	// stfs f25,528(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmr f25,f8
	ctx.f25.f64 = ctx.f8.f64;
	// stfs f21,440(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f24,464(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// stfs f22,412(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// stfs f3,436(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f3,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f3.f64 = double(temp.f32);
	// stfs f23,392(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f5,408(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// lfs f23,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f23.f64 = double(temp.f32);
	// lfs f5,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f5.f64 = double(temp.f32);
	// fmr f21,f25
	ctx.f21.f64 = ctx.f25.f64;
	// stfs f3,152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f25,f3,f25
	ctx.f25.f64 = double(float(ctx.f3.f64 * ctx.f25.f64));
	// stfs f9,136(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f3,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f5,f5,f19
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f19.f64));
	// lfs f9,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f9.f64 = double(temp.f32);
	// stfs f2,564(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// stfs f16,572(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// stfs f18,252(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f14,220(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f6,488(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmr f24,f21
	ctx.f24.f64 = ctx.f21.f64;
	// lfs f6,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f20,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f2,f13,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lfs f16,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f20,f20,f17
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f17.f64));
	// lfs f18,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f16,f16,f17
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f17.f64));
	// lfs f14,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f21,f31,f21
	ctx.f21.f64 = double(float(ctx.f31.f64 * ctx.f21.f64));
	// lfs f11,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f18,f4,f18
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f18.f64));
	// stfs f23,180(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f14,f12,f14
	ctx.f14.f64 = double(float(ctx.f12.f64 * ctx.f14.f64));
	// stfs f19,148(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f11,f11,f17
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f17.f64));
	// lfs f19,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f19.f64 = double(temp.f32);
	// fmr f22,f24
	ctx.f22.f64 = ctx.f24.f64;
	// lfs f3,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f3.f64 = double(temp.f32);
	// stfs f17,204(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f24,f29,f24
	ctx.f24.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// lfs f17,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f17.f64 = double(temp.f32);
	// stfs f10,540(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// stfs f7,1708(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1708, temp.u32);
	// stfs f0,948(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// stfs f30,2064(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2064, temp.u32);
	// stfs f28,980(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// stfs f26,932(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f27,952(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// fmuls f22,f23,f22
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f23,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f23.f64 = double(temp.f32);
	// stfs f1,984(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// fmuls f23,f19,f23
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// stfs f13,556(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// stfs f2,964(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// stfs f20,824(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// stfs f16,480(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 480, temp.u32);
	// stfs f6,1732(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1732, temp.u32);
	// stfs f8,444(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// stfs f25,1036(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// stfs f31,892(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// stfs f21,1928(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1928, temp.u32);
	// stfs f29,1320(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1320, temp.u32);
	// stfs f24,592(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// stfs f22,524(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// stfs f5,972(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// stfs f4,2092(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2092, temp.u32);
	// stfs f18,448(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// stfs f12,1600(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1600, temp.u32);
	// stfs f14,696(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// stfs f19,1832(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1832, temp.u32);
	// lfs f2,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f12,f3,f17
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// lfs f13,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f13,f2
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lfs f13,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f13.f64 = double(temp.f32);
	// lfs f31,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f31.f64 = double(temp.f32);
	// fmr f4,f13
	ctx.f4.f64 = ctx.f13.f64;
	// fmuls f30,f31,f13
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f13.f64));
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// fmr f13,f17
	ctx.f13.f64 = ctx.f17.f64;
	// stfs f0,896(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// lfs f0,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,968(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// lfs f0,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f0.f64 = double(temp.f32);
	// lfs f25,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f26.f64 = double(temp.f32);
	// stfs f9,992(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// lfs f9,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f9.f64 = double(temp.f32);
	// stfs f23,956(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// lfs f7,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f27,f0,f13
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f13,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f24,f25,f13
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f19,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f19.f64 = double(temp.f32);
	// fmr f13,f26
	ctx.f13.f64 = ctx.f26.f64;
	// lfs f17,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f17.f64 = double(temp.f32);
	// stfs f17,220(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f15,1220(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// lfs f15,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f16.f64 = double(temp.f32);
	// lfs f29,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f29.f64 = double(temp.f32);
	// stfs f15,180(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f4,f29,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// stfs f2,152(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfs f2,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f2.f64 = double(temp.f32);
	// lfs f21,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f21.f64 = double(temp.f32);
	// fmr f23,f13
	ctx.f23.f64 = ctx.f13.f64;
	// lfs f1,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f22,f9,f13
	ctx.f22.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f13,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f20,f25,f13
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f13,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f18,f7,f13
	ctx.f18.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lfs f6,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f6.f64 = double(temp.f32);
	// fmr f13,f19
	ctx.f13.f64 = ctx.f19.f64;
	// stfs f25,148(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f19,f17,f19
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// lfs f17,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f17.f64 = double(temp.f32);
	// stfs f17,204(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f16,f17,f16
	ctx.f16.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// lfs f17,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f25,f21
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f21.f64));
	// fmuls f17,f2,f17
	ctx.f17.f64 = double(float(ctx.f2.f64 * ctx.f17.f64));
	// stfs f11,796(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 796, temp.u32);
	// stfs f17,136(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f6,f1,f6
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// stfs f3,1204(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// lfs f11,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f26,f10,f26
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// fmuls f14,f15,f13
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f13.f64));
	// lfs f13,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f13.f64 = double(temp.f32);
	// lfs f15,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f23,f8,f23
	ctx.f23.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// fmuls f13,f15,f13
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f13.f64));
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f13,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f13.f64 = double(temp.f32);
	// lfs f28,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f28,f11,f28
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// lfs f17,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f17.f64 = double(temp.f32);
	// lfs f3,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f3.f64 = double(temp.f32);
	// stfd f13,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f13.u64);
	// fmuls f13,f17,f13
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f13.f64));
	// stfs f12,856(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 856, temp.u32);
	// fmuls f25,f25,f3
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f3.f64));
	// stfs f5,768(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// stfs f1,876(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f6,772(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 772, temp.u32);
	// stfs f31,1800(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1800, temp.u32);
	// stfs f13,884(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 884, temp.u32);
	// lfd f13,720(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f5,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f5.f64 = double(temp.f32);
	// stfs f8,1228(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// fmuls f8,f5,f13
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// lfs f31,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f31.f64 = double(temp.f32);
	// lfs f13,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f13.f64 = double(temp.f32);
	// stfs f30,608(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 608, temp.u32);
	// fmuls f30,f13,f31
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f27,304(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// lfs f13,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f13.f64 = double(temp.f32);
	// lfs f27,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f27.f64 = double(temp.f32);
	// stfs f26,648(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 648, temp.u32);
	// fmuls f26,f27,f13
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// stfs f14,872(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 872, temp.u32);
	// stfs f0,1080(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// stfs f10,1056(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// stfs f29,1344(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1344, temp.u32);
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f10.f64 = double(temp.f32);
	// lfs f29,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f29.f64 = double(temp.f32);
	// lfs f13,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// lfs f14,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f14.f64 = double(temp.f32);
	// stfs f9,1180(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// fmuls f14,f13,f14
	ctx.f14.f64 = double(float(ctx.f13.f64 * ctx.f14.f64));
	// lfs f12,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f12.f64 = double(temp.f32);
	// lfs f9,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f9.f64 = double(temp.f32);
	// stfs f0,728(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 728, temp.u32);
	// stfs f28,4752(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4752, temp.u32);
	// fmuls f28,f29,f10
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f10.f64));
	// stfs f20,4760(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4760, temp.u32);
	// stfs f18,4800(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4800, temp.u32);
	// lfs f0,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// lfs f20,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f18.f64 = double(temp.f32);
	// stfs f11,944(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// stfs f4,368(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 368, temp.u32);
	// stfs f22,852(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 852, temp.u32);
	// stfs f19,588(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// fmuls f19,f9,f12
	ctx.f19.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// stfs f3,600(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// lfs f11,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f6,f4,f11
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// lfs f22,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f1,f3,f11
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f14,388(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// stfs f17,640(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// fmuls f17,f20,f18
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// stfs f0,832(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 832, temp.u32);
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// lfs f14,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f14.f64 = double(temp.f32);
	// lfs f9,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f9.f64 = double(temp.f32);
	// stfs f21,4768(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4768, temp.u32);
	// fmuls f21,f10,f22
	ctx.f21.f64 = double(float(ctx.f10.f64 * ctx.f22.f64));
	// stfs f16,800(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// fmuls f16,f31,f11
	ctx.f16.f64 = double(float(ctx.f31.f64 * ctx.f11.f64));
	// stfs f15,1052(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// fmuls f15,f9,f12
	ctx.f15.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// stfs f24,4680(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4680, temp.u32);
	// stfs f2,628(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// stfs f17,336(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f17,f14,f0
	ctx.f17.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f2,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f2.f64 = double(temp.f32);
	// lfs f24,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f24.f64 = double(temp.f32);
	// lfs f10,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f11.f64 = double(temp.f32);
	// stfs f14,308(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f11,f10,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// lfs f14,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f14.f64 = double(temp.f32);
	// stfs f23,804(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 804, temp.u32);
	// fmuls f23,f24,f0
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// stfs f7,1108(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// fmuls f7,f2,f0
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f31,412(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// stfs f15,360(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// lfs f31,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f31.f64 = double(temp.f32);
	// lfs f15,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f14.f64 = double(temp.f32);
	// stfs f11,252(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f31,f10,f31
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// lfs f11,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f11,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f11.f64 = double(temp.f32);
	// lfs f11,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f11,f9,f0
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f14,324(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f14,f14,f0
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f10,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f15,f13,f15
	ctx.f15.f64 = double(float(ctx.f13.f64 * ctx.f15.f64));
	// lfs f10,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f10.f64 = double(temp.f32);
	// lfs f10,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,440(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f12,364(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f12,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f10.f64 = double(temp.f32);
	// lfs f10,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f10.f64 = double(temp.f32);
	// stfs f29,564(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmuls f29,f12,f13
	ctx.f29.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f6,216(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// stfs f3,464(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// lfs f3,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f3.f64 = double(temp.f32);
	// lfs f6,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f6.f64 = double(temp.f32);
	// stfs f26,384(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// fmuls f26,f16,f10
	ctx.f26.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// stfs f0,392(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// stfs f5,652(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// fmuls f5,f3,f6
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// stfs f16,680(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// stfs f28,780(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 780, temp.u32);
	// stfs f7,712(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 712, temp.u32);
	// stfs f8,644(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 644, temp.u32);
	// lfs f28,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f28.f64 = double(temp.f32);
	// lfs f7,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f7.f64 = double(temp.f32);
	// lfs f8,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f8.f64 = double(temp.f32);
	// lfs f16,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f16.f64 = double(temp.f32);
	// lfs f9,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f9.f64 = double(temp.f32);
	// stfs f30,844(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 844, temp.u32);
	// fmuls f30,f12,f0
	ctx.f30.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f1,348(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 348, temp.u32);
	// fmuls f1,f24,f0
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// stfs f19,532(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// fmuls f19,f17,f13
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f13.f64));
	// stfs f25,860(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 860, temp.u32);
	// fmuls f25,f7,f13
	ctx.f25.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// stfs f4,436(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f27,1176(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// fmuls f27,f28,f13
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// stfs f23,144(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fmuls f23,f7,f10
	ctx.f23.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// stfs f22,528(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmuls f22,f8,f10
	ctx.f22.f64 = double(float(ctx.f8.f64 * ctx.f10.f64));
	// stfs f21,176(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// stfs f20,1044(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// fmuls f20,f8,f13
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f16,688(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fmuls f16,f16,f0
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// stfs f24,572(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// stfs f2,792(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f11,204(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f17,488(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmuls f17,f14,f10
	ctx.f17.f64 = double(float(ctx.f14.f64 * ctx.f10.f64));
	// lfs f12,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f12.f64 = double(temp.f32);
	// lfs f21,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f21.f64 = double(temp.f32);
	// lfs f5,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f4.f64 = double(temp.f32);
	// lfs f24,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f24.f64 = double(temp.f32);
	// lfs f11,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f11.f64 = double(temp.f32);
	// stfd f6,3680(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3680, ctx.f6.u64);
	// fmuls f3,f12,f3
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// lfs f6,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f24,f24,f13
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f6,148(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f21,152(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f21,f21,f0
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// stfd f8,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f8.u64);
	// stfs f21,180(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f9,136(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfd f5,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f5.u64);
	// stfd f7,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f7.u64);
	// stfs f2,292(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
	// stfd f10,3696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3696, ctx.f10.u64);
	// fmuls f10,f7,f5
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// lfs f5,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f7.f64 = double(temp.f32);
	// stfs f30,1196(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// fmuls f7,f7,f9
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// stfs f15,836(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 836, temp.u32);
	// fmuls f15,f8,f13
	ctx.f15.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f28,1684(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1684, temp.u32);
	// fmuls f28,f5,f18
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// lfs f30,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f30.f64 = double(temp.f32);
	// stfs f1,224(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// stfd f4,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f4.u64);
	// fmuls f4,f31,f4
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// stfs f5,1088(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// lfs f2,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,3668(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3668, temp.u32);
	// lfs f2,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,1668(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1668, temp.u32);
	// lfd f8,720(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f1,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f2.f64 = double(temp.f32);
	// lfd f5,704(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// stfs f3,4660(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4660, temp.u32);
	// stfs f30,1136(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// stfs f31,664(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// stfs f7,4488(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4488, temp.u32);
	// lfs f6,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f6.f64 = double(temp.f32);
	// lfs f21,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f21.f64 = double(temp.f32);
	// lfs f3,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f21,f12,f21
	ctx.f21.f64 = double(float(ctx.f12.f64 * ctx.f21.f64));
	// lfs f31,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f3,f3,f5
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// lfs f30,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f30.f64 = double(temp.f32);
	// lfd f7,1096(r1)
	ctx.f7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// stfd f11,2216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 2216, ctx.f11.u64);
	// fmuls f11,f12,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f1,620(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 620, temp.u32);
	// fmuls f1,f8,f9
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f2,2348(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2348, temp.u32);
	// fmuls f2,f8,f5
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// stfs f27,612(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 612, temp.u32);
	// fmuls f27,f6,f0
	ctx.f27.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f29,656(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// fmuls f29,f7,f9
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// stfs f26,256(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// stfs f25,868(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 868, temp.u32);
	// stfs f23,172(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// fmuls f23,f30,f0
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f24,3464(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3464, temp.u32);
	// fmuls f24,f31,f0
	ctx.f24.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f26,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f25.f64 = double(temp.f32);
	// lfs f8,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f8.f64 = double(temp.f32);
	// stfs f18,408(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f22,840(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 840, temp.u32);
	// stfs f20,184(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// stfs f19,4776(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4776, temp.u32);
	// stfs f16,128(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f14,1128(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// stfs f17,264(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// stfs f10,4816(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4816, temp.u32);
	// stfs f4,3456(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3456, temp.u32);
	// stfs f11,672(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// stfs f15,3992(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3992, temp.u32);
	// fmuls f22,f8,f0
	ctx.f22.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f17,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f17.f64 = double(temp.f32);
	// lfs f8,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f20,f26,f0
	ctx.f20.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// fmuls f11,f17,f8
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f8.f64));
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f7,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f19,f25,f0
	ctx.f19.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// stfs f28,252(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f14,f7,f0
	ctx.f14.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f5,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f5.f64 = double(temp.f32);
	// lfs f28,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f28.f64 = double(temp.f32);
	// stfs f27,220(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f27,f28,f5
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f5.f64));
	// lfs f10,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f7.f64 = double(temp.f32);
	// stfs f31,1812(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1812, temp.u32);
	// lfd f4,1696(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// lfs f31,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f16,f17,f4
	ctx.f16.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// stfs f27,136(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f21,2556(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2556, temp.u32);
	// fmuls f21,f7,f10
	ctx.f21.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f27,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f10,f31,f0
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// stfd f9,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f9.u64);
	// stfd f8,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f8.u64);
	// lfs f9,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f8.f64 = double(temp.f32);
	// stfs f27,148(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f27,f27,f0
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfd f4,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f4.u64);
	// stfs f10,180(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f9,308(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f9,f12,f9
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
	// stfs f8,324(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f8,f12,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f4,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f4.f64 = double(temp.f32);
	// lfd f10,3696(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3696);
	// stfs f24,2564(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2564, temp.u32);
	// stfd f12,5064(r1)
	PPC_STORE_U64(ctx.r1.u32 + 5064, ctx.f12.u64);
	// stfd f5,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f5.u64);
	// stfs f27,152(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f26,1308(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1308, temp.u32);
	// stfs f20,92(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lfs f24,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f24.f64 = double(temp.f32);
	// lfs f18,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f18.f64 = double(temp.f32);
	// lfs f15,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f18,f18,f6
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// lfs f12,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f15,f28,f15
	ctx.f15.f64 = double(float(ctx.f28.f64 * ctx.f15.f64));
	// lfs f5,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f5.f64 = double(temp.f32);
	// lfs f27,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f27.f64 = double(temp.f32);
	// lfd f11,2216(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 2216);
	// lfs f26,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f26.f64 = double(temp.f32);
	// lfs f20,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f20.f64 = double(temp.f32);
	// stfs f22,2548(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2548, temp.u32);
	// fmuls f22,f4,f6
	ctx.f22.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// stfs f23,104(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// fmuls f23,f31,f10
	ctx.f23.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// stfs f3,3372(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3372, temp.u32);
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// lfs f3,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f20,f0
	ctx.f4.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// stfs f24,788(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// fmuls f24,f24,f13
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// stfs f30,1748(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1748, temp.u32);
	// fmuls f30,f28,f11
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// stfs f19,4696(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4696, temp.u32);
	// fmuls f19,f5,f12
	ctx.f19.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f25,4300(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4300, temp.u32);
	// fmuls f25,f27,f0
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfs f1,5024(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5024, temp.u32);
	// stfs f2,4620(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4620, temp.u32);
	// stfs f16,2244(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2244, temp.u32);
	// stfs f14,4176(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4176, temp.u32);
	// stfs f17,3488(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3488, temp.u32);
	// stfs f3,3984(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3984, temp.u32);
	// stfs f29,4208(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4208, temp.u32);
	// stfs f9,3132(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3132, temp.u32);
	// stfs f8,4720(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4720, temp.u32);
	// fmuls f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f15,3496(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3496, temp.u32);
	// lfs f15,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f13,f20,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// lfs f3,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,108(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// lfs f3,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f3.f64 = double(temp.f32);
	// stfs f25,3672(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3672, temp.u32);
	// lfs f25,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f25.f64 = double(temp.f32);
	// stfs f3,4672(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4672, temp.u32);
	// stfs f21,4808(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4808, temp.u32);
	// lfs f16,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f0,f0,f15
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f15.f64));
	// lfs f15,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f15.f64 = double(temp.f32);
	// fmr f8,f15
	ctx.f8.f64 = ctx.f15.f64;
	// lfs f3,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f3.f64 = double(temp.f32);
	// lfd f5,720(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// lfs f1,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f1.f64 = double(temp.f32);
	// stfs f19,4352(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4352, temp.u32);
	// fmr f19,f21
	ctx.f19.f64 = ctx.f21.f64;
	// stfs f3,3380(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3380, temp.u32);
	// fmuls f3,f12,f11
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f10,4316(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4316, temp.u32);
	// fmuls f10,f12,f5
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// stfs f16,392(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// fmuls f14,f1,f16
	ctx.f14.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// stfs f4,5060(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5060, temp.u32);
	// lfs f16,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f8,f25,f8
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// stfs f8,136(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfd f8,704(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f4,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f4.f64 = double(temp.f32);
	// stfs f27,364(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f27,f16,f8
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f8.f64));
	// stfs f23,4228(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4228, temp.u32);
	// fmuls f4,f4,f6
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// lfs f23,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f23.f64 = double(temp.f32);
	// lfs f2,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f19,f10,f19
	ctx.f19.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// stfs f31,764(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// fmuls f15,f23,f15
	ctx.f15.f64 = double(float(ctx.f23.f64 * ctx.f15.f64));
	// stfs f27,252(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f31,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f7,f7,f31
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// stfs f4,148(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f27,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f27.f64 = double(temp.f32);
	// lfd f4,1096(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// lfs f31,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f27,f4
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f4.f64));
	// lfs f9,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f31,f25,f31
	ctx.f31.f64 = double(float(ctx.f25.f64 * ctx.f31.f64));
	// stfs f18,4688(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4688, temp.u32);
	// fmuls f9,f25,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// stfs f24,3708(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3708, temp.u32);
	// stfs f22,3364(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3364, temp.u32);
	// stfs f30,4392(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4392, temp.u32);
	// stfs f2,1636(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1636, temp.u32);
	// lfs f5,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f21,f2,f21
	ctx.f21.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f29,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f30,f12
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f12.f64));
	// lfs f24,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f23,f29
	ctx.f29.f64 = double(float(ctx.f23.f64 * ctx.f29.f64));
	// lfs f22,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f5,f24
	ctx.f24.f64 = double(float(ctx.f5.f64 * ctx.f24.f64));
	// lfs f18,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f22,f23,f22
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f17,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f18,f6
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// stfs f4,336(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f17,f3,f17
	ctx.f17.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// lfs f4,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f4,f12,f8
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// stfs f25,808(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// stfs f26,408(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// lfs f25,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f26.f64 = double(temp.f32);
	// stfs f6,204(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f6,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f6.f64 = double(temp.f32);
	// stfs f9,116(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f6,152(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfd f6,3680(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3680);
	// stfs f12,220(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f12,f12,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// stfs f12,180(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfd f9,1696(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// fmuls f9,f9,f11
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// stfs f29,4544(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4544, temp.u32);
	// fmuls f29,f25,f26
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// stfs f0,2708(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2708, temp.u32);
	// stfs f1,936(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// lfs f26,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f26.f64 = double(temp.f32);
	// lfd f12,5064(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 5064);
	// lfs f1,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f12,f12,f8
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// stfs f30,4728(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4728, temp.u32);
	// fmuls f1,f1,f26
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f26.f64));
	// lfs f30,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f8,f16,f6
	ctx.f8.f64 = double(float(ctx.f16.f64 * ctx.f6.f64));
	// stfs f7,4712(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4712, temp.u32);
	// fmuls f7,f27,f6
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f6.f64));
	// stfs f31,4664(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4664, temp.u32);
	// fmuls f6,f30,f11
	ctx.f6.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// lfs f31,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f31.f64 = double(temp.f32);
	// lfs f26,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f26.f64 = double(temp.f32);
	// stfs f20,1428(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1428, temp.u32);
	// fmuls f26,f9,f26
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f26.f64));
	// stfs f13,4880(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4880, temp.u32);
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,3452(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3452, temp.u32);
	// lfs f0,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,1240(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// stfs f24,4552(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4552, temp.u32);
	// stfs f22,2676(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2676, temp.u32);
	// stfs f2,684(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// stfs f10,1676(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1676, temp.u32);
	// stfs f18,2692(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2692, temp.u32);
	// stfs f17,3204(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3204, temp.u32);
	// stfs f15,2668(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2668, temp.u32);
	// stfs f0,2660(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2660, temp.u32);
	// stfs f21,3180(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3180, temp.u32);
	// stfs f19,3688(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3688, temp.u32);
	// stfs f3,1420(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1420, temp.u32);
	// lfs f0,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f0.f64 = double(temp.f32);
	// lfs f21,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f19.f64 = double(temp.f32);
	// lfs f13,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f13,f4,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// lfs f5,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f5,f23,f5
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// lfs f2,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f10,f3,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// lfs f25,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f24,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f28,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// lfs f22,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f24,f21
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f20,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f22,f19
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// lfs f18,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f28,f20
	ctx.f20.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lfs f17,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f28,f18
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// lfs f15,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f17,f19
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// stfs f30,1484(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1484, temp.u32);
	// fmuls f30,f31,f11
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f11.f64));
	// stfs f14,1000(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// fmuls f15,f21,f15
	ctx.f15.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// stfs f0,3444(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3444, temp.u32);
	// stfs f9,748(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// lfs f0,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f11.f64 = double(temp.f32);
	// lfs f14,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f14.f64 = double(temp.f32);
	// lfs f9,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f9,f14,f9
	ctx.f9.f64 = double(float(ctx.f14.f64 * ctx.f9.f64));
	// stfs f29,220(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmr f14,f0
	ctx.f14.f64 = ctx.f0.f64;
	// lfs f29,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f29.f64 = double(temp.f32);
	// stfs f30,180(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f30,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// stfs f6,152(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmr f6,f30
	ctx.f6.f64 = ctx.f30.f64;
	// stfs f7,148(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f7,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f7.f64 = double(temp.f32);
	// stfs f8,136(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f7,f0,f7
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f8,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f8.f64 = double(temp.f32);
	// stfs f13,2684(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2684, temp.u32);
	// stfs f27,116(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f29,f14,f29
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// lfs f14,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f14,f30
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f30.f64));
	// lfs f14,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f14,f0,f14
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// lfs f27,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f0,f0,f8
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfs f0,4580(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4580, temp.u32);
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f6,f11,f6
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f6.f64));
	// fmr f13,f0
	ctx.f13.f64 = ctx.f0.f64;
	// lfs f8,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f8.f64 = double(temp.f32);
	// stfs f10,2700(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2700, temp.u32);
	// stfs f5,3580(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3580, temp.u32);
	// stfs f6,2644(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2644, temp.u32);
	// stfs f7,3196(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3196, temp.u32);
	// lfs f10,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f5.f64 = double(temp.f32);
	// stfs f4,1232(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// fmuls f4,f27,f8
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// stfs f3,1404(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1404, temp.u32);
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f2,3420(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3420, temp.u32);
	// fmuls f3,f5,f0
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f9,4484(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4484, temp.u32);
	// fmuls f9,f7,f0
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f30,4468(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4468, temp.u32);
	// fmuls f30,f6,f0
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f2,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f0.f64 = double(temp.f32);
	// stfs f29,3156(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3156, temp.u32);
	// fmuls f29,f2,f13
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmr f13,f0
	ctx.f13.f64 = ctx.f0.f64;
	// stfs f31,204(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f21,556(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// lfs f21,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f21.f64 = double(temp.f32);
	// lfs f31,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f28,f21
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f21.f64));
	// lfs f27,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f12,f31
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfs f26,3140(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3140, temp.u32);
	// stfs f28,744(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmuls f28,f0,f16
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f16.f64));
	// stfs f1,3564(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3564, temp.u32);
	// stfs f25,2604(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2604, temp.u32);
	// stfs f31,3788(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3788, temp.u32);
	// fmuls f26,f13,f27
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f27.f64));
	// lfs f1,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f31.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// lfs f25,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f25.f64 = double(temp.f32);
	// stfs f23,1112(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// stfs f24,2580(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2580, temp.u32);
	// stfs f22,2620(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2620, temp.u32);
	// stfs f20,3164(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3164, temp.u32);
	// stfs f18,2628(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2628, temp.u32);
	// stfs f19,1592(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1592, temp.u32);
	// stfs f17,2636(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2636, temp.u32);
	// stfs f15,3172(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3172, temp.u32);
	// stfs f21,2716(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2716, temp.u32);
	// stfs f12,3572(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3572, temp.u32);
	// stfs f11,1608(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1608, temp.u32);
	// stfs f14,3188(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3188, temp.u32);
	// fmuls f24,f25,f0
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f22,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f22.f64 = double(temp.f32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f1,f13
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// fmuls f21,f22,f0
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f13,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f20,f13,f0
	ctx.f20.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f15.f64 = double(temp.f32);
	// lfs f12,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f14,f15,f0
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// fmuls f19,f12,f0
	ctx.f19.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f0,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f0,116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f12,136(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f18,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f18.f64 = double(temp.f32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f17,f18,f13
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f13.f64));
	// lfs f13,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f13.f64 = double(temp.f32);
	// stfs f4,904(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// fmuls f4,f5,f13
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfd f13,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f13.u64);
	// stfs f16,308(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f11,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// lfs f16,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f16.f64 = double(temp.f32);
	// lfs f13,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f13.f64 = double(temp.f32);
	// stfs f27,252(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f27,f16,f11
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f11.f64));
	// stfs f13,180(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfd f11,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f11.u64);
	// stfd f0,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f0.u64);
	// lfs f11,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f11,204(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f11,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f11.f64 = double(temp.f32);
	// stfs f7,148(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f7,f31,f12
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f12.f64));
	// stfs f2,628(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// stfs f10,1532(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1532, temp.u32);
	// lfs f2,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f2.f64 = double(temp.f32);
	// lfs f10,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,152(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f6,1364(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1364, temp.u32);
	// lfs f11,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f11.f64 = double(temp.f32);
	// lfs f6,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f6.f64 = double(temp.f32);
	// stfs f30,4700(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4700, temp.u32);
	// stfs f29,4692(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4692, temp.u32);
	// stfs f28,3476(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3476, temp.u32);
	// stfs f26,3676(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3676, temp.u32);
	// stfs f24,5028(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5028, temp.u32);
	// stfs f25,1520(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1520, temp.u32);
	// stfs f1,1356(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1356, temp.u32);
	// stfs f23,4340(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4340, temp.u32);
	// stfs f22,1336(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1336, temp.u32);
	// stfs f21,4172(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4172, temp.u32);
	// stfs f20,3212(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3212, temp.u32);
	// stfs f19,4100(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4100, temp.u32);
	// stfs f17,4996(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4996, temp.u32);
	// stfs f18,2028(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2028, temp.u32);
	// stfs f15,1816(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1816, temp.u32);
	// stfs f14,4372(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4372, temp.u32);
	// stfs f8,2796(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2796, temp.u32);
	// stfs f9,3252(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3252, temp.u32);
	// stfs f3,4388(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4388, temp.u32);
	// stfs f2,816(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// stfs f10,4236(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4236, temp.u32);
	// stfs f5,1200(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// stfs f4,4668(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4668, temp.u32);
	// stfs f16,1728(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1728, temp.u32);
	// stfs f27,4676(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4676, temp.u32);
	// stfs f31,1452(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1452, temp.u32);
	// stfs f7,2788(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2788, temp.u32);
	// stfs f13,2804(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2804, temp.u32);
	// stfs f0,5044(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5044, temp.u32);
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfd f0,720(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f31,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f8,f6,f0
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfd f13,1096(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// lfs f0,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f31,f13
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f13.f64));
	// lfd f11,704(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f1,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f5,f0,f11
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f13,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f26,f1,f13
	ctx.f26.f64 = double(float(ctx.f1.f64 * ctx.f13.f64));
	// lfs f25,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f25.f64 = double(temp.f32);
	// lfs f13,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f22,f25,f13
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f2,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f2.f64 = double(temp.f32);
	// lfs f13,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f2,f12
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f19,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f19.f64 = double(temp.f32);
	// fmr f20,f13
	ctx.f20.f64 = ctx.f13.f64;
	// lfs f12,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f18,f19,f13
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// lfs f10,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f7,f0,f12
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// lfs f4,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f0,f10
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f4,f0,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f13,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// lfs f21,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f14,f17,f0
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmr f13,f0
	ctx.f13.f64 = ctx.f0.f64;
	// lfs f23,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f23.f64 = double(temp.f32);
	// lfs f29,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// stfs f21,148(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f28,f28,f29
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// stfs f4,360(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f17,388(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// stfs f6,136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f24,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f24.f64 = double(temp.f32);
	// lfs f17,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f17.f64 = double(temp.f32);
	// lfs f27,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f21,f13,f12
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f4,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f10,f13,f10
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// stfs f7,152(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f6,f13,f11
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f11.f64));
	// lfs f7,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// stfs f21,220(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f4,f4,f17
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f17.f64));
	// lfs f15,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f15.f64 = double(temp.f32);
	// lfs f21,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f21.f64 = double(temp.f32);
	// lfs f13,5096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5096);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f7,f12
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// stfs f24,336(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f21,f0,f21
	ctx.f21.f64 = double(float(ctx.f0.f64 * ctx.f21.f64));
	// stfs f1,180(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f29,324(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f16,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f16.f64 = double(temp.f32);
	// lfs f24,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f20,f16,f20
	ctx.f20.f64 = double(float(ctx.f16.f64 * ctx.f20.f64));
	// lfs f17,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f17.f64 = double(temp.f32);
	// lfs f29,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f29.f64 = double(temp.f32);
	// lfs f1,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f24,f29,f24
	ctx.f24.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// stfs f25,440(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmuls f25,f15,f0
	ctx.f25.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// lfs f12,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f17,f1
	ctx.f1.f64 = double(float(ctx.f17.f64 * ctx.f1.f64));
	// lfs f0,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f12.f64 = double(temp.f32);
	// stfs f28,2844(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2844, temp.u32);
	// stfs f8,3276(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3276, temp.u32);
	// stfs f31,1348(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1348, temp.u32);
	// stfs f16,1256(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// stfs f21,436(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f16,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f16.f64 = double(temp.f32);
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// stfs f9,3520(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3520, temp.u32);
	// fmuls f21,f16,f21
	ctx.f21.f64 = double(float(ctx.f16.f64 * ctx.f21.f64));
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// stfs f14,4384(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4384, temp.u32);
	// lfs f14,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f16.f64 = double(temp.f32);
	// stfs f0,116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f6,252(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmr f6,f16
	ctx.f6.f64 = ctx.f16.f64;
	// lfs f28,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f28.f64 = double(temp.f32);
	// stfs f19,1596(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1596, temp.u32);
	// fmuls f28,f15,f28
	ctx.f28.f64 = double(float(ctx.f15.f64 * ctx.f28.f64));
	// lfs f19,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f19.f64 = double(temp.f32);
	// stfs f24,3448(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3448, temp.u32);
	// stfs f26,3736(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3736, temp.u32);
	// stfs f15,204(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f15,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f15.f64 = double(temp.f32);
	// lfs f24,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f16,f15,f16
	ctx.f16.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f26,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f26.f64 = double(temp.f32);
	// stfs f23,4608(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4608, temp.u32);
	// fmuls f26,f26,f24
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// lfs f23,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f23.f64 = double(temp.f32);
	// stfs f15,488(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmr f24,f23
	ctx.f24.f64 = ctx.f23.f64;
	// fmr f15,f6
	ctx.f15.f64 = ctx.f6.f64;
	// stfs f1,4576(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4576, temp.u32);
	// lfs f31,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f31.f64 = double(temp.f32);
	// lfs f1,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f1.f64 = double(temp.f32);
	// stfs f2,1712(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1712, temp.u32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f0,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f2.f64 = double(temp.f32);
	// lfs f11,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f11.f64 = double(temp.f32);
	// stfs f30,4412(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4412, temp.u32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lfs f30,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f30.f64 = double(temp.f32);
	// lfs f8,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f24,f19,f24
	ctx.f24.f64 = double(float(ctx.f19.f64 * ctx.f24.f64));
	// lfs f31,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// stfs f9,4640(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4640, temp.u32);
	// fmuls f9,f2,f0
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f4,3440(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3440, temp.u32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// lfs f4,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f4,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f27,4512(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4512, temp.u32);
	// fmuls f7,f10,f0
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f18,4592(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4592, temp.u32);
	// fmuls f0,f3,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfs f20,4600(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4600, temp.u32);
	// stfs f29,888(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// stfs f5,464(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// stfs f22,4584(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4584, temp.u32);
	// stfs f25,4568(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4568, temp.u32);
	// stfs f13,412(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// lfs f22,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f22.f64 = double(temp.f32);
	// lfs f13,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f29,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f6,f13,f6
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f6.f64));
	// lfs f30,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f29,f3,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// lfs f27,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f30,f17,f30
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f30.f64));
	// lfs f25,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f25.f64 = double(temp.f32);
	// lfs f20,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f27,f25,f27
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f27.f64));
	// lfs f18,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f22,f20
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f5,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f18,f14,f18
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f18.f64));
	// stfs f4,180(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f5,f13,f5
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// fmr f4,f15
	ctx.f4.f64 = ctx.f15.f64;
	// stfs f10,116(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f3,148(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f3,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f3.f64 = double(temp.f32);
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// stfs f19,152(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f10,f3,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// lfs f19,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f19.f64 = double(temp.f32);
	// stfs f19,136(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f15,f19,f15
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f15.f64));
	// stfs f0,3808(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3808, temp.u32);
	// stfs f10,4012(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4012, temp.u32);
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// stfs f28,3432(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3432, temp.u32);
	// fmuls f28,f0,f10
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f8,4404(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4404, temp.u32);
	// stfs f6,4812(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4812, temp.u32);
	// lfs f0,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f6.f64 = double(temp.f32);
	// stfs f23,5076(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5076, temp.u32);
	// fmuls f23,f6,f10
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// stfs f24,4844(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4844, temp.u32);
	// fmuls f24,f8,f0
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f13,532(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f0.f64 = double(temp.f32);
	// stfs f9,4820(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4820, temp.u32);
	// fmr f9,f13
	ctx.f9.f64 = ctx.f13.f64;
	// stfs f22,528(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmuls f22,f0,f10
	ctx.f22.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// fmr f0,f13
	ctx.f0.f64 = ctx.f13.f64;
	// lfs f19,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f19.f64 = double(temp.f32);
	// stfs f27,3816(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3816, temp.u32);
	// fmuls f4,f19,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f4.f64));
	// lfs f27,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// stfs f26,4756(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4756, temp.u32);
	// stfs f19,920(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// lfs f19,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f19.f64 = double(temp.f32);
	// stfs f21,4740(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4740, temp.u32);
	// lfs f21,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f27,f9
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f9.f64));
	// lfs f9,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f9.f64 = double(temp.f32);
	// stfs f14,740(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// stfs f18,1132(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// fmuls f18,f19,f0
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// stfs f16,3932(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3932, temp.u32);
	// lfs f14,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f16.f64 = double(temp.f32);
	// stfs f12,4840(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4840, temp.u32);
	// stfs f1,5004(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5004, temp.u32);
	// stfs f31,4420(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4420, temp.u32);
	// stfs f17,1260(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1260, temp.u32);
	// stfs f20,2268(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2268, temp.u32);
	// fmuls f20,f21,f9
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f9.f64));
	// stfs f5,4940(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4940, temp.u32);
	// fmuls f9,f14,f13
	ctx.f9.f64 = double(float(ctx.f14.f64 * ctx.f13.f64));
	// lfs f12,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f12.f64 = double(temp.f32);
	// lfs f5,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f17.f64 = double(temp.f32);
	// lfs f0,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f0.f64 = double(temp.f32);
	// stfs f15,4548(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4548, temp.u32);
	// fmuls f15,f16,f10
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// stfs f29,4536(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4536, temp.u32);
	// fmuls f29,f12,f31
	ctx.f29.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// stfs f30,4632(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4632, temp.u32);
	// fmuls f30,f1,f12
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f11,3996(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3996, temp.u32);
	// fmuls f11,f25,f13
	ctx.f11.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// stfs f4,3968(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3968, temp.u32);
	// fmuls f4,f5,f12
	ctx.f4.f64 = double(float(ctx.f5.f64 * ctx.f12.f64));
	// stfs f7,4956(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4956, temp.u32);
	// fmuls f7,f2,f13
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f14,640(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// fmuls f0,f17,f0
	ctx.f0.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// lfs f10,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f10.f64 = double(temp.f32);
	// lfs f14,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f14.f64 = double(temp.f32);
	// stfs f0,116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f10,f5,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f10.f64));
	// stfs f9,136(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f10,148(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfd f12,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f12.u64);
	// stfs f25,572(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// lfs f25,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f14,f25,f14
	ctx.f14.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// stfs f25,152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f17,324(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f25,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f25.f64 = double(temp.f32);
	// lfs f17,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f17.f64 = double(temp.f32);
	// stfs f2,600(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// fmuls f17,f25,f17
	ctx.f17.f64 = double(float(ctx.f25.f64 * ctx.f17.f64));
	// stfs f25,336(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f9,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f9.f64 = double(temp.f32);
	// lfs f2,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f2.f64 = double(temp.f32);
	// lfs f25,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f25.f64 = double(temp.f32);
	// stfs f2,564(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmuls f2,f2,f13
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f25,180(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f25,f25,f9
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// stfs f8,2412(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2412, temp.u32);
	// stfd f9,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f9.u64);
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f12,f9,f0
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f11,3936(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3936, temp.u32);
	// stfs f2,4988(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4988, temp.u32);
	// lfs f11,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f11.f64 = double(temp.f32);
	// lfs f2,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f2.f64 = double(temp.f32);
	// stfs f8,4952(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4952, temp.u32);
	// lfs f10,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f8.f64 = double(temp.f32);
	// stfs f4,4944(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4944, temp.u32);
	// fmuls f4,f2,f11
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f1,1116(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// lfs f1,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f1.f64 = double(temp.f32);
	// stfs f8,4596(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4596, temp.u32);
	// stfd f13,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f13.u64);
	// fmuls f13,f10,f3
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f3.f64));
	// stfs f6,672(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// stfs f31,1272(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1272, temp.u32);
	// stfs f29,5084(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5084, temp.u32);
	// stfs f12,2260(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2260, temp.u32);
	// lfs f8,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f11.f64 = double(temp.f32);
	// lfs f31,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f29.f64 = double(temp.f32);
	// lfd f12,720(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// stfs f3,736(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// fmuls f3,f10,f1
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// stfs f28,3732(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3732, temp.u32);
	// fmuls f28,f12,f29
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// stfs f8,4960(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4960, temp.u32);
	// stfs f7,2252(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2252, temp.u32);
	// stfs f5,948(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// stfs f30,3608(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3608, temp.u32);
	// fmuls f30,f11,f31
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f7.f64 = double(temp.f32);
	// lfs f8,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f8.f64 = double(temp.f32);
	// stfs f27,680(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// stfs f26,4832(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4832, temp.u32);
	// stfs f24,4872(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4872, temp.u32);
	// stfs f23,4912(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4912, temp.u32);
	// stfs f22,4928(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4928, temp.u32);
	// stfs f21,792(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// stfs f20,3656(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3656, temp.u32);
	// stfs f19,688(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// stfs f18,3524(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3524, temp.u32);
	// stfs f16,308(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// stfs f15,3876(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3876, temp.u32);
	// stfs f14,3340(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3340, temp.u32);
	// stfs f17,2980(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2980, temp.u32);
	// stfs f25,4864(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4864, temp.u32);
	// stfs f13,5092(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5092, temp.u32);
	// stfs f9,1444(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1444, temp.u32);
	// fmr f21,f7
	ctx.f21.f64 = ctx.f7.f64;
	// stfs f31,652(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// fmuls f27,f7,f31
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f31.f64));
	// lfs f26,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f26.f64 = double(temp.f32);
	// lfs f23,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f26,f10
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// stfs f2,664(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// fmuls f23,f23,f10
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f10.f64));
	// lfs f2,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f2.f64 = double(temp.f32);
	// fmr f17,f0
	ctx.f17.f64 = ctx.f0.f64;
	// lfs f22,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f22.f64 = double(temp.f32);
	// lfs f31,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f10,f22,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f10.f64));
	// lfs f13,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f31,f2,f31
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// lfs f25,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f14,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f24,f8,f25
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f25.f64));
	// stfs f1,436(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f22,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f22.f64 = double(temp.f32);
	// stfs f31,136(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f22,f14,f22
	ctx.f22.f64 = double(float(ctx.f14.f64 * ctx.f22.f64));
	// lfs f18,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f18.f64 = double(temp.f32);
	// lfs f31,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f8,f8,f18
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f18.f64));
	// lfs f1,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f1.f64 = double(temp.f32);
	// lfs f16,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// stfs f2,204(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f15,f11,f16
	ctx.f15.f64 = double(float(ctx.f11.f64 * ctx.f16.f64));
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f21,f21,f16
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// lfd f13,1096(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// lfs f14,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f14,f0,f14
	ctx.f14.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f18,220(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f2,f0,f2
	ctx.f2.f64 = double(float(ctx.f0.f64 * ctx.f2.f64));
	// lfs f20,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f18.f64 = double(temp.f32);
	// stfs f1,148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f16,464(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fmuls f7,f7,f18
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f18.f64));
	// lfs f16,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f16.f64 = double(temp.f32);
	// stfs f31,412(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmuls f17,f16,f17
	ctx.f17.f64 = double(float(ctx.f16.f64 * ctx.f17.f64));
	// lfd f9,704(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f19,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f9,f9,f20
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f20.f64));
	// lfs f31,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f19,f19,f13
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// stfs f1,152(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f31,f31,f13
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f13.f64));
	// stfs f14,252(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f2,364(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f1,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f1.f64 = double(temp.f32);
	// lfs f18,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f1,f0,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// lfs f14,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f2.f64 = double(temp.f32);
	// stfs f30,4908(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4908, temp.u32);
	// fmuls f30,f11,f18
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// stfs f27,4884(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4884, temp.u32);
	// fmuls f27,f0,f14
	ctx.f27.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f6,668(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// fmuls f6,f2,f4
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f13,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f13.f64 = double(temp.f32);
	// stfs f5,4276(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4276, temp.u32);
	// stfs f3,4004(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4004, temp.u32);
	// stfs f26,3600(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3600, temp.u32);
	// stfs f25,932(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f24,2284(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2284, temp.u32);
	// stfs f23,4920(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4920, temp.u32);
	// stfs f29,928(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// stfs f28,3728(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3728, temp.u32);
	// stfs f20,788(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// stfs f9,3616(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3616, temp.u32);
	// stfs f8,3648(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3648, temp.u32);
	// stfs f15,4180(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4180, temp.u32);
	// stfs f12,2996(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2996, temp.u32);
	// fmr f28,f18
	ctx.f28.f64 = ctx.f18.f64;
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fmr f23,f18
	ctx.f23.f64 = ctx.f18.f64;
	// stfs f12,4684(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4684, temp.u32);
	// lfs f12,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,3660(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3660, temp.u32);
	// lfs f11,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f12.f64 = double(temp.f32);
	// stfs f10,2276(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2276, temp.u32);
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f21,4260(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4260, temp.u32);
	// lfs f18,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f18.f64 = double(temp.f32);
	// lfs f10,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f28,f6,f28
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f28.f64));
	// lfs f11,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f11.f64 = double(temp.f32);
	// lfs f21,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f11,f10,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// stfs f27,360(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmuls f21,f21,f18
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// lfs f27,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// stfs f18,392(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// fmr f18,f27
	ctx.f18.f64 = ctx.f27.f64;
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// lfs f10,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfs f3,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f3.f64 = double(temp.f32);
	// lfs f8,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f8.f64 = double(temp.f32);
	// stfs f19,4524(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4524, temp.u32);
	// fmuls f8,f3,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f8.f64));
	// stfs f17,4268(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4268, temp.u32);
	// stfs f22,4108(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4108, temp.u32);
	// lfs f26,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f26.f64 = double(temp.f32);
	// lfs f22,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f26,f4
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// lfs f19,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f19.f64 = double(temp.f32);
	// lfs f3,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f3.f64 = double(temp.f32);
	// lfs f25,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f3,f6,f3
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// lfs f17,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f25,f25,f22
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f22.f64));
	// stfs f1,388(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f17,f19,f17
	ctx.f17.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// lfs f5,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f5.f64 = double(temp.f32);
	// lfs f14,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f14.f64 = double(temp.f32);
	// lfs f1,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f1.f64 = double(temp.f32);
	// stfs f16,940(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// fmuls f27,f1,f27
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// stfs f26,440(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f19,408(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f31,180(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f5,944(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// stfs f6,152(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f22,136(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f29,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f23,f20,f23
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// lfs f0,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// lfs f5,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f13,f13,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// lfs f16,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f9,f29,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// lfs f26,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// lfs f19,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f26,f0,f26
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f26.f64));
	// lfs f31,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f19,f20,f19
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// lfs f6,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f6.f64 = double(temp.f32);
	// lfs f22,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f18,f6,f18
	ctx.f18.f64 = double(float(ctx.f6.f64 * ctx.f18.f64));
	// stfs f15,556(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// fmuls f15,f15,f4
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f4.f64));
	// stfs f2,148(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f31,f22,f31
	ctx.f31.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// stfs f30,116(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f2,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f30.f64 = double(temp.f32);
	// stfs f12,4916(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4916, temp.u32);
	// fmuls f2,f2,f14
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// stfs f18,4724(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4724, temp.u32);
	// lfs f12,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f12.f64 = double(temp.f32);
	// lfs f18,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f18.f64 = double(temp.f32);
	// stfs f0,1300(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1300, temp.u32);
	// lfs f0,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f0.f64 = double(temp.f32);
	// stfs f9,4508(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4508, temp.u32);
	// stfs f8,3988(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3988, temp.u32);
	// stfs f28,2988(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2988, temp.u32);
	// stfs f16,4636(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4636, temp.u32);
	// fmuls f16,f18,f12
	ctx.f16.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// lfs f9,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f9.f64 = double(temp.f32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// stfs f13,2300(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2300, temp.u32);
	// lfs f8,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f8.f64 = double(temp.f32);
	// lfs f28,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// stfs f11,900(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// stfs f1,656(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// stfs f31,4356(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4356, temp.u32);
	// stfs f5,684(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// fmuls f5,f7,f30
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// stfs f4,2144(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2144, temp.u32);
	// stfs f2,3004(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3004, temp.u32);
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// lfs f31,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f31.f64 = double(temp.f32);
	// lfs f1,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f1.f64 = double(temp.f32);
	// lfs f11,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f11.f64 = double(temp.f32);
	// lfs f2,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f2.f64 = double(temp.f32);
	// lfs f4,1136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1136);
	ctx.f4.f64 = double(temp.f32);
	// stfs f10,1572(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1572, temp.u32);
	// fmuls f30,f13,f4
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f4.f64));
	// stfs f27,5156(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5156, temp.u32);
	// fmuls f27,f9,f28
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f28.f64));
	// stfs f7,1052(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// fmuls f7,f9,f8
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f10,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f0,f11
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfs f29,808(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// fmuls f29,f13,f31
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f3,4748(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4748, temp.u32);
	// fmuls f3,f13,f1
	ctx.f3.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// stfs f6,1588(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1588, temp.u32);
	// fmuls f6,f13,f2
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// stfs f11,528(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmr f11,f10
	ctx.f11.f64 = ctx.f10.f64;
	// lfs f13,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f13.f64 = double(temp.f32);
	// stfs f17,4084(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4084, temp.u32);
	// fmuls f17,f18,f13
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f13.f64));
	// stfs f15,2308(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2308, temp.u32);
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f15,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f15.f64 = double(temp.f32);
	// stfs f14,764(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// fmuls f14,f15,f13
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f13.f64));
	// lfs f13,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f13.f64 = double(temp.f32);
	// stfs f22,748(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f22,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f22.f64 = double(temp.f32);
	// stfs f25,3956(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3956, temp.u32);
	// lfs f25,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f25.f64 = double(temp.f32);
	// stfs f26,3356(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3356, temp.u32);
	// stfs f20,904(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// stfs f31,336(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f26,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f26.f64 = double(temp.f32);
	// lfs f20,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f11,f26,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// lfs f12,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// lfs f31,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f31.f64 = double(temp.f32);
	// stfs f24,3924(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3924, temp.u32);
	// fmuls f24,f25,f10
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// stfs f23,4804(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4804, temp.u32);
	// fmuls f12,f31,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f12.f64));
	// stfs f21,4068(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4068, temp.u32);
	// fmuls f21,f22,f0
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfs f19,4308(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4308, temp.u32);
	// fmuls f19,f20,f0
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// stfs f1,488(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// stfs f25,324(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f23,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f23.f64 = double(temp.f32);
	// lfs f1,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f1.f64 = double(temp.f32);
	// lfs f25,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f25.f64 = double(temp.f32);
	// stfs f18,252(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f18,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f18.f64 = double(temp.f32);
	// stfs f9,152(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f1,f18,f1
	ctx.f1.f64 = double(float(ctx.f18.f64 * ctx.f1.f64));
	// lfs f9,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f9.f64 = double(temp.f32);
	// stfs f11,136(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f11,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f11.f64 = double(temp.f32);
	// stfd f9,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f9.u64);
	// fmuls f9,f9,f23
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f23.f64));
	// stfd f11,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f11.u64);
	// stfs f12,148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f4,204(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f22,220(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// lfs f12,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// lfs f22,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f22.f64 = double(temp.f32);
	// lfs f4,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f25,f25,f22
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f22.f64));
	// stfs f2,180(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f2,f4,f12
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfd f12,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f12.u64);
	// lfs f12,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f12.f64 = double(temp.f32);
	// stfd f0,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f0.u64);
	// fmuls f0,f11,f12
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f11,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,308(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f13,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// stfs f6,3776(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3776, temp.u32);
	// stfs f9,3532(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3532, temp.u32);
	// stfs f17,4188(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4188, temp.u32);
	// stfs f2,4348(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4348, temp.u32);
	// lfs f6,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f2.f64 = double(temp.f32);
	// lfd f9,704(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f17,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f17.f64 = double(temp.f32);
	// stfs f5,4644(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4644, temp.u32);
	// fmuls f5,f13,f23
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f23.f64));
	// stfs f4,1556(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1556, temp.u32);
	// stfs f8,936(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// stfs f11,4788(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4788, temp.u32);
	// lfd f11,720(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f8,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f4.f64 = double(temp.f32);
	// stfs f16,2292(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2292, temp.u32);
	// fmuls f16,f17,f10
	ctx.f16.f64 = double(float(ctx.f17.f64 * ctx.f10.f64));
	// stfs f15,1224(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// stfs f26,1740(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1740, temp.u32);
	// stfs f31,1548(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1548, temp.u32);
	// stfs f6,5164(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5164, temp.u32);
	// fmuls f6,f9,f2
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f15,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f15.f64 = double(temp.f32);
	// lfs f26,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f26.f64 = double(temp.f32);
	// lfs f31,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f31.f64 = double(temp.f32);
	// stfs f1,4292(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4292, temp.u32);
	// stfs f3,3976(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3976, temp.u32);
	// fmuls f3,f4,f11
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// stfs f7,5112(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5112, temp.u32);
	// fmuls f7,f8,f11
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// stfs f5,2332(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2332, temp.u32);
	// fmuls f5,f13,f2
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lfs f9,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f10.f64 = double(temp.f32);
	// lfs f1,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f1.f64 = double(temp.f32);
	// stfs f14,4284(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4284, temp.u32);
	// stfs f15,5080(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5080, temp.u32);
	// stfs f26,4496(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4496, temp.u32);
	// stfs f31,4876(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4876, temp.u32);
	// stfs f18,3348(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3348, temp.u32);
	// stfs f25,4324(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4324, temp.u32);
	// stfs f22,3012(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3012, temp.u32);
	// stfs f24,3920(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3920, temp.u32);
	// stfs f21,4504(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4504, temp.u32);
	// stfs f30,4888(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4888, temp.u32);
	// stfs f20,1216(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// stfs f19,5128(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5128, temp.u32);
	// stfs f12,1380(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1380, temp.u32);
	// stfs f0,5152(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5152, temp.u32);
	// stfs f29,4436(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4436, temp.u32);
	// stfs f28,876(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f27,3868(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3868, temp.u32);
	// lfd f12,1696(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// lfs f11,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f27,f9,f12
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfd f0,1096(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// fmuls f28,f11,f12
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// fmuls f25,f10,f12
	ctx.f25.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// lfs f19,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f30,f0,f1
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// lfs f31,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f18,f19,f13
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// lfs f12,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f29,f0,f31
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// stfs f1,360(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmr f1,f12
	ctx.f1.f64 = ctx.f12.f64;
	// stfs f31,532(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// fmuls f10,f10,f12
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f12.f64));
	// stfs f19,388(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// lfs f19,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f19,f31
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// stfs f2,408(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f31,148(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f31,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f31,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f2,152(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfs f22,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f22.f64 = double(temp.f32);
	// lfs f2,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f21,f22,f13
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// lfs f11,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f26,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f24,f11,f13
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f15,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f26,f26,f13
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// lfs f14,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f13,f14,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f13.f64));
	// stfs f2,180(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f20,f20,f15
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// stfs f17,392(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f19,136(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f19,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f17.f64 = double(temp.f32);
	// lfs f2,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f12,f17,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f12.f64));
	// lfs f0,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f15.f64 = double(temp.f32);
	// stfs f22,364(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f31,220(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f1,204(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f2,464(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fmuls f2,f2,f19
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f19.f64));
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f30,5172(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5172, temp.u32);
	// lfs f13,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f9,f9,f13
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// lfs f11,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f11.f64 = double(temp.f32);
	// lfs f22,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f22.f64 = double(temp.f32);
	// lfs f31,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f1,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// stfs f16,2316(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2316, temp.u32);
	// fmuls f16,f0,f15
	ctx.f16.f64 = double(float(ctx.f0.f64 * ctx.f15.f64));
	// stfs f23,412(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmuls f23,f11,f0
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f2,252(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f15,f19,f11
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f11.f64));
	// stfs f24,3784(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3784, temp.u32);
	// fmuls f24,f0,f13
	ctx.f24.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f29,3760(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3760, temp.u32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f2,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f29,f30,f13
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// stfs f5,3768(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3768, temp.u32);
	// stfs f6,5160(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5160, temp.u32);
	// stfs f8,1208(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// stfs f7,5072(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5072, temp.u32);
	// lfs f7,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f7.f64 = double(temp.f32);
	// fmr f5,f13
	ctx.f5.f64 = ctx.f13.f64;
	// stfs f30,440(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmr f30,f7
	ctx.f30.f64 = ctx.f7.f64;
	// lfs f6,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f6.f64 = double(temp.f32);
	// stfs f14,1044(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// stfs f6,1088(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// stfs f10,4528(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4528, temp.u32);
	// lfs f6,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f6.f64 = double(temp.f32);
	// lfs f14,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f14.f64 = double(temp.f32);
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// stfs f6,640(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// fmuls f6,f14,f10
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f10.f64));
	// stfs f12,2380(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2380, temp.u32);
	// stfs f14,572(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// stfs f18,5180(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5180, temp.u32);
	// lfs f18,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f18.f64 = double(temp.f32);
	// lfs f12,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f12.f64 = double(temp.f32);
	// lfs f14,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f14.f64 = double(temp.f32);
	// stfs f26,4376(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4376, temp.u32);
	// fmuls f30,f14,f30
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f30.f64));
	// lfs f26,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f26.f64 = double(temp.f32);
	// stfs f12,1720(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1720, temp.u32);
	// stfs f4,1628(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1628, temp.u32);
	// stfs f17,600(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// fmuls f17,f18,f5
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f5.f64));
	// lfs f12,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f12.f64 = double(temp.f32);
	// lfs f4,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f4.f64 = double(temp.f32);
	// stfs f18,336(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f14,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f18.f64 = double(temp.f32);
	// stfs f12,3044(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3044, temp.u32);
	// fmuls f18,f18,f14
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f14.f64));
	// stfs f19,308(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f12,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f12.f64 = double(temp.f32);
	// lfs f19,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f19.f64 = double(temp.f32);
	// stfs f1,204(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmr f14,f19
	ctx.f14.f64 = ctx.f19.f64;
	// fmr f1,f7
	ctx.f1.f64 = ctx.f7.f64;
	// stfs f21,3780(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3780, temp.u32);
	// lfs f21,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f21.f64 = double(temp.f32);
	// stfs f20,1736(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1736, temp.u32);
	// fmuls f20,f21,f5
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// stfs f12,2388(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2388, temp.u32);
	// stfs f21,436(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// stfs f28,2324(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2324, temp.u32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f28.f64 = double(temp.f32);
	// lfs f12,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f19,f28,f19
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f19.f64));
	// lfs f21,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f21.f64 = double(temp.f32);
	// stfs f15,180(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f3,3020(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3020, temp.u32);
	// stfs f31,152(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfs f8,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f31.f64 = double(temp.f32);
	// lfs f3,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f14,f31,f14
	ctx.f14.f64 = double(float(ctx.f31.f64 * ctx.f14.f64));
	// lfs f13,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f31,f1
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f1.f64));
	// lfs f15,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f13,f2,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// stfs f23,3928(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3928, temp.u32);
	// fmuls f23,f22,f12
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f12.f64));
	// stfs f25,4520(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4520, temp.u32);
	// fmuls f25,f26,f0
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// stfs f21,488(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmuls f21,f21,f0
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// stfs f29,324(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f29,f4,f0
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f27,3800(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3800, temp.u32);
	// fmuls f27,f28,f7
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// stfs f8,2340(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2340, temp.u32);
	// fmuls f8,f4,f7
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// stfd f0,2216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 2216, ctx.f0.u64);
	// fmuls f15,f3,f15
	ctx.f15.f64 = double(float(ctx.f3.f64 * ctx.f15.f64));
	// stfd f5,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, ctx.f5.u64);
	// fmuls f12,f16,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f12.f64));
	// lfs f11,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f0.f64 = double(temp.f32);
	// lfs f5,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f5.f64 = double(temp.f32);
	// stfs f24,148(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f6,136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfd f11,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f11.u64);
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// stfd f10,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f10.u64);
	// stfd f13,1696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1696, ctx.f13.u64);
	// lfs f6,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f6.f64 = double(temp.f32);
	// stfs f12,564(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmuls f12,f16,f6
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f6.f64));
	// stfs f16,220(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// lfs f5,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f5.f64 = double(temp.f32);
	// lfs f16,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f5,f5,f11
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64));
	// fmuls f16,f16,f11
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f11.f64));
	// lfs f24,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f24.f64 = double(temp.f32);
	// lfs f11,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f24,f24,f10
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// stfs f8,2364(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2364, temp.u32);
	// fmuls f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// stfs f1,2096(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2096, temp.u32);
	// lfs f10,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f10.f64 = double(temp.f32);
	// stfs f12,4052(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4052, temp.u32);
	// stfs f11,2436(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2436, temp.u32);
	// stfd f6,3696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3696, ctx.f6.u64);
	// lfs f1,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f1.f64 = double(temp.f32);
	// lfs f8,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,1984(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1984, temp.u32);
	// stfs f9,148(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfd f11,704(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f6,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f6.f64 = double(temp.f32);
	// lfs f12,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f6,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f8,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f9.f64 = double(temp.f32);
	// stfs f2,3472(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3472, temp.u32);
	// fmuls f2,f7,f10
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// stfs f30,4836(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4836, temp.u32);
	// fmuls f30,f8,f11
	ctx.f30.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// stfs f28,1128(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// fmuls f28,f11,f12
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// stfs f29,2444(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2444, temp.u32);
	// fmuls f29,f9,f11
	ctx.f29.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// stfs f4,1124(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// lfs f11,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f11.f64 = double(temp.f32);
	// lfs f4,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f4.f64 = double(temp.f32);
	// stfs f10,1976(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1976, temp.u32);
	// stfs f5,2420(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2420, temp.u32);
	// stfs f13,3076(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3076, temp.u32);
	// lfd f10,720(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f12,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f12.f64 = double(temp.f32);
	// lfd f5,1096(r1)
	ctx.f5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1096);
	// lfd f13,1696(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 1696);
	// stfs f25,2452(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2452, temp.u32);
	// fmuls f25,f11,f4
	ctx.f25.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// stfs f31,528(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// fmuls f31,f1,f10
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// stfs f3,628(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// stfs f23,2356(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2356, temp.u32);
	// stfs f27,3036(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3036, temp.u32);
	// fmuls f27,f11,f5
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f5.f64));
	// stfs f26,664(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// fmuls f26,f12,f11
	ctx.f26.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f24,2468(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2468, temp.u32);
	// fmuls f24,f11,f13
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f3,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f3.f64 = double(temp.f32);
	// lfs f4,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f4.f64 = double(temp.f32);
	// lfs f23,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f23.f64 = double(temp.f32);
	// stfs f18,3052(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3052, temp.u32);
	// stfs f19,2404(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2404, temp.u32);
	// stfs f14,1744(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1744, temp.u32);
	// stfs f15,1752(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1752, temp.u32);
	// stfs f22,1304(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1304, temp.u32);
	// stfs f20,3620(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3620, temp.u32);
	// stfs f17,3028(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3028, temp.u32);
	// stfs f0,2372(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2372, temp.u32);
	// stfs f6,1188(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// stfs f16,3060(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3060, temp.u32);
	// stfs f21,1760(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1760, temp.u32);
	// lfs f11,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f23,f5
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// lfs f20,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f11,f20
	ctx.f20.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// lfs f11,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,116(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f6,f11,f13
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f6,136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f2,388(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// lfs f2,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f2.f64 = double(temp.f32);
	// stfs f25,152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f25,f4,f12
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// lfs f18,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f4,f8,f4
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfd f10,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f10.u64);
	// fmr f10,f18
	ctx.f10.f64 = ctx.f18.f64;
	// stfs f23,364(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f23,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f23.f64 = double(temp.f32);
	// stfs f31,3084(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3084, temp.u32);
	// lfs f31,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f31.f64 = double(temp.f32);
	// stfs f26,180(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f26,f3,f12
	ctx.f26.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// stfs f5,2428(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2428, temp.u32);
	// fmuls f5,f9,f3
	ctx.f5.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// lfs f22,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f3,f31,f18
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f18.f64));
	// lfs f16,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f22,f13
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// stfs f27,792(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// fmuls f15,f16,f13
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f13.f64));
	// lfd f0,2216(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 2216);
	// lfs f27,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f19,f1,f0
	ctx.f19.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f14,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f13,f27,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// stfs f21,3396(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3396, temp.u32);
	// fmuls f21,f31,f14
	ctx.f21.f64 = double(float(ctx.f31.f64 * ctx.f14.f64));
	// lfs f18,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f18,672(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// lfs f18,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f17.f64 = double(temp.f32);
	// stfs f24,652(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f1,360(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f2,392(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// fmuls f2,f2,f23
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// stfs f28,2184(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2184, temp.u32);
	// stfs f15,2460(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2460, temp.u32);
	// stfs f18,2476(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2476, temp.u32);
	// stfs f27,1184(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// lfs f24,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f24.f64 = double(temp.f32);
	// lfs f1,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f24,f31,f24
	ctx.f24.f64 = double(float(ctx.f31.f64 * ctx.f24.f64));
	// lfs f11,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f1,f12
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// lfd f6,3696(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3696);
	// lfs f28,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f27.f64 = double(temp.f32);
	// lfs f15,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f14.f64 = double(temp.f32);
	// stfs f22,744(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmuls f22,f31,f17
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// stfs f19,3548(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3548, temp.u32);
	// fmuls f19,f31,f6
	ctx.f19.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// stfs f20,3068(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3068, temp.u32);
	// fmuls f20,f28,f23
	ctx.f20.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// stfs f16,1136(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// fmuls f17,f18,f10
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f10.f64));
	// stfs f2,3404(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3404, temp.u32);
	// fmuls f16,f27,f0
	ctx.f16.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfs f30,3092(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3092, temp.u32);
	// fmuls f14,f15,f14
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// lfs f30,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f2,f11,f0
	ctx.f2.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f13,2500(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2500, temp.u32);
	// stfs f26,2484(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2484, temp.u32);
	// stfs f25,3628(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3628, temp.u32);
	// stfs f29,2492(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2492, temp.u32);
	// stfs f4,2524(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2524, temp.u32);
	// stfs f5,1776(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1776, temp.u32);
	// stfs f7,2016(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2016, temp.u32);
	// fmuls f5,f18,f0
	ctx.f5.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// lfs f7,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f10,f18,f6
	ctx.f10.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f0,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f0.f64 = double(temp.f32);
	// stfs f3,2160(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2160, temp.u32);
	// fmuls f3,f8,f0
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f30,1176(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f29,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f0.f64 = double(temp.f32);
	// stfs f30,1808(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1808, temp.u32);
	// fmuls f30,f15,f0
	ctx.f30.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// fmuls f26,f29,f0
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// stfs f14,680(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// lfs f14,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f14.f64 = double(temp.f32);
	// stfs f22,2112(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2112, temp.u32);
	// fmr f22,f0
	ctx.f22.f64 = ctx.f0.f64;
	// stfs f7,220(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmr f7,f14
	ctx.f7.f64 = ctx.f14.f64;
	// lfs f13,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f13.f64 = double(temp.f32);
	// lfs f25,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f8,f8,f13
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f10,116(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f13,f9,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// stfs f16,532(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// fmuls f9,f9,f25
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f25.f64));
	// lfd f10,720(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f16,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f16.f64 = double(temp.f32);
	// stfs f21,1392(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1392, temp.u32);
	// stfs f24,2000(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2000, temp.u32);
	// lfs f21,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f21.f64 = double(temp.f32);
	// lfs f24,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f24.f64 = double(temp.f32);
	// stfs f19,1376(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1376, temp.u32);
	// stfs f2,412(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// stfs f17,464(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fmuls f17,f16,f10
	ctx.f17.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// lfs f19,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f10,f24,f10
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// lfs f25,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f2,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f25,f27,f25
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// stfs f20,2588(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2588, temp.u32);
	// fmuls f20,f21,f23
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// stfs f1,2572(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2572, temp.u32);
	// fmuls f2,f27,f2
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// lfs f1,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f1.f64 = double(temp.f32);
	// stfs f21,308(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// stfs f29,180(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f29,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f29.f64 = double(temp.f32);
	// lfs f21,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f29,f22
	ctx.f22.f64 = double(float(ctx.f29.f64 * ctx.f22.f64));
	// lfs f24,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f14,f29,f14
	ctx.f14.f64 = double(float(ctx.f29.f64 * ctx.f14.f64));
	// stfs f31,2008(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2008, temp.u32);
	// fmuls f24,f24,f21
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// stfs f30,556(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// fmuls f30,f30,f1
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f28,252(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f15,204(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f18,152(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f27,148(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f31,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f31,f1,f31
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f15,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f11,f28,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f18,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f27.f64 = double(temp.f32);
	// stfs f30,136(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f7,f27,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f7.f64));
	// stfs f29,324(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// stfs f8,1848(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1848, temp.u32);
	// fmuls f8,f15,f18
	ctx.f8.f64 = double(float(ctx.f15.f64 * ctx.f18.f64));
	// lfs f30,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f29.f64 = double(temp.f32);
	// lfs f21,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f21.f64 = double(temp.f32);
	// stfs f10,1904(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1904, temp.u32);
	// lfs f10,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f10.f64 = double(temp.f32);
	// stfs f7,1912(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1912, temp.u32);
	// fmr f7,f10
	ctx.f7.f64 = ctx.f10.f64;
	// stfs f4,1372(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1372, temp.u32);
	// fmuls f4,f16,f0
	ctx.f4.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// stfs f4,2192(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2192, temp.u32);
	// lfs f4,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f4.f64 = double(temp.f32);
	// lfs f18,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f18.f64 = double(temp.f32);
	// stfs f9,3148(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3148, temp.u32);
	// fmuls f9,f18,f29
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f29.f64));
	// stfs f12,2200(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2200, temp.u32);
	// fmuls f6,f18,f6
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// lfs f12,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,2128(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2128, temp.u32);
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f10,f7
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f7.f64));
	// stfs f23,624(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// fmr f7,f4
	ctx.f7.f64 = ctx.f4.f64;
	// stfs f17,3436(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3436, temp.u32);
	// fmuls f23,f31,f26
	ctx.f23.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// stfs f12,5120(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5120, temp.u32);
	// lfs f15,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f12,f1,f21
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// lfs f17,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f17.f64 = double(temp.f32);
	// stfs f2,1120(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// fmuls f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// stfs f23,2088(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2088, temp.u32);
	// lfs f2,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f2.f64 = double(temp.f32);
	// lfs f21,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f23.f64 = double(temp.f32);
	// stfs f20,2040(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2040, temp.u32);
	// fmuls f20,f1,f30
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// fmuls f7,f4,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// stfs f13,1864(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1864, temp.u32);
	// fmr f4,f29
	ctx.f4.f64 = ctx.f29.f64;
	// stfs f14,3428(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3428, temp.u32);
	// fmuls f13,f27,f0
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// stfs f5,2596(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2596, temp.u32);
	// lfs f14,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// lfs f15,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f5,f2,f26
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// stfs f28,3480(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3480, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f25,2032(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2032, temp.u32);
	// stfs f20,1416(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1416, temp.u32);
	// stfs f31,2892(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2892, temp.u32);
	// stfs f22,2136(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2136, temp.u32);
	// stfs f19,1312(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1312, temp.u32);
	// stfs f24,1888(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1888, temp.u32);
	// stfs f13,1400(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1400, temp.u32);
	// stfs f2,688(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// lfs f13,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f13.f64 = double(temp.f32);
	// lfs f31,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f29,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// lfs f28,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f4,f13,f4
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f4.f64));
	// lfs f25,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f29,f13,f29
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// lfs f24,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f28,f24,f28
	ctx.f28.f64 = double(float(ctx.f24.f64 * ctx.f28.f64));
	// lfs f20,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// lfs f21,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f13,f21
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// lfs f2,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f20,f19,f20
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// stfs f3,2048(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2048, temp.u32);
	// fmuls f3,f9,f26
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f26.f64));
	// stfs f11,2056(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2056, temp.u32);
	// stfs f8,1408(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1408, temp.u32);
	// fmuls f8,f18,f30
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f30.f64));
	// stfs f9,440(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmuls f9,f2,f26
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// lfs f11,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f11.f64 = double(temp.f32);
	// lfs f14,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f14.f64 = double(temp.f32);
	// stfs f2,436(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f2,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f14,f2
	ctx.f2.f64 = double(float(ctx.f14.f64 * ctx.f2.f64));
	// stfs f19,360(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// lfs f14,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f14,f19
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// stfs f16,392(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// lfs f14,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f14.f64 = double(temp.f32);
	// stfs f13,4704(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4704, temp.u32);
	// lfs f16,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f16.f64 = double(temp.f32);
	// stfs f0,1960(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1960, temp.u32);
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// stfs f10,1920(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1920, temp.u32);
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f10.f64 = double(temp.f32);
	// stfs f30,364(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,2756(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2756, temp.u32);
	// fmuls f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f14,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f14.f64 = double(temp.f32);
	// lfs f30,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f30.f64 = double(temp.f32);
	// stfs f5,3236(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3236, temp.u32);
	// fmuls f30,f30,f14
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f0,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// lfs f5,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f5.f64 = double(temp.f32);
	// stfs f27,336(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// stfs f4,1952(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1952, temp.u32);
	// fmuls f4,f13,f0
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f27,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f27.f64 = double(temp.f32);
	// stfs f30,1852(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1852, temp.u32);
	// fmuls f27,f1,f27
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f0,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f30.f64 = double(temp.f32);
	// stfs f22,408(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// fmuls f30,f30,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f29,1456(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1456, temp.u32);
	// fmuls f29,f13,f0
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f22,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f22.f64 = double(temp.f32);
	// stfs f27,388(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f22,f11,f22
	ctx.f22.f64 = double(float(ctx.f11.f64 * ctx.f22.f64));
	// stfs f25,1472(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1472, temp.u32);
	// lfs f13,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f13.f64 = double(temp.f32);
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// lfs f14,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f14.f64 = double(temp.f32);
	// lfs f0,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f0.f64 = double(temp.f32);
	// lfs f25,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f25.f64 = double(temp.f32);
	// stfs f7,1440(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1440, temp.u32);
	// fmuls f25,f25,f13
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// stfs f31,2208(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2208, temp.u32);
	// stfs f28,1968(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1968, temp.u32);
	// stfs f2,3468(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3468, temp.u32);
	// lfs f2,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f31.f64 = double(temp.f32);
	// lfs f7,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f7.f64 = double(temp.f32);
	// lfs f28,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f28.f64 = double(temp.f32);
	// stfs f12,2176(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2176, temp.u32);
	// fmuls f12,f14,f27
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// stfs f23,2732(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2732, temp.u32);
	// fmuls f23,f0,f13
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f21,3220(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3220, temp.u32);
	// fmuls f27,f28,f26
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// stfs f8,2748(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2748, temp.u32);
	// fmuls f8,f18,f7
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f7.f64));
	// stfs f3,1788(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1788, temp.u32);
	// fmuls f3,f6,f26
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// stfs f11,1460(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1460, temp.u32);
	// fmuls f11,f31,f2
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f22,2764(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2764, temp.u32);
	// lfs f22,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f22.f64 = double(temp.f32);
	// lfs f13,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f13.f64 = double(temp.f32);
	// lfs f21,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f21.f64 = double(temp.f32);
	// stfs f20,1764(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1764, temp.u32);
	// stfs f17,2772(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2772, temp.u32);
	// stfs f15,3588(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3588, temp.u32);
	// stfs f19,1488(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1488, temp.u32);
	// stfs f16,2780(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2780, temp.u32);
	// lfs f19,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f17,f22,f13
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// fmr f13,f19
	ctx.f13.f64 = ctx.f19.f64;
	// lfs f16,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f16.f64 = double(temp.f32);
	// stfs f24,180(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f20,f21,f26
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// stfs f18,204(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f24,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f24.f64 = double(temp.f32);
	// lfs f15,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f24,f18
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f18.f64));
	// stfs f24,308(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f24,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f24.f64 = double(temp.f32);
	// stfs f7,148(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f7,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f19,f19,f13
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// lfs f13,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f16,f16,f13
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f13.f64));
	// lfs f13,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f15,f15,f13
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f13.f64));
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f24,f13,f24
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f24.f64));
	// lfs f13,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f13.f64 = double(temp.f32);
	// stfs f1,152(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f7,f7,f13
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// stfs f23,136(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f13,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f13.f64 = double(temp.f32);
	// fmr f23,f13
	ctx.f23.f64 = ctx.f13.f64;
	// stfs f11,1868(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1868, temp.u32);
	// fmr f11,f13
	ctx.f11.f64 = ctx.f13.f64;
	// lfs f1,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f1.f64 = double(temp.f32);
	// stfs f9,1480(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1480, temp.u32);
	// lfs f9,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f9.f64 = double(temp.f32);
	// stfs f12,1844(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1844, temp.u32);
	// stfs f31,1500(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1500, temp.u32);
	// stfs f2,1292(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1292, temp.u32);
	// stfs f5,3260(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3260, temp.u32);
	// stfs f7,3492(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3492, temp.u32);
	// lfs f2,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f1,f23
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f23.f64));
	// stfs f1,1220(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// fmuls f23,f0,f13
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f0,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f0,f11
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f7,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f0,f9
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// lfs f0,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f31.f64 = double(temp.f32);
	// stfs f10,1496(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1496, temp.u32);
	// lfs f5,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f5.f64 = double(temp.f32);
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f2,f5
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// stfs f4,1876(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1876, temp.u32);
	// stfs f8,3484(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3484, temp.u32);
	// stfs f6,788(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// fmuls f6,f1,f0
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f3,2812(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2812, temp.u32);
	// fmuls f3,f2,f31
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// stfs f30,1956(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1956, temp.u32);
	// stfs f12,1328(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1328, temp.u32);
	// fmuls f12,f5,f7
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// lfs f8,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f30.f64 = double(temp.f32);
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f14,1644(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1644, temp.u32);
	// stfs f22,1436(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1436, temp.u32);
	// stfs f17,1804(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1804, temp.u32);
	// stfs f19,1504(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1504, temp.u32);
	// stfs f28,816(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// stfs f27,3268(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3268, temp.u32);
	// stfs f16,2828(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2828, temp.u32);
	// stfs f15,1916(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1916, temp.u32);
	// stfs f21,740(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// stfs f26,1144(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// stfs f20,2820(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2820, temp.u32);
	// stfs f18,1940(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1940, temp.u32);
	// stfs f29,1924(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1924, temp.u32);
	// stfs f24,1112(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// stfs f25,1252(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// lfs f20,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f13,f30,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// fmuls f20,f1,f20
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f20.f64));
	// stfs f3,336(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// stfs f1,252(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f3,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f1.f64 = double(temp.f32);
	// lfs f16,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f1,f3,f1
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// lfs f15,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f15.f64 = double(temp.f32);
	// lfs f25,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f15,f16,f15
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f15.f64));
	// stfs f3,220(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f0,f25,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f3,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f29.f64 = double(temp.f32);
	// lfs f22,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f22.f64 = double(temp.f32);
	// lfs f30,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f30,f29,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// stfs f16,204(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f24,f24,f22
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// lfs f28,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f27,f5,f28
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f28.f64));
	// lfs f17,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f17.f64 = double(temp.f32);
	// lfs f14,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f17,f18,f17
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// lfs f16,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f4,f14
	ctx.f14.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// stfs f25,152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f25,f5,f3
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// lfs f19,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f16,f4,f16
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// lfs f29,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f5,f29
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f29.f64));
	// lfs f21,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f5,f26
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// lfs f22,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f21,f8
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// stfs f18,180(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f22,f19,f22
	ctx.f22.f64 = double(float(ctx.f19.f64 * ctx.f22.f64));
	// stfs f25,148(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f4,136(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f28,116(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f28,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f18.f64 = double(temp.f32);
	// lfs f25,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f18,f5,f18
	ctx.f18.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// lfs f4,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f25,f28,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// stfs f13,3744(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3744, temp.u32);
	// stfs f12,2860(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2860, temp.u32);
	// fmuls f12,f10,f4
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// lfs f13,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f13.f64 = double(temp.f32);
	// stfs f23,684(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// stfs f30,2852(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2852, temp.u32);
	// stfs f11,736(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// stfs f9,668(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// stfs f7,1264(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1264, temp.u32);
	// stfs f27,1080(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// stfs f29,3284(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3284, temp.u32);
	// stfs f26,1056(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// stfs f0,3904(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3904, temp.u32);
	// stfs f6,5144(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5144, temp.u32);
	// stfs f24,3824(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3824, temp.u32);
	// stfs f8,1580(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1580, temp.u32);
	// stfs f21,2868(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2868, temp.u32);
	// stfs f19,764(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f22,2876(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2876, temp.u32);
	// stfs f20,3500(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3500, temp.u32);
	// stfs f31,488(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// stfs f5,656(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// stfs f3,1180(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// stfs f28,1944(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1944, temp.u32);
	// stfs f17,4224(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4224, temp.u32);
	// stfs f15,4240(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4240, temp.u32);
	// stfs f14,4344(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4344, temp.u32);
	// stfs f1,2900(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2900, temp.u32);
	// stfs f16,2908(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2908, temp.u32);
	// lfs f31,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f31.f64 = double(temp.f32);
	// fmr f29,f31
	ctx.f29.f64 = ctx.f31.f64;
	// lfs f19,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f19.f64 = double(temp.f32);
	// fmr f16,f19
	ctx.f16.f64 = ctx.f19.f64;
	// lfs f27,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f27.f64 = double(temp.f32);
	// lfs f23,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f23.f64 = double(temp.f32);
	// lfs f1,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// lfs f24,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f3,f1,f0
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f30,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f24,f23,f24
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f24.f64));
	// stfs f23,136(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f1,148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f26,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f27,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// lfs f6,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f6.f64 = double(temp.f32);
	// lfs f14,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f5,f18,f6
	ctx.f5.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// lfs f22,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f23,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f16,f14,f16
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// lfs f9,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f9.f64 = double(temp.f32);
	// lfs f4,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f8,f2,f9
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// lfs f28,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f21,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f28,f26,f28
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// lfs f1,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f21,f22,f21
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f21.f64));
	// lfs f17,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f1,f1,f23
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f23.f64));
	// lfs f7,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f19,f17,f19
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// lfs f11,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f11.f64 = double(temp.f32);
	// lfs f20,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f31,f11,f31
	ctx.f31.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// stfs f2,572(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// fmuls f20,f17,f20
	ctx.f20.f64 = double(float(ctx.f17.f64 * ctx.f20.f64));
	// stfs f6,528(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// stfs f14,408(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f25,364(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f26,324(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// stfs f22,152(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f30,116(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f12,3300(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3300, temp.u32);
	// lfs f2,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f2.f64 = double(temp.f32);
	// lfs f25,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f25.f64 = double(temp.f32);
	// lfs f15,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f15.f64 = double(temp.f32);
	// lfs f6,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f15,f10,f15
	ctx.f15.f64 = double(float(ctx.f10.f64 * ctx.f15.f64));
	// lfs f14,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f6,f10,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f26,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f14,f14,f7
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f7.f64));
	// lfs f22,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f26,f13,f26
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// lfs f30,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// lfs f23,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f30,f13,f30
	ctx.f30.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// lfs f12,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f12.f64 = double(temp.f32);
	// stfs f2,1064(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// fmuls f2,f18,f2
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f2.f64));
	// stfs f25,392(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// fmuls f25,f18,f25
	ctx.f25.f64 = double(float(ctx.f18.f64 * ctx.f25.f64));
	// stfs f4,4448(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4448, temp.u32);
	// fmuls f23,f12,f23
	ctx.f23.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// stfs f11,1072(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// stfs f31,4464(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4464, temp.u32);
	// stfs f29,4256(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4256, temp.u32);
	// stfs f28,4416(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4416, temp.u32);
	// stfs f27,4480(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4480, temp.u32);
	// stfs f24,5168(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5168, temp.u32);
	// stfs f21,2884(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2884, temp.u32);
	// stfs f20,3912(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3912, temp.u32);
	// stfs f17,360(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f19,3652(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3652, temp.u32);
	// lfs f4,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f4.f64 = double(temp.f32);
	// lfs f19,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f19.f64 = double(temp.f32);
	// stfs f18,3840(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3840, temp.u32);
	// fmr f18,f19
	ctx.f18.f64 = ctx.f19.f64;
	// stfs f3,4456(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4456, temp.u32);
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// stfs f5,4248(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4248, temp.u32);
	// lfs f5,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f5.f64 = double(temp.f32);
	// stfs f1,2916(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2916, temp.u32);
	// fmuls f5,f5,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f1,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f1.f64 = double(temp.f32);
	// lfs f4,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f4.f64 = double(temp.f32);
	// stfs f8,4232(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4232, temp.u32);
	// fmuls f4,f4,f1
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// stfs f7,1000(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// stfs f14,3604(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3604, temp.u32);
	// stfs f25,4440(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4440, temp.u32);
	// stfs f26,1980(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1980, temp.u32);
	// lfs f14,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f14.f64 = double(temp.f32);
	// lfs f8,1608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1608);
	ctx.f8.f64 = double(temp.f32);
	// lfs f28,1600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1600);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f24.f64 = double(temp.f32);
	// lfs f7,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f7.f64 = double(temp.f32);
	// lfs f26,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f7,f8,f7
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f25,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f28,f26
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f21,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f25,f24
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// lfs f17,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f21,f19
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// lfs f29,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f17,f14,f17
	ctx.f17.f64 = double(float(ctx.f14.f64 * ctx.f17.f64));
	// lfs f27,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f27.f64 = double(temp.f32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// stfs f15,2228(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2228, temp.u32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// stfs f9,1424(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1424, temp.u32);
	// stfs f6,4336(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4336, temp.u32);
	// stfs f10,1972(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1972, temp.u32);
	// stfs f22,3308(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3308, temp.u32);
	// stfs f30,4400(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4400, temp.u32);
	// stfs f8,412(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// stfs f16,3292(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3292, temp.u32);
	// stfs f2,4216(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4216, temp.u32);
	// stfs f0,4472(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4472, temp.u32);
	// stfs f14,388(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// stfs f28,180(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f0,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f18,f11,f18
	ctx.f18.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// lfs f9,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f13,f10
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f6,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f9,f13,f9
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// lfs f2,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f6,f6,f13
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f31,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f3,f2,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// lfs f30,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f13,f31
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// lfs f22,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f24,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f20,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f24,f20,f24
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f24.f64));
	// lfs f16,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f19,f16,f19
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// lfs f14,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f14.f64 = double(temp.f32);
	// lfs f28,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f29,136(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f8,f28,f8
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// lfs f29,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,116(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f12,532(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// stfs f2,3664(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3664, temp.u32);
	// stfs f1,4744(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4744, temp.u32);
	// lfs f12,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f12.f64 = double(temp.f32);
	// lfs f2,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f1.f64 = double(temp.f32);
	// stfs f11,1464(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1464, temp.u32);
	// stfs f7,3332(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3332, temp.u32);
	// fmuls f7,f1,f12
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f4,4736(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4736, temp.u32);
	// stfs f8,3324(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3324, temp.u32);
	// fmuls f8,f2,f12
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f4,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f4.f64 = double(temp.f32);
	// lfs f11,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f11.f64 = double(temp.f32);
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// stfs f3,4648(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4648, temp.u32);
	// fmuls f3,f11,f4
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// stfs f13,1204(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// fmr f13,f12
	ctx.f13.f64 = ctx.f12.f64;
	// lfs f11,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f11.f64 = double(temp.f32);
	// stfs f31,4824(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4824, temp.u32);
	// fmuls f31,f4,f11
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// fmr f11,f12
	ctx.f11.f64 = ctx.f12.f64;
	// stfs f26,4792(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4792, temp.u32);
	// lfs f26,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f26.f64 = double(temp.f32);
	// stfs f25,3516(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3516, temp.u32);
	// stfs f24,2236(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2236, temp.u32);
	// lfs f24,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f24.f64 = double(temp.f32);
	// stfs f23,3316(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3316, temp.u32);
	// stfs f21,4408(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4408, temp.u32);
	// fmuls f25,f26,f13
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// lfs f13,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f21,f24,f13
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// stfs f22,4992(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4992, temp.u32);
	// stfs f6,4424(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4424, temp.u32);
	// fmuls f23,f24,f11
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f11.f64));
	// lfs f22,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f22.f64 = double(temp.f32);
	// lfs f13,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f11.f64 = double(temp.f32);
	// lfs f6,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f6.f64 = double(temp.f32);
	// stfs f10,3508(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3508, temp.u32);
	// stfs f18,2924(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2924, temp.u32);
	// fmuls f18,f22,f13
	ctx.f18.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// stfs f17,4656(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4656, temp.u32);
	// fmuls f17,f11,f6
	ctx.f17.f64 = double(float(ctx.f11.f64 * ctx.f6.f64));
	// lfs f29,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f29.f64 = double(temp.f32);
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// stfs f16,464(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// lfs f16,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f16.f64 = double(temp.f32);
	// lfs f13,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f11.f64 = double(temp.f32);
	// stfs f9,4856(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4856, temp.u32);
	// fmuls f9,f10,f29
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// stfs f0,920(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// stfs f20,3388(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3388, temp.u32);
	// stfs f14,2508(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2508, temp.u32);
	// fmuls f14,f16,f11
	ctx.f14.f64 = double(float(ctx.f16.f64 * ctx.f11.f64));
	// stfs f15,2940(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2940, temp.u32);
	// fmuls f15,f16,f13
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f13.f64));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f20.f64 = double(temp.f32);
	// lfs f13,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f24,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// stfs f27,2024(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2024, temp.u32);
	// fmuls f27,f2,f0
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f19,2932(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2932, temp.u32);
	// fmuls f19,f20,f0
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f0.f64));
	// stfs f28,564(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmuls f28,f29,f0
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// stfs f22,324(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmr f22,f0
	ctx.f22.f64 = ctx.f0.f64;
	// stfs f5,3704(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3704, temp.u32);
	// fmuls f11,f1,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// stfs f30,4784(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4784, temp.u32);
	// stfd f0,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f0.u64);
	// lfs f5,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f5.f64 = double(temp.f32);
	// lfs f30,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f0.f64 = double(temp.f32);
	// stfs f16,308(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f13,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f13.f64 = double(temp.f32);
	// stfs f4,220(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f4,f13,f5
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// stfs f20,252(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfd f13,720(r1)
	PPC_STORE_U64(ctx.r1.u32 + 720, ctx.f13.u64);
	// stfs f11,136(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f13,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f13.f64 = double(temp.f32);
	// lfs f20,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f20.f64 = double(temp.f32);
	// stfs f13,148(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f20,f20,f30
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// stfs f20,152(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f20,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f20.f64 = double(temp.f32);
	// stfs f20,204(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f9,2948(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2948, temp.u32);
	// lfs f11,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f11.f64 = double(temp.f32);
	// stfs f10,1756(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1756, temp.u32);
	// fmuls f10,f29,f11
	ctx.f10.f64 = double(float(ctx.f29.f64 * ctx.f11.f64));
	// stfs f13,2240(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2240, temp.u32);
	// stfs f10,2232(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2232, temp.u32);
	// lfs f10,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f10.f64 = double(temp.f32);
	// stfs f6,748(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// stfs f8,4896(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4896, temp.u32);
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,2312(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2312, temp.u32);
	// stfs f2,888(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// stfs f4,5056(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5056, temp.u32);
	// lfd f13,720(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 720);
	// lfs f9,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f6,f13,f10
	ctx.f6.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// stfs f9,3796(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3796, temp.u32);
	// stfs f7,2248(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2248, temp.u32);
	// lfs f9,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,3624(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3624, temp.u32);
	// lfs f9,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,5008(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5008, temp.u32);
	// lfs f9,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f13,f9
	ctx.f8.f64 = double(float(ctx.f13.f64 * ctx.f9.f64));
	// lfs f16,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f16.f64 = double(temp.f32);
	// lfs f20,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f16,f22
	ctx.f22.f64 = double(float(ctx.f16.f64 * ctx.f22.f64));
	// lfs f2,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f0,f16,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f4.f64 = double(temp.f32);
	// stfs f1,928(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// stfs f29,436(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f1,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f29.f64 = double(temp.f32);
	// stfs f20,364(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f20,f20,f12
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f12.f64));
	// stfs f3,4976(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4976, temp.u32);
	// fmuls f3,f2,f4
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// stfs f31,5040(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5040, temp.u32);
	// fmuls f31,f13,f7
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f7.f64));
	// stfs f28,4904(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4904, temp.u32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// stfs f5,628(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// stfs f30,944(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// lfs f5,1592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1592);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f30.f64 = double(temp.f32);
	// lfs f13,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f13.f64 = double(temp.f32);
	// lfs f28,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,440(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f25,2956(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2956, temp.u32);
	// stfs f23,4848(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4848, temp.u32);
	// stfs f21,4428(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4428, temp.u32);
	// stfs f18,3740(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3740, temp.u32);
	// stfs f17,3764(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3764, temp.u32);
	// stfs f15,4764(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4764, temp.u32);
	// stfs f14,3772(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3772, temp.u32);
	// stfs f24,640(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// stfs f22,4932(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4932, temp.u32);
	// stfs f16,2972(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2972, temp.u32);
	// stfs f0,3836(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3836, temp.u32);
	// stfs f27,4968(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4968, temp.u32);
	// stfs f19,5000(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5000, temp.u32);
	// stfs f20,4924(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4924, temp.u32);
	// fmuls f27,f30,f13
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// lfs f26,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f26.f64 = double(temp.f32);
	// lfs f13,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f28,f26,f28
	ctx.f28.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// fmuls f24,f26,f13
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// lfs f13,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f13.f64 = double(temp.f32);
	// lfs f22,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f22,f13
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// lfs f19,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f19.f64 = double(temp.f32);
	// lfs f13,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f18,f19,f13
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f13.f64));
	// lfs f13,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f16,f13,f2
	ctx.f16.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lfs f13,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f14,f13,f5
	ctx.f14.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// lfs f15,1584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1584);
	ctx.f15.f64 = double(temp.f32);
	// lfs f13,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f15,f13
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f13.f64));
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f17,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f17.f64 = double(temp.f32);
	// stfs f4,336(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f17,f17,f2
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f2.f64));
	// stfs f2,204(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f4,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f2.f64 = double(temp.f32);
	// stfs f4,308(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f2,f4,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f13,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f13.f64 = double(temp.f32);
	// lfs f4,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f11,f11,f13
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// stfs f9,688(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fmuls f12,f4,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// lfs f0,1576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1576);
	ctx.f0.f64 = double(temp.f32);
	// lfs f23,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f23.f64 = double(temp.f32);
	// lfs f9,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f23,f23,f5
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// fmuls f9,f0,f9
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// stfs f11,180(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f0,136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f12,152(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f5,148(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfd f0,704(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f25,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f25.f64 = double(temp.f32);
	// lfs f20,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f26,f25
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f25.f64));
	// lfs f11,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f20,f22,f20
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f4,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f11,f0,f11
	ctx.f11.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f12,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f4,f0,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f5,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f12,f0,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// stfs f29,556(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// stfs f10,600(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// lfs f10,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f10.f64 = double(temp.f32);
	// lfs f29,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f29.f64 = double(temp.f32);
	// stfs f8,4984(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4984, temp.u32);
	// fmuls f8,f0,f5
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// stfs f7,808(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// fmuls f29,f10,f29
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// lfs f7,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f6,2964(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2964, temp.u32);
	// stfs f31,3612(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3612, temp.u32);
	// stfs f3,4936(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4936, temp.u32);
	// stfs f1,5032(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5032, temp.u32);
	// stfs f27,5048(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5048, temp.u32);
	// stfs f28,3756(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3756, temp.u32);
	// stfs f24,2272(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2272, temp.u32);
	// stfs f25,3748(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3748, temp.u32);
	// stfs f21,2296(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2296, temp.u32);
	// stfs f23,2280(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2280, temp.u32);
	// stfs f19,2516(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2516, temp.u32);
	// stfs f18,2320(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2320, temp.u32);
	// stfs f20,3828(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3828, temp.u32);
	// stfs f16,3820(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3820, temp.u32);
	// stfs f17,4772(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4772, temp.u32);
	// stfs f14,3804(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3804, temp.u32);
	// stfs f7,2256(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2256, temp.u32);
	// stfs f9,2264(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2264, temp.u32);
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f28.f64 = double(temp.f32);
	// lfs f23,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f23.f64 = double(temp.f32);
	// fmr f27,f28
	ctx.f27.f64 = ctx.f28.f64;
	// fmr f21,f23
	ctx.f21.f64 = ctx.f23.f64;
	// lfs f5,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,1568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1568);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f28,f15,f28
	ctx.f28.f64 = double(float(ctx.f15.f64 * ctx.f28.f64));
	// fmuls f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f13,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f13.f64 = double(temp.f32);
	// lfs f17,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f23,f15,f23
	ctx.f23.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// lfs f3,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f10,f10,f17
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f17.f64));
	// lfs f5,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f3,f13
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f13.f64));
	// stfs f29,4444(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4444, temp.u32);
	// fmuls f5,f5,f13
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f13.f64));
	// stfs f2,2304(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2304, temp.u32);
	// lfs f7,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f7.f64 = double(temp.f32);
	// lfs f2,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f7,f22,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// lfs f31,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f2,f22,f2
	ctx.f2.f64 = double(float(ctx.f22.f64 * ctx.f2.f64));
	// lfs f29,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f22,f31
	ctx.f31.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// lfs f25,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f29,f22,f29
	ctx.f29.f64 = double(float(ctx.f22.f64 * ctx.f29.f64));
	// lfs f19,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// stfs f13,252(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f19,f22,f19
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// lfs f24,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f24.f64 = double(temp.f32);
	// lfs f16,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f27,f24,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// lfs f14,1544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1544);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f21,f24,f21
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f18,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// lfs f13,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f17,f14,f17
	ctx.f17.f64 = double(float(ctx.f14.f64 * ctx.f17.f64));
	// stfs f0,220(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f13,f16,f13
	ctx.f13.f64 = double(float(ctx.f16.f64 * ctx.f13.f64));
	// stfs f22,136(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f9,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f9.f64 = double(temp.f32);
	// lfs f22,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f22.f64 = double(temp.f32);
	// lfs f1,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f1.f64 = double(temp.f32);
	// lfs f20,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f1,f1,f9
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// lfs f0,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f20,f30,f20
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f20.f64));
	// stfs f16,792(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// fmuls f0,f0,f22
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f22.f64));
	// stfs f8,116(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f14,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f26,f14
	ctx.f14.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f16,f26,f16
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f22,1432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1432);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f8,f11,f8
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// stfs f7,2288(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2288, temp.u32);
	// stfs f6,4452(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4452, temp.u32);
	// stfs f3,4460(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4460, temp.u32);
	// stfs f5,4780(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4780, temp.u32);
	// stfs f2,3812(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3812, temp.u32);
	// stfs f9,1584(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1584, temp.u32);
	// stfs f1,5124(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5124, temp.u32);
	// stfs f31,5136(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5136, temp.u32);
	// stfs f29,3852(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3852, temp.u32);
	// stfs f28,2376(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2376, temp.u32);
	// stfs f27,2384(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2384, temp.u32);
	// stfs f25,4476(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4476, temp.u32);
	// stfs f15,892(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// stfs f23,3844(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3844, temp.u32);
	// stfs f24,936(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// stfs f21,2328(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2328, temp.u32);
	// stfs f30,680(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// stfs f20,3884(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3884, temp.u32);
	// stfs f19,2336(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2336, temp.u32);
	// stfs f18,3892(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3892, temp.u32);
	// lfs f3,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f3.f64 = double(temp.f32);
	// lfs f7,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f7,f7,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// lfs f2,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f5,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmr f3,f2
	ctx.f3.f64 = ctx.f2.f64;
	// stfs f10,3900(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3900, temp.u32);
	// lfs f10,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f2,f12,f2
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f2.f64));
	// fmr f9,f10
	ctx.f9.f64 = ctx.f10.f64;
	// lfs f25,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f10,f22,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f10.f64));
	// stfs f8,4516(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4516, temp.u32);
	// fmuls f27,f27,f25
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lfs f6,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f6.f64 = double(temp.f32);
	// lfs f8,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f8.f64 = double(temp.f32);
	// lfs f21,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// stfs f11,948(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// fmuls f21,f21,f6
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// lfs f23,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f23.f64 = double(temp.f32);
	// fmr f1,f3
	ctx.f1.f64 = ctx.f3.f64;
	// lfs f11,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f11.f64 = double(temp.f32);
	// lfs f24,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f24,f23
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f23.f64));
	// stfs f14,2360(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2360, temp.u32);
	// fmuls f25,f25,f23
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// lfs f14,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f9,f4,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f9.f64));
	// stfs f6,672(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// lfs f6,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f6.f64 = double(temp.f32);
	// stfs f23,664(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// fmuls f6,f6,f11
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// lfs f23,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f23.f64 = double(temp.f32);
	// stfs f26,1384(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1384, temp.u32);
	// fmuls f23,f23,f14
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f14.f64));
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lfs f26,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f26.f64 = double(temp.f32);
	// lfs f28,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f28.f64 = double(temp.f32);
	// stfs f0,2352(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2352, temp.u32);
	// lfs f0,1368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1368);
	ctx.f0.f64 = double(temp.f32);
	// stfs f17,3908(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3908, temp.u32);
	// fmuls f3,f0,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// stfs f16,5012(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5012, temp.u32);
	// stfs f13,3940(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3940, temp.u32);
	// stfs f14,136(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f6,116(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f13,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f29,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f20,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f29,f26,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// lfs f19,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f13,f19
	ctx.f19.f64 = double(float(ctx.f13.f64 * ctx.f19.f64));
	// lfs f17,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// lfs f16,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f26,f17
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f17.f64));
	// lfs f15,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f26,f16
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f14,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f26,f15
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f15.f64));
	// lfs f6,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f6.f64 = double(temp.f32);
	// stfs f10,3948(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3948, temp.u32);
	// fmuls f10,f14,f6
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// stfs f22,2216(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2216, temp.u32);
	// stfs f4,876(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f9,2408(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2408, temp.u32);
	// stfs f8,2448(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2448, temp.u32);
	// stfs f7,2456(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2456, temp.u32);
	// stfs f5,2464(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2464, temp.u32);
	// stfs f12,932(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f2,5132(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5132, temp.u32);
	// stfs f3,5140(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5140, temp.u32);
	// stfs f0,5064(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5064, temp.u32);
	// lfs f12,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f12.f64 = double(temp.f32);
	// lfs f2,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f2.f64 = double(temp.f32);
	// stfs f13,1052(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// fmuls f13,f2,f12
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,2440(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2440, temp.u32);
	// stfs f30,1096(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1096, temp.u32);
	// stfs f11,900(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// lfs f0,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f0.f64 = double(temp.f32);
	// lfs f30,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f30.f64 = double(temp.f32);
	// lfs f9,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f0,f9
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f9.f64));
	// lfs f3,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f5,f0,f7
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f11,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f11.f64 = double(temp.f32);
	// stfs f18,904(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// fmuls f8,f0,f11
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// stfs f20,3916(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3916, temp.u32);
	// lfs f20,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f18.f64 = double(temp.f32);
	// stfs f31,3860(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3860, temp.u32);
	// fmuls f31,f2,f3
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// stfs f29,2344(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2344, temp.u32);
	// fmuls f29,f3,f30
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfs f24,4796(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4796, temp.u32);
	// fmuls f18,f18,f20
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// lfs f24,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f24.f64 = double(temp.f32);
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// stfs f27,4492(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4492, temp.u32);
	// stfs f25,4500(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4500, temp.u32);
	// stfs f21,5052(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5052, temp.u32);
	// stfs f26,3124(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3124, temp.u32);
	// stfs f15,2400(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2400, temp.u32);
	// stfs f23,4828(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4828, temp.u32);
	// stfs f12,652(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f9,392(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f7,324(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// stfs f29,252(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f31,220(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f1,5148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5148, temp.u32);
	// stfs f28,940(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// stfs f19,2368(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2368, temp.u32);
	// stfs f16,4948(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4948, temp.u32);
	// stfs f20,488(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// lfs f19,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f19.f64 = double(temp.f32);
	// lfs f4,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,1136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1136);
	ctx.f1.f64 = double(temp.f32);
	// lfs f28,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f27,f1
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f25,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f26,f24
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// lfs f23,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f23,f4,f23
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// lfs f21,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// lfs f16,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f15,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f15.f64 = double(temp.f32);
	// lfs f12,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f15,f15,f19
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f19.f64));
	// lfs f20,1832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1832);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f9,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f20,f9
	ctx.f9.f64 = double(float(ctx.f20.f64 * ctx.f9.f64));
	// lfs f29,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f4,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// lfs f31,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f29,f29,f19
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// stfs f17,2392(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2392, temp.u32);
	// fmuls f17,f0,f13
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f30,204(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f0,f16,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// stfs f2,180(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f30,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f30.f64 = double(temp.f32);
	// lfs f2,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// stfs f18,4588(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4588, temp.u32);
	// fmuls f2,f30,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// lfs f18,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f18.f64 = double(temp.f32);
	// stfs f30,148(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f16,116(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f17,2560(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2560, temp.u32);
	// fmr f17,f18
	ctx.f17.f64 = ctx.f18.f64;
	// stfs f4,3108(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3108, temp.u32);
	// stfs f28,1608(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1608, temp.u32);
	// lfs f4,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f4.f64 = double(temp.f32);
	// lfs f28,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,2424(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2424, temp.u32);
	// fmr f26,f28
	ctx.f26.f64 = ctx.f28.f64;
	// stfs f15,4540(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4540, temp.u32);
	// stfs f2,2488(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2488, temp.u32);
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// stfs f20,152(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f4,f14,f4
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f4.f64));
	// lfs f16,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f16.f64 = double(temp.f32);
	// fmr f15,f17
	ctx.f15.f64 = ctx.f17.f64;
	// lfs f20,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// lfs f30,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f20,f16,f20
	ctx.f20.f64 = double(float(ctx.f16.f64 * ctx.f20.f64));
	// stfs f24,656(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// stfs f21,2480(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2480, temp.u32);
	// stfs f10,4028(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4028, temp.u32);
	// fmuls f10,f3,f30
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// lfs f21,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f21.f64 = double(temp.f32);
	// lfs f24,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f24.f64 = double(temp.f32);
	// stfs f1,788(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// fmuls f24,f21,f24
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f24.f64));
	// stfs f25,4532(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4532, temp.u32);
	// stfs f6,3972(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3972, temp.u32);
	// stfs f7,2432(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2432, temp.u32);
	// stfs f20,2520(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2520, temp.u32);
	// stfs f27,4564(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4564, temp.u32);
	// stfs f22,764(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f23,4044(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4044, temp.u32);
	// stfs f5,2496(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2496, temp.u32);
	// stfs f11,744(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// stfs f0,2544(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2544, temp.u32);
	// stfs f13,4020(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4020, temp.u32);
	// stfs f12,4604(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4604, temp.u32);
	// stfs f19,5184(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5184, temp.u32);
	// stfs f29,4036(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4036, temp.u32);
	// stfs f31,5088(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5088, temp.u32);
	// lfs f27,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f27.f64 = double(temp.f32);
	// lfs f0,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,1816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1816);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f28,f0,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// lfs f12,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f26,f13,f26
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// lfs f11,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f17,f12,f17
	ctx.f17.f64 = double(float(ctx.f12.f64 * ctx.f17.f64));
	// lfs f7,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f15,f11,f15
	ctx.f15.f64 = double(float(ctx.f11.f64 * ctx.f15.f64));
	// lfs f6,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f16,f6
	ctx.f6.f64 = double(float(ctx.f16.f64 * ctx.f6.f64));
	// lfs f1,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f7,f7,f5
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// lfs f31,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f2,f31,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// lfs f29,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f25,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// lfs f23,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,1800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1800);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f25,f23,f25
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// lfs f20,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,1732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1732);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f22,f20
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// stfs f8,4964(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4964, temp.u32);
	// fmuls f18,f19,f18
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// stfs f9,2416(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2416, temp.u32);
	// stfs f21,528(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// lfs f9,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f8.f64 = double(temp.f32);
	// lfs f21,2092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2092);
	ctx.f21.f64 = double(temp.f32);
	// stfs f10,116(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f22,308(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f22,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f22.f64 = double(temp.f32);
	// stfs f19,152(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfs f19,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f19.f64 = double(temp.f32);
	// stfs f2,5020(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5020, temp.u32);
	// lfs f2,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// stfs f16,148(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmr f16,f19
	ctx.f16.f64 = ctx.f19.f64;
	// stfs f31,412(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmr f31,f2
	ctx.f31.f64 = ctx.f2.f64;
	// stfs f3,336(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f3,1344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1344);
	ctx.f3.f64 = double(temp.f32);
	// stfs f23,360(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmuls f19,f3,f19
	ctx.f19.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// lfs f23,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f23.f64 = double(temp.f32);
	// stfs f3,388(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f23,f21,f23
	ctx.f23.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// lfs f10,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f10.f64 = double(temp.f32);
	// lfs f3,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f3.f64 = double(temp.f32);
	// stfs f21,136(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f3,f10,f3
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f3.f64));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f16,f9,f16
	ctx.f16.f64 = double(float(ctx.f9.f64 * ctx.f16.f64));
	// stfs f14,364(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f21,f8,f21
	ctx.f21.f64 = double(float(ctx.f8.f64 * ctx.f21.f64));
	// stfs f27,572(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// stfs f28,2576(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2576, temp.u32);
	// lfs f27,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f27.f64 = double(temp.f32);
	// lfs f14,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f14.f64 = double(temp.f32);
	// lfs f28,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f14,f22,f14
	ctx.f14.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// stfs f4,3964(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3964, temp.u32);
	// fmuls f28,f28,f27
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// stfs f29,2612(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2612, temp.u32);
	// stfs f3,2512(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2512, temp.u32);
	// stfs f6,3980(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3980, temp.u32);
	// stfs f1,2472(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2472, temp.u32);
	// stfs f30,2504(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2504, temp.u32);
	// stfs f0,2168(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2168, temp.u32);
	// stfs f13,1512(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1512, temp.u32);
	// stfs f12,1932(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1932, temp.u32);
	// stfs f8,1892(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1892, temp.u32);
	// lfs f0,1336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1336);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,1328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1328);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,1320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1320);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f2,f13,f2
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f2.f64));
	// lfs f8,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f31,f12,f31
	ctx.f31.f64 = double(float(ctx.f12.f64 * ctx.f31.f64));
	// lfs f6,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f0,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f1,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,1304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1304);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f3,f1,f3
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// stfs f26,4116(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4116, temp.u32);
	// fmr f26,f27
	ctx.f26.f64 = ctx.f27.f64;
	// stfs f5,408(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// fmuls f5,f8,f6
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f6.f64));
	// stfs f7,2528(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2528, temp.u32);
	// fmuls f29,f30,f29
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f11,3752(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3752, temp.u32);
	// stfs f9,1964(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1964, temp.u32);
	// lfs f11,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,1312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1312);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f7.f64 = double(temp.f32);
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// stfs f25,4124(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4124, temp.u32);
	// stfs f24,4616(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4616, temp.u32);
	// stfs f20,4164(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4164, temp.u32);
	// stfs f18,4148(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4148, temp.u32);
	// stfs f17,2600(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2600, temp.u32);
	// stfs f15,2616(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2616, temp.u32);
	// stfs f23,2632(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2632, temp.u32);
	// stfs f14,4980(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4980, temp.u32);
	// stfs f22,2116(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2116, temp.u32);
	// stfs f19,2640(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2640, temp.u32);
	// stfs f16,2672(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2672, temp.u32);
	// stfs f21,4076(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4076, temp.u32);
	// lfs f24,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f11,f26
	ctx.f26.f64 = double(float(ctx.f11.f64 * ctx.f26.f64));
	// fmr f23,f24
	ctx.f23.f64 = ctx.f24.f64;
	// lfs f16,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f27,f9,f27
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// lfs f15,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f19,f19,f16
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// lfs f21,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f15,f16,f15
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f15.f64));
	// stfs f8,740(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// stfs f30,204(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f10,180(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f16,220(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// lfs f30,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f30.f64 = double(temp.f32);
	// lfs f16,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f16.f64 = double(temp.f32);
	// fmr f22,f23
	ctx.f22.f64 = ctx.f23.f64;
	// lfs f10,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f23,f21,f23
	ctx.f23.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// stfs f21,684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// lfs f21,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f10,f10,f30
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// lfs f25,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f25.f64 = double(temp.f32);
	// lfs f30,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f24,f25,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// lfs f17,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f17.f64 = double(temp.f32);
	// stfs f0,2836(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2836, temp.u32);
	// fmuls f17,f7,f17
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f17.f64));
	// stfs f25,116(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f25,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f25.f64 = double(temp.f32);
	// lfs f0,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f12,f12,f25
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f25.f64));
	// fmr f20,f22
	ctx.f20.f64 = ctx.f22.f64;
	// stfs f7,3832(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3832, temp.u32);
	// fmuls f22,f8,f22
	ctx.f22.f64 = double(float(ctx.f8.f64 * ctx.f22.f64));
	// lfs f8,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f16,f16,f8
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f8.f64));
	// stfs f8,252(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f8,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f8,f8,f30
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f30.f64));
	// lfs f30,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f7,f30,f0
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f8,2784(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2784, temp.u32);
	// lfs f18,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f18.f64 = double(temp.f32);
	// lfs f0,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f0.f64 = double(temp.f32);
	// lfs f14,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f14.f64 = double(temp.f32);
	// lfs f8,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f14,f21,f14
	ctx.f14.f64 = double(float(ctx.f21.f64 * ctx.f14.f64));
	// stfs f12,2760(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2760, temp.u32);
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// lfs f12,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f12.f64 = double(temp.f32);
	// stfs f5,4252(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4252, temp.u32);
	// fmuls f5,f0,f8
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfs f13,1900(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1900, temp.u32);
	// fmr f0,f25
	ctx.f0.f64 = ctx.f25.f64;
	// stfs f2,2568(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2568, temp.u32);
	// stfs f11,1948(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1948, temp.u32);
	// fmuls f11,f18,f6
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// stfs f9,2652(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2652, temp.u32);
	// fmuls f9,f1,f12
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f10,2752(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2752, temp.u32);
	// lfs f10,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f13.f64 = double(temp.f32);
	// lfs f2,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f2.f64 = double(temp.f32);
	// stfs f4,2536(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2536, temp.u32);
	// stfs f3,2552(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2552, temp.u32);
	// stfs f31,2584(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2584, temp.u32);
	// stfs f29,4852(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4852, temp.u32);
	// stfs f28,5176(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5176, temp.u32);
	// stfs f26,4156(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4156, temp.u32);
	// stfs f27,4140(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4140, temp.u32);
	// stfs f24,4628(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4628, temp.u32);
	// stfs f23,4204(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4204, temp.u32);
	// stfs f22,4196(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4196, temp.u32);
	// stfs f20,4868(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4868, temp.u32);
	// stfs f19,4892(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4892, temp.u32);
	// stfs f17,2712(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2712, temp.u32);
	// stfs f15,2720(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2720, temp.u32);
	// stfs f14,2736(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2736, temp.u32);
	// stfs f21,2076(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2076, temp.u32);
	// stfs f16,2792(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2792, temp.u32);
	// lfs f3,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f2,f13
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f13.f64));
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f13,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// fmr f31,f25
	ctx.f31.f64 = ctx.f25.f64;
	// fmuls f29,f13,f0
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f27,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f27.f64 = double(temp.f32);
	// fmr f0,f25
	ctx.f0.f64 = ctx.f25.f64;
	// lfs f26,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f26.f64 = double(temp.f32);
	// lfs f13,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f13.f64 = double(temp.f32);
	// lfs f24,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f26,f13
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// lfs f13,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f13.f64 = double(temp.f32);
	// lfs f16,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f16.f64 = double(temp.f32);
	// stfs f30,464(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// lfs f30,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f14.f64 = double(temp.f32);
	// stfs f26,308(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f28,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f27,f0
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// lfs f0,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f24,f0
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f21,f13,f0
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f0,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// fmuls f17,f0,f13
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f0,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f15,f16,f0
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f0,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f0,f30
	ctx.f30.f64 = double(float(ctx.f0.f64 * ctx.f30.f64));
	// lfs f13,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f14,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f13.f64));
	// lfs f26,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f0,f24,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f20,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f22.f64 = double(temp.f32);
	// lfs f19,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f22,f20,f22
	ctx.f22.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// stfs f8,336(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f19,f10,f19
	ctx.f19.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// stfs f2,220(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f18,148(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f16,152(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f0,136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f16,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f16.f64 = double(temp.f32);
	// lfs f8,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f26,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// lfs f13,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f2,f16,f2
	ctx.f2.f64 = double(float(ctx.f16.f64 * ctx.f2.f64));
	// lfs f0,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f18.f64 = double(temp.f32);
	// stfs f7,2800(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2800, temp.u32);
	// fmuls f18,f18,f13
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f13.f64));
	// stfs f5,2808(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2808, temp.u32);
	// fmuls f7,f16,f0
	ctx.f7.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// stfs f4,2824(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2824, temp.u32);
	// stfs f1,4572(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4572, temp.u32);
	// stfs f12,3792(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3792, temp.u32);
	// stfs f9,4716(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4716, temp.u32);
	// stfs f3,2832(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2832, temp.u32);
	// stfs f29,2840(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2840, temp.u32);
	// stfs f28,3596(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3596, temp.u32);
	// stfs f31,4612(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4612, temp.u32);
	// stfs f27,4972(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4972, temp.u32);
	// stfs f25,4132(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4132, temp.u32);
	// stfs f23,4624(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4624, temp.u32);
	// stfs f21,4860(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4860, temp.u32);
	// stfs f20,1884(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1884, temp.u32);
	// stfs f22,2592(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2592, temp.u32);
	// stfs f17,2608(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2608, temp.u32);
	// stfs f10,3540(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3540, temp.u32);
	// stfs f19,2624(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2624, temp.u32);
	// stfs f15,4212(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4212, temp.u32);
	// stfs f11,4220(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4220, temp.u32);
	// stfs f6,2688(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2688, temp.u32);
	// lfs f21,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f21.f64 = double(temp.f32);
	// fmr f20,f21
	ctx.f20.f64 = ctx.f21.f64;
	// lfs f3,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f3.f64 = double(temp.f32);
	// lfs f4,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f4.f64 = double(temp.f32);
	// stfs f24,4060(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4060, temp.u32);
	// fmuls f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f29,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f29.f64 = double(temp.f32);
	// lfs f22,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f22.f64 = double(temp.f32);
	// lfs f31,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f31.f64 = double(temp.f32);
	// lfs f24,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f29,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// lfs f3,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f24,f22,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// stfs f8,2664(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2664, temp.u32);
	// fmr f1,f3
	ctx.f1.f64 = ctx.f3.f64;
	// stfs f22,388(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmr f19,f20
	ctx.f19.f64 = ctx.f20.f64;
	// stfs f29,816(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// lfs f8,1484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1484);
	ctx.f8.f64 = double(temp.f32);
	// lfs f22,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f5,f14,f8
	ctx.f5.f64 = double(float(ctx.f14.f64 * ctx.f8.f64));
	// lfs f29,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f29.f64 = double(temp.f32);
	// lfs f12,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f29,f22,f29
	ctx.f29.f64 = double(float(ctx.f22.f64 * ctx.f29.f64));
	// stfs f14,436(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// stfs f30,4652(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4652, temp.u32);
	// stfs f22,364(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f27,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f27.f64 = double(temp.f32);
	// lfs f14,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f14.f64 = double(temp.f32);
	// lfs f30,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f19,f27,f19
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f19.f64));
	// lfs f22,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// stfs f12,668(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// stfs f27,148(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f8,136(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f16,204(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f14,324(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f16,1468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1468);
	ctx.f16.f64 = double(temp.f32);
	// lfs f27,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f27.f64 = double(temp.f32);
	// lfs f14,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f14.f64 = double(temp.f32);
	// lfs f8,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f14,f16,f14
	ctx.f14.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// fmuls f8,f27,f8
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f8.f64));
	// stfs f26,2744(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2744, temp.u32);
	// stfs f2,2696(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2696, temp.u32);
	// stfs f12,4244(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4244, temp.u32);
	// stfs f18,2656(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2656, temp.u32);
	// stfs f16,116(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f12,1272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1272);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,1264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1264);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f9,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f12,f3
	ctx.f3.f64 = double(float(ctx.f12.f64 * ctx.f3.f64));
	// lfs f10,1684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1684);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f9,f1
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// lfs f6,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f13.f64));
	// lfs f2,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f6,f12,f6
	ctx.f6.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f28,1596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1596);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f26,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f20,f12,f20
	ctx.f20.f64 = double(float(ctx.f12.f64 * ctx.f20.f64));
	// lfs f25,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f28,f26
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f23,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f9,f25
	ctx.f25.f64 = double(float(ctx.f9.f64 * ctx.f25.f64));
	// lfs f18,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f21,f23,f21
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// lfs f17,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f18,f17
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f17.f64));
	// lfs f16,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f15,f11
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f11.f64));
	// lfs f27,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// stfs f8,180(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f27,f16,f27
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f27.f64));
	// lfs f8,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f8.f64 = double(temp.f32);
	// stfs f13,2728(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2728, temp.u32);
	// fmuls f13,f28,f8
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// stfs f6,2680(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2680, temp.u32);
	// lfs f6,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f6.f64 = double(temp.f32);
	// lfs f8,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f8.f64 = double(temp.f32);
	// stfs f5,5036(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5036, temp.u32);
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f5,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f6.f64 = double(temp.f32);
	// stfs f4,2768(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2768, temp.u32);
	// fmuls f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f4,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f5.f64 = double(temp.f32);
	// stfs f20,2968(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2968, temp.u32);
	// fmuls f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// lfs f16,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f16.f64 = double(temp.f32);
	// lfs f20,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f20.f64 = double(temp.f32);
	// stfs f2,2816(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2816, temp.u32);
	// fmuls f20,f20,f16
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// stfs f3,2848(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2848, temp.u32);
	// stfs f1,4900(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4900, temp.u32);
	// lfs f3,1712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1712);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,1452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1452);
	ctx.f2.f64 = double(temp.f32);
	// lfs f4,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f3,f4
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// stfs f24,2928(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2928, temp.u32);
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// stfs f22,2872(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2872, temp.u32);
	// stfs f14,2904(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2904, temp.u32);
	// stfs f16,556(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// lfs f24,1580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1580);
	ctx.f24.f64 = double(temp.f32);
	// lfs f14,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f14.f64 = double(temp.f32);
	// lfs f22,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f22,f22,f24
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// stfs f7,2704(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2704, temp.u32);
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// stfs f31,4332(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4332, temp.u32);
	// stfs f30,2864(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2864, temp.u32);
	// stfs f25,2912(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2912, temp.u32);
	// stfs f21,2960(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2960, temp.u32);
	// stfs f17,1988(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1988, temp.u32);
	// stfs f15,2856(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2856, temp.u32);
	// stfs f29,2920(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2920, temp.u32);
	// stfs f3,564(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// stfs f2,532(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// stfs f0,2776(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2776, temp.u32);
	// stfs f26,2896(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2896, temp.u32);
	// stfs f19,2976(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2976, temp.u32);
	// stfs f11,1108(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// lfs f19,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f19.f64 = double(temp.f32);
	// lfs f0,1588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1588);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,1460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1460);
	ctx.f11.f64 = double(temp.f32);
	// lfs f7,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f7.f64 = double(temp.f32);
	// lfs f31,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f7,f18,f7
	ctx.f7.f64 = double(float(ctx.f18.f64 * ctx.f7.f64));
	// lfs f30,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f23,f31
	ctx.f31.f64 = double(float(ctx.f23.f64 * ctx.f31.f64));
	// lfs f29,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f0,f29
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// lfs f25,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// lfs f21,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f25,f9
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// lfs f17,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f21,f19
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// lfs f15,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f11,f17
	ctx.f17.f64 = double(float(ctx.f11.f64 * ctx.f17.f64));
	// lfs f3,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f15,f15,f9
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f9.f64));
	// lfs f2,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f3,f19
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// stfs f10,2944(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2944, temp.u32);
	// fmuls f2,f12,f2
	ctx.f2.f64 = double(float(ctx.f12.f64 * ctx.f2.f64));
	// stfs f14,360(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f24,392(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// lfs f10,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f10.f64 = double(temp.f32);
	// lfs f14,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f14.f64 = double(temp.f32);
	// lfs f24,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f24.f64 = double(temp.f32);
	// stfs f27,152(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f14,f14,f10
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f10.f64));
	// lfs f27,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// stfs f12,116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f24,f27,f24
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// stfs f2,3120(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3120, temp.u32);
	// lfs f2,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f2.f64 = double(temp.f32);
	// stfs f31,3096(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3096, temp.u32);
	// fmr f31,f2
	ctx.f31.f64 = ctx.f2.f64;
	// lfs f12,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f12.f64 = double(temp.f32);
	// stfs f13,2880(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2880, temp.u32);
	// fmuls f13,f23,f12
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f12.f64));
	// stfs f28,1000(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// stfs f18,1228(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// stfs f5,3056(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3056, temp.u32);
	// stfs f4,2004(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2004, temp.u32);
	// stfs f26,3556(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3556, temp.u32);
	// stfs f30,3104(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3104, temp.u32);
	// stfs f25,3880(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3880, temp.u32);
	// stfs f22,3872(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3872, temp.u32);
	// stfs f21,3944(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3944, temp.u32);
	// stfs f20,3152(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3152, temp.u32);
	// stfs f19,4364(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4364, temp.u32);
	// stfs f24,3168(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3168, temp.u32);
	// stfs f8,2888(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2888, temp.u32);
	// stfs f7,4396(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4396, temp.u32);
	// stfs f6,2936(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2936, temp.u32);
	// stfs f1,3072(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3072, temp.u32);
	// stfs f0,4380(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4380, temp.u32);
	// stfs f29,3864(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3864, temp.u32);
	// stfs f11,3100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3100, temp.u32);
	// stfs f16,3136(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3136, temp.u32);
	// stfs f3,3888(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3888, temp.u32);
	// stfs f10,1784(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1784, temp.u32);
	// stfs f27,1768(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1768, temp.u32);
	// lfs f1,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f31,f1,f31
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f0,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f2,f3
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// lfs f12,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,1444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1444);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,1644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1644);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// lfs f30,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f1,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// lfs f29,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f23,f30
	ctx.f30.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// lfs f28,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f28,f29
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// lfs f26,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f26,f27
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// lfs f24,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f25,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f22,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f12,f24
	ctx.f24.f64 = double(float(ctx.f12.f64 * ctx.f24.f64));
	// lfs f21,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f22,f8
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f8.f64));
	// lfs f20,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f21,f7
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f7.f64));
	// lfs f19,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f1,f20
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f20.f64));
	// lfs f18,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f19,f6
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f6.f64));
	// lfs f16,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f16.f64 = double(temp.f32);
	// stfs f17,3848(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3848, temp.u32);
	// fmuls f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// stfs f9,4092(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4092, temp.u32);
	// fmuls f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f15,3856(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3856, temp.u32);
	// stfs f14,3952(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3952, temp.u32);
	// lfs f17,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f14.f64 = double(temp.f32);
	// stfs f10,628(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// fmuls f15,f16,f15
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f15.f64));
	// lfs f10,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f17,f14,f17
	ctx.f17.f64 = double(float(ctx.f14.f64 * ctx.f17.f64));
	// fmuls f10,f16,f10
	ctx.f10.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// stfs f16,600(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfs f27,440(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// lfs f16,1436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1436);
	ctx.f16.f64 = double(temp.f32);
	// lfs f27,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f27.f64 = double(temp.f32);
	// stfs f11,136(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f27,f27,f16
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// stfs f16,116(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f16,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f16.f64 = double(temp.f32);
	// lfs f11,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f11,f16,f11
	ctx.f11.f64 = double(float(ctx.f16.f64 * ctx.f11.f64));
	// stfs f12,3116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3116, temp.u32);
	// lfs f16,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f16.f64 = double(temp.f32);
	// lfs f12,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f12.f64 = double(temp.f32);
	// stfs f8,1044(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// lfs f8,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f8.f64 = double(temp.f32);
	// stfs f11,4040(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4040, temp.u32);
	// fmuls f11,f12,f16
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f16.f64));
	// stfs f5,3008(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3008, temp.u32);
	// fmuls f12,f12,f8
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// lfs f5,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,1676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1676);
	ctx.f8.f64 = double(temp.f32);
	// stfs f29,488(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fmuls f8,f8,f5
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// stfs f31,3040(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3040, temp.u32);
	// stfs f1,664(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// lfs f31,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// lfs f1,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f29,f14,f29
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// stfs f3,928(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f3,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// fmr f31,f16
	ctx.f31.f64 = ctx.f16.f64;
	// lfs f5,1420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1420);
	ctx.f5.f64 = double(temp.f32);
	// stfs f29,4024(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4024, temp.u32);
	// fmuls f5,f5,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// fmr f3,f16
	ctx.f3.f64 = ctx.f16.f64;
	// stfs f30,3064(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3064, temp.u32);
	// fmr f29,f16
	ctx.f29.f64 = ctx.f16.f64;
	// lfs f30,1636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1636);
	ctx.f30.f64 = double(temp.f32);
	// stfs f4,3024(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3024, temp.u32);
	// stfs f6,2104(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2104, temp.u32);
	// stfs f13,3144(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3144, temp.u32);
	// stfs f2,3016(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3016, temp.u32);
	// stfs f9,3048(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3048, temp.u32);
	// stfs f0,4732(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4732, temp.u32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// stfs f10,3160(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3160, temp.u32);
	// lfs f13,1572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1572);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,1428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1428);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f10,f29
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// lfs f9,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f6,f14,f6
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// lfs f2,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f4,f13
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// stfs f28,3080(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3080, temp.u32);
	// fmuls f3,f2,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// stfs f7,736(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// fmuls f7,f9,f0
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f30,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f28.f64 = double(temp.f32);
	// stfs f23,672(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// stfs f26,3960(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3960, temp.u32);
	// stfs f25,2984(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2984, temp.u32);
	// stfs f24,2992(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2992, temp.u32);
	// stfs f22,3032(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3032, temp.u32);
	// stfs f21,3000(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3000, temp.u32);
	// stfs f20,1996(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1996, temp.u32);
	// stfs f19,3088(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3088, temp.u32);
	// stfs f18,3128(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3128, temp.u32);
	// stfs f17,3896(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3896, temp.u32);
	// stfs f15,3112(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3112, temp.u32);
	// stfs f27,4016(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4016, temp.u32);
	// fmr f20,f16
	ctx.f20.f64 = ctx.f16.f64;
	// lfs f19,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f10,f10,f30
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// lfs f18,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f28,f14
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f14.f64));
	// lfs f30,1404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1404);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f30,f27
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// lfs f25,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f26,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f25.f64));
	// stfs f30,4112(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4112, temp.u32);
	// fmuls f27,f27,f13
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// stfs f3,5116(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5116, temp.u32);
	// stfs f1,4032(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4032, temp.u32);
	// fmuls f20,f20,f19
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// lfs f19,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// fmr f30,f18
	ctx.f30.f64 = ctx.f18.f64;
	// stfs f29,4048(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4048, temp.u32);
	// stfs f26,3224(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3224, temp.u32);
	// fmr f3,f18
	ctx.f3.f64 = ctx.f18.f64;
	// stfs f27,4272(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4272, temp.u32);
	// fmr f1,f18
	ctx.f1.f64 = ctx.f18.f64;
	// fmr f29,f18
	ctx.f29.f64 = ctx.f18.f64;
	// lfs f24,1556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	ctx.f24.f64 = double(temp.f32);
	// fmr f27,f18
	ctx.f27.f64 = ctx.f18.f64;
	// stfs f31,4008(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4008, temp.u32);
	// fmr f26,f18
	ctx.f26.f64 = ctx.f18.f64;
	// lfs f31,1548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f0,f24,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f22,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f22.f64 = double(temp.f32);
	// lfs f23,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f23.f64 = double(temp.f32);
	// lfs f24,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f14,f23
	ctx.f23.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// stfs f28,4080(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4080, temp.u32);
	// fmuls f24,f24,f22
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// stfs f25,1568(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1568, temp.u32);
	// fmuls f31,f31,f30
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// stfs f12,4192(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4192, temp.u32);
	// stfs f10,4096(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4096, temp.u32);
	// stfs f13,1432(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1432, temp.u32);
	// stfs f0,4104(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4104, temp.u32);
	// fmuls f27,f20,f27
	ctx.f27.f64 = double(float(ctx.f20.f64 * ctx.f27.f64));
	// stfs f9,1696(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1696, temp.u32);
	// fmuls f26,f19,f26
	ctx.f26.f64 = double(float(ctx.f19.f64 * ctx.f26.f64));
	// lfs f21,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f21,f22
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// lfs f0,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f3,f0,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// lfs f12,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f13,f1
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// lfs f10,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f29,f12,f29
	ctx.f29.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// lfs f9,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f9.f64 = double(temp.f32);
	// lfs f30,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f14
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f25,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f28,f28,f10
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// stfs f11,4072(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4072, temp.u32);
	// fmuls f11,f17,f18
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// stfs f6,4056(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4056, temp.u32);
	// fmuls f25,f25,f9
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// stfs f8,4064(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4064, temp.u32);
	// stfs f4,3232(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3232, temp.u32);
	// stfs f5,4280(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4280, temp.u32);
	// stfs f7,4160(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4160, temp.u32);
	// stfs f23,3200(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3200, temp.u32);
	// stfs f24,3176(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3176, temp.u32);
	// fmr f24,f16
	ctx.f24.f64 = ctx.f16.f64;
	// lfs f8,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f4.f64 = double(temp.f32);
	// lfs f23,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f23.f64 = double(temp.f32);
	// lfs f20,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f24,f8,f24
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f24.f64));
	// fmr f19,f20
	ctx.f19.f64 = ctx.f20.f64;
	// lfs f17,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f17.f64 = double(temp.f32);
	// stfs f2,408(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// fmuls f23,f8,f23
	ctx.f23.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// lfs f2,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f20,f20,f7
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f7.f64));
	// stfs f17,220(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// lfs f18,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f18.f64 = double(temp.f32);
	// fmr f16,f18
	ctx.f16.f64 = ctx.f18.f64;
	// stfs f22,640(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// stfs f4,1872(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1872, temp.u32);
	// stfs f3,4200(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4200, temp.u32);
	// stfs f21,412(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// lfs f21,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f19,f19,f17
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// lfs f17,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f17,f4,f17
	ctx.f17.f64 = double(float(ctx.f4.f64 * ctx.f17.f64));
	// stfs f17,116(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f4,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f21,f21,f5
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// fmr f3,f4
	ctx.f3.f64 = ctx.f4.f64;
	// stfs f8,148(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f0,3696(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3696, temp.u32);
	// fmr f22,f16
	ctx.f22.f64 = ctx.f16.f64;
	// lfs f17,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f17.f64 = double(temp.f32);
	// lfs f8,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f16,f16,f6
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f6.f64));
	// lfs f15,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f8,f8,f17
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f17.f64));
	// stfs f1,4304(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4304, temp.u32);
	// fmuls f18,f18,f15
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f15.f64));
	// stfs f5,2532(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2532, temp.u32);
	// stfs f11,4152(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4152, temp.u32);
	// stfs f12,5104(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5104, temp.u32);
	// stfs f30,4000(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4000, temp.u32);
	// stfs f2,2120(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2120, temp.u32);
	// lfs f12,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f22,f22,f2
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f2.f64));
	// lfs f11,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f5,f17
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f17.f64));
	// lfs f1,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f2,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f30,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f30.f64 = double(temp.f32);
	// stfs f13,3680(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3680, temp.u32);
	// fmuls f13,f11,f12
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f12.f64));
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f0,3360(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3360, temp.u32);
	// lfs f0,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f0.f64 = double(temp.f32);
	// stfs f31,4312(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4312, temp.u32);
	// fmuls f3,f3,f0
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// stfs f10,1880(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1880, temp.u32);
	// stfs f9,3412(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3412, temp.u32);
	// stfs f7,4556(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4556, temp.u32);
	// stfs f6,1824(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1824, temp.u32);
	// stfs f8,3408(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3408, temp.u32);
	// lfs f10,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,1380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1380);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,1628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1628);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,1372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f6.f64 = double(temp.f32);
	// lfs f31,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f31.f64 = double(temp.f32);
	// stfs f29,3248(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3248, temp.u32);
	// stfs f14,1320(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1320, temp.u32);
	// stfs f28,4184(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4184, temp.u32);
	// stfs f27,4320(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4320, temp.u32);
	// stfs f26,4328(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4328, temp.u32);
	// stfs f25,4128(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4128, temp.u32);
	// stfs f24,4168(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4168, temp.u32);
	// stfs f23,3192(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3192, temp.u32);
	// stfs f20,3216(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3216, temp.u32);
	// stfs f19,3240(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3240, temp.u32);
	// stfs f15,788(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// stfs f18,4296(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4296, temp.u32);
	// stfs f16,3280(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3280, temp.u32);
	// stfs f22,3288(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3288, temp.u32);
	// stfs f21,3296(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3296, temp.u32);
	// lfs f22,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f31,f31,f10
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// lfs f24,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f24.f64 = double(temp.f32);
	// fmr f21,f22
	ctx.f21.f64 = ctx.f22.f64;
	// fmuls f24,f24,f10
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f10.f64));
	// stfs f10,180(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f15,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f22,f22,f30
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f30.f64));
	// lfs f10,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f10.f64 = double(temp.f32);
	// fmr f14,f15
	ctx.f14.f64 = ctx.f15.f64;
	// stfs f17,152(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmr f17,f10
	ctx.f17.f64 = ctx.f10.f64;
	// lfs f23,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f15,f7,f15
	ctx.f15.f64 = double(float(ctx.f7.f64 * ctx.f15.f64));
	// lfs f26,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f26,f30
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// lfs f27,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f25,f23,f25
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// stfs f11,364(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f30,252(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f23,204(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f19,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f19.f64 = double(temp.f32);
	// lfs f23,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f19,f21
	ctx.f21.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f20,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f14,f6,f14
	ctx.f14.f64 = double(float(ctx.f6.f64 * ctx.f14.f64));
	// lfs f18,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f19,f20
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lfs f11,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f18,f19,f18
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// lfs f30,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f11,f23,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f11.f64));
	// fmuls f30,f23,f30
	ctx.f30.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// stfs f19,148(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f23,136(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f19,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f19.f64 = double(temp.f32);
	// lfs f23,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f10,f19,f10
	ctx.f10.f64 = double(float(ctx.f19.f64 * ctx.f10.f64));
	// lfs f29,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f17,f23,f17
	ctx.f17.f64 = double(float(ctx.f23.f64 * ctx.f17.f64));
	// lfs f28,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f16,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f28,f28,f9
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f9.f64));
	// stfs f12,116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f16,f8,f16
	ctx.f16.f64 = double(float(ctx.f8.f64 * ctx.f16.f64));
	// stfs f0,3636(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3636, temp.u32);
	// stfs f11,3368(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3368, temp.u32);
	// lfs f12,1364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,1668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1668);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f11.f64 = double(temp.f32);
	// stfs f5,2012(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2012, temp.u32);
	// stfs f2,764(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f4,4360(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4360, temp.u32);
	// stfs f3,3264(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3264, temp.u32);
	// stfs f1,3272(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3272, temp.u32);
	// stfs f31,3304(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3304, temp.u32);
	// stfs f27,748(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// stfs f29,4432(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4432, temp.u32);
	// stfs f9,920(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// stfs f28,3328(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3328, temp.u32);
	// stfs f26,3352(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3352, temp.u32);
	// stfs f25,3392(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3392, temp.u32);
	// stfs f24,3400(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3400, temp.u32);
	// stfs f22,4120(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4120, temp.u32);
	// stfs f21,4136(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4136, temp.u32);
	// stfs f20,3184(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3184, temp.u32);
	// stfs f18,3208(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3208, temp.u32);
	// stfs f13,4264(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4264, temp.u32);
	// stfs f8,888(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// stfs f16,3256(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3256, temp.u32);
	// stfs f7,1856(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1856, temp.u32);
	// stfs f15,4288(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4288, temp.u32);
	// stfs f6,1372(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1372, temp.u32);
	// stfs f14,3344(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3344, temp.u32);
	// stfs f30,3376(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3376, temp.u32);
	// stfs f19,528(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// stfs f10,3424(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3424, temp.u32);
	// stfs f23,2396(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2396, temp.u32);
	// stfs f17,4560(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4560, temp.u32);
	// lfs f2,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f2.f64 = double(temp.f32);
	// fmr f1,f2
	ctx.f1.f64 = ctx.f2.f64;
	// lfs f24,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f24.f64 = double(temp.f32);
	// lfs f26,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f26,f24
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// lfs f25,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f24.f64 = double(temp.f32);
	// lfs f27,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f27.f64 = double(temp.f32);
	// lfs f21,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f27,f25,f27
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f27.f64));
	// lfs f18,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f21,f24,f21
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f15,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f15.f64 = double(temp.f32);
	// stfs f25,152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f24,148(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lfs f24,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f25.f64 = double(temp.f32);
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f25,f25,f24
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// lfs f4,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f13,f11,f10
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f28,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f4,f12,f4
	ctx.f4.f64 = double(float(ctx.f12.f64 * ctx.f4.f64));
	// lfs f20,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f28,f12,f28
	ctx.f28.f64 = double(float(ctx.f12.f64 * ctx.f28.f64));
	// lfs f19,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// lfs f9,1356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f8,1532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f2,f9,f2
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f7,1348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	ctx.f7.f64 = double(temp.f32);
	// fmr f29,f31
	ctx.f29.f64 = ctx.f31.f64;
	// lfs f30,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f30.f64 = double(temp.f32);
	// lfs f22,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f3,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f31,f8,f31
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// lfs f23,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f3,f0,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// stfs f10,324(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// stfs f18,136(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f12,308(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// stfs f25,116(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f25,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f25.f64 = double(temp.f32);
	// lfs f6,1728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1728);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,1944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1944);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f29,f7,f29
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f29.f64));
	// lfs f17,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// lfs f14,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f16,f6,f16
	ctx.f16.f64 = double(float(ctx.f6.f64 * ctx.f16.f64));
	// lfs f10,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f10.f64 = double(temp.f32);
	// lfs f12,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f10,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// lfs f18,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f14,f14,f12
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f12.f64));
	// lfs f24,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f18,f25,f18
	ctx.f18.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// stfs f4,2036(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2036, temp.u32);
	// fmuls f4,f25,f24
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// stfs f0,948(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// stfs f3,2044(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2044, temp.u32);
	// stfs f9,1380(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1380, temp.u32);
	// stfs f2,2052(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2052, temp.u32);
	// stfs f30,652(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f1,4368(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4368, temp.u32);
	// stfs f8,1668(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1668, temp.u32);
	// stfs f31,3312(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3312, temp.u32);
	// stfs f7,1356(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1356, temp.u32);
	// stfs f29,3320(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3320, temp.u32);
	// stfs f28,3384(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3384, temp.u32);
	// stfs f27,3336(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3336, temp.u32);
	// stfs f26,3416(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3416, temp.u32);
	// stfs f22,1992(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1992, temp.u32);
	// stfs f23,2020(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2020, temp.u32);
	// stfs f21,2068(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2068, temp.u32);
	// stfs f20,1528(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1528, temp.u32);
	// stfs f13,1552(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1552, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f10,636(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f10,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f10.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lfs f13,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// fmuls f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// lfs f13,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f7.f64 = double(temp.f32);
	// fmr f8,f13
	ctx.f8.f64 = ctx.f13.f64;
	// stfs f6,2952(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2952, temp.u32);
	// fmuls f6,f7,f13
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f13.f64));
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f2,f25,f13
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f13,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f30.f64 = double(temp.f32);
	// fmr f27,f13
	ctx.f27.f64 = ctx.f13.f64;
	// lfs f26,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f30,f13
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// fmuls f24,f26,f13
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f13.f64));
	// lfs f28,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f28.f64 = double(temp.f32);
	// lfs f13,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f21,f28,f13
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// lfs f13,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// stfs f18,908(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// stfs f16,812(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// lfs f18,1620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1620);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,1524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	ctx.f16.f64 = double(temp.f32);
	// stfs f19,2084(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2084, temp.u32);
	// fmuls f19,f13,f28
	ctx.f19.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// stfs f30,336(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f13,f16,f18
	ctx.f13.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// lfs f23,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f23.f64 = double(temp.f32);
	// lfs f30,1324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1324);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// stfs f14,820(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// stfs f10,392(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// lfs f14,1332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	ctx.f14.f64 = double(temp.f32);
	// lfs f10,1704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1704);
	ctx.f10.f64 = double(temp.f32);
	// stfs f13,148(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f13,f30,f18
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// stfs f26,220(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fadds f26,f10,f14
	ctx.f26.f64 = double(float(ctx.f10.f64 + ctx.f14.f64));
	// stfs f0,1536(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1536, temp.u32);
	// stfs f13,204(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f17,1152(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// stfs f12,1840(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1840, temp.u32);
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// lfs f17,1340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1340);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f11,2080(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2080, temp.u32);
	// stfs f5,876(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f26,180(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lfs f11,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f11,f12,f11
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f3,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f8,f5,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f8.f64));
	// lfs f1,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f3,f1,f3
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f22,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f20,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f28,f22
	ctx.f22.f64 = double(float(ctx.f28.f64 * ctx.f22.f64));
	// lfs f26,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f20,f20,f28
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f28.f64));
	// stfs f15,1896(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1896, temp.u32);
	// fmuls f15,f17,f18
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// stfs f13,116(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f13,-19536(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19536);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// stfs f18,136(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f18,f26,f18
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f18.f64));
	// stfs f4,916(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// stfs f0,1560(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1560, temp.u32);
	// lfs f4,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f4,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// stfs f12,684(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// stfs f0,116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f0,-19108(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19108);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f12,-19112(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19112);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f12,576(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 576, temp.u32);
	// lfs f12,1316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1316);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,1168(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// fmuls f11,f15,f0
	ctx.f11.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// stfs f13,700(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 700, temp.u32);
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// stfs f19,3592(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3592, temp.u32);
	// stfs f20,2164(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2164, temp.u32);
	// stfs f17,2740(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2740, temp.u32);
	// stfs f10,1120(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// stfs f6,660(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// lfs f10,-19116(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19116);
	ctx.f10.f64 = double(temp.f32);
	// lfs f17,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f17.f64 = double(temp.f32);
	// lfs f6,1508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1508);
	ctx.f6.f64 = double(temp.f32);
	// stfs f28,2540(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2540, temp.u32);
	// stfs f30,3228(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3228, temp.u32);
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lfs f28,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f28.f64 = double(temp.f32);
	// lfs f10,1908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1908);
	ctx.f10.f64 = double(temp.f32);
	// lfs f30,1740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1740);
	ctx.f30.f64 = double(temp.f32);
	// stfs f31,1664(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1664, temp.u32);
	// fmuls f30,f30,f28
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// lfs f20,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f11,f17,f20,f11
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f20.f64 - ctx.f11.f64));
	// stfs f24,3512(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3512, temp.u32);
	// fmsubs f13,f6,f19,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f19.f64 - ctx.f13.f64));
	// stfs f27,1680(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1680, temp.u32);
	// stfs f9,1160(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// stfs f7,1064(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// stfs f5,1184(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// stfs f25,656(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// stfs f2,548(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// stfs f3,1248(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// stfs f29,3504(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3504, temp.u32);
	// stfs f23,488(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// stfs f26,1448(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1448, temp.u32);
	// lfs f5,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,1612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1612);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,1308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1308);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,1812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1812);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,1748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1748);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f2,f28
	ctx.f28.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// lfs f29,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f31,f29
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f29.f64));
	// lfs f26,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,1300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1300);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f27,f27,f26
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// lfs f24,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,1660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1660);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f25,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// lfs f20,1500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1500);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,1284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1284);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f17.f64 = double(temp.f32);
	// stfs f8,1032(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f1,1792(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1792, temp.u32);
	// fmuls f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// stfs f21,2100(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2100, temp.u32);
	// fmuls f1,f3,f5
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// stfs f22,2108(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2108, temp.u32);
	// fmuls f21,f23,f5
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// stfs f16,1780(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1780, temp.u32);
	// fmuls f16,f20,f17
	ctx.f16.f64 = double(float(ctx.f20.f64 * ctx.f17.f64));
	// stfs f14,1112(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// fmuls f14,f19,f5
	ctx.f14.f64 = double(float(ctx.f19.f64 * ctx.f5.f64));
	// stfs f4,944(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// fmuls f4,f7,f5
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// lfs f22,1292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1292);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f15.f64 = double(temp.f32);
	// stfs f14,436(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// fmuls f15,f22,f15
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// lfs f14,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f30,f5
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f5.f64));
	// stfs f21,252(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// lfs f21,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f21.f64 = double(temp.f32);
	// fmr f14,f21
	ctx.f14.f64 = ctx.f21.f64;
	// stfs f17,136(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f17,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f17.f64 = double(temp.f32);
	// stfs f30,360(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmadds f8,f17,f21,f8
	ctx.f8.f64 = double(float(ctx.f17.f64 * ctx.f21.f64 + ctx.f8.f64));
	// lfs f30,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f30.f64 = double(temp.f32);
	// stfs f3,808(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// fmr f3,f30
	ctx.f3.f64 = ctx.f30.f64;
	// stfs f6,900(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// stfs f10,3644(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3644, temp.u32);
	// lfs f21,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f21.f64 = double(temp.f32);
	// lfs f6,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f17,f20,f21
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f21.f64));
	// lfs f10,1604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1604);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f0,f31,f14,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f14.f64 + ctx.f0.f64));
	// lfs f14,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// stfs f4,388(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// lfs f31,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f1,f14
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f14.f64));
	// lfs f4,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f13,f4,f31,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f31.f64 + ctx.f13.f64));
	// stfs f29,688(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fmuls f31,f27,f5
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// stfs f31,464(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// lfs f31,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f11,f31,f30,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f30.f64 + ctx.f11.f64));
	// lfs f29,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f29.f64 = double(temp.f32);
	// fmr f31,f14
	ctx.f31.f64 = ctx.f14.f64;
	// stfs f12,1072(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// stfs f9,3460(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3460, temp.u32);
	// fmuls f12,f28,f5
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f5.f64));
	// lfs f27,-19508(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19508);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f29,f20
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f20.f64));
	// lfs f9,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f9.f64 = double(temp.f32);
	// stfs f24,3528(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3528, temp.u32);
	// stfs f2,940(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// fmadds f13,f10,f6,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f6.f64 + ctx.f13.f64));
	// lfs f2,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f2.f64 = double(temp.f32);
	// lfs f4,3216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3216);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f30.f64 = double(temp.f32);
	// lfs f6,1276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1276);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f11,f18,f3,f11
	ctx.f11.f64 = double(float(-(ctx.f18.f64 * ctx.f3.f64 - ctx.f11.f64)));
	// lfs f24,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// stfs f23,2724(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2724, temp.u32);
	// fmuls f24,f17,f24
	ctx.f24.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// stfs f27,596(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 596, temp.u32);
	// stfs f26,440(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f7,904(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// fmuls f7,f9,f31
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f31.f64));
	// lfs f3,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f31,f2,f15
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f27,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f13,f6,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// lfs f26,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f3,f9,f3
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// lfs f23,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f25,f27
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f27.f64));
	// lfs f20,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f26,f26,f16
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// stfs f4,504(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 504, temp.u32);
	// fmuls f4,f21,f15
	ctx.f4.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// stfs f19,1772(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1772, temp.u32);
	// fmuls f19,f2,f22
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f22.f64));
	// stfs f12,412(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmuls f22,f21,f22
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// lfs f18,1492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f23,f23,f25
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// lfs f30,1268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1268);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f20,f25,f20
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f20.f64));
	// lfs f17,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f14.f64 = double(temp.f32);
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f17,f17,f12
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f12.f64));
	// stfs f17,116(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f15,f15,f12
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f12.f64));
	// stfs f12,136(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f18,f18,f12
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// stfs f1,408(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// fmuls f30,f30,f12
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f12.f64));
	// lfs f12,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f12.f64 = double(temp.f32);
	// stfs f15,148(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f14,f14,f12
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f12.f64));
	// lfs f1,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f11,f8,f5,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f5.f64 + ctx.f11.f64));
	// stfs f14,152(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f1,f1,f12
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f1,180(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f17,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f17.f64 = double(temp.f32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lfs f14,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f14.f64 = double(temp.f32);
	// fmr f15,f17
	ctx.f15.f64 = ctx.f17.f64;
	// lfs f1,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f1.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f0,308(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmadds f13,f14,f1,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f1.f64 + ctx.f13.f64));
	// lfs f0,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f0.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// stfs f10,1088(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// fmuls f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f12,204(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// lfs f12,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f17,f12,f17
	ctx.f17.f64 = double(float(ctx.f12.f64 * ctx.f17.f64));
	// stfs f0,3712(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3712, temp.u32);
	// stfs f12,564(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// lfs f12,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f0,f12,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f10,2156(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2156, temp.u32);
	// lfs f10,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,4088(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4088, temp.u32);
	// lfs f10,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,2196(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2196, temp.u32);
	// lfs f10,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f10.f64 = double(temp.f32);
	// stfs f7,3536(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3536, temp.u32);
	// lfs f7,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f7.f64 = double(temp.f32);
	// lfs f11,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f15,f7,f15
	ctx.f15.f64 = double(float(ctx.f7.f64 * ctx.f15.f64));
	// lfs f12,1724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1724);
	ctx.f12.f64 = double(temp.f32);
	// stfs f10,3632(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3632, temp.u32);
	// fnmsubs f13,f12,f11,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// lfs f10,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f10.f64 = double(temp.f32);
	// stfs f9,1276(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1276, temp.u32);
	// stfs f10,3640(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3640, temp.u32);
	// lfs f10,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,2144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2144);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f8.f64 = double(temp.f32);
	// stfs f3,2124(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2124, temp.u32);
	// stfs f27,2132(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2132, temp.u32);
	// stfs f4,3552(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3552, temp.u32);
	// stfs f31,3584(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3584, temp.u32);
	// stfs f26,3560(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3560, temp.u32);
	// stfs f16,2180(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2180, temp.u32);
	// stfs f24,3568(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3568, temp.u32);
	// stfs f29,4144(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4144, temp.u32);
	// stfs f28,2140(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2140, temp.u32);
	// stfs f2,3244(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3244, temp.u32);
	// stfs f19,3576(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3576, temp.u32);
	// stfs f21,1796(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1796, temp.u32);
	// stfs f22,2148(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2148, temp.u32);
	// stfs f6,932(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f23,2172(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2172, temp.u32);
	// stfs f25,1576(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1576, temp.u32);
	// stfs f20,2188(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2188, temp.u32);
	// stfs f18,2204(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2204, temp.u32);
	// stfs f30,3544(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3544, temp.u32);
	// stfs f14,1136(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// stfs f17,2212(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2212, temp.u32);
	// stfs f7,792(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// stfs f15,5016(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5016, temp.u32);
	// lfs f30,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f9,f11
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// fmadds f30,f30,f5,f0
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f0.f64));
	// lfs f3,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f3.f64 = double(temp.f32);
	// lfs f16,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f16.f64 = double(temp.f32);
	// lis r6,-32254
	ctx.r6.s64 = -2113798144;
	// lfs f7,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f3,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// stfs f3,532(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// lfs f3,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f3.f64 = double(temp.f32);
	// stfs f5,308(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f14,1288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1288);
	ctx.f14.f64 = double(temp.f32);
	// lfs f5,1280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1280);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,336(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f5,f5,f14
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f14.f64));
	// stfs f14,220(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fnmsubs f10,f16,f10,f30
	ctx.f10.f64 = double(float(-(ctx.f16.f64 * ctx.f10.f64 - ctx.f30.f64)));
	// lfs f16,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f3,f16
	ctx.f16.f64 = double(float(ctx.f3.f64 * ctx.f16.f64));
	// stfs f16,324(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f16,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f16.f64 = double(temp.f32);
	// fmr f14,f16
	ctx.f14.f64 = ctx.f16.f64;
	// lfs f6,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f6.f64 = double(temp.f32);
	// stfs f5,204(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f8,f6,f8
	ctx.f8.f64 = double(float(ctx.f6.f64 * ctx.f8.f64));
	// lfs f29,1260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1260);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,1640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1640);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f28,f29
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// lfs f23,1616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1616);
	ctx.f23.f64 = double(temp.f32);
	// lfs f2,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f2.f64 = double(temp.f32);
	// lfs f17,1820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1820);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f2,f27,f2
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// lfs f30,2072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2072);
	ctx.f30.f64 = double(temp.f32);
	// lfs f18,1296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1296);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f15,f30,f23
	ctx.f15.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmr f5,f14
	ctx.f5.f64 = ctx.f14.f64;
	// lfs f1,1936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1936);
	ctx.f1.f64 = double(temp.f32);
	// lfs f4,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f14,f17,f14
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// stfs f24,116(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmuls f16,f18,f16
	ctx.f16.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f27,1632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1632);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f4,f6,f4
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f25,1624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1624);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f1,f29
	ctx.f26.f64 = double(float(ctx.f1.f64 * ctx.f29.f64));
	// lfs f21,1360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1360);
	ctx.f21.f64 = double(temp.f32);
	// fadds f22,f25,f27
	ctx.f22.f64 = double(float(ctx.f25.f64 + ctx.f27.f64));
	// lfs f0,2916(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2916);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f20,f21,f23
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// lfs f19,1352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1352);
	ctx.f19.f64 = double(temp.f32);
	// lfs f24,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f13,f19,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f15,180(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f14,152(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fnmsubs f10,f24,f5,f10
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f5.f64 - ctx.f10.f64)));
	// lfs f15,904(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 904);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,-19120(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19120);
	ctx.f14.f64 = double(temp.f32);
	// stfs f16,148(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f29,136(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f16,-19124(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19124);
	ctx.f16.f64 = double(temp.f32);
	// lfs f29,-19128(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -19128);
	ctx.f29.f64 = double(temp.f32);
	// stfs f8,2152(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2152, temp.u32);
	// stfs f7,1688(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1688, temp.u32);
	// stfs f6,1672(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1672, temp.u32);
	// stfs f4,3720(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3720, temp.u32);
	// stfs f11,1344(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1344, temp.u32);
	// stfs f9,2648(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2648, temp.u32);
	// stfs f31,704(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 704, temp.u32);
	// stfs f2,720(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 720, temp.u32);
	// stfs f12,668(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// stfs f3,816(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// stfs f1,1544(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1544, temp.u32);
	// stfs f28,1600(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1600, temp.u32);
	// stfs f25,2092(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2092, temp.u32);
	// stfs f27,1328(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1328, temp.u32);
	// stfs f21,1684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1684, temp.u32);
	// stfs f0,432(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 432, temp.u32);
	// stfs f19,1176(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// stfs f15,676(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 676, temp.u32);
	// stfs f14,396(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 396, temp.u32);
	// stfs f16,280(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// fmr f6,f5
	ctx.f6.f64 = ctx.f5.f64;
	// fmr f0,f14
	ctx.f0.f64 = ctx.f14.f64;
	// lfs f11,-19520(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -19520);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f12,f22,f23
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f8,1564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1564);
	ctx.f8.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f11,960(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 960, temp.u32);
	// fnmsubs f13,f8,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// stfs f17,1444(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1444, temp.u32);
	// fmr f5,f14
	ctx.f5.f64 = ctx.f14.f64;
	// lfs f11,1476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1476);
	ctx.f11.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f17,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f17.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lfs f22,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f22.f64 = double(temp.f32);
	// fmr f15,f17
	ctx.f15.f64 = ctx.f17.f64;
	// lfs f14,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f14.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// fmuls f6,f20,f6
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f6.f64));
	// stfs f29,512(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 512, temp.u32);
	// fmuls f0,f26,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f26,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f20,f22,f11
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f11.f64));
	// fmadds f17,f26,f17,f14
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f17.f64 + ctx.f14.f64));
	// stfs f20,136(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f20,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f20.f64 = double(temp.f32);
	// lfs f4,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f3.f64 = double(temp.f32);
	// lfs f9,1652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1652);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f10,f3,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f3,1692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1692);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f7,f9,f11
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// lfs f4,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f31,f3,f11
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// lfs f14,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f20,f20,f14
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f14.f64));
	// lfs f14,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f14.f64 = double(temp.f32);
	// fmsubs f12,f12,f14,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f14.f64 - ctx.f6.f64));
	// stfs f11,252(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fmr f6,f14
	ctx.f6.f64 = ctx.f14.f64;
	// lfs f11,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f11.f64 = double(temp.f32);
	// lfs f24,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f0,f4,f5,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f5.f64 - ctx.f0.f64));
	// fmadds f11,f24,f15,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f15.f64 + ctx.f11.f64));
	// lfs f15,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f15.f64 = double(temp.f32);
	// lfs f21,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f13,f21,f15,f13
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f15.f64 + ctx.f13.f64));
	// lfs f15,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f7,f7,f15
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f15.f64));
	// stfs f18,1816(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1816, temp.u32);
	// lfs f15,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f15.f64 = double(temp.f32);
	// lfs f19,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f10,f14,f6,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f14,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f0,f14,f15,f0
	ctx.f0.f64 = double(float(ctx.f14.f64 * ctx.f15.f64 + ctx.f0.f64));
	// lfs f6,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f6.f64 = double(temp.f32);
	// lfs f15,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f15.f64 = double(temp.f32);
	// fadds f16,f18,f19
	ctx.f16.f64 = double(float(ctx.f18.f64 + ctx.f19.f64));
	// stfs f6,180(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f6,f6,f15
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// stfs f30,1256(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1256, temp.u32);
	// lfs f5,1412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1412);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,1396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1396);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f1.f64 = double(temp.f32);
	// fadds f2,f4,f5
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// lfs f30,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f1,f29
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f29.f64));
	// lfs f27,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f27.f64 = double(temp.f32);
	// stfs f6,152(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fadds f25,f27,f30
	ctx.f25.f64 = double(float(ctx.f27.f64 + ctx.f30.f64));
	// stfs f7,148(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f16,116(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f14,-19132(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19132);
	ctx.f14.f64 = double(temp.f32);
	// lfs f6,-19504(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19504);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,-19492(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19492);
	ctx.f7.f64 = double(temp.f32);
	// lfs f16,-19480(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19480);
	ctx.f16.f64 = double(temp.f32);
	// stfs f7,580(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 580, temp.u32);
	// fmadds f12,f17,f23,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f23.f64 + ctx.f12.f64));
	// stfs f9,1124(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1124, temp.u32);
	// fmsubs f11,f11,f23,f20
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f23.f64 - ctx.f20.f64));
	// stfs f8,1220(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// fmuls f8,f2,f15
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f7,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f7.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f9,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f9.f64 = double(temp.f32);
	// stfs f4,1264(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1264, temp.u32);
	// fmadds f13,f9,f7,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f13.f64));
	// lfs f2,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f2.f64 = double(temp.f32);
	// lfs f4,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f4.f64 = double(temp.f32);
	// stfs f30,936(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// fnmsubs f10,f2,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// fmr f30,f14
	ctx.f30.f64 = ctx.f14.f64;
	// stfs f6,732(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 732, temp.u32);
	// stfs f3,1468(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1468, temp.u32);
	// lfs f3,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f3.f64 = double(temp.f32);
	// lfs f7,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f7.f64 = double(temp.f32);
	// stfs f14,420(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 420, temp.u32);
	// stfs f18,1272(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1272, temp.u32);
	// lfs f6,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f13,f7,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f14,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f18.f64 = double(temp.f32);
	// lfs f2,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f10,f14,f18,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f18.f64 + ctx.f10.f64));
	// lfs f17,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f0,f28,f30,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 + ctx.f0.f64));
	// stfs f5,1460(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1460, temp.u32);
	// fmuls f5,f25,f6
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// stfs f27,744(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmuls f2,f31,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// lfs f31,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// lfs f30,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f31,f29
	ctx.f28.f64 = double(float(ctx.f31.f64 * ctx.f29.f64));
	// lfs f27,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f25.f64 = double(temp.f32);
	// lfs f18,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f14.f64 = double(temp.f32);
	// stfs f17,360(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmuls f17,f18,f29
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f29.f64));
	// stfs f26,1252(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1252, temp.u32);
	// fmuls f26,f30,f29
	ctx.f26.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f24,1428(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1428, temp.u32);
	// fmuls f24,f27,f29
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// stfs f21,2144(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2144, temp.u32);
	// fmuls f21,f25,f29
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// stfs f29,572(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// fmuls f29,f14,f29
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// lfs f4,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f4.f64 = double(temp.f32);
	// stfs f17,464(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// stfs f29,436(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f17,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f17.f64 = double(temp.f32);
	// lfs f29,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f29.f64 = double(temp.f32);
	// stfs f1,1732(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1732, temp.u32);
	// fmuls f1,f4,f6
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// stfs f5,364(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f5,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f5.f64 = double(temp.f32);
	// stfs f16,924(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 924, temp.u32);
	// stfs f11,204(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f11,f29,f23
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// stfs f6,388(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f6,f17,f23
	ctx.f6.f64 = double(float(ctx.f17.f64 * ctx.f23.f64));
	// lfs f16,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f16.f64 = double(temp.f32);
	// lfs f20,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f20.f64 = double(temp.f32);
	// stfs f23,392(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// fmuls f23,f5,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f23.f64));
	// stfs f22,1304(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1304, temp.u32);
	// fmadds f13,f20,f16,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 + ctx.f13.f64));
	// lfs f3,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f3.f64 = double(temp.f32);
	// lfs f22,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f22.f64 = double(temp.f32);
	// stfs f19,1336(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1336, temp.u32);
	// fmuls f19,f22,f3
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f3.f64));
	// stfs f11,220(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f23,412(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// lfs f11,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f11.f64 = double(temp.f32);
	// lfs f16,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f23.f64 = double(temp.f32);
	// stfs f12,116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f6,136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f6,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f12,f12,f16,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f16.f64 - ctx.f6.f64));
	// lfs f6,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f6.f64 = double(temp.f32);
	// stfs f12,148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmsubs f8,f8,f6,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f6.f64 - ctx.f2.f64));
	// lfs f12,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f16,f11,f15
	ctx.f16.f64 = double(float(ctx.f11.f64 * ctx.f15.f64));
	// stfs f28,152(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmuls f12,f12,f23
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f23.f64));
	// stfs f29,1420(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1420, temp.u32);
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// stfs f16,408(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// lfs f16,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f0,f1,f16,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// stfs f30,1368(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1368, temp.u32);
	// stfs f27,1592(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1592, temp.u32);
	// lfs f27,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f30.f64 = double(temp.f32);
	// stfs f5,1196(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// stfs f13,5200(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5200, temp.u32);
	// lfs f6,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f6.f64 = double(temp.f32);
	// lfs f13,-19464(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19464);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f29,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f29.f64 = double(temp.f32);
	// lfs f23,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f23.f64 = double(temp.f32);
	// lfs f28,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f2,f28,f2,f29
	ctx.f2.f64 = double(float(-(ctx.f28.f64 * ctx.f2.f64 - ctx.f29.f64)));
	// lfs f28,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f28.f64 = double(temp.f32);
	// fmr f29,f16
	ctx.f29.f64 = ctx.f16.f64;
	// lfs f5,-19136(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19136);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f30,f27,f30,f28
	ctx.f30.f64 = double(float(-(ctx.f27.f64 * ctx.f30.f64 - ctx.f28.f64)));
	// lfs f28,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f28.f64 = double(temp.f32);
	// stfs f4,440(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f7,1180(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// stfs f26,556(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// stfs f21,600(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfs f25,1832(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1832, temp.u32);
	// stfs f3,1200(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1200, temp.u32);
	// stfs f10,180(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// stfs f11,1644(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1644, temp.u32);
	// fmadds f0,f28,f29,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 + ctx.f0.f64));
	// stfs f13,404(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 404, temp.u32);
	// stfs f5,244(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 244, temp.u32);
	// lfs f5,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f7,f23,f7
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f7.f64));
	// lfs f3,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f4,f5
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// lfs f1,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f29,f5
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// lfs f26,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,1656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1656);
	ctx.f25.f64 = double(temp.f32);
	// lfs f21,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f21.f64 = double(temp.f32);
	// stfs f6,1580(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1580, temp.u32);
	// fmuls f6,f11,f15
	ctx.f6.f64 = double(float(ctx.f11.f64 * ctx.f15.f64));
	// stfs f9,1080(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// fmuls f9,f13,f10
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f10.f64));
	// stfs f31,1800(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1800, temp.u32);
	// fmuls f31,f3,f1
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// stfs f24,628(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// fmuls f24,f26,f28
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// stfs f19,640(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// stfs f22,1936(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1936, temp.u32);
	// stfs f20,1056(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// fmuls f20,f25,f21
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f21.f64));
	// lfs f22,1648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1648);
	ctx.f22.f64 = double(temp.f32);
	// lfs f19,1388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1388);
	ctx.f19.f64 = double(temp.f32);
	// stfs f18,1484(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1484, temp.u32);
	// stfs f14,892(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// stfs f17,1132(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// lfs f18,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f14,f19,f10
	ctx.f14.f64 = double(float(ctx.f19.f64 * ctx.f10.f64));
	// stfs f24,308(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f17,f22,f18
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// lfs f24,1716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1716);
	ctx.f24.f64 = double(temp.f32);
	// stfs f18,364(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f18,f24,f18
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f18.f64));
	// stfs f3,204(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f20,680(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// stfs f18,336(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f3,1836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1836);
	ctx.f3.f64 = double(temp.f32);
	// lfs f20,1516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1516);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,1828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1828);
	ctx.f18.f64 = double(temp.f32);
	// stfs f27,220(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmuls f27,f20,f10
	ctx.f27.f64 = double(float(ctx.f20.f64 * ctx.f10.f64));
	// stfs f17,116(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fadds f17,f18,f3
	ctx.f17.f64 = double(float(ctx.f18.f64 + ctx.f3.f64));
	// stfs f27,136(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f17,388(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// lfs f27,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f27.f64 = double(temp.f32);
	// lfs f17,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f7,f7,f27
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// fmuls f31,f31,f17
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// stfs f31,324(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// stfs f7,252(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f27,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f27.f64 = double(temp.f32);
	// lfs f31,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f31.f64 = double(temp.f32);
	// lfs f7,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f31,f27,f7
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f27.f64 + ctx.f7.f64));
	// lfs f17,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f17.f64 = double(temp.f32);
	// lfs f27,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f12,f27,f17,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f17.f64 - ctx.f12.f64));
	// lfs f31,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f2,f17,f31,f2
	ctx.f2.f64 = double(float(-(ctx.f17.f64 * ctx.f31.f64 - ctx.f2.f64)));
	// lfs f27,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f27.f64 = double(temp.f32);
	// lfs f17,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f0,f17,f27,f0
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f27.f64 - ctx.f0.f64)));
	// lfs f31,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f31.f64 = double(temp.f32);
	// stfs f13,1712(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1712, temp.u32);
	// fnmsubs f9,f9,f31,f8
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f31.f64 - ctx.f8.f64)));
	// stfs f10,1724(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1724, temp.u32);
	// lfs f27,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f27.f64 = double(temp.f32);
	// lfs f13,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f6,f6,f27,f30
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f27.f64 - ctx.f30.f64)));
	// lfs f10,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,-19140(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19140);
	ctx.f8.f64 = double(temp.f32);
	// stfs f1,740(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// stfs f8,544(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 544, temp.u32);
	// lfs f16,1540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1540);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f0,f10,f13,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f31,-19512(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19512);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f30.f64 = double(temp.f32);
	// lfs f27,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f27.f64 = double(temp.f32);
	// lfs f13,1928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1928);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f10.f64 = double(temp.f32);
	// lfs f8,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f8.f64 = double(temp.f32);
	// lfs f1,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f1.f64 = double(temp.f32);
	// stfs f15,392(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// fmuls f15,f16,f15
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f15.f64));
	// stfs f11,1588(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1588, temp.u32);
	// fmuls f11,f27,f30
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// stfs f29,1616(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1616, temp.u32);
	// fmuls f1,f13,f1
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// stfs f28,460(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 460, temp.u32);
	// stfs f31,500(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 500, temp.u32);
	// fmuls f31,f10,f5
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// stfs f3,1436(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1436, temp.u32);
	// fmuls f3,f8,f5
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f29,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f28.f64 = double(temp.f32);
	// stfs f23,1008(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1008, temp.u32);
	// stfs f26,1260(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1260, temp.u32);
	// stfs f25,528(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// stfs f21,1128(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// stfs f22,1312(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1312, temp.u32);
	// stfs f19,1596(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1596, temp.u32);
	// stfs f16,1452(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1452, temp.u32);
	// stfs f20,1188(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1188, temp.u32);
	// stfs f24,1116(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1116, temp.u32);
	// stfs f18,1572(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1572, temp.u32);
	// lfs f25,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f29,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// lfs f27,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f27.f64 = double(temp.f32);
	// fmr f29,f25
	ctx.f29.f64 = ctx.f25.f64;
	// lfs f26,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f7,f14,f25,f7
	ctx.f7.f64 = double(float(-(ctx.f14.f64 * ctx.f25.f64 - ctx.f7.f64)));
	// fnmsubs f4,f4,f26,f27
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f26.f64 - ctx.f27.f64)));
	// lfs f27,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f12,f15,f27,f12
	ctx.f12.f64 = double(float(-(ctx.f15.f64 * ctx.f27.f64 - ctx.f12.f64)));
	// lfs f27,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f2,f27,f26,f2
	ctx.f2.f64 = double(float(-(ctx.f27.f64 * ctx.f26.f64 - ctx.f2.f64)));
	// lfs f26,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f26.f64 = double(temp.f32);
	// lfs f18,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f15,f18,f5
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f5.f64));
	// lfs f28,2064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2064);
	ctx.f28.f64 = double(temp.f32);
	// stfs f15,464(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// lfs f15,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f9,f26,f29,f9
	ctx.f9.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f9.f64));
	// lfs f26,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f26.f64 = double(temp.f32);
	// lfs f16,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f25,f28,f26
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f21,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f21.f64 = double(temp.f32);
	// stfs f28,412(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// fmuls f28,f15,f16
	ctx.f28.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f14,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f19,f21,f5
	ctx.f19.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// stfs f28,408(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// lfs f28,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f28.f64 = double(temp.f32);
	// stfs f19,360(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmuls f19,f14,f5
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f5.f64));
	// stfs f3,204(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fmuls f3,f28,f15
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f15.f64));
	// stfs f15,152(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f19,436(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// lfs f15,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f15.f64 = double(temp.f32);
	// lfs f19,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f19.f64 = double(temp.f32);
	// stfs f3,440(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmuls f19,f15,f19
	ctx.f19.f64 = double(float(ctx.f15.f64 * ctx.f19.f64));
	// lfs f29,1708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1708);
	ctx.f29.f64 = double(temp.f32);
	// lfs f3,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f3.f64 = double(temp.f32);
	// lfs f15,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f3,f29,f3
	ctx.f3.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// stfs f3,180(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmr f3,f15
	ctx.f3.f64 = ctx.f15.f64;
	// lfs f27,2060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2060);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f27,f26
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// lfs f20,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f15,f24,f15
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f15.f64));
	// fmuls f17,f20,f26
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f26.f64));
	// stfs f26,148(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f15,220(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f17,136(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f23,116(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f22,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f15.f64 = double(temp.f32);
	// lfs f26,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f26.f64 = double(temp.f32);
	// lfs f17,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f3,f22,f3,f26
	ctx.f3.f64 = double(float(ctx.f22.f64 * ctx.f3.f64 - ctx.f26.f64));
	// lfs f23,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f23,f17,f15,f23
	ctx.f23.f64 = double(float(ctx.f17.f64 * ctx.f15.f64 + ctx.f23.f64));
	// lfs f26,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f26.f64 = double(temp.f32);
	// lfs f15,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f6,f15,f26,f6
	ctx.f6.f64 = double(float(-(ctx.f15.f64 * ctx.f26.f64 - ctx.f6.f64)));
	// lfs f26,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f26.f64 = double(temp.f32);
	// lfs f17,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// fmadds f11,f11,f17,f1
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f17.f64 + ctx.f1.f64));
	// lfs f1,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f1.f64 = double(temp.f32);
	// lfs f26,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f26.f64 = double(temp.f32);
	// stfs f30,488(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// fnmsubs f1,f1,f26,f0
	ctx.f1.f64 = double(float(-(ctx.f1.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// stfs f13,1352(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1352, temp.u32);
	// stfs f10,1636(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1636, temp.u32);
	// stfs f27,1548(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1548, temp.u32);
	// stfs f29,1224(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1224, temp.u32);
	// stfs f8,1364(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1364, temp.u32);
	// stfs f24,2072(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2072, temp.u32);
	// stfs f22,1624(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1624, temp.u32);
	// stfs f21,1628(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1628, temp.u32);
	// fmr f0,f26
	ctx.f0.f64 = ctx.f26.f64;
	// lfs f13,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f13.f64 = double(temp.f32);
	// stfs f9,5224(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5224, temp.u32);
	// fmr f9,f26
	ctx.f9.f64 = ctx.f26.f64;
	// lfs f8,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f8.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f2,5212(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5212, temp.u32);
	// fmr f2,f26
	ctx.f2.f64 = ctx.f26.f64;
	// lfs f10,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f10.f64 = double(temp.f32);
	// stfs f12,5220(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5220, temp.u32);
	// fmsubs f12,f3,f5,f19
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 - ctx.f19.f64));
	// stfs f20,1404(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1404, temp.u32);
	// lfs f3,-19144(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19144);
	ctx.f3.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f16,532(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// stfs f28,564(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmadds f13,f13,f0,f1
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f1.f64));
	// lfs f28,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f0,f8,f0,f23
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f23.f64)));
	// lfs f16,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f16.f64 = double(temp.f32);
	// lfs f20,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f20.f64 = double(temp.f32);
	// stfs f3,472(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 472, temp.u32);
	// fmuls f15,f20,f16
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// lfs f8,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f8.f64 = double(temp.f32);
	// lfs f3,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f12,f11,f5,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 + ctx.f12.f64));
	// stfs f25,252(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f14,1632(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1632, temp.u32);
	// lfs f26,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f26.f64 = double(temp.f32);
	// lfs f14,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f23,f26,f5
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f5.f64));
	// fnmsubs f13,f10,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f9,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f0,f9,f2,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 + ctx.f0.f64));
	// lfs f2,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f2.f64 = double(temp.f32);
	// lfs f10,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f25,f2,f28
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// stfs f25,324(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f27,f10,f28
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// stfs f4,5204(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5204, temp.u32);
	// fmuls f25,f14,f16
	ctx.f25.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// stfs f7,5216(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5216, temp.u32);
	// stfs f15,336(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f19,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f19.f64 = double(temp.f32);
	// lfs f7,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f8,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f17,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f4,f4,f7
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// lfs f8,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f8.f64 = double(temp.f32);
	// lfs f3,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f3.f64 = double(temp.f32);
	// lfs f22,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f22.f64 = double(temp.f32);
	// stfs f23,364(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmuls f23,f17,f19
	ctx.f23.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// stfs f25,388(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// fmuls f25,f8,f15
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f15.f64));
	// stfs f27,308(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f27,f3,f5
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// stfs f18,1640(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1640, temp.u32);
	// fmuls f18,f22,f19
	ctx.f18.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// stfs f25,392(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f27,148(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f19,136(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f23,116(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f19,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f19.f64 = double(temp.f32);
	// lfs f23,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f23.f64 = double(temp.f32);
	// lfs f11,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f11.f64 = double(temp.f32);
	// lfs f25,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f25,f11,f25,f19
	ctx.f25.f64 = double(float(ctx.f11.f64 * ctx.f25.f64 + ctx.f19.f64));
	// lfs f1,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// lfs f24,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f1,f5
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfs f30,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f21,f24,f5
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// lfs f9,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f9.f64 = double(temp.f32);
	// lfs f19,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f19.f64 = double(temp.f32);
	// lfs f23,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f23.f64 = double(temp.f32);
	// stfs f27,152(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// lfs f27,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f27,f9,f19,f27
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f19.f64 + ctx.f27.f64));
	// lfs f19,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f6,f4,f19,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f19.f64 + ctx.f6.f64));
	// stfs f27,180(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmr f4,f30
	ctx.f4.f64 = ctx.f30.f64;
	// lfs f27,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f27,f23
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f23.f64));
	// lfs f23,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f31,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// lfs f23,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f23.f64 = double(temp.f32);
	// stfs f11,1556(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1556, temp.u32);
	// fmr f11,f19
	ctx.f11.f64 = ctx.f19.f64;
	// stfs f10,1240(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1240, temp.u32);
	// lfs f10,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f10.f64 = double(temp.f32);
	// stfs f26,1820(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1820, temp.u32);
	// fmr f26,f19
	ctx.f26.f64 = ctx.f19.f64;
	// stfs f1,1280(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1280, temp.u32);
	// stfs f2,1216(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1216, temp.u32);
	// lfs f2,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f0,f23,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// lfs f4,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f4,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// stfs f13,5208(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5208, temp.u32);
	// lfs f13,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f18,f13,f6
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f13.f64 + ctx.f6.f64));
	// lfs f1,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f10,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// stfs f29,204(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f29,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f4,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,-19148(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19148);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f6,f21,f4
	ctx.f6.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// lfs f30,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f30.f64 = double(temp.f32);
	// stfs f30,1676(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1676, temp.u32);
	// fmadds f0,f25,f28,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f25,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f25.f64 = double(temp.f32);
	// stfs f31,260(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 260, temp.u32);
	// stfs f22,680(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// fnmsubs f13,f25,f29,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// lfs f25,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f12,f25,f26,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f26.f64 + ctx.f12.f64));
	// lfs f26,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f1,f5
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfs f25,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f25.f64 = double(temp.f32);
	// stfs f8,1288(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1288, temp.u32);
	// lfs f11,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f29,f26,f29,f0
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f0.f64));
	// lfs f0,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f0.f64 = double(temp.f32);
	// lfs f26,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f23,f0,f25
	ctx.f23.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// lfs f8,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f10,f31
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// lfs f22,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f4,f30
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// lfs f21,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f18.f64 = double(temp.f32);
	// stfs f24,1232(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1232, temp.u32);
	// fmuls f24,f26,f0
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// stfs f9,1360(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1360, temp.u32);
	// fmuls f9,f11,f7
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f7.f64));
	// stfs f20,556(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// fmuls f20,f22,f28
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f28.f64));
	// stfs f17,688(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fmuls f17,f21,f18
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// stfs f14,572(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// stfs f16,1532(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1532, temp.u32);
	// stfs f15,616(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 616, temp.u32);
	// fmuls f15,f19,f28
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f28.f64));
	// stfs f3,1268(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1268, temp.u32);
	// fmuls f3,f8,f5
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f16,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f14.f64 = double(temp.f32);
	// lfs f0,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,116(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f9,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f20,220(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// lfs f20,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f0,f3,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f9,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f9,f20,f9
	ctx.f9.f64 = double(float(ctx.f20.f64 * ctx.f9.f64));
	// stfs f2,464(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// lfs f2,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f20,f2,f20
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f20.f64));
	// stfs f1,408(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f20,136(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// lfs f1,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f1.f64 = double(temp.f32);
	// lfs f20,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f20.f64 = double(temp.f32);
	// stfs f6,148(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmsubs f30,f1,f20,f30
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f20.f64 - ctx.f30.f64));
	// stfs f15,412(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// lfs f15,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f15.f64 = double(temp.f32);
	// lfs f6,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f6.f64 = double(temp.f32);
	// stfs f9,360(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// fmsubs f6,f15,f6,f31
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f6.f64 - ctx.f31.f64));
	// lfs f31,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f31.f64 = double(temp.f32);
	// lfs f20,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f20.f64 = double(temp.f32);
	// stfs f11,672(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// lfs f9,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f9.f64 = double(temp.f32);
	// stfs f30,116(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f30,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f30,f31,f12
	ctx.f12.f64 = double(float(-(ctx.f30.f64 * ctx.f31.f64 - ctx.f12.f64)));
	// lfs f31,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f31,f20,f31,f29
	ctx.f31.f64 = double(float(-(ctx.f20.f64 * ctx.f31.f64 - ctx.f29.f64)));
	// lfs f11,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f11.f64 = double(temp.f32);
	// lfs f3,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f3.f64 = double(temp.f32);
	// stfs f9,792(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// fmuls f3,f9,f3
	ctx.f3.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// lfs f9,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f9.f64 = double(temp.f32);
	// fmsubs f6,f6,f5,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f5.f64 - ctx.f0.f64));
	// stfs f25,436(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// fadds f25,f14,f16
	ctx.f25.f64 = double(float(ctx.f14.f64 + ctx.f16.f64));
	// stfs f10,1508(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1508, temp.u32);
	// lfs f10,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f10.f64 = double(temp.f32);
	// stfs f28,640(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// lfs f28,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f12,f11,f5,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 + ctx.f12.f64));
	// lfs f11,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f9,f11,f31
	ctx.f11.f64 = double(float(-(ctx.f9.f64 * ctx.f11.f64 - ctx.f31.f64)));
	// lfs f9,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f9.f64 = double(temp.f32);
	// stfs f26,1660(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1660, temp.u32);
	// stfs f2,1300(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1300, temp.u32);
	// lfs f30,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f31.f64 = double(temp.f32);
	// lfs f26,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f12,f27,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f27,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f27.f64 = double(temp.f32);
	// lfs f10,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f11,f27,f9,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f9.f64 + ctx.f11.f64));
	// stfs f4,1612(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1612, temp.u32);
	// fmuls f4,f25,f5
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// stfs f7,1728(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1728, temp.u32);
	// fmadds f10,f28,f10,f24
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f10.f64 + ctx.f24.f64));
	// stfs f8,1316(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1316, temp.u32);
	// fadds f8,f29,f30
	ctx.f8.f64 = double(float(ctx.f29.f64 + ctx.f30.f64));
	// stfs f1,1348(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1348, temp.u32);
	// fmr f1,f2
	ctx.f1.f64 = ctx.f2.f64;
	// lfs f7,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f7.f64 = double(temp.f32);
	// fmr f9,f0
	ctx.f9.f64 = ctx.f0.f64;
	// lfs f27,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f26,f26,f31,f23
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f31.f64 + ctx.f23.f64));
	// lfs f25,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f24.f64 = double(temp.f32);
	// stfs f22,664(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// stfs f21,652(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f18,1236(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1236, temp.u32);
	// stfs f19,1208(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1208, temp.u32);
	// stfs f16,1944(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1944, temp.u32);
	// stfs f14,1192(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1192, temp.u32);
	// stfs f15,1524(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1524, temp.u32);
	// lfs f14,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f12,f7,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// fmadds f13,f17,f25,f13
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f25.f64 + ctx.f13.f64));
	// lfs f25,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f25.f64 = double(temp.f32);
	// fadds f8,f8,f14
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f14.f64));
	// stfs f8,252(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// lfs f2,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f21,f25,f5
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f5.f64));
	// lfs f18,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f2,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f8,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f23,f24,f5
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// stfs f21,152(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmadds f8,f18,f8,f3
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f8.f64 + ctx.f3.f64));
	// lfs f11,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f21,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f21.f64 = double(temp.f32);
	// lfs f3,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f3,f11,f3,f21
	ctx.f3.f64 = double(float(-(ctx.f11.f64 * ctx.f3.f64 - ctx.f21.f64)));
	// lfs f21,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f12,f21,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f21,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f21.f64 = double(temp.f32);
	// stfs f23,308(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// lfs f23,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f0,f21,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f21.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f9,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f10,f9,f23,f10
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f23.f64 + ctx.f10.f64));
	// lfs f20,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f20.f64 = double(temp.f32);
	// lfs f9,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f17,f20,f5
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f5.f64));
	// stfs f17,204(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// lfs f7,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f7.f64 = double(temp.f32);
	// lfs f22,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f22,f5
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// fnmsubs f12,f9,f27,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f27.f64 - ctx.f12.f64)));
	// stfs f19,180(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f17,f17,f7
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// stfs f17,116(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f17.f64 = double(temp.f32);
	// stfs f11,656(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// fmsubs f26,f26,f19,f17
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f19.f64 - ctx.f17.f64));
	// lfs f11,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f11.f64 = double(temp.f32);
	// lfs f19,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f19.f64 = double(temp.f32);
	// lfs f16,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f16.f64 = double(temp.f32);
	// stfs f13,5228(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5228, temp.u32);
	// fmuls f15,f16,f5
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f5.f64));
	// lfs f2,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f11,f19,f12
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f19.f64 - ctx.f12.f64)));
	// lfs f21,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f21.f64 = double(temp.f32);
	// lfs f13,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f13.f64 = double(temp.f32);
	// stfs f15,324(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// fmuls f15,f2,f5
	ctx.f15.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// fnmsubs f13,f13,f21,f0
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f21.f64 - ctx.f0.f64)));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f11.f64 = double(temp.f32);
	// stfs f15,336(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// lfs f15,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f15.f64 = double(temp.f32);
	// lfs f1,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// stfs f7,628(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// fmadds f7,f4,f0,f6
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f6.f64));
	// lfs f9,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f1,f15,f1
	ctx.f1.f64 = double(float(ctx.f15.f64 * ctx.f1.f64));
	// fmadds f12,f11,f0,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f11,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f11.f64 = double(temp.f32);
	// lfs f6,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f6.f64 = double(temp.f32);
	// stfs f29,1332(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1332, temp.u32);
	// stfs f30,1620(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1620, temp.u32);
	// stfs f31,440(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// stfs f28,1212(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1212, temp.u32);
	// stfs f24,1296(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1296, temp.u32);
	// stfs f25,1812(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1812, temp.u32);
	// stfs f22,1324(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1324, temp.u32);
	// stfs f20,1908(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1908, temp.u32);
	// stfs f18,600(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfs f16,788(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 788, temp.u32);
	// stfs f2,1704(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1704, temp.u32);
	// stfs f14,1308(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1308, temp.u32);
	// stfs f15,1340(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1340, temp.u32);
	// stfs f23,364(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lfs f2,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f2.f64 = double(temp.f32);
	// fmr f28,f21
	ctx.f28.f64 = ctx.f21.f64;
	// lfs f31,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f3,f11,f2,f3
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + ctx.f3.f64));
	// fmuls f8,f8,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// lfs f2,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f2,f9,f2,f26
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 + ctx.f26.f64));
	// fmuls f31,f6,f31
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// lfs f4,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f30.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f22,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f30,f4
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// fmuls f20,f22,f5
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// lfs f27,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f27.f64 = double(temp.f32);
	// stfs f20,220(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fnmsubs f12,f27,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f20,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f20.f64 = double(temp.f32);
	// lfs f14,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f14.f64 = double(temp.f32);
	// lfs f23,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f7,f20,f14,f7
	ctx.f7.f64 = double(float(-(ctx.f20.f64 * ctx.f14.f64 - ctx.f7.f64)));
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f10,f10,f0,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f8.f64));
	// lfs f18,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f1,f23,f15,f1
	ctx.f1.f64 = double(float(ctx.f23.f64 * ctx.f15.f64 + ctx.f1.f64));
	// lfs f19,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f18,f0,f2
	ctx.f0.f64 = double(float(-(ctx.f18.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// lfs f20,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f20.f64 = double(temp.f32);
	// lfs f26,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f31,f19,f20,f31
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f20.f64 + ctx.f31.f64));
	// lfs f29,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f13,f26,f28,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f28.f64 + ctx.f13.f64));
	// lfs f21,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f29,f5
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// lfs f15,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f15.f64 = double(temp.f32);
	// lfs f2,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f3,f21,f15,f3
	ctx.f3.f64 = double(float(-(ctx.f21.f64 * ctx.f15.f64 - ctx.f3.f64)));
	// lfs f18,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f30,f14,f18,f30
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f18.f64 + ctx.f30.f64));
	// stfs f5,136(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmuls f20,f2,f20
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f20.f64));
	// lfs f25,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f28.f64 = double(temp.f32);
	// lfs f8,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f8.f64 = double(temp.f32);
	// fadds f24,f28,f27
	ctx.f24.f64 = double(float(ctx.f28.f64 + ctx.f27.f64));
	// lfs f17,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f16,f25
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f25.f64));
	// lfs f5,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f15,f8,f15
	ctx.f15.f64 = double(float(ctx.f8.f64 * ctx.f15.f64));
	// stfs f1,148(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fmuls f5,f17,f5
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f5.f64));
	// lfs f14,-19152(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19152);
	ctx.f14.f64 = double(temp.f32);
	// lfs f1,-19600(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19600);
	ctx.f1.f64 = double(temp.f32);
	// stfs f20,116(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f20,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f20.f64 = double(temp.f32);
	// stfs f13,5232(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5232, temp.u32);
	// stfs f11,808(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 808, temp.u32);
	// stfs f9,1748(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1748, temp.u32);
	// stfs f6,752(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 752, temp.u32);
	// stfs f28,1740(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1740, temp.u32);
	// stfs f27,1052(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1052, temp.u32);
	// stfs f14,200(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// stfs f25,392(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f1,296(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 296, temp.u32);
	// stfs f4,388(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// stfs f23,748(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 748, temp.u32);
	// stfs f22,764(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 764, temp.u32);
	// stfs f21,412(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// stfs f19,452(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 452, temp.u32);
	// stfs f8,408(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// stfs f2,360(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f18,1604(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1604, temp.u32);
	// stfs f26,152(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f29,488(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 488, temp.u32);
	// lfs f11,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f11,f20,f7
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f20.f64 - ctx.f7.f64)));
	// lfs f13,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f13.f64 = double(temp.f32);
	// lfs f9,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f9,f13,f9,f3
	ctx.f9.f64 = double(float(-(ctx.f13.f64 * ctx.f9.f64 - ctx.f3.f64)));
	// lfs f7,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f7.f64 = double(temp.f32);
	// lfs f8,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f0,f16,f7,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// fmadds f10,f24,f8,f10
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f8.f64 + ctx.f10.f64));
	// lfs f26,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f26.f64 = double(temp.f32);
	// lfs f2,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f2.f64 = double(temp.f32);
	// lfs f28,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f28.f64 = double(temp.f32);
	// lfs f6,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f29.f64 = double(temp.f32);
	// fadds f1,f3,f6
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f6.f64));
	// fmadds f11,f26,f28,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f28.f64 + ctx.f11.f64));
	// lfs f26,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f29,f2
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f2.f64));
	// lfs f27,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f9,f26,f27,f9
	ctx.f9.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 + ctx.f9.f64));
	// lfs f27,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f27.f64 = double(temp.f32);
	// stfs f17,684(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 684, temp.u32);
	// fmadds f0,f30,f27,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f0.f64));
	// lfs f28,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f28.f64 = double(temp.f32);
	// lfs f20,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f10,f31,f28,f10
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f28.f64 - ctx.f10.f64));
	// lfs f18,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,3464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3464);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f20,f18
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// lfs f19,3456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3456);
	ctx.f19.f64 = double(temp.f32);
	// lfs f30,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f14,f17,f19
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// lfs f16,3472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3472);
	ctx.f16.f64 = double(temp.f32);
	// fmr f28,f30
	ctx.f28.f64 = ctx.f30.f64;
	// stfs f17,3456(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3456, temp.u32);
	// fmuls f1,f1,f16
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// stfs f20,3472(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3472, temp.u32);
	// lfs f17,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f17.f64 = double(temp.f32);
	// lfs f20,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f20.f64 = double(temp.f32);
	// lfs f8,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f29,f17,f20,f29
	ctx.f29.f64 = double(float(ctx.f17.f64 * ctx.f20.f64 + ctx.f29.f64));
	// lfs f7,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f7.f64 = double(temp.f32);
	// lfs f22,4696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4696);
	ctx.f22.f64 = double(temp.f32);
	// fadds f4,f7,f8
	ctx.f4.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// stfs f1,3464(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3464, temp.u32);
	// stfs f29,4696(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4696, temp.u32);
	// lfs f1,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f29.f64 = double(temp.f32);
	// lfs f17,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f11,f29,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f25,4680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4680);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f9,f25,f17,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f17.f64 + ctx.f9.f64));
	// lfs f27,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f10,f15,f1,f10
	ctx.f10.f64 = double(float(ctx.f15.f64 * ctx.f1.f64 + ctx.f10.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f12,f27,f28,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f28.f64 + ctx.f12.f64));
	// lfs f17,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f17.f64 = double(temp.f32);
	// lfs f31,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f0,f17,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f1,3480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3480);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f5,f31,f30,f5
	ctx.f5.f64 = double(float(ctx.f31.f64 * ctx.f30.f64 + ctx.f5.f64));
	// lfs f23,4688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4688);
	ctx.f23.f64 = double(temp.f32);
	// lfs f30,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f17,f23,f1
	ctx.f17.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// lfs f28,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f27.f64 = double(temp.f32);
	// fadds f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f28.f64));
	// lfs f26,4672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4672);
	ctx.f26.f64 = double(temp.f32);
	// lfs f21,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f24,f26,f27
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// lfs f29,3488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3488);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f21,f21,f30
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// stfs f17,3480(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3480, temp.u32);
	// fmuls f17,f22,f29
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f29.f64));
	// lfs f15,3496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3496);
	ctx.f15.f64 = double(temp.f32);
	// stfs f13,252(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// stfs f7,736(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 736, temp.u32);
	// stfs f8,436(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 436, temp.u32);
	// stfs f30,204(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f12,5236(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5236, temp.u32);
	// lfs f13,4752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4752);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f13,f12,f9
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f12.f64 + ctx.f9.f64));
	// lfs f8,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f11,f9,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f9.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f8,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,4760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4760);
	ctx.f9.f64 = double(temp.f32);
	// stfs f6,896(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 896, temp.u32);
	// fmr f6,f8
	ctx.f6.f64 = ctx.f8.f64;
	// stfs f31,324(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f31,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f30.f64 = double(temp.f32);
	// stfs f2,1292(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1292, temp.u32);
	// lfs f2,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f9,f8,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f8.f64 - ctx.f12.f64)));
	// stfs f27,3488(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3488, temp.u32);
	// fmadds f0,f21,f2,f0
	ctx.f0.f64 = double(float(ctx.f21.f64 * ctx.f2.f64 + ctx.f0.f64));
	// stfs f20,1284(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1284, temp.u32);
	// fmadds f11,f30,f31,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f30,4808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4808);
	ctx.f30.f64 = double(temp.f32);
	// lfs f21,4620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4620);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f27,f30,f15
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// lfs f20,3708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3708);
	ctx.f20.f64 = double(temp.f32);
	// stfs f18,1500(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1500, temp.u32);
	// fmuls f18,f21,f16
	ctx.f18.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// stfs f27,3708(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3708, temp.u32);
	// lfs f7,4768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4768);
	ctx.f7.f64 = double(temp.f32);
	// lfs f27,3668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3668);
	ctx.f27.f64 = double(temp.f32);
	// stfs f26,1652(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1652, temp.u32);
	// lfs f26,5024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5024);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f12,f7,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// stfs f3,4688(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4688, temp.u32);
	// stfs f18,3668(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3668, temp.u32);
	// fmuls f18,f20,f15
	ctx.f18.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// stfs f5,3496(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3496, temp.u32);
	// lfs f5,4776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4776);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// stfs f22,1028(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1028, temp.u32);
	// fmuls f22,f26,f16
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// stfs f28,904(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 904, temp.u32);
	// fnmsubs f10,f5,f3,f10
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f3.f64 - ctx.f10.f64)));
	// lfs f6,3364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3364);
	ctx.f6.f64 = double(temp.f32);
	// lfs f28,4816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4816);
	ctx.f28.f64 = double(temp.f32);
	// stfs f18,4808(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4808, temp.u32);
	// lfs f18,2244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2244);
	ctx.f18.f64 = double(temp.f32);
	// stfs f22,3364(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3364, temp.u32);
	// stfs f25,464(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 464, temp.u32);
	// fmuls f25,f28,f16
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f3,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f3.f64 = double(temp.f32);
	// lfs f22,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f22.f64 = double(temp.f32);
	// stfs f16,2244(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2244, temp.u32);
	// fmuls f16,f27,f19
	ctx.f16.f64 = double(float(ctx.f27.f64 * ctx.f19.f64));
	// fnmsubs f11,f22,f3,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f3.f64 - ctx.f11.f64)));
	// stfs f16,4620(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4620, temp.u32);
	// lfs f16,4176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4176);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,3372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3372);
	ctx.f22.f64 = double(temp.f32);
	// stfs f31,3372(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3372, temp.u32);
	// fmuls f31,f16,f18
	ctx.f31.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// stfs f31,5024(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5024, temp.u32);
	// lfs f31,3380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3380);
	ctx.f31.f64 = double(temp.f32);
	// stfs f1,3380(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3380, temp.u32);
	// fmuls f1,f6,f1
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// stfs f31,4776(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4776, temp.u32);
	// fmuls f31,f31,f15
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f15.f64));
	// stfs f23,1172(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1172, temp.u32);
	// stfs f1,4768(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4768, temp.u32);
	// lfs f1,2412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2412);
	ctx.f1.f64 = double(temp.f32);
	// lfs f23,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f23.f64 = double(temp.f32);
	// lfs f3,2348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2348);
	ctx.f3.f64 = double(temp.f32);
	// stfs f31,4176(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4176, temp.u32);
	// lfs f31,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f31.f64 = double(temp.f32);
	// stfs f15,2348(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2348, temp.u32);
	// fmuls f15,f1,f23
	ctx.f15.f64 = double(float(ctx.f1.f64 * ctx.f23.f64));
	// stfs f15,2412(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2412, temp.u32);
	// fmuls f31,f24,f31
	ctx.f31.f64 = double(float(ctx.f24.f64 * ctx.f31.f64));
	// lfs f2,4800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4800);
	ctx.f2.f64 = double(temp.f32);
	// lfs f8,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f8.f64 = double(temp.f32);
	// lfs f15,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f15.f64 = double(temp.f32);
	// lfs f24,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f10,f4,f24,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f24.f64 + ctx.f10.f64));
	// lfs f24,3472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3472);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f14,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f24,f24,f14,f0
	ctx.f24.f64 = double(float(-(ctx.f24.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// lfs f0,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f0.f64 = double(temp.f32);
	// stfs f30,152(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// fmadds f12,f2,f8,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f12.f64));
	// lfs f30,3464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3464);
	ctx.f30.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f4,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f14,f3,f29
	ctx.f14.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// fmuls f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f8,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f11,f0,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f0.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f0,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f0.f64 = double(temp.f32);
	// stfs f13,1476(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1476, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f6,1492(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1492, temp.u32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lfs f6,-19156(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19156);
	ctx.f6.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f14,4816(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4816, temp.u32);
	// fmuls f14,f22,f29
	ctx.f14.f64 = double(float(ctx.f22.f64 * ctx.f29.f64));
	// stfs f23,1068(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1068, temp.u32);
	// lfs f23,4352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4352);
	ctx.f23.f64 = double(temp.f32);
	// stfs f10,4800(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4800, temp.u32);
	// stfs f6,128(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fmsubs f13,f17,f0,f4
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f0.f64 - ctx.f4.f64));
	// lfs f17,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f17.f64 = double(temp.f32);
	// lfs f10,3496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3496);
	ctx.f10.f64 = double(temp.f32);
	// lfs f6,3372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3372);
	ctx.f6.f64 = double(temp.f32);
	// stfs f14,4352(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4352, temp.u32);
	// fmuls f14,f1,f17
	ctx.f14.f64 = double(float(ctx.f1.f64 * ctx.f17.f64));
	// stfs f26,1016(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1016, temp.u32);
	// fmadds f11,f10,f6,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f6.f64 + ctx.f11.f64));
	// lfs f26,4208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4208);
	ctx.f26.f64 = double(temp.f32);
	// stfs f22,784(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 784, temp.u32);
	// stfs f12,5240(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5240, temp.u32);
	// fmsubs f12,f25,f0,f15
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f0.f64 - ctx.f15.f64));
	// lfs f22,2244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2244);
	ctx.f22.f64 = double(temp.f32);
	// lfs f4,3364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3364);
	ctx.f4.f64 = double(temp.f32);
	// stfs f14,4208(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4208, temp.u32);
	// fmadds f13,f4,f0,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f10,-19160(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19160);
	ctx.f10.f64 = double(temp.f32);
	// lfs f15,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,4488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4488);
	ctx.f14.f64 = double(temp.f32);
	// stfs f21,1024(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1024, temp.u32);
	// fmuls f21,f26,f22
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f22.f64));
	// stfs f22,4488(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4488, temp.u32);
	// fmuls f22,f1,f15
	ctx.f22.f64 = double(float(ctx.f1.f64 * ctx.f15.f64));
	// stfs f28,4672(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4672, temp.u32);
	// stfs f5,936(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 936, temp.u32);
	// stfs f20,3496(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3496, temp.u32);
	// stfs f19,4760(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4760, temp.u32);
	// stfs f9,220(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// stfs f3,776(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 776, temp.u32);
	// stfs f10,368(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 368, temp.u32);
	// lfs f9,3488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3488);
	ctx.f9.f64 = double(temp.f32);
	// lfs f4,4316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4316);
	ctx.f4.f64 = double(temp.f32);
	// lfs f8,5060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5060);
	ctx.f8.f64 = double(temp.f32);
	// lfs f10,4660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4660);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,4228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4228);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,3984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3984);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,3992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3992);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,3456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3456);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f30,f10
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f20,4392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4392);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,3380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3380);
	ctx.f19.f64 = double(temp.f32);
	// stfs f27,3472(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3472, temp.u32);
	// fmuls f27,f3,f28
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// stfs f18,180(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmuls f18,f23,f19
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f19.f64));
	// stfs f16,484(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 484, temp.u32);
	// fmuls f16,f5,f20
	ctx.f16.f64 = double(float(ctx.f5.f64 * ctx.f20.f64));
	// stfs f7,308(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f7,f8,f9
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// stfs f2,336(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// fmuls f2,f4,f9
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f9.f64));
	// stfs f22,4316(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4316, temp.u32);
	// lfs f22,2548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2548);
	ctx.f22.f64 = double(temp.f32);
	// stfs f20,2548(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2548, temp.u32);
	// fmuls f20,f14,f29
	ctx.f20.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// stfs f20,4228(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4228, temp.u32);
	// lfs f20,3672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3672);
	ctx.f20.f64 = double(temp.f32);
	// stfs f19,3672(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3672, temp.u32);
	// lfs f19,2412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2412);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f19,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f12,2556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2556);
	ctx.f12.f64 = double(temp.f32);
	// stfs f9,2556(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2556, temp.u32);
	// fmuls f19,f22,f10
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f10.f64));
	// lfs f9,3132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3132);
	ctx.f9.f64 = double(temp.f32);
	// stfs f1,3132(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3132, temp.u32);
	// lfs f1,2348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2348);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f7,f20,f1,f7
	ctx.f7.f64 = double(float(ctx.f20.f64 * ctx.f1.f64 + ctx.f7.f64));
	// stfs f30,4752(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4752, temp.u32);
	// stfs f7,3372(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3372, temp.u32);
	// fmuls f30,f12,f30
	ctx.f30.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// lfs f7,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f7.f64 = double(temp.f32);
	// stfs f1,4392(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4392, temp.u32);
	// fmuls f7,f7,f9
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// stfs f30,2348(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2348, temp.u32);
	// lfs f1,3668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3668);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f1,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// stfs f7,2244(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2244, temp.u32);
	// lfs f1,3480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3480);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f1,f7,f31
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f7.f64 + ctx.f31.f64));
	// stfs f11,5244(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5244, temp.u32);
	// lfs f31,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f31.f64 = double(temp.f32);
	// lfs f11,3708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3708);
	ctx.f11.f64 = double(temp.f32);
	// stfs f6,4680(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4680, temp.u32);
	// stfs f3,3480(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3480, temp.u32);
	// lfs f6,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,4620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4620);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f0,f3,f6,f0
	ctx.f0.f64 = double(float(-(ctx.f3.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// stfs f4,3364(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3364, temp.u32);
	// stfs f8,1564(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1564, temp.u32);
	// stfs f10,136(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// fmsubs f11,f11,f31,f7
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64 - ctx.f7.f64));
	// lfs f8,4696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4696);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,4176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4176);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f10.f64 = double(temp.f32);
	// stfs f9,116(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// fmadds f10,f8,f10,f24
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f10.f64 + ctx.f24.f64));
	// stfs f2,3984(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3984, temp.u32);
	// stfs f12,148(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// lfs f1,-19164(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19164);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,-19168(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19168);
	ctx.f30.f64 = double(temp.f32);
	// lfs f12,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f11,f6,f4,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f11.f64)));
	// lfs f9,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4544);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f9,f12
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfs f7,4552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4552);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,3132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3132);
	ctx.f2.f64 = double(temp.f32);
	// stfs f5,744(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 744, temp.u32);
	// fmuls f5,f8,f7
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// stfs f1,800(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 800, temp.u32);
	// fmadds f1,f2,f4,f19
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f19.f64));
	// stfs f30,256(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 256, temp.u32);
	// lfs f6,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,4664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4664);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f30.f64 = double(temp.f32);
	// stfs f16,3992(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3992, temp.u32);
	// stfs f17,1048(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1048, temp.u32);
	// stfs f15,1140(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1140, temp.u32);
	// stfs f29,3488(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3488, temp.u32);
	// stfs f14,1076(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1076, temp.u32);
	// stfs f27,4660(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4660, temp.u32);
	// stfs f28,3668(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3668, temp.u32);
	// stfs f25,5060(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5060, temp.u32);
	// stfs f22,3464(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3464, temp.u32);
	// stfs f21,2412(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2412, temp.u32);
	// stfs f26,3456(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3456, temp.u32);
	// stfs f18,3380(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3380, temp.u32);
	// stfs f23,528(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 528, temp.u32);
	// stfs f20,3708(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3708, temp.u32);
	// lfs f28,4816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4816);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f10,f9,f6,f10
	ctx.f10.f64 = double(float(-(ctx.f9.f64 * ctx.f6.f64 - ctx.f10.f64)));
	// lfs f29,5024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5024);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f30,f28,f30,f0
	ctx.f30.f64 = double(float(-(ctx.f28.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// fmadds f13,f29,f31,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f29,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,4808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4808);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f11,f27,f29,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f29,4728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4728);
	ctx.f29.f64 = double(temp.f32);
	// lfs f16,4392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4392);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,3672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3672);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f29,f23
	ctx.f22.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// lfs f31,4704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4704);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f15,f16,f21
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f21.f64));
	// lfs f27,4880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4880);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f6,f3,f31
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f14,3140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3140);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,3420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3420);
	ctx.f18.f64 = double(temp.f32);
	// lfs f9,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f9.f64 = double(temp.f32);
	// stfs f16,4880(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4880, temp.u32);
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// lfs f28,4720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4720);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f9,f5,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f9.f64));
	// lfs f20,2548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2548);
	ctx.f20.f64 = double(temp.f32);
	// stfs f22,3420(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3420, temp.u32);
	// fmuls f19,f27,f20
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// stfs f31,4728(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4728, temp.u32);
	// lfs f0,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,4800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4800);
	ctx.f31.f64 = double(temp.f32);
	// lfs f22,4208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4208);
	ctx.f22.f64 = double(temp.f32);
	// fadds f10,f31,f10
	ctx.f10.f64 = double(float(ctx.f31.f64 + ctx.f10.f64));
	// stfs f16,4720(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4720, temp.u32);
	// fmadds f30,f22,f0,f30
	ctx.f30.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f30.f64));
	// lfs f5,4712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4712);
	ctx.f5.f64 = double(temp.f32);
	// lfs f24,2564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2564);
	ctx.f24.f64 = double(temp.f32);
	// lfs f26,2556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2556);
	ctx.f26.f64 = double(temp.f32);
	// lfs f16,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f25,f5,f26
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// stfs f19,2564(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2564, temp.u32);
	// fmuls f16,f16,f28
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f28.f64));
	// lfs f19,4352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4352);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,3564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3564);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f13,f19,f0,f13
	ctx.f13.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f31,2580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2580);
	ctx.f31.f64 = double(temp.f32);
	// stfs f25,2580(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2580, temp.u32);
	// stfs f16,3140(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3140, temp.u32);
	// fmuls f16,f22,f18
	ctx.f16.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// lfs f17,4488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4488);
	ctx.f17.f64 = double(temp.f32);
	// lfs f25,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f1,f24,f17,f1
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f17.f64 + ctx.f1.f64));
	// stfs f22,3564(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3564, temp.u32);
	// fmuls f25,f25,f17
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f17.f64));
	// lfs f0,2604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2604);
	ctx.f0.f64 = double(temp.f32);
	// lfs f22,4776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4776);
	ctx.f22.f64 = double(temp.f32);
	// stfs f0,4712(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4712, temp.u32);
	// fmuls f0,f22,f0
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// stfs f17,4544(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4544, temp.u32);
	// lfs f17,3156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3156);
	ctx.f17.f64 = double(temp.f32);
	// stfs f0,2604(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2604, temp.u32);
	// stfs f25,3156(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3156, temp.u32);
	// lfs f25,4768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4768);
	ctx.f25.f64 = double(temp.f32);
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f0,f25,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f19,4688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4688);
	ctx.f19.f64 = double(temp.f32);
	// lfs f11,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f25,f23,f31
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f31.f64));
	// stfs f5,1396(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1396, temp.u32);
	// fmuls f5,f19,f17
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// stfs f2,2556(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2556, temp.u32);
	// fmadds f11,f6,f11,f9
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f11.f64 + ctx.f9.f64));
	// stfs f4,1040(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1040, temp.u32);
	// stfs f12,4664(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4664, temp.u32);
	// stfs f28,3132(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3132, temp.u32);
	// stfs f29,4352(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4352, temp.u32);
	// stfs f7,4704(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4704, temp.u32);
	// stfs f8,4696(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4696, temp.u32);
	// stfs f20,2548(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2548, temp.u32);
	// stfs f27,364(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f24,4552(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4552, temp.u32);
	// stfs f3,4488(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4488, temp.u32);
	// stfs f10,5248(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5248, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f10,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f10.f64 = double(temp.f32);
	// lfs f12,3992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3992);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f12,f12,f10,f0
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f0,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,4316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4316);
	ctx.f9.f64 = double(temp.f32);
	// fmr f2,f0
	ctx.f2.f64 = ctx.f0.f64;
	// lfs f8,4228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4228);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f9,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fnmsubs f8,f8,f0,f30
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// lfs f9,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,3984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3984);
	ctx.f10.f64 = double(temp.f32);
	// fmr f4,f9
	ctx.f4.f64 = ctx.f9.f64;
	// lfs f7,4660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4660);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f6.f64 = double(temp.f32);
	// lfs f3,5060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5060);
	ctx.f3.f64 = double(temp.f32);
	// stfs f22,920(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 920, temp.u32);
	// lfs f22,3372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3372);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f10,f9,f12
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f9.f64 - ctx.f12.f64)));
	// stfs f31,4176(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4176, temp.u32);
	// lfs f31,2412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2412);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f13,f7,f6,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f13.f64));
	// stfs f21,4208(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4208, temp.u32);
	// fmadds f8,f3,f2,f8
	ctx.f8.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f8.f64));
	// lfs f21,3364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3364);
	ctx.f21.f64 = double(temp.f32);
	// fmr f3,f0
	ctx.f3.f64 = ctx.f0.f64;
	// stfs f14,1692(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1692, temp.u32);
	// lfs f20,2628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2628);
	ctx.f20.f64 = double(temp.f32);
	// lfs f29,3788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3788);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,4468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4468);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,4484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4484);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f24,f28,f29
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// lfs f9,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f22,f4,f12
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f22,2348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2348);
	ctx.f22.f64 = double(temp.f32);
	// lfs f10,3380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3380);
	ctx.f10.f64 = double(temp.f32);
	// fmr f30,f9
	ctx.f30.f64 = ctx.f9.f64;
	// fnmsubs f13,f31,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f7,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f0,f22,f0,f8
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f8,3164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3164);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f14,f21,f8
	ctx.f14.f64 = double(float(ctx.f21.f64 * ctx.f8.f64));
	// stfs f14,2628(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2628, temp.u32);
	// lfs f14,2644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2644);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f22,f26,f27
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// stfs f11,2644(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2644, temp.u32);
	// fmr f11,f4
	ctx.f11.f64 = ctx.f4.f64;
	// stfs f17,3164(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3164, temp.u32);
	// fmr f2,f7
	ctx.f2.f64 = ctx.f7.f64;
	// lfs f17,3172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3172);
	ctx.f17.f64 = double(temp.f32);
	// stfs f25,3172(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3172, temp.u32);
	// lfs f25,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f12,f10,f9,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 + ctx.f12.f64));
	// fmadds f25,f15,f25,f16
	ctx.f25.f64 = double(float(ctx.f15.f64 * ctx.f25.f64 + ctx.f16.f64));
	// lfs f9,2244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2244);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f13,f9,f7,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f13.f64));
	// lfs f16,3156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3156);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f0,f1,f3,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f31,2620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2620);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f10,f10,f16,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f16.f64 + ctx.f5.f64));
	// lfs f1,2604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2604);
	ctx.f1.f64 = double(temp.f32);
	// stfs f19,1004(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1004, temp.u32);
	// fmuls f19,f23,f31
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f31.f64));
	// stfs f18,3672(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3672, temp.u32);
	// fmuls f11,f1,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// lfs f18,2636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2636);
	ctx.f18.f64 = double(temp.f32);
	// lfs f9,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,3564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3564);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f9,f24,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f9.f64));
	// stfs f19,2636(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2636, temp.u32);
	// fadds f19,f17,f18
	ctx.f19.f64 = double(float(ctx.f17.f64 + ctx.f18.f64));
	// stfs f29,4484(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4484, temp.u32);
	// fmadds f29,f20,f5,f22
	ctx.f29.f64 = double(float(ctx.f20.f64 * ctx.f5.f64 + ctx.f22.f64));
	// lfs f6,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,4760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4760);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,3572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3572);
	ctx.f3.f64 = double(temp.f32);
	// lfs f1,-19496(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19496);
	ctx.f1.f64 = double(temp.f32);
	// stfs f28,492(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 492, temp.u32);
	// stfs f31,4228(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4228, temp.u32);
	// stfs f27,340(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 340, temp.u32);
	// lfs f27,2580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2580);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f12,f27,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// stfs f8,5060(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5060, temp.u32);
	// lfs f8,3140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3140);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f8,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f8,3420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3420);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,4752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4752);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f31,f7
	ctx.f28.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// fmadds f10,f10,f27,f9
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f27.f64 + ctx.f9.f64));
	// lfs f6,2668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2668);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f2.f64 = double(temp.f32);
	// stfs f1,768(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 768, temp.u32);
	// fmuls f2,f2,f16
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// stfs f20,2412(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2412, temp.u32);
	// lfs f20,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f12,f8,f30,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f30.f64 + ctx.f12.f64));
	// lfs f30,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f30.f64 = double(temp.f32);
	// fmsubs f11,f25,f30,f11
	ctx.f11.f64 = double(float(ctx.f25.f64 * ctx.f30.f64 - ctx.f11.f64));
	// lfs f8,3444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3444);
	ctx.f8.f64 = double(temp.f32);
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f13,2564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2564);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,5252(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5252, temp.u32);
	// lfs f15,4720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4720);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,3708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3708);
	ctx.f16.f64 = double(temp.f32);
	// stfs f18,3992(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3992, temp.u32);
	// lfs f18,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f18.f64 = double(temp.f32);
	// stfs f26,3788(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3788, temp.u32);
	// fmuls f26,f14,f3
	ctx.f26.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// fmsubs f10,f28,f18,f10
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f18.f64 - ctx.f10.f64));
	// stfs f17,3984(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3984, temp.u32);
	// fmadds f0,f13,f4,f12
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f4.f64 + ctx.f12.f64));
	// stfs f0,5256(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5256, temp.u32);
	// lfs f0,3180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3180);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f11,f15,f20,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f20.f64 - ctx.f11.f64)));
	// lfs f12,2660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2660);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f1,f6,f0
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// fadds f4,f8,f12
	ctx.f4.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// lfs f15,4712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4712);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f29,f16,f15,f29
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f15.f64 + ctx.f29.f64));
	// stfs f14,1084(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1084, temp.u32);
	// stfs f7,2660(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2660, temp.u32);
	// fmuls f13,f19,f23
	ctx.f13.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// stfs f29,2668(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2668, temp.u32);
	// lfs f17,4728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4728);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,3688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3688);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,2684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2684);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,3496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3496);
	ctx.f14.f64 = double(temp.f32);
	// lfs f7,3196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3196);
	ctx.f7.f64 = double(temp.f32);
	// lfs f29,2692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2692);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,3188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3188);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f4,f17
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f17.f64));
	// lfs f19,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f1,f1,f19
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f19.f64));
	// lfs f20,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f2,f22,f30,f2
	ctx.f2.f64 = double(float(ctx.f22.f64 * ctx.f30.f64 + ctx.f2.f64));
	// stfs f5,2684(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2684, temp.u32);
	// fmuls f5,f14,f15
	ctx.f5.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f23,3196(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3196, temp.u32);
	// fmuls f23,f17,f29
	ctx.f23.f64 = double(float(ctx.f17.f64 * ctx.f29.f64));
	// stfs f31,3688(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3688, temp.u32);
	// lfs f9,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f9.f64 = double(temp.f32);
	// lfs f27,4880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4880);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,2676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2676);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f27,f9
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f9.f64));
	// lfs f31,3452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3452);
	ctx.f31.f64 = double(temp.f32);
	// stfs f3,2620(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2620, temp.u32);
	// fmuls f3,f20,f30
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// stfs f21,1412(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1412, temp.u32);
	// fmuls f21,f0,f24
	ctx.f21.f64 = double(float(ctx.f0.f64 * ctx.f24.f64));
	// stfs f13,3444(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3444, temp.u32);
	// fmuls f13,f17,f28
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f28.f64));
	// stfs f30,3188(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3188, temp.u32);
	// fmuls f30,f17,f18
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f18.f64));
	// stfs f5,3572(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3572, temp.u32);
	// stfs f23,3452(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3452, temp.u32);
	// lfs f19,3668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3668);
	ctx.f19.f64 = double(temp.f32);
	// lfs f5,2700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2700);
	ctx.f5.f64 = double(temp.f32);
	// lfs f23,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f23.f64 = double(temp.f32);
	// stfs f17,2700(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2700, temp.u32);
	// fmuls f25,f25,f23
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// fmuls f17,f17,f5
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f5.f64));
	// stfs f17,2692(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2692, temp.u32);
	// stfs f25,3180(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3180, temp.u32);
	// fmadds f2,f19,f7,f2
	ctx.f2.f64 = double(float(ctx.f19.f64 * ctx.f7.f64 + ctx.f2.f64));
	// lfs f17,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f17.f64 = double(temp.f32);
	// lfs f25,2644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2644);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f25,f21,f17,f25
	ctx.f25.f64 = double(float(-(ctx.f21.f64 * ctx.f17.f64 - ctx.f25.f64)));
	// lfs f23,3488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3488);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f23,f21
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// stfs f0,2564(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2564, temp.u32);
	// stfs f23,2644(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2644, temp.u32);
	// stfs f21,2676(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2676, temp.u32);
	// lfs f21,3172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3172);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f17.f64 = double(temp.f32);
	// lfs f23,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f11,f21,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f21.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// lfs f0,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f4,f4,f23,f1
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f23.f64 - ctx.f1.f64));
	// stfs f12,3420(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3420, temp.u32);
	// fmr f12,f0
	ctx.f12.f64 = ctx.f0.f64;
	// lfs f21,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f0,f13,f0,f25
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f0.f64 - ctx.f25.f64)));
	// stfs f9,4660(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4660, temp.u32);
	// fmr f13,f17
	ctx.f13.f64 = ctx.f17.f64;
	// fmadds f10,f26,f21,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f21.f64 + ctx.f10.f64));
	// lfs f26,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f26.f64 = double(temp.f32);
	// fmr f9,f21
	ctx.f9.f64 = ctx.f21.f64;
	// stfs f8,2604(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2604, temp.u32);
	// fmuls f8,f31,f26
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// stfs f31,4392(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4392, temp.u32);
	// lfs f31,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f31.f64 = double(temp.f32);
	// fmr f25,f21
	ctx.f25.f64 = ctx.f21.f64;
	// stfs f28,4720(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4720, temp.u32);
	// fmr f28,f31
	ctx.f28.f64 = ctx.f31.f64;
	// stfs f7,3172(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3172, temp.u32);
	// lfs f7,2636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2636);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f12,f30,f12,f4
	ctx.f12.f64 = double(float(-(ctx.f30.f64 * ctx.f12.f64 - ctx.f4.f64)));
	// stfs f27,4468(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4468, temp.u32);
	// lfs f27,3452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3452);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f13,f7,f13,f11
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// stfs f29,4712(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4712, temp.u32);
	// fmadds f0,f27,f31,f0
	ctx.f0.f64 = double(float(ctx.f27.f64 * ctx.f31.f64 + ctx.f0.f64));
	// lfs f27,2692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2692);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f11,f3,f9,f10
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f29,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f29.f64 = double(temp.f32);
	// stfs f22,1244(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1244, temp.u32);
	// fmuls f8,f8,f29
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f29.f64));
	// lfs f22,2628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2628);
	ctx.f22.f64 = double(temp.f32);
	// stfs f5,4880(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4880, temp.u32);
	// lfs f1,4580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4580);
	ctx.f1.f64 = double(temp.f32);
	// lfs f10,3204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3204);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f23,f19,f1
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// fnmsubs f12,f27,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f28,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f28.f64 = double(temp.f32);
	// lfs f9,2708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2708);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,2700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2700);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f13,f22,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f5,3580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3580);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f9,f26
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f26.f64));
	// lfs f3,4704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4704);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f11,f2,f25,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f25.f64 + ctx.f11.f64));
	// lfs f29,3472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3472);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f5,f3
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// lfs f31,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,3196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3196);
	ctx.f27.f64 = double(temp.f32);
	// stfs f6,888(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 888, temp.u32);
	// fmuls f6,f7,f10
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// stfs f24,3488(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3488, temp.u32);
	// fmuls f31,f27,f31
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lfs f28,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f24,f29,f1
	ctx.f24.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// lfs f2,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f2.f64 = double(temp.f32);
	// stfs f16,532(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 532, temp.u32);
	// stfs f20,1164(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1164, temp.u32);
	// stfs f18,3564(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3564, temp.u32);
	// stfs f15,4316(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4316, temp.u32);
	// stfs f14,572(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 572, temp.u32);
	// stfs f19,4580(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4580, temp.u32);
	// lfs f25,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f8,f4,f2,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 - ctx.f8.f64));
	// lfs f20,2684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2684);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f12,f30,f25,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f25.f64 + ctx.f12.f64));
	// lfs f16,2676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2676);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f0,f6,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// lfs f30,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f31,f20
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// lfs f14,3188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3188);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f16,f30
	ctx.f30.f64 = double(float(ctx.f16.f64 * ctx.f30.f64));
	// lfs f4,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f4.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f6,3212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3212);
	ctx.f6.f64 = double(temp.f32);
	// lfs f22,2716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2716);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,3480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3480);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f19,f22,f20
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f28,3688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3688);
	ctx.f28.f64 = double(temp.f32);
	// lfs f17,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f25,f21,f28
	ctx.f25.f64 = double(float(ctx.f21.f64 * ctx.f28.f64));
	// stfs f31,2708(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2708, temp.u32);
	// fmuls f31,f4,f14
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// stfs f16,3204(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3204, temp.u32);
	// fmuls f17,f17,f6
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f6.f64));
	// lfs f16,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f16.f64 = double(temp.f32);
	// stfs f31,3212(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3212, temp.u32);
	// fnmsubs f11,f23,f16,f11
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f16.f64 - ctx.f11.f64)));
	// stfs f6,3188(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3188, temp.u32);
	// fmr f6,f16
	ctx.f6.f64 = ctx.f16.f64;
	// lfs f15,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f15.f64 = double(temp.f32);
	// fmr f23,f16
	ctx.f23.f64 = ctx.f16.f64;
	// lfs f31,2668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2668);
	ctx.f31.f64 = double(temp.f32);
	// stfs f25,2716(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2716, temp.u32);
	// fmadds f13,f31,f15,f13
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f15.f64 + ctx.f13.f64));
	// stfs f19,3580(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3580, temp.u32);
	// lfs f18,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f18.f64 = double(temp.f32);
	// lfs f19,4664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4664);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f24,f18,f14,f24
	ctx.f24.f64 = double(float(ctx.f18.f64 * ctx.f14.f64 + ctx.f24.f64));
	// lfs f25,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f25,f25,f19
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// stfs f10,4728(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4728, temp.u32);
	// stfs f0,5260(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5260, temp.u32);
	// lfs f2,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f11,f30,f6,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f6.f64 + ctx.f11.f64));
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,3444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3444);
	ctx.f10.f64 = double(temp.f32);
	// lfs f31,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f13,f10,f0,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f13.f64));
	// stfs f3,2580(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2580, temp.u32);
	// fmuls f31,f31,f2
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f8,5264(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5264, temp.u32);
	// fmr f10,f15
	ctx.f10.f64 = ctx.f15.f64;
	// stfs f7,3496(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3496, temp.u32);
	// fmr f0,f15
	ctx.f0.f64 = ctx.f15.f64;
	// stfs f9,4704(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4704, temp.u32);
	// stfs f5,740(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 740, temp.u32);
	// lfs f9,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,3464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3464);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f9,f8
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// lfs f5,2660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2660);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f3.f64 = double(temp.f32);
	// lfs f6,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f3,f17,f3
	ctx.f3.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// stfs f1,3196(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3196, temp.u32);
	// fmuls f6,f25,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// stfs f4,1036(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1036, temp.u32);
	// fmuls f4,f7,f5
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// stfs f12,5268(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5268, temp.u32);
	// fmuls f12,f24,f23
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f23.f64));
	// stfs f2,2636(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2636, temp.u32);
	// lfs f2,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// stfs f22,2628(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2628, temp.u32);
	// stfs f27,3156(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3156, temp.u32);
	// stfs f29,2700(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2700, temp.u32);
	// stfs f26,1204(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1204, temp.u32);
	// stfs f20,3140(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3140, temp.u32);
	// stfs f28,2692(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2692, temp.u32);
	// stfs f21,2684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2684, temp.u32);
	// stfs f18,1156(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1156, temp.u32);
	// stfs f14,3452(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3452, temp.u32);
	// lfs f30,3180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3180);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// fnmsubs f13,f30,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f22,3572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3572);
	ctx.f22.f64 = double(temp.f32);
	// fmr f1,f16
	ctx.f1.f64 = ctx.f16.f64;
	// lfs f10,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f10.f64 = double(temp.f32);
	// lfs f30,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,3212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3212);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f8,f30,f10,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f10.f64 + ctx.f8.f64));
	// stfs f10,3212(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3212, temp.u32);
	// lfs f27,5028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5028);
	ctx.f27.f64 = double(temp.f32);
	// lfs f10,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f10,f27,f10,f6
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f10.f64 - ctx.f6.f64));
	// lfs f6,3580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3580);
	ctx.f6.f64 = double(temp.f32);
	// lfs f30,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f13,f22,f0,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f17,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f12,f29,f1,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 - ctx.f12.f64));
	// stfs f9,3580(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3580, temp.u32);
	// fmadds f3,f31,f14,f3
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f14.f64 + ctx.f3.f64));
	// lfs f16,2716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2716);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f17,f30
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f30.f64));
	// lfs f18,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f18.f64 = double(temp.f32);
	// lfs f9,2708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2708);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f11,f16,f18,f11
	ctx.f11.f64 = double(float(-(ctx.f16.f64 * ctx.f18.f64 - ctx.f11.f64)));
	// lfs f1,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,4100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4100);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,4108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4108);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f14.f64 = double(temp.f32);
	// stfs f5,4100(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4100, temp.u32);
	// fmadds f8,f1,f14,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f14.f64 + ctx.f8.f64));
	// fnmsubs f13,f6,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f6,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f2,f6
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// lfs f2,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f12,f4,f2,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 + ctx.f12.f64));
	// stfs f11,4108(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4108, temp.u32);
	// lfs f4,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f5.f64 = double(temp.f32);
	// lfs f11,4172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4172);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f5,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// stfs f7,2716(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2716, temp.u32);
	// lfs f24,3164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3164);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,4552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4552);
	ctx.f22.f64 = double(temp.f32);
	// lfs f28,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f20,f22,f24
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// fnmsubs f0,f9,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f0,5272(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5272, temp.u32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f11,f4,f3
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f4.f64 - ctx.f3.f64)));
	// lfs f26,3456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3456);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f0,f17,f0,f10
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f21,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f23,f26,f24
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// lfs f18,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f18,f18,f29
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f29.f64));
	// lfs f15,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f16,f28
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f28.f64));
	// lfs f31,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f15,f15,f19
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f19.f64));
	// lfs f14,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f31,f25,f31
	ctx.f31.f64 = double(float(ctx.f25.f64 * ctx.f31.f64));
	// lfs f9,4236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4236);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f14,f21
	ctx.f2.f64 = double(float(ctx.f14.f64 * ctx.f21.f64));
	// lfs f7,4668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4668);
	ctx.f7.f64 = double(temp.f32);
	// stfs f1,3164(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3164, temp.u32);
	// fadds f3,f7,f9
	ctx.f3.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// lfs f10,4680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4680);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,4676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4676);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,4692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4692);
	ctx.f1.f64 = double(temp.f32);
	// stfs f30,2660(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2660, temp.u32);
	// stfs f29,3444(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3444, temp.u32);
	// stfs f28,4552(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4552, temp.u32);
	// stfs f19,3572(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3572, temp.u32);
	// stfs f27,2348(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2348, temp.u32);
	// stfs f26,880(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 880, temp.u32);
	// stfs f25,2676(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2676, temp.u32);
	// stfs f22,1060(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1060, temp.u32);
	// stfs f21,4664(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4664, temp.u32);
	// lfs f29,4672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4672);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f3,f3,f10
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// fadds f27,f29,f1
	ctx.f27.f64 = double(float(ctx.f29.f64 + ctx.f1.f64));
	// lfs f28,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f12,f23,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f28,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f28.f64 = double(temp.f32);
	// lfs f19,2788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2788);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f0,f8,f28,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f28.f64 + ctx.f0.f64));
	// fmuls f17,f10,f19
	ctx.f17.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// lfs f8,4300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4300);
	ctx.f8.f64 = double(temp.f32);
	// lfs f25,4700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4700);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f30,f10,f4
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// lfs f22,2620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2620);
	ctx.f22.f64 = double(temp.f32);
	// stfs f17,4700(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4700, temp.u32);
	// fmuls f21,f25,f22
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f22.f64));
	// lfs f17,3252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3252);
	ctx.f17.f64 = double(temp.f32);
	// stfs f3,3252(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3252, temp.u32);
	// stfs f10,4692(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4692, temp.u32);
	// fmuls f27,f27,f24
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// stfs f27,4300(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4300, temp.u32);
	// lfs f27,2796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2796);
	ctx.f27.f64 = double(temp.f32);
	// fadds f3,f17,f27
	ctx.f3.f64 = double(float(ctx.f17.f64 + ctx.f27.f64));
	// stfs f21,2796(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2796, temp.u32);
	// lfs f21,2804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2804);
	ctx.f21.f64 = double(temp.f32);
	// stfs f3,5028(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5028, temp.u32);
	// fmuls f10,f10,f21
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f21.f64));
	// lfs f26,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// lfs f3,3676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3676);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f13,f18,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f18.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// stfs f2,2804(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2804, temp.u32);
	// stfs f5,3676(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3676, temp.u32);
	// stfs f10,4668(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4668, temp.u32);
	// lfs f2,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f2.f64 = double(temp.f32);
	// lfs f5,3204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3204);
	ctx.f5.f64 = double(temp.f32);
	// lfs f26,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// lfs f10,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// lfs f28,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,3476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3476);
	ctx.f18.f64 = double(temp.f32);
	// lfs f2,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f2.f64 = double(temp.f32);
	// stfs f10,3476(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3476, temp.u32);
	// fmuls f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f10,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f10.f64 = double(temp.f32);
	// stfs f2,4676(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4676, temp.u32);
	// fmuls f10,f10,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfs f2,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f2.f64 = double(temp.f32);
	// stfs f10,2788(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2788, temp.u32);
	// fmuls f2,f2,f18
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f18.f64));
	// lfs f23,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f23.f64 = double(temp.f32);
	// lfs f10,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f10.f64 = double(temp.f32);
	// stfs f2,4236(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4236, temp.u32);
	// fmuls f10,f10,f23
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f23.f64));
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// stfs f10,4172(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4172, temp.u32);
	// fmuls f2,f30,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// lfs f10,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f10.f64 = double(temp.f32);
	// lfs f30,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f20,f10,f12
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f10.f64 + ctx.f12.f64));
	// lfs f20,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f13,f6,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// lfs f14,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f30,f16,f20,f0
	ctx.f30.f64 = double(float(-(ctx.f16.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f10,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f10.f64 = double(temp.f32);
	// lfs f6,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f10,f14,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f10.f64));
	// lfs f20,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f6,f6,f3
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// stfs f11,3364(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3364, temp.u32);
	// stfs f26,600(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 600, temp.u32);
	// stfs f8,2620(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2620, temp.u32);
	// stfs f28,604(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 604, temp.u32);
	// stfs f7,3668(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3668, temp.u32);
	// stfs f9,4816(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4816, temp.u32);
	// stfs f4,4620(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4620, temp.u32);
	// stfs f22,3204(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3204, temp.u32);
	// stfs f25,828(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 828, temp.u32);
	// stfs f29,756(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 756, temp.u32);
	// stfs f1,1020(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1020, temp.u32);
	// stfs f23,3180(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3180, temp.u32);
	// stfs f19,5024(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5024, temp.u32);
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// fnmsubs f13,f15,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f15.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f8,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f9,f31,f0,f30
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// lfs f30,3676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3676);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f5,f8,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f8.f64 - ctx.f12.f64)));
	// lfs f29,2804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2804);
	ctx.f29.f64 = double(temp.f32);
	// fmr f8,f0
	ctx.f8.f64 = ctx.f0.f64;
	// stfs f3,2708(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2708, temp.u32);
	// fmr f7,f0
	ctx.f7.f64 = ctx.f0.f64;
	// lfs f3,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f3.f64 = double(temp.f32);
	// lfs f11,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f11.f64 = double(temp.f32);
	// lfs f28,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f28.f64 = double(temp.f32);
	// lfs f23,3476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3476);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,4700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4700);
	ctx.f22.f64 = double(temp.f32);
	// stfs f17,3380(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3380, temp.u32);
	// fmadds f13,f30,f0,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f30,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f0,f29,f0,f9
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f29,3252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3252);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f2,f29,f30,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f30.f64 - ctx.f2.f64));
	// lfs f29,2796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2796);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f29,f3,f12
	ctx.f12.f64 = double(float(-(ctx.f29.f64 * ctx.f3.f64 - ctx.f12.f64)));
	// lfs f17,2844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2844);
	ctx.f17.f64 = double(temp.f32);
	// fmr f30,f3
	ctx.f30.f64 = ctx.f3.f64;
	// stfs f14,3688(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3688, temp.u32);
	// lfs f14,3276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3276);
	ctx.f14.f64 = double(temp.f32);
	// fmr f3,f8
	ctx.f3.f64 = ctx.f8.f64;
	// stfs f21,2244(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2244, temp.u32);
	// fmr f25,f8
	ctx.f25.f64 = ctx.f8.f64;
	// lfs f1,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f1.f64 = double(temp.f32);
	// fmr f19,f8
	ctx.f19.f64 = ctx.f8.f64;
	// lfs f5,4340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4340);
	ctx.f5.f64 = double(temp.f32);
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// stfs f18,520(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 520, temp.u32);
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// fmadds f13,f23,f11,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f11.f64 + ctx.f13.f64));
	// lfs f23,2788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2788);
	ctx.f23.f64 = double(temp.f32);
	// lfs f11,5044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5044);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f7,f23,f7,f0
	ctx.f7.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 + ctx.f0.f64));
	// fnmsubs f2,f22,f28,f2
	ctx.f2.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f2.f64)));
	// lfs f22,4300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4300);
	ctx.f22.f64 = double(temp.f32);
	// lfs f23,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f23.f64 = double(temp.f32);
	// fmr f18,f1
	ctx.f18.f64 = ctx.f1.f64;
	// fmadds f12,f22,f30,f12
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f30.f64 + ctx.f12.f64));
	// lfs f22,4692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4692);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f22,f11
	ctx.f21.f64 = double(float(ctx.f22.f64 * ctx.f11.f64));
	// stfs f22,3276(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3276, temp.u32);
	// fmuls f15,f22,f17
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f17.f64));
	// stfs f24,2668(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2668, temp.u32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// stfs f22,2844(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2844, temp.u32);
	// fmr f16,f23
	ctx.f16.f64 = ctx.f23.f64;
	// lfs f22,4676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4676);
	ctx.f22.f64 = double(temp.f32);
	// lfs f26,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f26.f64 = double(temp.f32);
	// fmr f24,f1
	ctx.f24.f64 = ctx.f1.f64;
	// fmadds f13,f22,f8,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f8.f64 + ctx.f13.f64));
	// lfs f22,4668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4668);
	ctx.f22.f64 = double(temp.f32);
	// stfs f27,3372(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3372, temp.u32);
	// fnmsubs f7,f5,f1,f7
	ctx.f7.f64 = double(float(-(ctx.f5.f64 * ctx.f1.f64 - ctx.f7.f64)));
	// fmadds f2,f22,f23,f2
	ctx.f2.f64 = double(float(ctx.f22.f64 * ctx.f23.f64 + ctx.f2.f64));
	// lfs f22,4236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4236);
	ctx.f22.f64 = double(temp.f32);
	// lfs f27,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f27.f64 = double(temp.f32);
	// fmr f30,f26
	ctx.f30.f64 = ctx.f26.f64;
	// lfs f0,-19172(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19172);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f12,f22,f26,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f26.f64 - ctx.f12.f64)));
	// lfs f8,4404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4404);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f27,f27,f20
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// lfs f1,5004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5004);
	ctx.f1.f64 = double(temp.f32);
	// lfs f23,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f23.f64 = double(temp.f32);
	// stfs f11,4800(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4800, temp.u32);
	// fmr f26,f16
	ctx.f26.f64 = ctx.f16.f64;
	// lfs f4,4996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4996);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f23,f23,f1
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// lfs f9,4372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4372);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f11,f8,f0
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f28,4388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4388);
	ctx.f28.f64 = double(temp.f32);
	// stfs f20,2804(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2804, temp.u32);
	// stfs f17,4776(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4776, temp.u32);
	// stfs f14,3708(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3708, temp.u32);
	// fmadds f13,f4,f31,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f31.f64 + ctx.f13.f64));
	// stfs f0,144(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// fmadds f0,f6,f30,f12
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f30.f64 + ctx.f12.f64));
	// stfs f5,3476(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3476, temp.u32);
	// lfs f5,4172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4172);
	ctx.f5.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fnmsubs f7,f5,f3,f7
	ctx.f7.f64 = double(float(-(ctx.f5.f64 * ctx.f3.f64 - ctx.f7.f64)));
	// stfs f9,4300(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4300, temp.u32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// fnmsubs f5,f21,f16,f2
	ctx.f5.f64 = double(float(-(ctx.f21.f64 * ctx.f16.f64 - ctx.f2.f64)));
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// fmr f3,f16
	ctx.f3.f64 = ctx.f16.f64;
	// stfs f8,4768(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4768, temp.u32);
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// stfs f4,4692(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4692, temp.u32);
	// lfs f31,2844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2844);
	ctx.f31.f64 = double(temp.f32);
	// lfs f4,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f9,f29,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// lfs f9,4108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4108);
	ctx.f9.f64 = double(temp.f32);
	// fadds f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// lfs f9,5028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5028);
	ctx.f9.f64 = double(temp.f32);
	// stfs f0,5276(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5276, temp.u32);
	// fnmsubs f12,f10,f25,f7
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f25.f64 - ctx.f7.f64)));
	// stfs f28,2796(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2796, temp.u32);
	// fmadds f10,f15,f26,f5
	ctx.f10.f64 = double(float(ctx.f15.f64 * ctx.f26.f64 + ctx.f5.f64));
	// lfs f6,3276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3276);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,4756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4756);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,4420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4420);
	ctx.f30.f64 = double(temp.f32);
	// lfs f25,3736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3736);
	ctx.f25.f64 = double(temp.f32);
	// lfs f21,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f6,f25
	ctx.f22.f64 = double(float(ctx.f6.f64 * ctx.f25.f64));
	// fmadds f0,f9,f24,f13
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f24.f64 + ctx.f13.f64));
	// lfs f9,-19180(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19180);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,264(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 264, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f9,-19184(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19184);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f27,f19,f12
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f19.f64 + ctx.f12.f64));
	// stfs f9,608(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 608, temp.u32);
	// lfs f12,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f10,f31,f3,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f3.f64 - ctx.f10.f64)));
	// lfs f19,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f11,f23,f19,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f19.f64 - ctx.f11.f64));
	// fnmsubs f0,f28,f18,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f18.f64 - ctx.f0.f64)));
	// lfs f23,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f23.f64 = double(temp.f32);
	// lfs f9,4412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4412);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f18,f21,f30
	ctx.f18.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// lfs f3,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f5,f6,f9
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f9.f64));
	// lfs f31,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f3,f3,f2
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// lfs f24,4384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4384);
	ctx.f24.f64 = double(temp.f32);
	// lfs f19,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f19.f64 = double(temp.f32);
	// lfs f27,1136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1136);
	ctx.f27.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f0,5280(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5280, temp.u32);
	// lfs f0,-19176(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19176);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f0,216(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// lfs f0,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f8,f0,f12
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// fmuls f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f28,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f28.f64 = double(temp.f32);
	// lfs f7,-19196(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19196);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// stfs f7,480(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 480, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f7,3188(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 3188);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f29,f0,f31
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// stfs f7,840(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 840, temp.u32);
	// fmuls f20,f0,f24
	ctx.f20.f64 = double(float(ctx.f0.f64 * ctx.f24.f64));
	// lfs f7,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f19,f8,f19
	ctx.f19.f64 = double(float(ctx.f8.f64 * ctx.f19.f64));
	// fmuls f7,f0,f7
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f17,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f13,f13,f17
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f17.f64));
	// fmuls f7,f7,f23
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f23.f64));
	// lfs f23,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f23.f64 = double(temp.f32);
	// lfs f17,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f26,f0,f28
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// fmuls f8,f8,f17
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f17.f64));
	// lfs f17,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f3,f3,f17
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// lfs f17,-19192(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19192);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f13,f11,f0,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 - ctx.f13.f64));
	// stfs f12,5044(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5044, temp.u32);
	// fmadds f4,f27,f23,f4
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f23.f64 + ctx.f4.f64));
	// lfs f12,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f12.f64 = double(temp.f32);
	// fmr f11,f17
	ctx.f11.f64 = ctx.f17.f64;
	// stfs f25,4700(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4700, temp.u32);
	// lfs f15,-19200(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19200);
	ctx.f15.f64 = double(temp.f32);
	// lfs f25,4568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4568);
	ctx.f25.f64 = double(temp.f32);
	// stfs f27,4808(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4808, temp.u32);
	// lfs f23,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f23.f64 = double(temp.f32);
	// lfs f27,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f10,f5,f23,f10
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f23.f64 - ctx.f10.f64)));
	// stfs f17,832(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 832, temp.u32);
	// fmuls f17,f0,f25
	ctx.f17.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// stfs f15,176(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// lfs f15,4584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4584);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f13,f29,f27,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f27.f64 + ctx.f13.f64));
	// stfs f17,4584(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4584, temp.u32);
	// fmuls f17,f6,f15
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// fmadds f12,f12,f11,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64 + ctx.f4.f64));
	// stfs f9,4108(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4108, temp.u32);
	// lfs f9,3808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3808);
	ctx.f9.f64 = double(temp.f32);
	// lfs f29,4536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4536);
	ctx.f29.f64 = double(temp.f32);
	// stfs f24,3480(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3480, temp.u32);
	// fmuls f24,f21,f29
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// stfs f17,3808(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3808, temp.u32);
	// lfs f17,4600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4600);
	ctx.f17.f64 = double(temp.f32);
	// stfs f2,4600(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4600, temp.u32);
	// lfs f5,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f0,f5
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f5.f64));
	// stfs f30,668(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 668, temp.u32);
	// lfs f23,4512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4512);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f12,f12,f0,f7
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f7.f64));
	// lfs f7,4592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4592);
	ctx.f7.f64 = double(temp.f32);
	// stfs f24,4592(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4592, temp.u32);
	// fadds f2,f17,f7
	ctx.f2.f64 = double(float(ctx.f17.f64 + ctx.f7.f64));
	// lfs f24,4608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4608);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f16,f6,f23
	ctx.f16.f64 = double(float(ctx.f6.f64 * ctx.f23.f64));
	// stfs f2,4536(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4536, temp.u32);
	// stfs f20,4608(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4608, temp.u32);
	// lfs f2,4632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4632);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,4640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4640);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f1,f2,f1
	ctx.f1.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// stfs f5,4632(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4632, temp.u32);
	// fmuls f5,f0,f20
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f20.f64));
	// lfs f11,3732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3732);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,-19188(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -19188);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f10,f22,f30,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f30.f64 + ctx.f10.f64));
	// stfs f1,3732(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3732, temp.u32);
	// stfs f5,4512(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4512, temp.u32);
	// lfs f4,4100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4100);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,3432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3432);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,3440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3440);
	ctx.f5.f64 = double(temp.f32);
	// stfs f14,172(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// lfs f22,4576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4576);
	ctx.f22.f64 = double(temp.f32);
	// lfs f14,3212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3212);
	ctx.f14.f64 = double(temp.f32);
	// stfs f8,4640(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4640, temp.u32);
	// fmuls f8,f0,f1
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f1.f64));
	// stfs f26,3432(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3432, temp.u32);
	// fmuls f26,f21,f5
	ctx.f26.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// stfs f31,3464(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3464, temp.u32);
	// fmuls f31,f4,f11
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f11.f64));
	// stfs f4,4384(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4384, temp.u32);
	// fmuls f4,f14,f22
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f22.f64));
	// stfs f28,4680(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4680, temp.u32);
	// lfs f28,2644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2644);
	ctx.f28.f64 = double(temp.f32);
	// lfs f30,3816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3816);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f3,f28,f9,f3
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f9.f64 + ctx.f3.f64));
	// stfs f8,4756(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4756, temp.u32);
	// fmuls f27,f21,f30
	ctx.f27.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// stfs f26,4412(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4412, temp.u32);
	// lfs f8,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f8.f64 = double(temp.f32);
	// lfs f26,3448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3448);
	ctx.f26.f64 = double(temp.f32);
	// stfs f4,4576(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4576, temp.u32);
	// lfs f4,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f4.f64 = double(temp.f32);
	// stfs f8,3448(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3448, temp.u32);
	// stfs f26,3440(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3440, temp.u32);
	// fmuls f8,f0,f8
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// fmuls f26,f0,f26
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f26.f64));
	// stfs f8,3276(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3276, temp.u32);
	// stfs f26,2844(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2844, temp.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f26,3520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3520);
	ctx.f26.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f8,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f8.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f8,3520(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3520, temp.u32);
	// fmuls f8,f0,f8
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// stfs f8,4388(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4388, temp.u32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// stfs f6,5004(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5004, temp.u32);
	// fmuls f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfs f8,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// stfs f6,4420(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4420, temp.u32);
	// fmuls f8,f4,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 * ctx.f8.f64));
	// lfs f6,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f6.f64 = double(temp.f32);
	// stfs f8,4568(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4568, temp.u32);
	// fmr f8,f6
	ctx.f8.f64 = ctx.f6.f64;
	// stfs f11,4340(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4340, temp.u32);
	// fmuls f6,f24,f6
	ctx.f6.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// lfs f11,3448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3448);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,3456(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3456, temp.u32);
	// stfs f9,4672(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4672, temp.u32);
	// stfs f6,3816(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3816, temp.u32);
	// lfs f6,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f6,f31,f6,f3
	ctx.f6.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f3.f64)));
	// stfs f7,4668(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4668, temp.u32);
	// lfs f7,3232(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3232);
	ctx.f7.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f11,3440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3440);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f10,f16,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f16.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// lfs f8,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f13,f18,f8,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f8.f64 + ctx.f13.f64));
	// lfs f8,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f8.f64 = double(temp.f32);
	// stfs f11,2644(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2644, temp.u32);
	// fmr f9,f8
	ctx.f9.f64 = ctx.f8.f64;
	// fmadds f12,f19,f8,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f8.f64 + ctx.f12.f64));
	// lfs f8,3432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3432);
	ctx.f8.f64 = double(temp.f32);
	// lfs f11,3520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3520);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,4172(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4172, temp.u32);
	// lfs f11,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f11.f64 = double(temp.f32);
	// stfs f4,4688(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4688, temp.u32);
	// stfs f5,3472(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3472, temp.u32);
	// stfs f7,728(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 728, temp.u32);
	// lfs f3,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f13,f8,f11,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// lfs f11,4640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4640);
	ctx.f11.f64 = double(temp.f32);
	// lfs f8,4832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4832);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f3,f0,f3
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// fnmsubs f12,f11,f9,f12
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f9.f64 - ctx.f12.f64)));
	// lfs f11,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,4544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4544);
	ctx.f5.f64 = double(temp.f32);
	// lfs f7,3132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3132);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f6,f5,f8,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f8.f64 - ctx.f6.f64)));
	// lfs f4,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,4632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4632);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f7,f4
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// stfs f2,4996(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4996, temp.u32);
	// stfs f1,3212(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3212, temp.u32);
	// fmr f1,f9
	ctx.f1.f64 = ctx.f9.f64;
	// lfs f2,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f13,f31,f11,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// stfs f23,4676(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4676, temp.u32);
	// stfs f29,640(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 640, temp.u32);
	// stfs f25,4752(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4752, temp.u32);
	// stfs f28,4404(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4404, temp.u32);
	// stfs f14,3252(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3252, temp.u32);
	// stfs f22,816(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// stfs f15,5028(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5028, temp.u32);
	// stfs f17,4236(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4236, temp.u32);
	// stfs f24,2788(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2788, temp.u32);
	// stfs f20,4760(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4760, temp.u32);
	// stfs f27,3736(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3736, temp.u32);
	// stfs f30,4372(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4372, temp.u32);
	// stfs f21,3676(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3676, temp.u32);
	// stfs f26,4100(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4100, temp.u32);
	// lfs f31,4608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4608);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f31,f9,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f9.f64 + ctx.f12.f64));
	// lfs f19,4592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4592);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f13,f19,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f19,4584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4584);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,4600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4600);
	ctx.f31.f64 = double(temp.f32);
	// lfs f11,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,4840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4840);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// lfs f16,4536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4536);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f28,f0,f30
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f30.f64));
	// lfs f15,3816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3816);
	ctx.f15.f64 = double(temp.f32);
	// lfs f25,4912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4912);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f23.f64 = double(temp.f32);
	// stfs f28,4840(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4840, temp.u32);
	// fmuls f21,f23,f25
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// fnmsubs f12,f19,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f1,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f1,f16,f1,f15
	ctx.f1.f64 = double(float(ctx.f16.f64 * ctx.f1.f64 + ctx.f15.f64));
	// lfs f19,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f19.f64 = double(temp.f32);
	// lfs f15,3808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3808);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f10,f15,f19,f10
	ctx.f10.f64 = double(float(-(ctx.f15.f64 * ctx.f19.f64 - ctx.f10.f64)));
	// lfs f28,3732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3732);
	ctx.f28.f64 = double(temp.f32);
	// lfs f9,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f13,f28,f16,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f16.f64 - ctx.f13.f64)));
	// lfs f17,4568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4568);
	ctx.f17.f64 = double(temp.f32);
	// lfs f19,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f19.f64 = double(temp.f32);
	// lfs f2,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f6,f4,f19,f6
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f19.f64 - ctx.f6.f64)));
	// lfs f15,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f2,f9,f2,f17
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 + ctx.f17.f64));
	// lfs f14,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,4512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4512);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f19,f14,f31
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f31.f64));
	// lfs f29,4864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4864);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f12,f16,f15,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f15.f64 + ctx.f12.f64));
	// lfs f18,4576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4576);
	ctx.f18.f64 = double(temp.f32);
	// lfs f20,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f20.f64 = double(temp.f32);
	// lfs f17,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f18,f20
	ctx.f20.f64 = double(float(ctx.f18.f64 * ctx.f20.f64));
	// stfs f2,4864(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4864, temp.u32);
	// fmuls f17,f29,f17
	ctx.f17.f64 = double(float(ctx.f29.f64 * ctx.f17.f64));
	// lfs f27,4872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4872);
	ctx.f27.f64 = double(temp.f32);
	// lfs f16,2556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2556);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,-19212(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19212);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f11,f11,f14
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f14.f64));
	// lfs f28,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f26.f64 = double(temp.f32);
	// stfs f0,4872(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4872, temp.u32);
	// fmuls f0,f16,f28
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f28.f64));
	// stfs f29,4568(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4568, temp.u32);
	// fmuls f24,f26,f27
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// stfs f2,860(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 860, temp.u32);
	// lfs f22,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,-19552(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19552);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f18.f64 = double(temp.f32);
	// lfs f4,5096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5096);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f18,f18,f31
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f31.f64));
	// lfs f2,4928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4928);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f22,f4
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// lfs f29,4384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4384);
	ctx.f29.f64 = double(temp.f32);
	// stfs f16,4912(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4912, temp.u32);
	// lfs f14,-19204(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19204);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,-19208(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19208);
	ctx.f16.f64 = double(temp.f32);
	// stfs f9,4632(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4632, temp.u32);
	// stfs f30,4640(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4640, temp.u32);
	// fmr f30,f15
	ctx.f30.f64 = ctx.f15.f64;
	// stfs f8,3448(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3448, temp.u32);
	// stfs f27,948(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 948, temp.u32);
	// fmadds f27,f29,f2,f21
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f2.f64 + ctx.f21.f64));
	// lfs f9,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,4920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4920);
	ctx.f8.f64 = double(temp.f32);
	// stfs f26,3432(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3432, temp.u32);
	// stfs f23,3440(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3440, temp.u32);
	// stfs f25,940(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 940, temp.u32);
	// stfs f15,696(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 696, temp.u32);
	// stfs f14,224(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 224, temp.u32);
	// stfs f22,928(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 928, temp.u32);
	// stfs f16,952(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 952, temp.u32);
	// lfs f23,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f30,f9,f30,f17
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f30.f64 - ctx.f17.f64));
	// lfs f25,3736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3736);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f13,f25,f23,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f23.f64 + ctx.f13.f64));
	// lfs f25,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f3,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// lfs f22,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f22.f64 = double(temp.f32);
	// fmr f3,f23
	ctx.f3.f64 = ctx.f23.f64;
	// lfs f26,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f6,f24,f22,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f22.f64 - ctx.f6.f64)));
	// lfs f24,4420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4420);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f26,f26,f8
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// lfs f21,4756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4756);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f11,f18,f23,f11
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f23.f64 - ctx.f11.f64));
	// lfs f23,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,4580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4580);
	ctx.f25.f64 = double(temp.f32);
	// lfs f15,3648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3648);
	ctx.f15.f64 = double(temp.f32);
	// stfs f31,3648(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3648, temp.u32);
	// lfs f18,3600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3600);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f10,f24,f3,f10
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f3.f64 - ctx.f10.f64)));
	// lfs f3,4944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4944);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f30,f3,f22,f30
	ctx.f30.f64 = double(float(-(ctx.f3.f64 * ctx.f22.f64 - ctx.f30.f64)));
	// lfs f22,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f13,f21,f22,f13
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f22.f64 + ctx.f13.f64));
	// lfs f24,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f24.f64 = double(temp.f32);
	// fmr f22,f14
	ctx.f22.f64 = ctx.f14.f64;
	// lfs f16,3616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3616);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f26,f26,f23
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f23,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f6,f27,f23,f6
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f23.f64 + ctx.f6.f64));
	// lfs f27,4412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4412);
	ctx.f27.f64 = double(temp.f32);
	// lfs f23,4952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4952);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f25,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// fmuls f21,f23,f31
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f31.f64));
	// lfs f14,3656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3656);
	ctx.f14.f64 = double(temp.f32);
	// lfs f31,4404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4404);
	ctx.f31.f64 = double(temp.f32);
	// stfs f21,3600(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3600, temp.u32);
	// fadds f21,f15,f16
	ctx.f21.f64 = double(float(ctx.f15.f64 + ctx.f16.f64));
	// stfs f31,3656(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3656, temp.u32);
	// lfs f17,3608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3608);
	ctx.f17.f64 = double(temp.f32);
	// stfs f21,3608(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3608, temp.u32);
	// fnmsubs f12,f27,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// lfs f22,5004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5004);
	ctx.f22.f64 = double(temp.f32);
	// lfs f27,1608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1608);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f10,f1,f22,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f22.f64 + ctx.f10.f64));
	// lfs f1,4960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4960);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f31,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// stfs f31,4960(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4960, temp.u32);
	// fnmsubs f6,f5,f14,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f14.f64 - ctx.f6.f64)));
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// lfs f31,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f0,f0,f21
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f21.f64));
	// fnmsubs f11,f19,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// lfs f19,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f19.f64 = double(temp.f32);
	// lfs f21,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f26,f24,f19,f26
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f19.f64 - ctx.f26.f64));
	// lfs f31,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f1,f21
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// fnmsubs f31,f17,f31,f30
	ctx.f31.f64 = double(float(-(ctx.f17.f64 * ctx.f31.f64 - ctx.f30.f64)));
	// lfs f30,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f30.f64 = double(temp.f32);
	// lfs f19,3276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3276);
	ctx.f19.f64 = double(temp.f32);
	// stfs f21,3616(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3616, temp.u32);
	// fnmsubs f13,f19,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// stfs f28,4928(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4928, temp.u32);
	// lfs f21,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f21.f64 = double(temp.f32);
	// lfs f24,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f21,f21,f18
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// lfs f28,2844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2844);
	ctx.f28.f64 = double(temp.f32);
	// lfs f30,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f28,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// stfs f9,3276(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3276, temp.u32);
	// stfs f8,4944(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4944, temp.u32);
	// stfs f29,4920(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4920, temp.u32);
	// stfs f2,4832(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4832, temp.u32);
	// stfs f25,900(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 900, temp.u32);
	// stfs f3,3132(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3132, temp.u32);
	// stfs f27,932(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 932, temp.u32);
	// stfs f23,4608(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4608, temp.u32);
	// stfs f1,3816(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3816, temp.u32);
	// stfs f10,5284(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5284, temp.u32);
	// fmuls f9,f7,f30
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f30.f64));
	// lfs f10,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f11,f4,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f8,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f8.f64 = double(temp.f32);
	// lfs f10,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f12,f20,f8,f12
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f8.f64 + ctx.f12.f64));
	// lfs f4,4388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4388);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f4,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f3,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f0,f0,f3,f6
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f3.f64 + ctx.f6.f64));
	// lfs f10,5092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5092);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f4,f10,f4,f31
	ctx.f4.f64 = double(float(-(ctx.f10.f64 * ctx.f4.f64 - ctx.f31.f64)));
	// lfs f6,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f6.f64 = double(temp.f32);
	// lfs f25,4840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4840);
	ctx.f25.f64 = double(temp.f32);
	// lfs f31,4912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4912);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,4864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4864);
	ctx.f27.f64 = double(temp.f32);
	// lfs f8,5076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5076);
	ctx.f8.f64 = double(temp.f32);
	// lfs f28,4872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4872);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f12,f25,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f25.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// lfs f25,3656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3656);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f31,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f1,3876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3876);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f13,f27,f28,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f28.f64 + ctx.f13.f64));
	// fmadds f0,f25,f1,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f1.f64 + ctx.f0.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f27.f64 = double(temp.f32);
	// lfs f3,3728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3728);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,5084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5084);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f6,f3,f29,f26
	ctx.f6.f64 = double(float(-(ctx.f3.f64 * ctx.f29.f64 - ctx.f26.f64)));
	// stfs f22,1108(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1108, temp.u32);
	// fmadds f4,f2,f27,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f27.f64 + ctx.f4.f64));
	// lfs f22,3936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3936);
	ctx.f22.f64 = double(temp.f32);
	// stfs f16,4384(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4384, temp.u32);
	// fmuls f16,f5,f22
	ctx.f16.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// stfs f15,4544(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4544, temp.u32);
	// lfs f26,4884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4884);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,4276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4276);
	ctx.f27.f64 = double(temp.f32);
	// fadds f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f15,2252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2252);
	ctx.f15.f64 = double(temp.f32);
	// fadds f20,f27,f26
	ctx.f20.f64 = double(float(ctx.f27.f64 + ctx.f26.f64));
	// lfs f19,3968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3968);
	ctx.f19.f64 = double(temp.f32);
	// stfs f16,2252(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2252, temp.u32);
	// stfs f14,3520(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3520, temp.u32);
	// lfs f16,3648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3648);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,2980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2980);
	ctx.f14.f64 = double(temp.f32);
	// stfs f9,3968(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3968, temp.u32);
	// fmuls f9,f25,f15
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f15.f64));
	// stfs f17,4584(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4584, temp.u32);
	// lfs f17,3340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3340);
	ctx.f17.f64 = double(temp.f32);
	// lfs f30,4844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4844);
	ctx.f30.f64 = double(temp.f32);
	// stfs f20,2980(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2980, temp.u32);
	// fmuls f20,f14,f16
	ctx.f20.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// lfs f23,4988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4988);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f11,f31,f30,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f30.f64 + ctx.f11.f64));
	// stfs f9,3340(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3340, temp.u32);
	// lfs f9,3524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3524);
	ctx.f9.f64 = double(temp.f32);
	// stfs f18,4952(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4952, temp.u32);
	// fmuls f18,f5,f23
	ctx.f18.f64 = double(float(ctx.f5.f64 * ctx.f23.f64));
	// stfs f31,3936(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3936, temp.u32);
	// fmuls f31,f5,f19
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f19.f64));
	// stfs f20,4988(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4988, temp.u32);
	// fmadds f0,f5,f9,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f0.f64));
	// lfs f12,2260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2260);
	ctx.f12.f64 = double(temp.f32);
	// lfs f20,2268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2268);
	ctx.f20.f64 = double(temp.f32);
	// stfs f18,2260(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2260, temp.u32);
	// stfs f5,3524(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3524, temp.u32);
	// stfs f16,2268(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2268, temp.u32);
	// stfs f31,4884(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4884, temp.u32);
	// lfs f16,3616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3616);
	ctx.f16.f64 = double(temp.f32);
	// lfs f31,2700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2700);
	ctx.f31.f64 = double(temp.f32);
	// lfs f18,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f18.f64 = double(temp.f32);
	// lfs f5,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f18,f17,f18,f16
	ctx.f18.f64 = double(float(ctx.f17.f64 * ctx.f18.f64 + ctx.f16.f64));
	// lfs f29,4596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4596);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f5,f31,f5,f21
	ctx.f5.f64 = double(float(ctx.f31.f64 * ctx.f5.f64 + ctx.f21.f64));
	// lfs f16,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f24,f25,f29
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f21,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f21.f64 = double(temp.f32);
	// stfs f5,4276(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4276, temp.u32);
	// fnmsubs f11,f25,f20,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f20.f64 - ctx.f11.f64)));
	// stfs f13,5288(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5288, temp.u32);
	// stfs f10,4580(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4580, temp.u32);
	// fnmsubs f4,f12,f21,f4
	ctx.f4.f64 = double(float(-(ctx.f12.f64 * ctx.f21.f64 - ctx.f4.f64)));
	// lfs f13,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f10,3600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3600);
	ctx.f10.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f9,4864(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4864, temp.u32);
	// fmr f9,f21
	ctx.f9.f64 = ctx.f21.f64;
	// fnmsubs f0,f10,f13,f0
	ctx.f0.f64 = double(float(-(ctx.f10.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// stfs f12,2844(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2844, temp.u32);
	// lfs f12,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lfs f13,4960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4960);
	ctx.f13.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// lfs f5,3608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3608);
	ctx.f5.f64 = double(temp.f32);
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// fmadds f6,f5,f16,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f16.f64 + ctx.f6.f64));
	// lfs f5,4908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4908);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f13,f13,f12,f11
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f12.f64 - ctx.f11.f64)));
	// stfs f7,3728(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3728, temp.u32);
	// lfs f12,4740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4740);
	ctx.f12.f64 = double(temp.f32);
	// lis r6,-32254
	ctx.r6.s64 = -2113798144;
	// lfs f11,4916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4916);
	ctx.f11.f64 = double(temp.f32);
	// lis r5,-32254
	ctx.r5.s64 = -2113798144;
	// lfs f10,4748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4748);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f9,f5,f9,f4
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// lfs f4,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f4,f24,f4,f0
	ctx.f4.f64 = double(float(-(ctx.f24.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// stfs f8,3648(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3648, temp.u32);
	// fmuls f8,f11,f10
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f3,3736(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3736, temp.u32);
	// stfs f2,4536(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4536, temp.u32);
	// lfs f3,1600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1600);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f13,f7,f12,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f12.f64 + ctx.f13.f64));
	// lfs f2,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f2.f64 = double(temp.f32);
	// stfs f23,5076(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5076, temp.u32);
	// fmadds f6,f3,f2,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f6.f64));
	// lfs f23,2268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2268);
	ctx.f23.f64 = double(temp.f32);
	// stfs f27,3808(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3808, temp.u32);
	// stfs f19,3608(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3608, temp.u32);
	// stfs f20,4840(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4840, temp.u32);
	// fmadds f4,f18,f23,f4
	ctx.f4.f64 = double(float(ctx.f18.f64 * ctx.f23.f64 + ctx.f4.f64));
	// stfs f29,4592(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4592, temp.u32);
	// stfs f26,4404(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4404, temp.u32);
	// stfs f22,4576(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4576, temp.u32);
	// stfs f1,4600(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4600, temp.u32);
	// stfs f31,912(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 912, temp.u32);
	// stfs f30,5092(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5092, temp.u32);
	// lfs f1,1584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1584);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,5044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5044);
	ctx.f3.f64 = double(temp.f32);
	// lfs f0,-25384(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25384);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,1592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1592);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f9,f2,f0,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f30,2988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2988);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f31,f12,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f12.f64 - ctx.f13.f64)));
	// lfs f29,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f8,f29,f30,f8
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f30.f64 + ctx.f8.f64));
	// lfs f26,1568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1568);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f27,f3
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// lfs f24,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,2276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2276);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f24,f26
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// lfs f21,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f21,f22
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// lfs f19,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f20,f1
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f1.f64));
	// stfs f17,4512(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4512, temp.u32);
	// fmuls f19,f19,f1
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// stfs f28,5084(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5084, temp.u32);
	// lfs f28,1576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1576);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,1544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1544);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,3348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3348);
	ctx.f16.f64 = double(temp.f32);
	// stfs f15,3732(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3732, temp.u32);
	// stfs f14,4872(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4872, temp.u32);
	// stfs f0,780(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 780, temp.u32);
	// fnmsubs f6,f18,f17,f6
	ctx.f6.f64 = double(float(-(ctx.f18.f64 * ctx.f17.f64 - ctx.f6.f64)));
	// stfs f1,4740(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4740, temp.u32);
	// fmuls f17,f28,f16
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f1,-19220(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19220);
	ctx.f1.f64 = double(temp.f32);
	// lfs f18,1432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1432);
	ctx.f18.f64 = double(temp.f32);
	// lfs f15,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f15.f64 = double(temp.f32);
	// stfs f1,384(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// fmuls f15,f15,f18
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f18.f64));
	// stfs f11,2268(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2268, temp.u32);
	// fmuls f1,f19,f16
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// stfs f15,3348(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3348, temp.u32);
	// stfs f7,4916(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4916, temp.u32);
	// lfs f15,1368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1368);
	ctx.f15.f64 = double(temp.f32);
	// lfs f7,-19224(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19224);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,304(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// lfs f7,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f7.f64 = double(temp.f32);
	// stfs f10,4596(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4596, temp.u32);
	// lfs f0,4940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4940);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,3524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3524);
	ctx.f10.f64 = double(temp.f32);
	// stfs f25,2276(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2276, temp.u32);
	// fmadds f13,f10,f0,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f25,-19236(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -19236);
	ctx.f25.f64 = double(temp.f32);
	// stfs f12,4912(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4912, temp.u32);
	// fmuls f12,f24,f16
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f16.f64));
	// stfs f28,3616(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3616, temp.u32);
	// lfs f11,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f15,f11,f9
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f11.f64 - ctx.f9.f64)));
	// lfs f9,2260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2260);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f9,f9,f7,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f4.f64));
	// lfs f7,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f7,f26
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// stfs f25,872(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 872, temp.u32);
	// lfs f28,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f6,f27,f28,f6
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f28.f64 + ctx.f6.f64));
	// lfs f24,2980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2980);
	ctx.f24.f64 = double(temp.f32);
	// lfs f27,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f27.f64 = double(temp.f32);
	// stfs f18,4748(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4748, temp.u32);
	// stfs f21,2988(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2988, temp.u32);
	// stfs f23,3876(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3876, temp.u32);
	// fmadds f11,f24,f25,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f25.f64 + ctx.f11.f64));
	// lfs f24,2252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2252);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f9,f24,f27,f9
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f27.f64 - ctx.f9.f64)));
	// stfs f29,148(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f5,5004(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5004, temp.u32);
	// fmuls f7,f7,f16
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f16.f64));
	// stfs f2,4388(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4388, temp.u32);
	// stfs f31,4844(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4844, temp.u32);
	// lfs f14,-19216(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19216);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,-19228(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -19228);
	ctx.f18.f64 = double(temp.f32);
	// lfs f21,-19232(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -19232);
	ctx.f21.f64 = double(temp.f32);
	// lfs f2,4804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4804);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,3932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3932);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,3452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3452);
	ctx.f28.f64 = double(temp.f32);
	// lfs f5,4508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4508);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f13,f31,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f4,3924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3924);
	ctx.f4.f64 = double(temp.f32);
	// lfs f29,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,4952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4952);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f29,f5,f29
	ctx.f29.f64 = double(float(ctx.f5.f64 * ctx.f29.f64));
	// lfs f27,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f19,f27,f16
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// lfs f23,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f24,f22
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// stfs f3,3600(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3600, temp.u32);
	// fmuls f3,f20,f16
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// stfs f14,348(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 348, temp.u32);
	// fmuls f23,f23,f25
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// stfs f18,108(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f30,3656(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3656, temp.u32);
	// fmuls f30,f2,f4
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// stfs f21,648(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 648, temp.u32);
	// stfs f1,4804(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4804, temp.u32);
	// lfs f21,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,4524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4524);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,1832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1832);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,3956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3956);
	ctx.f14.f64 = double(temp.f32);
	// lfs f1,4812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4812);
	ctx.f1.f64 = double(temp.f32);
	// stfs f28,3932(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3932, temp.u32);
	// stfs f3,3956(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3956, temp.u32);
	// fmuls f28,f14,f4
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f4.f64));
	// lfs f3,1816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1816);
	ctx.f3.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fadds f3,f21,f3
	ctx.f3.f64 = double(float(ctx.f21.f64 + ctx.f3.f64));
	// stfs f28,4524(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4524, temp.u32);
	// stfs f3,4812(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4812, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f28,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f28.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lfs f3,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f8,f8,f28
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f28.f64));
	// fmuls f3,f17,f3
	ctx.f3.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// lfs f17,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f11,f18,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f18.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// fmuls f28,f28,f20
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lfs f17,2276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2276);
	ctx.f17.f64 = double(temp.f32);
	// stfs f28,4940(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4940, temp.u32);
	// fnmsubs f13,f17,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f17.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// stfs f3,3924(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3924, temp.u32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// lfs f28,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f28.f64 = double(temp.f32);
	// lis r7,-32254
	ctx.r7.s64 = -2113798144;
	// lfs f3,3348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3348);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f3,f3,f28,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64 + ctx.f29.f64));
	// stfs f3,4508(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4508, temp.u32);
	// lfs f3,3988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3988);
	ctx.f3.f64 = double(temp.f32);
	// stfs f17,3988(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3988, temp.u32);
	// lfs f17,3340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3340);
	ctx.f17.f64 = double(temp.f32);
	// lfs f29,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f9,f17,f29,f9
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f29.f64 + ctx.f9.f64));
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// stfs f0,2980(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2980, temp.u32);
	// fmuls f29,f29,f3
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// stfs f10,2260(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2260, temp.u32);
	// lfs f10,3968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3968);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f0,f10,f0,f13
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f28,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f28.f64 = double(temp.f32);
	// stfs f1,4960(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4960, temp.u32);
	// fmuls f30,f30,f28
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// stfs f5,2700(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2700, temp.u32);
	// lfs f28,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f28.f64 = double(temp.f32);
	// lfs f17,2988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2988);
	ctx.f17.f64 = double(temp.f32);
	// lfs f1,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f6,f17,f28,f6
	ctx.f6.f64 = double(float(ctx.f17.f64 * ctx.f28.f64 + ctx.f6.f64));
	// lfs f5,1800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1800);
	ctx.f5.f64 = double(temp.f32);
	// stfs f26,2252(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2252, temp.u32);
	// fmadds f11,f5,f1,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f11.f64));
	// stfs f21,4420(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4420, temp.u32);
	// stfs f23,4908(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4908, temp.u32);
	// lfs f13,4820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4820);
	ctx.f13.f64 = double(temp.f32);
	// lfs f26,3936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3936);
	ctx.f26.f64 = double(temp.f32);
	// lfs f23,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f0,f26,f13,f0
	ctx.f0.f64 = double(float(-(ctx.f26.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// lfs f21,4988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4988);
	ctx.f21.f64 = double(temp.f32);
	// stfs f2,3452(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3452, temp.u32);
	// fmuls f2,f29,f16
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f16.f64));
	// stfs f31,3340(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3340, temp.u32);
	// fnmsubs f9,f21,f23,f9
	ctx.f9.f64 = double(float(-(ctx.f21.f64 * ctx.f23.f64 - ctx.f9.f64)));
	// stfs f24,2276(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2276, temp.u32);
	// stfs f22,3524(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3524, temp.u32);
	// stfs f7,3348(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3348, temp.u32);
	// stfs f25,1184(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1184, temp.u32);
	// stfs f27,4756(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4756, temp.u32);
	// lfs f10,3996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3996);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,4548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4548);
	ctx.f7.f64 = double(temp.f32);
	// lfs f1,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,4004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4004);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,4956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4956);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,1732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1732);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,4012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4012);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f22.f64 = double(temp.f32);
	// stfs f15,4952(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4952, temp.u32);
	// stfs f20,876(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 876, temp.u32);
	// stfs f18,4412(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4412, temp.u32);
	// stfs f14,5044(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5044, temp.u32);
	// stfs f19,2988(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2988, temp.u32);
	// fmadds f11,f31,f29,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f23,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f12,f12,f23,f8
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f23.f64 - ctx.f8.f64));
	// lfs f8,4068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4068);
	ctx.f8.f64 = double(temp.f32);
	// lfs f18,2996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2996);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f8,f4
	ctx.f20.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfs f4,2996(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2996, temp.u32);
	// lfs f21,4748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4748);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,4916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4916);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f22,f21
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f21.f64));
	// stfs f21,4012(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4012, temp.u32);
	// fmadds f0,f17,f10,f0
	ctx.f0.f64 = double(float(ctx.f17.f64 * ctx.f10.f64 + ctx.f0.f64));
	// stfs f2,4068(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4068, temp.u32);
	// lfs f21,1344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1344);
	ctx.f21.f64 = double(temp.f32);
	// lfs f2,1336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1336);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f27,f25,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f25.f64 - ctx.f11.f64)));
	// lfs f25,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f25.f64 = double(temp.f32);
	// fmr f4,f25
	ctx.f4.f64 = ctx.f25.f64;
	// lfs f23,4084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4084);
	ctx.f23.f64 = double(temp.f32);
	// lfs f19,2284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2284);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f2,f2,f21
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// stfs f21,2284(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2284, temp.u32);
	// stfs f2,4084(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4084, temp.u32);
	// lfs f21,3956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3956);
	ctx.f21.f64 = double(temp.f32);
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f21,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f21,4524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4524);
	ctx.f21.f64 = double(temp.f32);
	// stfs f13,4820(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4820, temp.u32);
	// lfs f13,4884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4884);
	ctx.f13.f64 = double(temp.f32);
	// stfs f17,4004(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4004, temp.u32);
	// fmadds f6,f18,f4,f6
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f4.f64 + ctx.f6.f64));
	// lfs f4,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f4,f21,f4,f30
	ctx.f4.f64 = double(float(ctx.f21.f64 * ctx.f4.f64 - ctx.f30.f64));
	// lfs f30,3988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3988);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f0,f30,f7,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f7.f64 + ctx.f0.f64));
	// lfs f15,2092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2092);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,4812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4812);
	ctx.f17.f64 = double(temp.f32);
	// fadds f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 + ctx.f15.f64));
	// stfs f8,2556(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2556, temp.u32);
	// lfs f29,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f29.f64 = double(temp.f32);
	// lfs f15,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f15.f64 = double(temp.f32);
	// lfs f8,4276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4276);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f15,f15,f3
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f3.f64));
	// fmadds f11,f8,f29,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f29.f64 + ctx.f11.f64));
	// stfs f10,3996(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3996, temp.u32);
	// lfs f10,-19252(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19252);
	ctx.f10.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// lfs f14,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,-19240(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19240);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f14,f23,f14
	ctx.f14.f64 = double(float(ctx.f23.f64 * ctx.f14.f64));
	// fmadds f0,f13,f1,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f1.f64 + ctx.f0.f64));
	// lfs f13,3932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3932);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,1568(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1568, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f5,3956(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3956, temp.u32);
	// fmuls f5,f22,f2
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f2.f64));
	// stfs f10,848(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 848, temp.u32);
	// lfs f10,4636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4636);
	ctx.f10.f64 = double(temp.f32);
	// stfs f23,136(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f3,4956(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4956, temp.u32);
	// fnmsubs f11,f19,f25,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f25.f64 - ctx.f11.f64)));
	// stfs f31,3968(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3968, temp.u32);
	// stfs f26,944(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 944, temp.u32);
	// stfs f27,4916(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4916, temp.u32);
	// fnmsubs f0,f30,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f30.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// stfs f7,3988(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3988, temp.u32);
	// stfs f19,4748(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4748, temp.u32);
	// stfs f30,4548(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4548, temp.u32);
	// stfs f28,4812(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4812, temp.u32);
	// stfs f24,4524(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4524, temp.u32);
	// stfs f18,3936(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3936, temp.u32);
	// stfs f21,184(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// fmadds f0,f24,f13,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f13,-19244(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19244);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,104(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// fmuls f13,f15,f16
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// fadds f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// stfs f0,5292(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5292, temp.u32);
	// lfs f0,1328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1328);
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f17,f0
	ctx.f0.f64 = double(float(ctx.f17.f64 + ctx.f0.f64));
	// fmr f3,f2
	ctx.f3.f64 = ctx.f2.f64;
	// lfs f2,4804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4804);
	ctx.f2.f64 = double(temp.f32);
	// lfs f9,4180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4180);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f9,f7,f6
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f7.f64 + ctx.f6.f64));
	// lfs f6,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f6,f20,f6,f4
	ctx.f6.f64 = double(float(-(ctx.f20.f64 * ctx.f6.f64 - ctx.f4.f64)));
	// lfs f4,1312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1312);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,1252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1252);
	ctx.f1.f64 = double(temp.f32);
	// lfs f8,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f28,f1,f16
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// lfs f31,2996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2996);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f8,f10,f8,f14
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 + ctx.f14.f64));
	// lfs f27,4188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4188);
	ctx.f27.f64 = double(temp.f32);
	// stfs f28,2996(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2996, temp.u32);
	// fnmsubs f12,f2,f3,f12
	ctx.f12.f64 = double(float(-(ctx.f2.f64 * ctx.f3.f64 - ctx.f12.f64)));
	// stfs f11,4180(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4180, temp.u32);
	// fmr f3,f21
	ctx.f3.f64 = ctx.f21.f64;
	// lfs f21,4260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4260);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f2,f4,f16
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// stfs f2,4188(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4188, temp.u32);
	// lfs f2,3924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3924);
	ctx.f2.f64 = double(temp.f32);
	// lfs f28,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f28.f64 = double(temp.f32);
	// lfs f11,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f7,f21,f28,f7
	ctx.f7.f64 = double(float(-(ctx.f21.f64 * ctx.f28.f64 - ctx.f7.f64)));
	// lfs f28,1264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1264);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,1124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f8,f8,f31,f6
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f31.f64 + ctx.f6.f64));
	// lfs f23,4876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4876);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f29,f31
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// lfs f20,1188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1188);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f12,f2,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// lfs f11,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f0,f0,f3,f5
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f3.f64 + ctx.f5.f64));
	// lfs f2,1684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1684);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f11,f28,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f28,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,4876(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4876, temp.u32);
	// fmuls f19,f20,f31
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f31.f64));
	// lfs f26,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,4644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4644);
	ctx.f25.f64 = double(temp.f32);
	// lfs f6,2292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2292);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f22,f25,f16
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f16.f64));
	// lfs f5,1320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1320);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,1304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1304);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f24,f5,f27
	ctx.f24.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// lfs f18,1272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1272);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f30,f3,f31
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f15,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f5,f6
	ctx.f17.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fmadds f0,f2,f28,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f28,4740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4740);
	ctx.f28.f64 = double(temp.f32);
	// lfs f14,2284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2284);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f26,f26,f28
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// stfs f28,4636(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4636, temp.u32);
	// fmuls f18,f18,f14
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f14.f64));
	// stfs f0,4644(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4644, temp.u32);
	// fmuls f15,f23,f15
	ctx.f15.f64 = double(float(ctx.f23.f64 * ctx.f15.f64));
	// stfs f13,4260(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4260, temp.u32);
	// stfs f19,2292(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2292, temp.u32);
	// lfs f2,-19248(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19248);
	ctx.f2.f64 = double(temp.f32);
	// lfs f28,-19256(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -19256);
	ctx.f28.f64 = double(temp.f32);
	// lfs f0,-19264(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19264);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-19268(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19268);
	ctx.f13.f64 = double(temp.f32);
	// lfs f19,-19260(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19260);
	ctx.f19.f64 = double(temp.f32);
	// stfs f10,152(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f2,980(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 980, temp.u32);
	// stfs f4,4740(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4740, temp.u32);
	// stfs f28,972(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 972, temp.u32);
	// stfs f9,4804(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4804, temp.u32);
	// stfs f3,4884(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4884, temp.u32);
	// stfs f1,3924(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3924, temp.u32);
	// stfs f0,804(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 804, temp.u32);
	// stfs f29,388(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 388, temp.u32);
	// stfs f13,552(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 552, temp.u32);
	// stfs f27,1120(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1120, temp.u32);
	// stfs f19,612(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 612, temp.u32);
	// stfs f25,4988(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4988, temp.u32);
	// stfs f23,360(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 360, temp.u32);
	// stfs f21,3932(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3932, temp.u32);
	// stfs f20,4276(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4276, temp.u32);
	// lfs f13,4508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4508);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f9,f24,f14
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f14.f64));
	// fmadds f13,f13,f16,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f16.f64 + ctx.f12.f64));
	// lfs f12,4940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4940);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f17,f14
	ctx.f4.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// fnmsubs f0,f12,f0,f7
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// stfs f5,116(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lfs f12,4268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4268);
	ctx.f12.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,3348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3348);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f5.f64 = double(temp.f32);
	// stfs f6,1136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1136, temp.u32);
	// fmuls f6,f26,f16
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f26,4084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4084);
	ctx.f26.f64 = double(temp.f32);
	// lfs f2,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f13,f7,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// lfs f5,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f8,f26,f5,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f8.f64));
	// lfs f3,2276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2276);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f0,f12,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f1,2988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2988);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// fmr f7,f2
	ctx.f7.f64 = ctx.f2.f64;
	// lfs f26,3660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3660);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f25.f64 = double(temp.f32);
	// stfs f6,3660(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3660, temp.u32);
	// lfs f6,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f22,f6
	ctx.f6.f64 = double(float(ctx.f22.f64 * ctx.f6.f64));
	// lfs f28,4908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4908);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f13,f1,f29,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f29.f64 + ctx.f13.f64));
	// lfs f22,4068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4068);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f8,f18,f25,f8
	ctx.f8.f64 = double(float(-(ctx.f18.f64 * ctx.f25.f64 - ctx.f8.f64)));
	// stfs f4,4268(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4268, temp.u32);
	// fmadds f0,f3,f2,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f0.f64));
	// lfs f4,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f4.f64 = double(temp.f32);
	// fmr f24,f27
	ctx.f24.f64 = ctx.f27.f64;
	// lfs f1,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,1596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1596);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// lfs f10,4684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4684);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f20,f2,f31
	ctx.f20.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// lfs f17,1588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1588);
	ctx.f17.f64 = double(temp.f32);
	// stfs f4,4684(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4684, temp.u32);
	// fmuls f17,f17,f14
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// lfs f25,1256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1256);
	ctx.f25.f64 = double(temp.f32);
	// lfs f4,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f22,f27,f13
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f27.f64 - ctx.f13.f64)));
	// stfs f2,4508(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4508, temp.u32);
	// fnmsubs f8,f30,f1,f8
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f1.f64 - ctx.f8.f64)));
	// lfs f29,1116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f0,f28,f7,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// lfs f7,1460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1460);
	ctx.f7.f64 = double(temp.f32);
	// lfs f28,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f4,f25,f4,f15
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f4.f64 + ctx.f15.f64));
	// fmsubs f11,f7,f28,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f28.f64 - ctx.f11.f64));
	// lfs f28,2292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2292);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f1,f28,f1
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// lfs f22,4012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4012);
	ctx.f22.f64 = double(temp.f32);
	// lfs f5,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f18,f29,f16
	ctx.f18.f64 = double(float(ctx.f29.f64 * ctx.f16.f64));
	// lfs f23,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f23.f64 = double(temp.f32);
	// lfs f19,1468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1468);
	ctx.f19.f64 = double(temp.f32);
	// lfs f27,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f27,f27,f22
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// lfs f28,2300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2300);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f22
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f22.f64));
	// lfs f2,4260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4260);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f0,f10,f5,f0
	ctx.f0.f64 = double(float(-(ctx.f10.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// stfs f20,2284(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2284, temp.u32);
	// fmuls f20,f19,f31
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// stfs f17,2300(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2300, temp.u32);
	// fmr f17,f23
	ctx.f17.f64 = ctx.f23.f64;
	// lfs f21,1484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1484);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f13,f2,f24,f13
	ctx.f13.f64 = double(float(-(ctx.f2.f64 * ctx.f24.f64 - ctx.f13.f64)));
	// lfs f3,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f3.f64 = double(temp.f32);
	// lfs f15,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f15.f64 = double(temp.f32);
	// lfs f7,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f7.f64 = double(temp.f32);
	// stfs f29,4908(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4908, temp.u32);
	// fnmsubs f0,f26,f23,f0
	ctx.f0.f64 = double(float(-(ctx.f26.f64 * ctx.f23.f64 - ctx.f0.f64)));
	// stfs f12,4084(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4084, temp.u32);
	// lfs f12,4876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4876);
	ctx.f12.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f10,4876(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4876, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f10,4644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4644);
	ctx.f10.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// fmadds f13,f10,f16,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f16.f64 + ctx.f13.f64));
	// lfs f10,4188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4188);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f11,f11,f14,f1
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f14.f64 - ctx.f1.f64));
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// lfs f1,4636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4636);
	ctx.f1.f64 = double(temp.f32);
	// stfs f21,4260(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4260, temp.u32);
	// fnmsubs f12,f12,f15,f8
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f15.f64 - ctx.f8.f64)));
	// lfs f2,-19280(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19280);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,856(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 856, temp.u32);
	// fmr f2,f15
	ctx.f2.f64 = ctx.f15.f64;
	// fmadds f0,f21,f3,f0
	ctx.f0.f64 = double(float(ctx.f21.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f3,-19272(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19272);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,292(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 292, temp.u32);
	// fmuls f3,f27,f16
	ctx.f3.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// lfs f23,5080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5080);
	ctx.f23.f64 = double(temp.f32);
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// lfs f21,4596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4596);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f13,f10,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f7,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f7,f5
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// lfs f5,-19276(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19276);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,92(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// fmuls f10,f4,f16
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// lfs f5,4180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4180);
	ctx.f5.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f19,4940(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4940, temp.u32);
	// fmuls f19,f23,f21
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// fnmsubs f11,f20,f2,f11
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f2.f64 - ctx.f11.f64)));
	// lfs f20,4708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4708);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f0,f28,f17,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f17.f64 + ctx.f0.f64));
	// lfs f17,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f17.f64 = double(temp.f32);
	// stfs f25,4012(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4012, temp.u32);
	// fmuls f20,f20,f17
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f17.f64));
	// lfs f25,4308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4308);
	ctx.f25.f64 = double(temp.f32);
	// lfs f15,1452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1452);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f17.f64 = double(temp.f32);
	// stfs f19,4308(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4308, temp.u32);
	// fmuls f17,f15,f17
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f17.f64));
	// lfs f2,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f2.f64 = double(temp.f32);
	// lfs f29,2996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2996);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f13,f29,f2,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f2.f64 + ctx.f13.f64));
	// stfs f26,4068(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4068, temp.u32);
	// lfs f8,1712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1712);
	ctx.f8.f64 = double(temp.f32);
	// fadds f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 + ctx.f0.f64));
	// lfs f5,-19284(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19284);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,836(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 836, temp.u32);
	// fmuls f4,f8,f31
	ctx.f4.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// lfs f5,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f5,f5,f1,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f30.f64));
	// lfs f19,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f9,f9,f19,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f19.f64 - ctx.f6.f64));
	// lfs f30,4956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4956);
	ctx.f30.f64 = double(temp.f32);
	// lfs f19,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f19.f64 = double(temp.f32);
	// lfs f15,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f15.f64 = double(temp.f32);
	// lfs f6,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f10,f18,f15,f10
	ctx.f10.f64 = double(float(ctx.f18.f64 * ctx.f15.f64 - ctx.f10.f64));
	// lfs f18,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f7,f6,f18,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f18.f64 + ctx.f7.f64));
	// fmadds f5,f19,f30,f5
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f30.f64 + ctx.f5.f64));
	// lfs f2,4284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4284);
	ctx.f2.f64 = double(temp.f32);
	// fmr f19,f18
	ctx.f19.f64 = ctx.f18.f64;
	// lfs f29,4292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4292);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,2268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2268);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f2,f16
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// lfs f6,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f24,f29,f26
	ctx.f24.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// lfs f18,1580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1580);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f6,f6,f30
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// stfs f22,2292(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2292, temp.u32);
	// fmuls f22,f25,f31
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f31.f64));
	// lfs f15,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f18,f18,f14
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f14.f64));
	// stfs f7,5080(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5080, temp.u32);
	// fmuls f6,f6,f16
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f16.f64));
	// lfs f7,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f11,f4,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// stfs f27,4708(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4708, temp.u32);
	// lfs f27,772(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 772);
	ctx.f27.f64 = double(temp.f32);
	// lfs f7,-19476(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19476);
	ctx.f7.f64 = double(temp.f32);
	// stfs f8,4636(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4636, temp.u32);
	// fmr f8,f27
	ctx.f8.f64 = ctx.f27.f64;
	// stfs f7,796(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 796, temp.u32);
	// lfs f4,2300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2300);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f9,f4,f19,f9
	ctx.f9.f64 = double(float(-(ctx.f4.f64 * ctx.f19.f64 - ctx.f9.f64)));
	// lfs f4,3660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3660);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f10,f4,f15,f10
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f15.f64 - ctx.f10.f64)));
	// stfs f13,5300(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5300, temp.u32);
	// fmr f4,f15
	ctx.f4.f64 = ctx.f15.f64;
	// stfs f2,4180(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4180, temp.u32);
	// fmuls f13,f5,f16
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// lfs f5,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,4684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4684);
	ctx.f2.f64 = double(temp.f32);
	// stfs f0,5296(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5296, temp.u32);
	// stfs f26,4188(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4188, temp.u32);
	// fmadds f8,f20,f8,f17
	ctx.f8.f64 = double(float(ctx.f20.f64 * ctx.f8.f64 + ctx.f17.f64));
	// lfs f0,5156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5156);
	ctx.f0.f64 = double(temp.f32);
	// stfs f28,4644(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4644, temp.u32);
	// fmuls f7,f0,f31
	ctx.f7.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// lfs f26,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f9,f2,f5,f9
	ctx.f9.f64 = double(float(-(ctx.f2.f64 * ctx.f5.f64 - ctx.f9.f64)));
	// lfs f5,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f5.f64 = double(temp.f32);
	// lfs f28,1444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1444);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f11,f22,f5,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// fnmsubs f10,f3,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// stfs f25,4956(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4956, temp.u32);
	// fmuls f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f25,1644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1644);
	ctx.f25.f64 = double(temp.f32);
	// stfs f23,2996(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2996, temp.u32);
	// lfs f23,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f23.f64 = double(temp.f32);
	// stfs f1,4284(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4284, temp.u32);
	// stfs f29,3660(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3660, temp.u32);
	// lfs f26,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,3004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3004);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f8,f25,f26,f8
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f26.f64 + ctx.f8.f64));
	// lfs f1,4356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4356);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f25,f18,f23
	ctx.f25.f64 = double(float(ctx.f18.f64 * ctx.f23.f64));
	// stfs f27,644(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 644, temp.u32);
	// fadds f27,f29,f1
	ctx.f27.f64 = double(float(ctx.f29.f64 + ctx.f1.f64));
	// lfs f26,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f26.f64 = double(temp.f32);
	// lfs f22,4268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4268);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f9,f22,f26,f9
	ctx.f9.f64 = double(float(-(ctx.f22.f64 * ctx.f26.f64 - ctx.f9.f64)));
	// lfs f23,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f10,f24,f23,f10
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f23.f64 - ctx.f10.f64)));
	// lfs f20,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f11,f7,f26,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f26.f64 + ctx.f11.f64));
	// lfs f7,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f17,f20,f16
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// lfs f24,2284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2284);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,2308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2308);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f24,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f18,1428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1428);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f8,f8,f14,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f14.f64 - ctx.f25.f64));
	// stfs f17,2308(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2308, temp.u32);
	// fmuls f17,f18,f16
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// stfs f21,4292(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4292, temp.u32);
	// fmuls f27,f27,f31
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lfs f5,4324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4324);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,4348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4348);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,4724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4724);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f4,f5
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// lfs f26,1436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1436);
	ctx.f26.f64 = double(temp.f32);
	// lfs f7,1572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1572);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f24,f26,f16
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f21,3356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3356);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f7,f16
	ctx.f22.f64 = double(float(ctx.f7.f64 * ctx.f16.f64));
	// stfs f30,2300(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2300, temp.u32);
	// fmuls f30,f3,f31
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f17,3004(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3004, temp.u32);
	// fmuls f19,f21,f23
	ctx.f19.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// lfs f15,2316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2316);
	ctx.f15.f64 = double(temp.f32);
	// lfs f25,1676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1676);
	ctx.f25.f64 = double(temp.f32);
	// lfs f17,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f2,f2,f17,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f17.f64 + ctx.f28.f64));
	// stfs f22,3356(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3356, temp.u32);
	// fmr f28,f17
	ctx.f28.f64 = ctx.f17.f64;
	// lfs f22,1420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1420);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f22,f16
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f16.f64));
	// stfs f22,2316(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2316, temp.u32);
	// lfs f22,4708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4708);
	ctx.f22.f64 = double(temp.f32);
	// stfs f12,5304(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5304, temp.u32);
	// lfs f12,5080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5080);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,2284(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2284, temp.u32);
	// stfs f3,3348(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3348, temp.u32);
	// lfs f17,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f17.f64 = double(temp.f32);
	// lfs f3,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f3.f64 = double(temp.f32);
	// stfs f20,2988(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2988, temp.u32);
	// fnmsubs f9,f22,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// lfs f28,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f13,f13,f28,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f28.f64 + ctx.f10.f64));
	// lfs f10,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f0,f25,f10
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// lfs f10,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f22,f15,f17
	ctx.f22.f64 = double(float(ctx.f15.f64 * ctx.f17.f64));
	// lfs f17,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f11,f30,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f30.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// stfs f29,392(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 392, temp.u32);
	// stfs f7,2268(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2268, temp.u32);
	// lfs f7,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f7.f64 = double(temp.f32);
	// lfs f29,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f29.f64 = double(temp.f32);
	// stfs f4,408(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 408, temp.u32);
	// lfs f4,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f12,f12,f14,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f14.f64 + ctx.f9.f64));
	// stfs f26,4596(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4596, temp.u32);
	// fmr f9,f28
	ctx.f9.f64 = ctx.f28.f64;
	// lfs f28,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f28.f64 = double(temp.f32);
	// lfs f20,2316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2316);
	ctx.f20.f64 = double(temp.f32);
	// stfs f1,2276(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2276, temp.u32);
	// fmadds f11,f27,f29,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 + ctx.f11.f64));
	// stfs f23,628(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 628, temp.u32);
	// stfs f31,5156(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5156, temp.u32);
	// lfs f26,3800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3800);
	ctx.f26.f64 = double(temp.f32);
	// lfs f1,4944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4944);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,4928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4928);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,3784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3784);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f31,f30
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// fnmsubs f13,f6,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f6,3776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3776);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f3,f6,f3,f0
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f3.f64 - ctx.f0.f64));
	// lfs f0,4308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4308);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f12,f0,f10,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 + ctx.f12.f64));
	// lfs f0,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f0.f64 = double(temp.f32);
	// lfs f9,3768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3768);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,2716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2716);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f4,f9,f4,f22
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f4.f64 - ctx.f22.f64));
	// lfs f23,4004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4004);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f10,f0
	ctx.f27.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f5,440(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 440, temp.u32);
	// fmuls f22,f23,f26
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f26.f64));
	// stfs f21,4684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4684, temp.u32);
	// stfs f18,4268(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4268, temp.u32);
	// stfs f15,564(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 564, temp.u32);
	// fmadds f13,f2,f16,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f16.f64 + ctx.f13.f64));
	// lfs f2,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f8,f19,f2,f8
	ctx.f8.f64 = double(float(-(ctx.f19.f64 * ctx.f2.f64 - ctx.f8.f64)));
	// lfs f2,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f12,f24,f7,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f7,2692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2692);
	ctx.f7.f64 = double(temp.f32);
	// stfs f25,556(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 556, temp.u32);
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f5,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f0,f7,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f25,3524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3524);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,1240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1240);
	ctx.f24.f64 = double(temp.f32);
	// lfs f21,3920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3920);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f13,f20,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f28,3172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3172);
	ctx.f28.f64 = double(temp.f32);
	// lfs f20,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f26,f28
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// lfs f15,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f20,f29,f20
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f20.f64));
	// fmuls f17,f17,f25
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// stfs f0,4356(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4356, temp.u32);
	// fmuls f15,f15,f1
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f1.f64));
	// stfs f15,3768(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3768, temp.u32);
	// stfs f17,3776(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3776, temp.u32);
	// lfs f0,3928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3928);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,3580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3580);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,3356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3356);
	ctx.f17.f64 = double(temp.f32);
	// stfs f0,4324(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4324, temp.u32);
	// fnmsubs f12,f17,f5,f12
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f5.f64 - ctx.f12.f64)));
	// fmuls f0,f0,f15
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f15.f64));
	// stfs f15,3356(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3356, temp.u32);
	// lfs f15,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f15.f64 = double(temp.f32);
	// lfs f5,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f4,f24,f15,f4
	ctx.f4.f64 = double(float(-(ctx.f24.f64 * ctx.f15.f64 - ctx.f4.f64)));
	// fmuls f5,f29,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// lfs f15,2324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2324);
	ctx.f15.f64 = double(temp.f32);
	// stfs f5,2316(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2316, temp.u32);
	// lfs f5,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f5.f64 = double(temp.f32);
	// stfs f30,2324(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2324, temp.u32);
	// fmadds f5,f21,f5,f3
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f5.f64 + ctx.f3.f64));
	// stfs f10,1368(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1368, temp.u32);
	// lfs f10,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f10.f64 = double(temp.f32);
	// lfs f3,2308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2308);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// stfs f9,4944(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4944, temp.u32);
	// stfs f25,1044(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1044, temp.u32);
	// fmadds f12,f3,f18,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f18.f64 + ctx.f12.f64));
	// lfs f9,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f9.f64 = double(temp.f32);
	// fmr f25,f18
	ctx.f25.f64 = ctx.f18.f64;
	// stfs f13,3784(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3784, temp.u32);
	// fmadds f9,f20,f9,f4
	ctx.f9.f64 = double(float(ctx.f20.f64 * ctx.f9.f64 + ctx.f4.f64));
	// lfs f13,3976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3976);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f13,f10,f5
	ctx.f10.f64 = double(float(-(ctx.f13.f64 * ctx.f10.f64 - ctx.f5.f64)));
	// stfs f24,4928(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4928, temp.u32);
	// lfs f17,3012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3012);
	ctx.f17.f64 = double(temp.f32);
	// lfs f24,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f24.f64 = double(temp.f32);
	// stfs f21,3524(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3524, temp.u32);
	// lfs f21,3004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3004);
	ctx.f21.f64 = double(temp.f32);
	// fmsubs f0,f22,f24,f0
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f24.f64 - ctx.f0.f64));
	// stfs f19,3800(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3800, temp.u32);
	// fmuls f19,f23,f17
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f17.f64));
	// lfs f22,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f21,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// stfs f28,1000(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1000, temp.u32);
	// stfs f19,3012(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3012, temp.u32);
	// stfs f31,4348(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4348, temp.u32);
	// lfs f24,2324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2324);
	ctx.f24.f64 = double(temp.f32);
	// stfs f2,3928(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3928, temp.u32);
	// fnmsubs f9,f24,f22,f9
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f22.f64 - ctx.f9.f64)));
	// stfs f6,4308(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4308, temp.u32);
	// stfs f1,1128(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1128, temp.u32);
	// stfs f29,1088(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1088, temp.u32);
	// stfs f7,1220(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1220, temp.u32);
	// lfs f19,4844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4844);
	ctx.f19.f64 = double(temp.f32);
	// lfs f7,4376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4376);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,2708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2708);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,4496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4496);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,1232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1232);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f5,f16
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// lfs f2,4504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4504);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,2260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2260);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,4520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4520);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f1,f7
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f29,4920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4920);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f28,f28,f6
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// lfs f21,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f25,f4
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// stfs f8,5308(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5308, temp.u32);
	// fmuls f8,f19,f26
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f26.f64));
	// stfs f11,5312(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5312, temp.u32);
	// fmuls f11,f19,f17
	ctx.f11.f64 = double(float(ctx.f19.f64 * ctx.f17.f64));
	// stfs f27,3920(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3920, temp.u32);
	// fmuls f27,f29,f31
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// lfs f24,4528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4528);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f10,f2,f21,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f21.f64 + ctx.f10.f64));
	// lfs f22,4372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4372);
	ctx.f22.f64 = double(temp.f32);
	// stfs f14,1176(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1176, temp.u32);
	// lfs f14,4832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4832);
	ctx.f14.f64 = double(temp.f32);
	// stfs f29,2324(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2324, temp.u32);
	// fmuls f29,f23,f14
	ctx.f29.f64 = double(float(ctx.f23.f64 * ctx.f14.f64));
	// stfs f11,3976(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3976, temp.u32);
	// stfs f29,4520(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4520, temp.u32);
	// lfs f11,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f11.f64 = double(temp.f32);
	// lfs f29,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f11,f15,f11
	ctx.f11.f64 = double(float(ctx.f15.f64 * ctx.f11.f64));
	// fmadds f30,f30,f29,f27
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f29.f64 + ctx.f27.f64));
	// stfs f11,4528(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4528, temp.u32);
	// lfs f29,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f29.f64 = double(temp.f32);
	// lfs f11,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f12,f3,f29,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f29.f64 + ctx.f12.f64));
	// fmuls f11,f28,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f29,3012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3012);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f0,f29,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// lfs f28,3928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3928);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f9,f28,f27,f9
	ctx.f9.f64 = double(float(-(ctx.f28.f64 * ctx.f27.f64 - ctx.f9.f64)));
	// fmuls f29,f25,f29
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f25,4888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4888);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f28.f64 = double(temp.f32);
	// stfs f6,1432(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1432, temp.u32);
	// fnmsubs f10,f25,f28,f10
	ctx.f10.f64 = double(float(-(ctx.f25.f64 * ctx.f28.f64 - ctx.f10.f64)));
	// fmr f6,f27
	ctx.f6.f64 = ctx.f27.f64;
	// stfs f5,4004(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4004, temp.u32);
	// stfs f7,3012(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3012, temp.u32);
	// stfs f13,2260(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2260, temp.u32);
	// lfs f7,3920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3920);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,5128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5128);
	ctx.f13.f64 = double(temp.f32);
	// stfs f31,4376(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4376, temp.u32);
	// stfs f23,4496(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4496, temp.u32);
	// stfs f4,4920(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4920, temp.u32);
	// fmadds f10,f13,f5,f10
	ctx.f10.f64 = double(float(ctx.f13.f64 * ctx.f5.f64 + ctx.f10.f64));
	// lfs f5,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f9,f7,f6,f9
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f6.f64 - ctx.f9.f64)));
	// lfs f23,3876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3876);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f0,f8,f5,f0
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// lfs f21,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,4484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4484);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f21,f24
	ctx.f18.f64 = double(float(ctx.f21.f64 * ctx.f24.f64));
	// lfs f31,3196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3196);
	ctx.f31.f64 = double(temp.f32);
	// lfs f4,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f4.f64 = double(temp.f32);
	// lfs f8,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,5160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5160);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,5164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5164);
	ctx.f6.f64 = double(temp.f32);
	// stfs f19,3004(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3004, temp.u32);
	// lfs f19,1636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1636);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f10,f6,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// lfs f3,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f9,f7,f4,f9
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f4.f64 - ctx.f9.f64)));
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f3,f3,f19
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// stfs f1,4724(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4724, temp.u32);
	// fmuls f1,f24,f20
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f20.f64));
	// stfs f31,2308(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2308, temp.u32);
	// fmuls f31,f26,f31
	ctx.f31.f64 = double(float(ctx.f26.f64 * ctx.f31.f64));
	// stfs f23,4504(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4504, temp.u32);
	// fmuls f23,f23,f22
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// stfs f26,5080(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5080, temp.u32);
	// fmr f26,f28
	ctx.f26.f64 = ctx.f28.f64;
	// stfs f22,4832(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4832, temp.u32);
	// stfs f24,892(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 892, temp.u32);
	// stfs f2,4844(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4844, temp.u32);
	// lfs f5,5172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5172);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,1404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1404);
	ctx.f2.f64 = double(temp.f32);
	// lfs f27,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f27.f64 = double(temp.f32);
	// lfs f4,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f4.f64 = double(temp.f32);
	// lfs f8,5152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5152);
	ctx.f8.f64 = double(temp.f32);
	// lfs f24,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,3800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3800);
	ctx.f22.f64 = double(temp.f32);
	// stfs f15,1180(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1180, temp.u32);
	// stfs f16,4888(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4888, temp.u32);
	// stfs f21,672(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 672, temp.u32);
	// stfs f20,652(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 652, temp.u32);
	// stfs f14,3876(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3876, temp.u32);
	// stfs f17,4708(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4708, temp.u32);
	// stfs f19,3928(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3928, temp.u32);
	// fmadds f0,f22,f28,f0
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f28,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f9,f5,f27,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f27.f64 - ctx.f9.f64)));
	// lfs f21,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f10,f2,f4,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f10.f64));
	// stfs f13,3800(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3800, temp.u32);
	// fmr f19,f27
	ctx.f19.f64 = ctx.f27.f64;
	// lfs f17,3760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3760);
	ctx.f17.f64 = double(temp.f32);
	// fmr f20,f24
	ctx.f20.f64 = ctx.f24.f64;
	// lfs f13,3768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3768);
	ctx.f13.f64 = double(temp.f32);
	// stfs f29,3760(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3760, temp.u32);
	// fmr f15,f28
	ctx.f15.f64 = ctx.f28.f64;
	// lfs f29,3784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3784);
	ctx.f29.f64 = double(temp.f32);
	// fmr f14,f21
	ctx.f14.f64 = ctx.f21.f64;
	// lfs f22,5112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5112);
	ctx.f22.f64 = double(temp.f32);
	// fadds f12,f29,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 + ctx.f12.f64));
	// stfs f12,5316(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5316, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f27,1556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1556);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f31,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f31.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// lfs f31,4436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4436);
	ctx.f31.f64 = double(temp.f32);
	// stfs f3,4436(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4436, temp.u32);
	// fmr f26,f21
	ctx.f26.f64 = ctx.f21.f64;
	// lfs f3,3776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3776);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f10,f8,f28,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f28.f64 + ctx.f10.f64));
	// fmadds f9,f3,f24,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f24.f64 + ctx.f9.f64));
	// lfs f4,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f4.f64 = double(temp.f32);
	// fmr f16,f19
	ctx.f16.f64 = ctx.f19.f64;
	// lfs f28,3780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3780);
	ctx.f28.f64 = double(temp.f32);
	// lfs f3,5180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5180);
	ctx.f3.f64 = double(temp.f32);
	// stfs f8,5180(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5180, temp.u32);
	// lfs f8,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f8.f64 = double(temp.f32);
	// lfs f29,3204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3204);
	ctx.f29.f64 = double(temp.f32);
	// stfs f5,3784(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3784, temp.u32);
	// fmadds f0,f30,f21,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f21.f64 + ctx.f0.f64));
	// lfs f30,4548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4548);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f11,f1,f26,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f26.f64 + ctx.f11.f64));
	// lfs f5,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f12,f22,f19,f10
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f19.f64 + ctx.f10.f64));
	// lfs f10,2316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2316);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f13,f13,f20,f9
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f20.f64 - ctx.f9.f64)));
	// stfs f3,3776(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3776, temp.u32);
	// fmr f1,f24
	ctx.f1.f64 = ctx.f24.f64;
	// lfs f24,1224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1224);
	ctx.f24.f64 = double(temp.f32);
	// fmr f26,f19
	ctx.f26.f64 = ctx.f19.f64;
	// stfs f7,5160(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5160, temp.u32);
	// stfs f6,3768(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3768, temp.u32);
	// stfs f2,5172(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5172, temp.u32);
	// fmr f2,f19
	ctx.f2.f64 = ctx.f19.f64;
	// lfs f7,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,4788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4788);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f0,f23,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// stfs f25,5128(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5128, temp.u32);
	// fnmsubs f11,f3,f29,f11
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// lfs f3,3868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3868);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f12,f27,f15,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f15.f64 - ctx.f12.f64)));
	// stfs f22,4548(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4548, temp.u32);
	// fmadds f13,f17,f16,f13
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f16.f64 + ctx.f13.f64));
	// stfs f17,3920(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3920, temp.u32);
	// fmr f9,f1
	ctx.f9.f64 = ctx.f1.f64;
	// stfs f27,5112(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5112, temp.u32);
	// fmr f21,f1
	ctx.f21.f64 = ctx.f1.f64;
	// stfs f21,3780(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3780, temp.u32);
	// lfs f21,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f21.f64 = double(temp.f32);
	// fmr f4,f5
	ctx.f4.f64 = ctx.f5.f64;
	// stfs f28,4372(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4372, temp.u32);
	// stfs f31,5152(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5152, temp.u32);
	// stfs f24,5164(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5164, temp.u32);
	// fmadds f0,f18,f14,f0
	ctx.f0.f64 = double(float(ctx.f18.f64 * ctx.f14.f64 + ctx.f0.f64));
	// fmadds f12,f31,f26,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 + ctx.f12.f64));
	// fnmsubs f13,f10,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// lfs f10,4356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4356);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f0,f30,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f30.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// fnmsubs f12,f24,f21,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f21.f64 - ctx.f12.f64)));
	// fnmsubs f13,f10,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f10,5072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5072);
	ctx.f10.f64 = double(temp.f32);
	// fmr f9,f19
	ctx.f9.f64 = ctx.f19.f64;
	// fnmsubs f13,f10,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f9,1216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1216);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f9,f8,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f12.f64));
	// lfs f8,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f7,f8,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f8.f64 - ctx.f13.f64)));
	// lfs f24,4528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4528);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f12,f6,f5,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f5.f64 + ctx.f12.f64));
	// lfs f27,-19288(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19288);
	ctx.f27.f64 = double(temp.f32);
	// stfs f27,540(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 540, temp.u32);
	// fmr f28,f21
	ctx.f28.f64 = ctx.f21.f64;
	// lfs f1,1208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1208);
	ctx.f1.f64 = double(temp.f32);
	// fmr f31,f4
	ctx.f31.f64 = ctx.f4.f64;
	// lfs f8,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f8.f64 = double(temp.f32);
	// fmr f25,f4
	ctx.f25.f64 = ctx.f4.f64;
	// lfs f23,4520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4520);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,4504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4504);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,1720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1720);
	ctx.f19.f64 = double(temp.f32);
	// stfs f30,3868(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3868, temp.u32);
	// fmuls f30,f19,f21
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f21.f64));
	// lfs f27,2332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2332);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f13,f24,f4,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 + ctx.f13.f64));
	// lfs f24,2340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2340);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f3,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f7,3532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3532);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f23,f24,f29,f23
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f29.f64 - ctx.f23.f64));
	// lfs f5,1548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1548);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,1380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1380);
	ctx.f2.f64 = double(temp.f32);
	// stfs f30,2332(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2332, temp.u32);
	// fmuls f2,f2,f21
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f30,1372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1372);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f21,f30
	ctx.f30.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// lfs f20,2684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2684);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,4376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4376);
	ctx.f15.f64 = double(temp.f32);
	// stfs f0,4788(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4788, temp.u32);
	// fmuls f14,f20,f15
	ctx.f14.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// stfs f30,1720(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1720, temp.u32);
	// fmadds f13,f1,f8,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f8.f64 + ctx.f13.f64));
	// lfs f0,1736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1736);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f12,f7,f31,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f31.f64 + ctx.f12.f64));
	// lfs f30,3976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3976);
	ctx.f30.f64 = double(temp.f32);
	// lfs f1,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f1.f64 = double(temp.f32);
	// stfs f0,3532(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3532, temp.u32);
	// fmuls f0,f0,f21
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f21.f64));
	// lfs f16,1628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1628);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f1,f30,f1,f23
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64 + ctx.f23.f64));
	// lfs f8,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f8.f64 = double(temp.f32);
	// stfs f14,1736(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1736, temp.u32);
	// fmuls f8,f8,f16
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f16.f64));
	// stfs f21,2340(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2340, temp.u32);
	// stfs f9,2316(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2316, temp.u32);
	// lfs f9,3760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3760);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f5,f28,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f28.f64 + ctx.f13.f64));
	// lfs f14,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,4436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4436);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f12,f27,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// lfs f30,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f21,f21,f14
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f14.f64));
	// lfs f26,3020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3020);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f11,f9,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f9.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// stfs f8,3020(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3020, temp.u32);
	// lfs f18,4496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4496);
	ctx.f18.f64 = double(temp.f32);
	// lfs f22,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f22.f64 = double(temp.f32);
	// lfs f8,3996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3996);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f17,f18,f22
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f22.f64));
	// lfs f4,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,3688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3688);
	ctx.f31.f64 = double(temp.f32);
	// lfs f14,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f13,f26,f4,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f4.f64 + ctx.f13.f64));
	// lfs f9,2324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2324);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f2,f14
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// stfs f10,4528(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4528, temp.u32);
	// fmuls f10,f18,f31
	ctx.f10.f64 = double(float(ctx.f18.f64 * ctx.f31.f64));
	// stfs f6,4496(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4496, temp.u32);
	// fmuls f6,f9,f8
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f8.f64));
	// stfs f3,4436(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4436, temp.u32);
	// stfs f7,4520(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4520, temp.u32);
	// stfs f5,4504(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4504, temp.u32);
	// stfs f27,3760(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3760, temp.u32);
	// stfs f26,3780(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3780, temp.u32);
	// stfs f24,4356(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4356, temp.u32);
	// stfs f29,1576(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1576, temp.u32);
	// stfs f22,1132(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1132, temp.u32);
	// stfs f15,1080(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1080, temp.u32);
	// stfs f20,1228(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1228, temp.u32);
	// stfs f19,3996(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3996, temp.u32);
	// fadds f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f20,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f20.f64 = double(temp.f32);
	// stfs f13,5320(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5320, temp.u32);
	// fmadds f11,f21,f20,f11
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f20.f64 + ctx.f11.f64));
	// lfs f13,1200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1200);
	ctx.f13.f64 = double(temp.f32);
	// lfs f29,3028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3028);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,2356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2356);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f29,f13
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f13.f64));
	// lfs f19,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f19.f64 = double(temp.f32);
	// lfs f4,4996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4996);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f20,f27,f19
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f19.f64));
	// lfs f23,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f23,f23,f4
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// lfs f21,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f21.f64 = double(temp.f32);
	// lfs f25,3620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3620);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f0,f21,f1
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f21.f64 - ctx.f1.f64)));
	// lfs f19,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f22,f25,f13
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// fmadds f2,f17,f19,f2
	ctx.f2.f64 = double(float(ctx.f17.f64 * ctx.f19.f64 + ctx.f2.f64));
	// stfs f16,5072(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5072, temp.u32);
	// lfs f21,2364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2364);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f20,f21,f15,f20
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f15.f64 - ctx.f20.f64));
	// lfs f15,1720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1720);
	ctx.f15.f64 = double(temp.f32);
	// lfs f12,3356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3356);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f26,f23,f16,f26
	ctx.f26.f64 = double(float(ctx.f23.f64 * ctx.f16.f64 + ctx.f26.f64));
	// lfs f16,1736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1736);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f11,f16,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f16.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// fmadds f0,f15,f14,f0
	ctx.f0.f64 = double(float(ctx.f15.f64 * ctx.f14.f64 + ctx.f0.f64));
	// lfs f17,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,1668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1668);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f15,1356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1356);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f10,f10,f17,f2
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f17.f64 - ctx.f2.f64)));
	// fadds f22,f16,f15
	ctx.f22.f64 = double(float(ctx.f16.f64 + ctx.f15.f64));
	// lfs f3,3004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3004);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,2676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2676);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f28,f18,f12
	ctx.f28.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// lfs f15,2340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2340);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f24,f3,f12
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f12.f64));
	// lfs f7,2308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2308);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f14,f2,f15
	ctx.f14.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f17,3036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3036);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f5,f12,f7
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// lfs f16,5084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5084);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f30,f3,f31
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f24,3028(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3028, temp.u32);
	// fmuls f24,f17,f16
	ctx.f24.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// stfs f7,1720(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1720, temp.u32);
	// stfs f14,3620(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3620, temp.u32);
	// stfs f28,2364(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2364, temp.u32);
	// lfs f7,2372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2372);
	ctx.f7.f64 = double(temp.f32);
	// lfs f14,1976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1976);
	ctx.f14.f64 = double(temp.f32);
	// lfs f28,3188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3188);
	ctx.f28.f64 = double(temp.f32);
	// stfs f31,2372(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2372, temp.u32);
	// fmuls f31,f14,f28
	ctx.f31.f64 = double(float(ctx.f14.f64 * ctx.f28.f64));
	// stfs f24,3036(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3036, temp.u32);
	// lfs f24,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f24.f64 = double(temp.f32);
	// lfs f1,1364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1364);
	ctx.f1.f64 = double(temp.f32);
	// lfs f19,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f19.f64 = double(temp.f32);
	// stfs f31,1736(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1736, temp.u32);
	// fmr f31,f24
	ctx.f31.f64 = ctx.f24.f64;
	// fmuls f19,f19,f1
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// stfs f30,1976(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1976, temp.u32);
	// lfs f30,3020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3020);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f30,f24
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// lfs f24,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f11,f6,f24,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f24.f64 + ctx.f11.f64));
	// lfs f6,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f6.f64 = double(temp.f32);
	// stfs f8,3020(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3020, temp.u32);
	// fmsubs f8,f20,f16,f23
	ctx.f8.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 - ctx.f23.f64));
	// lfs f24,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f19,f31
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// stfs f31,2356(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2356, temp.u32);
	// lfs f19,1532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1532);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,2332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2332);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f26,f7,f19,f26
	ctx.f26.f64 = double(float(ctx.f7.f64 * ctx.f19.f64 + ctx.f26.f64));
	// fnmsubs f0,f31,f6,f0
	ctx.f0.f64 = double(float(-(ctx.f31.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// stfs f13,180(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// fmadds f10,f5,f24,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f24.f64 + ctx.f10.f64));
	// lfs f13,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f30,f13,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f6,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f6.f64 = double(temp.f32);
	// fmr f13,f24
	ctx.f13.f64 = ctx.f24.f64;
	// stfs f29,1592(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1592, temp.u32);
	// fmadds f8,f26,f6,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f6.f64 + ctx.f8.f64));
	// lfs f6,1976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1976);
	ctx.f6.f64 = double(temp.f32);
	// fmr f29,f24
	ctx.f29.f64 = ctx.f24.f64;
	// stfs f4,792(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 792, temp.u32);
	// lfs f4,4348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4348);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,4724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4724);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f30,f16,f4
	ctx.f30.f64 = double(float(ctx.f16.f64 * ctx.f4.f64));
	// lfs f31,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f31.f64 = double(temp.f32);
	// lfs f23,3036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3036);
	ctx.f23.f64 = double(temp.f32);
	// stfs f27,5084(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5084, temp.u32);
	// stfs f9,1064(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1064, temp.u32);
	// fmuls f9,f22,f15
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// stfs f12,1544(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1544, temp.u32);
	// fnmsubs f13,f6,f13,f10
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f13.f64 - ctx.f10.f64)));
	// lfs f10,2380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2380);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f11,f5,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f27,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f8,f23,f31,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f31.f64 + ctx.f8.f64));
	// lfs f23,2364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2364);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f26.f64 = double(temp.f32);
	// lfs f12,3532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3532);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f30,f30,f27
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// stfs f7,1600(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1600, temp.u32);
	// fmuls f7,f12,f19
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// lfs f27,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f27.f64 = double(temp.f32);
	// stfs f25,1608(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1608, temp.u32);
	// lfs f6,3044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3044);
	ctx.f6.f64 = double(temp.f32);
	// lfs f25,3868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3868);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f13,f23,f29,f13
	ctx.f13.f64 = double(float(-(ctx.f23.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// lfs f23,3620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3620);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f11,f23,f26,f11
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f26.f64 - ctx.f11.f64)));
	// lfs f23,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// lfs f31,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f0,f25,f6,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f6.f64 + ctx.f0.f64));
	// lfs f24,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f24.f64 = double(temp.f32);
	// stfs f21,4996(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4996, temp.u32);
	// lfs f21,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f8,f7,f24,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f24.f64 + ctx.f8.f64));
	// lfs f7,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f7.f64 = double(temp.f32);
	// stfs f2,3004(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3004, temp.u32);
	// stfs f28,3172(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3172, temp.u32);
	// stfs f3,1112(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1112, temp.u32);
	// fmadds f13,f9,f31,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f31,2356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2356);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f11,f31,f21,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f21.f64 - ctx.f11.f64)));
	// stfs f15,1056(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1056, temp.u32);
	// lfs f3,4324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4324);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f27,f7,f24,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f24.f64 + ctx.f27.f64));
	// lfs f2,2372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2372);
	ctx.f2.f64 = double(temp.f32);
	// lfs f28,2388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2388);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f0,f5,f28,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f15,3728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3728);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f22,f26,f16
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f31,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f31.f64 = double(temp.f32);
	// lfs f21,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f31,f15,f31
	ctx.f31.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// stfs f1,2340(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2340, temp.u32);
	// fmuls f1,f3,f2
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// stfs f18,688(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 688, temp.u32);
	// fmr f24,f21
	ctx.f24.f64 = ctx.f21.f64;
	// stfs f17,2716(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2716, temp.u32);
	// stfs f14,1584(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1584, temp.u32);
	// stfs f19,2380(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2380, temp.u32);
	// stfs f4,3044(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3044, temp.u32);
	// lfs f29,2668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2668);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,3520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3520);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f17.f64 = double(temp.f32);
	// lfs f9,2980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2980);
	ctx.f9.f64 = double(temp.f32);
	// lfs f14,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,1348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1348);
	ctx.f19.f64 = double(temp.f32);
	// lfs f4,4340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4340);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f0,f20,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f20.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// stfs f12,2308(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2308, temp.u32);
	// lfs f12,3028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3028);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f8,f30,f18,f8
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f18.f64 - ctx.f8.f64)));
	// fmadds f13,f12,f17,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f17.f64 + ctx.f13.f64));
	// stfs f31,2388(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2388, temp.u32);
	// fmadds f12,f9,f29,f11
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f11,1736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1736);
	ctx.f11.f64 = double(temp.f32);
	// stfs f10,4376(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4376, temp.u32);
	// fmuls f31,f19,f16
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// lfs f10,4788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4788);
	ctx.f10.f64 = double(temp.f32);
	// stfs f20,1976(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1976, temp.u32);
	// lfs f20,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f20.f64 = double(temp.f32);
	// stfs f7,2708(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2708, temp.u32);
	// stfs f9,1736(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1736, temp.u32);
	// fnmsubs f0,f11,f21,f0
	ctx.f0.f64 = double(float(-(ctx.f11.f64 * ctx.f21.f64 - ctx.f0.f64)));
	// lfs f7,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f11,f22,f14,f8
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f14.f64 - ctx.f8.f64)));
	// lfs f9,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f1,f24,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f24.f64 + ctx.f13.f64));
	// lfs f8,3044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3044);
	ctx.f8.f64 = double(temp.f32);
	// fadds f12,f10,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 + ctx.f12.f64));
	// lfs f10,1720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1720);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,1072(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1072, temp.u32);
	// fmuls f9,f9,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// stfs f6,2324(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2324, temp.u32);
	// stfs f28,3356(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3356, temp.u32);
	// lfs f6,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f6.f64 = double(temp.f32);
	// stfs f4,3976(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3976, temp.u32);
	// fmuls f6,f6,f16
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f16.f64));
	// stfs f5,252(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 252, temp.u32);
	// fnmsubs f0,f4,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f4.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f4,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f27,f16,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f16.f64 + ctx.f11.f64));
	// lfs f10,2388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2388);
	ctx.f10.f64 = double(temp.f32);
	// fadds f5,f4,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 + ctx.f5.f64));
	// fmadds f13,f10,f20,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f20.f64 + ctx.f13.f64));
	// lfs f10,4556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4556);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f7.f64 = double(temp.f32);
	// stfs f3,656(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 656, temp.u32);
	// fmuls f7,f7,f10
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f10.f64));
	// lfs f3,788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 788);
	ctx.f3.f64 = double(temp.f32);
	// lfs f4,764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	ctx.f4.f64 = double(temp.f32);
	// fadds f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f3.f64));
	// lfs f3,1728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1728);
	ctx.f3.f64 = double(temp.f32);
	// stfs f2,1196(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1196, temp.u32);
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// lfs f12,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f12,f8,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f12.f64));
	// stfs f29,664(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 664, temp.u32);
	// lfs f2,4572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4572);
	ctx.f2.f64 = double(temp.f32);
	// lfs f29,4052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4052);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f2,f3
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// lfs f24,2380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2380);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f27,f29,f3
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// stfs f26,3204(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3204, temp.u32);
	// stfs f23,3520(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3520, temp.u32);
	// stfs f25,412(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 412, temp.u32);
	// lfs f25,4060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4060);
	ctx.f25.f64 = double(temp.f32);
	// stfs f15,680(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 680, temp.u32);
	// fadds f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f0,1944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1944);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 684);
	ctx.f13.f64 = double(temp.f32);
	// fadds f28,f13,f0
	ctx.f28.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f13,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f13.f64 = double(temp.f32);
	// fmr f0,f18
	ctx.f0.f64 = ctx.f18.f64;
	// stfs f19,4324(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4324, temp.u32);
	// fmsubs f12,f12,f0,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 - ctx.f9.f64));
	// lfs f0,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f9,f6,f13
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f13.f64));
	// lfs f13,748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 748);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f7,f7,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// lfs f0,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f0.f64 = double(temp.f32);
	// fadds f6,f5,f13
	ctx.f6.f64 = double(float(ctx.f5.f64 + ctx.f13.f64));
	// lfs f13,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f4,f3
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f4,4836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4836);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f0,f3
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f3.f64));
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f4,f24
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// fmsubs f12,f12,f16,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f16.f64 - ctx.f9.f64));
	// lfs f9,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f7,f28,f0,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f7.f64));
	// stfs f4,308(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 308, temp.u32);
	// fmuls f9,f9,f16
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f16.f64));
	// lfs f19,3052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3052);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f0,f5,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f15,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f6,f6,f3
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// lfs f4,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f4.f64 = double(temp.f32);
	// lfs f28,1620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1620);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f27,f15
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f15.f64));
	// stfs f23,3052(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3052, temp.u32);
	// fmuls f28,f28,f16
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// fmuls f21,f25,f13
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// stfs f1,5324(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5324, temp.u32);
	// lfs f1,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f1.f64 = double(temp.f32);
	// lfs f17,1340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1340);
	ctx.f17.f64 = double(temp.f32);
	// lfs f14,1524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1524);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f17,f17,f3
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// fmuls f7,f7,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// lfs f22,3540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3540);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f12,f9,f4,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f4,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f4.f64 = double(temp.f32);
	// lfs f9,1704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1704);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f14,f14,f3
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// fmsubs f6,f6,f4,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64 - ctx.f0.f64));
	// stfs f29,3540(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3540, temp.u32);
	// fmuls f9,f9,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lfs f4,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f13,f22,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f13.f64));
	// lfs f20,2396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2396);
	ctx.f20.f64 = double(temp.f32);
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// lfs f15,1332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1332);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f5,f30,f29
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f25,2372(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2372, temp.u32);
	// fmuls f15,f15,f16
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f21,f3
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// fmsubs f7,f27,f4,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f4.f64 - ctx.f7.f64));
	// lfs f25,1192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1192);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f28,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// stfs f8,4348(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4348, temp.u32);
	// fmuls f8,f26,f20
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f20.f64));
	// lfs f26,1324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1324);
	ctx.f26.f64 = double(temp.f32);
	// stfs f24,204(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// fnmsubs f11,f31,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// fmadds f9,f25,f0,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f0.f64 + ctx.f9.f64));
	// stfs f15,2396(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2396, temp.u32);
	// lfs f23,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f26,f0
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f0.f64));
	// lfs f24,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f13,f13,f3
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// lfs f4,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f6,f14,f24,f6
	ctx.f6.f64 = double(float(-(ctx.f14.f64 * ctx.f24.f64 - ctx.f6.f64)));
	// lfs f25,1908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1908);
	ctx.f25.f64 = double(temp.f32);
	// lfs f15,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f15.f64 = double(temp.f32);
	// lfs f27,1316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1316);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f17,f0,f7
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// lfs f21,3052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3052);
	ctx.f21.f64 = double(temp.f32);
	// fadds f27,f25,f27
	ctx.f27.f64 = double(float(ctx.f25.f64 + ctx.f27.f64));
	// lfs f18,3676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3676);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f12,f21,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f1,4092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4092);
	ctx.f1.f64 = double(temp.f32);
	// lfs f28,808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	ctx.f28.f64 = double(temp.f32);
	// lfs f31,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f31.f64 = double(temp.f32);
	// stfs f2,2364(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2364, temp.u32);
	// fmuls f2,f15,f23
	ctx.f2.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// stfs f10,4556(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4556, temp.u32);
	// fmuls f10,f19,f18
	ctx.f10.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// stfs f22,4060(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4060, temp.u32);
	// fmuls f31,f31,f1
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f1.f64));
	// lfs f25,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f23,f28,f16
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f24,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f22.f64 = double(temp.f32);
	// lfs f7,2404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2404);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,1744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1744);
	ctx.f4.f64 = double(temp.f32);
	// stfs f20,4836(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4836, temp.u32);
	// stfs f19,3196(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3196, temp.u32);
	// stfs f18,220(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fmadds f0,f8,f22,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f22.f64 + ctx.f0.f64));
	// lfs f21,2096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2096);
	ctx.f21.f64 = double(temp.f32);
	// fmr f8,f22
	ctx.f8.f64 = ctx.f22.f64;
	// lfs f22,2396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2396);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f22,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f14,1308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1308);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f6,f5,f25,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f25.f64 - ctx.f6.f64)));
	// lfs f25,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f17,f21,f16
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// stfs f17,2404(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2404, temp.u32);
	// lfs f17,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f25,f30,f25
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// fmadds f2,f14,f17,f2
	ctx.f2.f64 = double(float(ctx.f14.f64 * ctx.f17.f64 + ctx.f2.f64));
	// lfs f14,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,1612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1612);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f7,f16
	ctx.f20.f64 = double(float(ctx.f7.f64 * ctx.f16.f64));
	// lfs f24,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f18,f4,f16
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// stfs f7,3676(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3676, temp.u32);
	// fmuls f27,f27,f3
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// lfs f5,1752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1752);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f23,f24,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f24.f64 + ctx.f11.f64));
	// fmr f22,f8
	ctx.f22.f64 = ctx.f8.f64;
	// lfs f17,1812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1812);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f8,f31,f8,f26
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f8.f64 + ctx.f26.f64));
	// lfs f31,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f12,f10,f14,f12
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f14.f64 - ctx.f12.f64)));
	// stfs f8,1744(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1744, temp.u32);
	// fmr f10,f31
	ctx.f10.f64 = ctx.f31.f64;
	// lfs f8,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f8.f64 = double(temp.f32);
	// fmr f7,f8
	ctx.f7.f64 = ctx.f8.f64;
	// stfs f18,1752(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1752, temp.u32);
	// fnmsubs f0,f29,f31,f0
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f0.f64)));
	// lfs f18,1508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1508);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f26,f19,f16
	ctx.f26.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// stfs f28,4340(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4340, temp.u32);
	// fmuls f17,f17,f3
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// stfs f17,2096(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2096, temp.u32);
	// fmuls f18,f18,f3
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// stfs f5,2692(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2692, temp.u32);
	// fmuls f17,f5,f16
	ctx.f17.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// lfs f28,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f13,f13,f22,f6
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f22.f64 + ctx.f6.f64));
	// stfs f30,2356(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2356, temp.u32);
	// fmr f5,f22
	ctx.f5.f64 = ctx.f22.f64;
	// lfs f30,2420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2420);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f2,f16,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f16.f64 + ctx.f12.f64));
	// stfs f1,3052(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3052, temp.u32);
	// stfs f4,3580(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3580, temp.u32);
	// lfs f4,4468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4468);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f0,f27,f7,f0
	ctx.f0.f64 = double(float(ctx.f27.f64 * ctx.f7.f64 + ctx.f0.f64));
	// lfs f2,2636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2636);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f26,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f26,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f26.f64 = double(temp.f32);
	// fmr f8,f22
	ctx.f8.f64 = ctx.f22.f64;
	// lfs f1,3060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3060);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f6,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f6.f64 = double(temp.f32);
	// lfs f24,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f4,f2
	ctx.f31.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// fnmsubs f13,f25,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f25,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f25.f64 = double(temp.f32);
	// fmr f10,f22
	ctx.f10.f64 = ctx.f22.f64;
	// lfs f23,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f30,f25
	ctx.f26.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f25,3156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3156);
	ctx.f25.f64 = double(temp.f32);
	// stfs f15,3728(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3728, temp.u32);
	// lfs f22,1752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1752);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f0,f18,f7,f0
	ctx.f0.f64 = double(float(-(ctx.f18.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// lfs f7,2628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2628);
	ctx.f7.f64 = double(temp.f32);
	// stfs f21,4724(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4724, temp.u32);
	// stfs f19,2980(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2980, temp.u32);
	// fmadds f13,f9,f3,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f3.f64 + ctx.f13.f64));
	// lfs f9,3672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3672);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f12,f20,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f20.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f10,3788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3788);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f29,f10,f1
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// fadds f27,f7,f9
	ctx.f27.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// fmadds f12,f22,f8,f12
	ctx.f12.f64 = double(float(ctx.f22.f64 * ctx.f8.f64 + ctx.f12.f64));
	// lfs f8,1052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	ctx.f8.f64 = double(temp.f32);
	// lfs f22,2096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2096);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f13,f22,f6,f13
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f6.f64 - ctx.f13.f64)));
	// lfs f22,1744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1744);
	ctx.f22.f64 = double(temp.f32);
	// lfs f14,1300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1300);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f0,f22,f3,f0
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f22,1984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1984);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f14,f25,f14
	ctx.f14.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// lfs f21,1760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1760);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f15,f4,f22
	ctx.f15.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// stfs f14,2420(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2420, temp.u32);
	// fmuls f23,f25,f23
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// lfs f14,2404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2404);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f8,f25,f8
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// stfs f10,1760(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1760, temp.u32);
	// fmuls f10,f10,f21
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f21.f64));
	// stfs f15,1984(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1984, temp.u32);
	// fnmsubs f12,f14,f5,f12
	ctx.f12.f64 = double(float(-(ctx.f14.f64 * ctx.f5.f64 - ctx.f12.f64)));
	// lfs f15,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f15.f64 = double(temp.f32);
	// stfs f10,3060(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3060, temp.u32);
	// fmuls f31,f31,f15
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f15.f64));
	// lfs f10,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f10.f64 = double(temp.f32);
	// lfs f20,3444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3444);
	ctx.f20.f64 = double(temp.f32);
	// fmr f15,f10
	ctx.f15.f64 = ctx.f10.f64;
	// fmuls f10,f29,f10
	ctx.f10.f64 = double(float(ctx.f29.f64 * ctx.f10.f64));
	// stfs f11,2404(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2404, temp.u32);
	// fmuls f11,f27,f20
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// lfs f18,1740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1740);
	ctx.f18.f64 = double(temp.f32);
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// stfs f0,5332(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5332, temp.u32);
	// fmuls f18,f25,f18
	ctx.f18.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// lfs f6,1748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1748);
	ctx.f6.f64 = double(temp.f32);
	// lfs f19,5076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5076);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f6,f25,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// fnmsubs f0,f17,f24,f12
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// stfs f0,4092(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4092, temp.u32);
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// lfs f14,2548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2548);
	ctx.f14.f64 = double(temp.f32);
	// stfs f3,2380(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2380, temp.u32);
	// fmuls f3,f14,f19
	ctx.f3.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// fmsubs f29,f28,f15,f26
	ctx.f29.f64 = double(float(ctx.f28.f64 * ctx.f15.f64 - ctx.f26.f64));
	// stfs f2,4572(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4572, temp.u32);
	// fmsubs f12,f31,f0,f10
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 - ctx.f10.f64));
	// lfs f10,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// stfs f9,2396(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2396, temp.u32);
	// stfs f23,1752(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1752, temp.u32);
	// stfs f19,3868(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3868, temp.u32);
	// stfs f6,2096(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2096, temp.u32);
	// lfs f5,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f5,f5,f7
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// lfs f11,3664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3664);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f25,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// lfs f9,2428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2428);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,3068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3068);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,1976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1976);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f18,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f31,2436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2436);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f0,f29,f4,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f0.f64));
	// lfs f28,3388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3388);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f25,f31
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f31.f64));
	// lfs f26,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f24,f25,f28
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f23,2804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2804);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f4,f26
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// lfs f19,3540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3540);
	ctx.f19.f64 = double(temp.f32);
	// stfs f1,5076(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5076, temp.u32);
	// fmuls f1,f14,f2
	ctx.f1.f64 = double(float(ctx.f14.f64 * ctx.f2.f64));
	// stfs f22,3688(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3688, temp.u32);
	// fmuls f17,f25,f19
	ctx.f17.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// stfs f30,2332(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2332, temp.u32);
	// fmuls f30,f4,f6
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// stfs f21,2684(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2684, temp.u32);
	// fmuls f21,f9,f23
	ctx.f21.f64 = double(float(ctx.f9.f64 * ctx.f23.f64));
	// stfs f8,1744(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1744, temp.u32);
	// fmuls f8,f25,f11
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f11.f64));
	// stfs f20,324(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 324, temp.u32);
	// lfs f22,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// lfs f20,2444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2444);
	ctx.f20.f64 = double(temp.f32);
	// lfs f10,3076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3076);
	ctx.f10.f64 = double(temp.f32);
	// lfs f29,5092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5092);
	ctx.f29.f64 = double(temp.f32);
	// stfs f16,3788(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3788, temp.u32);
	// stfs f23,4052(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4052, temp.u32);
	// fmuls f16,f14,f20
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f20.f64));
	// lfs f23,1760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1760);
	ctx.f23.f64 = double(temp.f32);
	// stfs f16,3068(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3068, temp.u32);
	// fmuls f16,f23,f10
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f10.f64));
	// stfs f16,2444(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2444, temp.u32);
	// fmuls f16,f4,f29
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// lfs f18,2452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2452);
	ctx.f18.f64 = double(temp.f32);
	// stfs f13,3388(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3388, temp.u32);
	// stfs f17,2452(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2452, temp.u32);
	// fmuls f17,f23,f18
	ctx.f17.f64 = double(float(ctx.f23.f64 * ctx.f18.f64));
	// stfs f16,2428(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2428, temp.u32);
	// lfs f13,3548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3548);
	ctx.f13.f64 = double(temp.f32);
	// lfs f16,2460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2460);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,4820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4820);
	ctx.f15.f64 = double(temp.f32);
	// stfs f23,2460(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2460, temp.u32);
	// fmuls f23,f23,f13
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f13.f64));
	// stfs f17,2436(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2436, temp.u32);
	// lfs f17,3084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3084);
	ctx.f17.f64 = double(temp.f32);
	// stfs f1,3548(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3548, temp.u32);
	// fmuls f1,f4,f15
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f15.f64));
	// stfs f23,3084(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3084, temp.u32);
	// lfs f23,736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 736);
	ctx.f23.f64 = double(temp.f32);
	// stfs f1,3664(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3664, temp.u32);
	// fmuls f23,f25,f23
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// lfs f1,2184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2184);
	ctx.f1.f64 = double(temp.f32);
	// stfs f25,3540(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3540, temp.u32);
	// stfs f3,2184(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2184, temp.u32);
	// fmuls f3,f4,f16
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// lfs f25,1984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1984);
	ctx.f25.f64 = double(temp.f32);
	// stfs f3,3076(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3076, temp.u32);
	// fmadds f12,f25,f22,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f22.f64 + ctx.f12.f64));
	// lfs f3,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// lfs f25,2468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2468);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f5,f5,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// stfs f25,1984(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1984, temp.u32);
	// fmuls f25,f25,f7
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// lfs f3,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f3.f64 = double(temp.f32);
	// stfs f25,2468(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2468, temp.u32);
	// fmr f25,f3
	ctx.f25.f64 = ctx.f3.f64;
	// fmuls f8,f8,f3
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// lfs f22,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f22.f64 = double(temp.f32);
	// lfs f3,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f26,f26,f22
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f22.f64));
	// fmuls f3,f30,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// lfs f22,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f22.f64 = double(temp.f32);
	// lfs f30,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f30.f64 = double(temp.f32);
	// stfs f13,2804(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2804, temp.u32);
	// fmr f13,f30
	ctx.f13.f64 = ctx.f30.f64;
	// stfs f11,3036(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3036, temp.u32);
	// stfs f10,5092(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5092, temp.u32);
	// fmuls f10,f4,f17
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f17.f64));
	// stfs f29,3044(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3044, temp.u32);
	// fmuls f29,f14,f1
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f1.f64));
	// stfs f20,3188(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3188, temp.u32);
	// fmuls f25,f21,f25
	ctx.f25.f64 = double(float(ctx.f21.f64 * ctx.f25.f64));
	// stfs f25,1760(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1760, temp.u32);
	// lfs f25,2420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2420);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f0,f25,f22,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f22.f64 + ctx.f0.f64));
	// lfs f25,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f25.f64 = double(temp.f32);
	// lfs f11,2184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2184);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f25,f24,f25
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// lfs f24,3060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3060);
	ctx.f24.f64 = double(temp.f32);
	// lfs f21,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f12,f24,f30,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f30.f64 - ctx.f12.f64)));
	// stfs f18,4820(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4820, temp.u32);
	// fmuls f27,f27,f21
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f21.f64));
	// stfs f7,2388(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2388, temp.u32);
	// stfs f15,3028(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3028, temp.u32);
	// stfs f9,2636(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2636, temp.u32);
	// stfs f6,2420(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2420, temp.u32);
	// stfs f31,3620(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3620, temp.u32);
	// fnmsubs f13,f11,f13,f0
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// lfs f11,1752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1752);
	ctx.f11.f64 = double(temp.f32);
	// stfs f28,1720(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1720, temp.u32);
	// fmr f0,f22
	ctx.f0.f64 = ctx.f22.f64;
	// stfs f2,3532(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3532, temp.u32);
	// stfs f16,3060(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3060, temp.u32);
	// stfs f17,1976(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1976, temp.u32);
	// stfs f19,2676(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2676, temp.u32);
	// stfs f1,4788(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4788, temp.u32);
	// fnmsubs f12,f11,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f11,2096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2096);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f13,f11,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f6,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f6.f64 = double(temp.f32);
	// fmr f31,f30
	ctx.f31.f64 = ctx.f30.f64;
	// lfs f9,1744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1744);
	ctx.f9.f64 = double(temp.f32);
	// fmr f1,f6
	ctx.f1.f64 = ctx.f6.f64;
	// lfs f30,3084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3084);
	ctx.f30.f64 = double(temp.f32);
	// fmr f7,f22
	ctx.f7.f64 = ctx.f22.f64;
	// lfs f18,3548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3548);
	ctx.f18.f64 = double(temp.f32);
	// lfs f19,3092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3092);
	ctx.f19.f64 = double(temp.f32);
	// fmr f11,f6
	ctx.f11.f64 = ctx.f6.f64;
	// lfs f16,2492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2492);
	ctx.f16.f64 = double(temp.f32);
	// lfs f28,3396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3396);
	ctx.f28.f64 = double(temp.f32);
	// stfs f4,3396(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3396, temp.u32);
	// lfs f24,2460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2460);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f9,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f9,2404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2404);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f23,f0,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f23,4292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4292);
	ctx.f23.f64 = double(temp.f32);
	// fmr f0,f31
	ctx.f0.f64 = ctx.f31.f64;
	// lfs f21,1660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1660);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f3,f30,f1,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f1.f64 + ctx.f3.f64));
	// lfs f30,4092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4092);
	ctx.f30.f64 = double(temp.f32);
	// fadds f9,f9,f30
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f30.f64));
	// lfs f30,2476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2476);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f20,f4,f30
	ctx.f20.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// lfs f4,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f22,f24,f28
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f28.f64));
	// stfs f22,2184(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2184, temp.u32);
	// stfs f20,3092(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3092, temp.u32);
	// lfs f20,2452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2452);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f20,f22
	ctx.f22.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// stfs f22,2476(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2476, temp.u32);
	// lfs f22,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f22.f64 = double(temp.f32);
	// fmr f20,f6
	ctx.f20.f64 = ctx.f6.f64;
	// fmadds f12,f5,f0,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f5,4060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4060);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f3,f18,f1,f3
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f1.f64 - ctx.f3.f64));
	// lfs f1,2484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2484);
	ctx.f1.f64 = double(temp.f32);
	// fadds f17,f1,f19
	ctx.f17.f64 = double(float(ctx.f1.f64 + ctx.f19.f64));
	// stfs f17,2492(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2492, temp.u32);
	// lfs f17,3388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3388);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f22,f21
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f21.f64));
	// stfs f14,2484(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2484, temp.u32);
	// lfs f14,3076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3076);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,3628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3628);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f15,f18,f23
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f23.f64));
	// stfs f5,3084(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3084, temp.u32);
	// fmuls f23,f16,f23
	ctx.f23.f64 = double(float(ctx.f16.f64 * ctx.f23.f64));
	// stfs f23,3628(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3628, temp.u32);
	// lfs f23,2444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2444);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f12,f17,f7,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f7,4364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4364);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f11,f14,f11,f3
	ctx.f11.f64 = double(float(ctx.f14.f64 * ctx.f11.f64 + ctx.f3.f64));
	// lfs f3,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f5,f3
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f3.f64));
	// lfs f5,3068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3068);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f13,f23,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f23,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f23.f64 = double(temp.f32);
	// fmr f17,f23
	ctx.f17.f64 = ctx.f23.f64;
	// lfs f14,4888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4888);
	ctx.f14.f64 = double(temp.f32);
	// stfs f9,5328(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5328, temp.u32);
	// fmuls f23,f23,f14
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f14.f64));
	// lfs f2,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f2.f64 = double(temp.f32);
	// stfs f28,1744(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1744, temp.u32);
	// stfs f30,2460(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2460, temp.u32);
	// stfs f21,2404(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2404, temp.u32);
	// fnmsubs f12,f5,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f5,2428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2428);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f29,f4,f11
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f11.f64));
	// lfs f4,2436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2436);
	ctx.f4.f64 = double(temp.f32);
	// stfs f1,4060(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4060, temp.u32);
	// fnmsubs f13,f4,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f19,4888(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4888, temp.u32);
	// stfs f18,3076(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3076, temp.u32);
	// fmuls f9,f17,f7
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// stfs f16,3388(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3388, temp.u32);
	// fnmsubs f12,f5,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// fadds f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// lfs f12,2468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2468);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f12,f0,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f12,3664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3664);
	ctx.f12.f64 = double(temp.f32);
	// lfs f5,4380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4380);
	ctx.f5.f64 = double(temp.f32);
	// lfs f28,2492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2492);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,1500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1500);
	ctx.f29.f64 = double(temp.f32);
	// lfs f21,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,3556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3556);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f21,f21,f29
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// lfs f1,2500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2500);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f30,f24,f1
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f1.f64));
	// stfs f18,3556(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3556, temp.u32);
	// lfs f16,3628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3628);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f13,f12,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f12,1760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1760);
	ctx.f12.f64 = double(temp.f32);
	// fmr f0,f2
	ctx.f0.f64 = ctx.f2.f64;
	// fmadds f13,f26,f6,f13
	ctx.f13.f64 = double(float(ctx.f26.f64 * ctx.f6.f64 + ctx.f13.f64));
	// lfs f6,4732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4732);
	ctx.f6.f64 = double(temp.f32);
	// lfs f26,3108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3108);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f19,f26,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f19.f64));
	// fnmsubs f13,f8,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f8,1292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1292);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f13,f12,f31,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f12,2660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2660);
	ctx.f12.f64 = double(temp.f32);
	// lfs f31,3404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3404);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f28,f28,f31,f15
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f31.f64 + ctx.f15.f64));
	// lfs f15,1284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1284);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f13,f27,f0,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f27,2508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2508);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f0,f25,f0,f13
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f25,2516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2516);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f13,f10,f20,f0
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f20,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f20.f64 = double(temp.f32);
	// fmr f0,f17
	ctx.f0.f64 = ctx.f17.f64;
	// lfs f17,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f10,f3,f14
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// lfs f3,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f3.f64 = double(temp.f32);
	// stfs f13,2516(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2516, temp.u32);
	// fmuls f20,f27,f20
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// lfs f13,1604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1604);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f4,f0,f12
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f12.f64));
	// lfs f0,3100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3100);
	ctx.f0.f64 = double(temp.f32);
	// fadds f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f0.f64));
	// fmr f0,f3
	ctx.f0.f64 = ctx.f3.f64;
	// fmuls f3,f8,f3
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// fmuls f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// fmadds f2,f6,f0,f22
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f22.f64));
	// lfs f22,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f25,f22
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f22.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f0,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f0,f5,f0,f3
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f3.f64));
	// lfs f5,2524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2524);
	ctx.f5.f64 = double(temp.f32);
	// stfs f10,2524(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2524, temp.u32);
	// lfs f10,3092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3092);
	ctx.f10.f64 = double(temp.f32);
	// lfs f3,3052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3052);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f16,f3,f23,f16
	ctx.f16.f64 = double(float(ctx.f3.f64 * ctx.f23.f64 + ctx.f16.f64));
	// stfs f16,3108(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3108, temp.u32);
	// lfs f16,2484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2484);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f2,f18,f17,f2
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f17.f64 + ctx.f2.f64));
	// lfs f18,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// stfs f22,3100(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3100, temp.u32);
	// lfs f22,3656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3656);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f11,f10,f18,f11
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f18.f64 - ctx.f11.f64)));
	// stfs f22,3404(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3404, temp.u32);
	// fmuls f22,f5,f22
	ctx.f22.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// stfs f22,2500(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2500, temp.u32);
	// lfs f22,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f22.f64 = double(temp.f32);
	// lfs f10,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// lfs f17,3448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3448);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f10,f10,f15,f21
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f15.f64 + ctx.f21.f64));
	// lfs f18,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f30,f16,f17,f30
	ctx.f30.f64 = double(float(ctx.f16.f64 * ctx.f17.f64 + ctx.f30.f64));
	// lfs f21,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f21.f64 = double(temp.f32);
	// stfs f22,2508(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2508, temp.u32);
	// fmuls f2,f2,f14
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// fmuls f20,f20,f18
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// lfs f18,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f18.f64 = double(temp.f32);
	// lfs f22,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f9,f19,f21,f9
	ctx.f9.f64 = double(float(ctx.f19.f64 * ctx.f21.f64 - ctx.f9.f64));
	// stfs f7,2668(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2668, temp.u32);
	// fmadds f7,f18,f13,f0
	ctx.f7.f64 = double(float(ctx.f18.f64 * ctx.f13.f64 + ctx.f0.f64));
	// fmuls f4,f4,f22
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// stfs f6,4092(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4092, temp.u32);
	// fmr f0,f22
	ctx.f0.f64 = ctx.f22.f64;
	// lfs f6,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f6.f64 = double(temp.f32);
	// stfs f13,3628(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3628, temp.u32);
	// fmadds f11,f30,f6,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f6.f64 + ctx.f11.f64));
	// stfs f5,2444(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2444, temp.u32);
	// lfs f21,2396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2396);
	ctx.f21.f64 = double(temp.f32);
	// lfs f5,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f5.f64 = double(temp.f32);
	// lfs f13,1984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1984);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f6,f5,f14
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f14.f64));
	// stfs f12,2396(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2396, temp.u32);
	// fmuls f13,f13,f21
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// stfs f8,3548(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3548, temp.u32);
	// lfs f8,3116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3116);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f9,f9,f14,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f14.f64 - ctx.f4.f64));
	// stfs f3,3656(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3656, temp.u32);
	// fmadds f12,f2,f0,f28
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f28.f64));
	// stfs f31,3672(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3672, temp.u32);
	// fmadds f10,f10,f0,f20
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64 + ctx.f20.f64));
	// lfs f0,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f7,f8,f0,f7
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f3,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f0,f25,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f28,2524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2524);
	ctx.f28.f64 = double(temp.f32);
	// lfs f4,936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,3540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3540);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f13,f13,f3,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f3.f64 + ctx.f11.f64));
	// lfs f31,3412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3412);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f2,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// stfs f1,2096(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2096, temp.u32);
	// fmuls f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// lfs f5,2532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2532);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f28,f30,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f30.f64 - ctx.f12.f64)));
	// fmadds f10,f10,f14,f9
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f14.f64 + ctx.f9.f64));
	// stfs f29,2468(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2468, temp.u32);
	// fmuls f1,f1,f5
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,3556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3556);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,4380(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4380, temp.u32);
	// fmadds f7,f28,f29,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f29.f64 + ctx.f7.f64));
	// stfs f24,2484(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2484, temp.u32);
	// lfs f26,2476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2476);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,2516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2516);
	ctx.f24.f64 = double(temp.f32);
	// lfs f11,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f11.f64 = double(temp.f32);
	// stfs f27,3448(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3448, temp.u32);
	// fnmsubs f11,f26,f11,f24
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f11.f64 - ctx.f24.f64)));
	// stfs f17,2452(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2452, temp.u32);
	// lfs f29,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f10,f6,f22,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f22.f64 - ctx.f10.f64)));
	// lfs f27,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f4,f4,f29,f13
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64 + ctx.f13.f64));
	// lfs f17,3108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3108);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f1,f1,f14
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f14.f64));
	// lfs f30,3648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3648);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f17,f27,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f27.f64 + ctx.f12.f64));
	// lfs f9,2540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2540);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f19,f9,f14
	ctx.f19.f64 = double(float(ctx.f9.f64 * ctx.f14.f64));
	// lfs f26,1768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1768);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f3,f3,f14
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// lfs f24,3396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3396);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f26,f14
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// stfs f23,3092(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3092, temp.u32);
	// fmuls f23,f24,f30
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// lfs f20,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,4836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4836);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,1992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1992);
	ctx.f27.f64 = double(temp.f32);
	// lfs f17,1776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1776);
	ctx.f17.f64 = double(temp.f32);
	// lfs f29,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f29.f64 = double(temp.f32);
	// lfs f6,2104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2104);
	ctx.f6.f64 = double(temp.f32);
	// stfs f16,3444(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3444, temp.u32);
	// stfs f15,2548(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2548, temp.u32);
	// lfs f13,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f7,f8,f20,f7
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f20.f64 + ctx.f7.f64));
	// stfs f13,2104(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2104, temp.u32);
	// fmuls f0,f0,f14
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f0,1776(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1776, temp.u32);
	// fmuls f6,f3,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// lfs f16,2508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2508);
	ctx.f16.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,2184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2184);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f15,f16,f18
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// stfs f31,1768(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1768, temp.u32);
	// fmuls f31,f27,f16
	ctx.f31.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// fnmsubs f11,f0,f29,f11
	ctx.f11.f64 = double(float(-(ctx.f0.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// stfs f27,4364(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4364, temp.u32);
	// lfs f27,2500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2500);
	ctx.f27.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f13,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f13.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// fnmsubs f10,f1,f13,f10
	ctx.f10.f64 = double(float(-(ctx.f1.f64 * ctx.f13.f64 - ctx.f10.f64)));
	// lfs f20,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f7,f7,f14
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f14.f64));
	// stfs f2,3556(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3556, temp.u32);
	// stfs f9,3156(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3156, temp.u32);
	// fmuls f2,f19,f20
	ctx.f2.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lfs f9,3100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3100);
	ctx.f9.f64 = double(temp.f32);
	// lfs f22,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f4,f23,f22,f4
	ctx.f4.f64 = double(float(-(ctx.f23.f64 * ctx.f22.f64 - ctx.f4.f64)));
	// lfs f3,3404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3404);
	ctx.f3.f64 = double(temp.f32);
	// stfs f5,3412(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3412, temp.u32);
	// fmuls f29,f17,f3
	ctx.f29.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// stfs f8,1992(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1992, temp.u32);
	// fmr f8,f20
	ctx.f8.f64 = ctx.f20.f64;
	// stfs f3,2628(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2628, temp.u32);
	// lfs f3,2252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2252);
	ctx.f3.f64 = double(temp.f32);
	// stfs f28,2508(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2508, temp.u32);
	// fmadds f10,f7,f13,f10
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f10.f64));
	// lfs f7,3124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3124);
	ctx.f7.f64 = double(temp.f32);
	// lfs f0,2104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2104);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f12,f27,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// stfs f30,2500(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2500, temp.u32);
	// lfs f28,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f28.f64 = double(temp.f32);
	// fadds f11,f11,f4
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f4.f64));
	// lfs f30,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f30.f64 = double(temp.f32);
	// fadds f30,f30,f28
	ctx.f30.f64 = double(float(ctx.f30.f64 + ctx.f28.f64));
	// lfs f28,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f28.f64 = double(temp.f32);
	// lfs f4,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f8,f26,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// stfs f25,2524(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2524, temp.u32);
	// fadds f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f28.f64));
	// stfs f24,2516(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2516, temp.u32);
	// lfs f28,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,3564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3564);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f9,f9,f0,f12
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f12,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f5,f7,f12
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// lfs f23,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f12,f2,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f12.f64));
	// lfs f2,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f2,f3
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// lfs f25,2604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2604);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,1784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1784);
	ctx.f1.f64 = double(temp.f32);
	// fadds f25,f24,f25
	ctx.f25.f64 = double(float(ctx.f24.f64 + ctx.f25.f64));
	// lfs f27,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f7,f27
	ctx.f27.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// lfs f13,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f26,f1,f26
	ctx.f26.f64 = double(float(ctx.f1.f64 * ctx.f26.f64));
	// lfs f24,2580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2580);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,2000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2000);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f9,f15,f0,f9
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f0.f64 + ctx.f9.f64));
	// stfs f21,2492(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2492, temp.u32);
	// fmuls f5,f5,f23
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f23.f64));
	// lfs f23,3140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3140);
	ctx.f23.f64 = double(temp.f32);
	// stfs f18,3108(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3108, temp.u32);
	// fmuls f2,f2,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// lfs f28,4284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4284);
	ctx.f28.f64 = double(temp.f32);
	// stfs f16,4732(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4732, temp.u32);
	// stfs f17,3068(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3068, temp.u32);
	// fnmsubs f9,f31,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f31,1792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1792);
	ctx.f31.f64 = double(temp.f32);
	// stfs f8,2000(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2000, temp.u32);
	// fmuls f4,f4,f23
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// lfs f8,1776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1776);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f26,f26,f14
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// fnmsubs f9,f8,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// stfs f11,5336(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5336, temp.u32);
	// lfs f17,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f20,f22,f24
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// fmuls f27,f27,f17
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f17.f64));
	// lfs f8,2944(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2944);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f5,f5,f17
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f17.f64));
	// lfs f17,3420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3420);
	ctx.f17.f64 = double(temp.f32);
	// fadds f25,f25,f17
	ctx.f25.f64 = double(float(ctx.f25.f64 + ctx.f17.f64));
	// stfs f25,1792(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1792, temp.u32);
	// lfs f25,1376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1376);
	ctx.f25.f64 = double(temp.f32);
	// stfs f8,1376(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1376, temp.u32);
	// lfs f21,2160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2160);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f15,f21,f24
	ctx.f15.f64 = double(float(ctx.f21.f64 * ctx.f24.f64));
	// stfs f15,2160(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2160, temp.u32);
	// lfs f15,-19292(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19292);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f11,f6,f0,f9
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f0,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f0.f64 = double(temp.f32);
	// fadds f9,f13,f0
	ctx.f9.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f16,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f16.f64 = double(temp.f32);
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f19,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f23,f19,f23
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// lfs f19,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f31,f19
	ctx.f19.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// stfs f31,3052(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3052, temp.u32);
	// lfs f31,4880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4880);
	ctx.f31.f64 = double(temp.f32);
	// stfs f3,1784(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1784, temp.u32);
	// lfs f3,1276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1276);
	ctx.f3.f64 = double(temp.f32);
	// lfs f6,2564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2564);
	ctx.f6.f64 = double(temp.f32);
	// stfs f1,3540(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3540, temp.u32);
	// fmadds f1,f25,f6,f20
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f6.f64 + ctx.f20.f64));
	// fnmsubs f12,f12,f16,f11
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f16.f64 - ctx.f11.f64)));
	// stfs f21,3100(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3100, temp.u32);
	// fmuls f11,f9,f28
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f28.f64));
	// stfs f24,4292(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4292, temp.u32);
	// fmadds f13,f13,f7,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f7.f64 + ctx.f2.f64));
	// lfs f2,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f21,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f21.f64 = double(temp.f32);
	// fmr f2,f15
	ctx.f2.f64 = ctx.f15.f64;
	// lfs f24,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f24.f64 = double(temp.f32);
	// lfs f9,3616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3616);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f8,f19,f14
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f14.f64));
	// lfs f20,1792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1792);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,2292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2292);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,-19500(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19500);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f0,f0,f18
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f18.f64));
	// stfs f22,2428(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2428, temp.u32);
	// fmuls f1,f1,f21
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// stfs f15,588(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// fmadds f11,f30,f3,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f3.f64 + ctx.f11.f64));
	// lfs f30,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f10,f26,f30,f10
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f30.f64 - ctx.f10.f64)));
	// lfs f30,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f30.f64 = double(temp.f32);
	// fmr f26,f15
	ctx.f26.f64 = ctx.f15.f64;
	// stfs f17,824(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 824, temp.u32);
	// fmsubs f2,f31,f2,f27
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64 - ctx.f27.f64));
	// lfs f31,1376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1376);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f13,f23,f31,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f23,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f29,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f29.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// lfs f31,4728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4728);
	ctx.f31.f64 = double(temp.f32);
	// fmr f29,f15
	ctx.f29.f64 = ctx.f15.f64;
	// lfs f23,2160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2160);
	ctx.f23.f64 = double(temp.f32);
	// lfs f27,4052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4052);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// lfs f21,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f30,f30,f27
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// fmadds f11,f24,f9,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f9.f64 + ctx.f11.f64));
	// fmsubs f5,f31,f26,f5
	ctx.f5.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 - ctx.f5.f64));
	// lfs f31,2008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2008);
	ctx.f31.f64 = double(temp.f32);
	// lfs f26,1808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1808);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f24,f31,f7
	ctx.f24.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// fmuls f22,f26,f31
	ctx.f22.f64 = double(float(ctx.f26.f64 * ctx.f31.f64));
	// fmadds f2,f20,f29,f2
	ctx.f2.f64 = double(float(ctx.f20.f64 * ctx.f29.f64 + ctx.f2.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f10,f8,f21,f10
	ctx.f10.f64 = double(float(-(ctx.f8.f64 * ctx.f21.f64 - ctx.f10.f64)));
	// lfs f19,2000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2000);
	ctx.f19.f64 = double(temp.f32);
	// fmr f21,f16
	ctx.f21.f64 = ctx.f16.f64;
	// lfs f17,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f0,f0,f29,f13
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// lfs f13,4488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4488);
	ctx.f13.f64 = double(temp.f32);
	// fmr f8,f15
	ctx.f8.f64 = ctx.f15.f64;
	// lfs f16,4392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4392);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f2,f2,f31,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f31.f64 + ctx.f1.f64));
	// lfs f1,4720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4720);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f11,f11,f29,f4
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f29.f64 + ctx.f4.f64));
	// lfs f29,1384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1384);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f30,f14
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// stfs f25,3664(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3664, temp.u32);
	// stfs f27,1752(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1752, temp.u32);
	// fmuls f16,f29,f16
	ctx.f16.f64 = double(float(ctx.f29.f64 * ctx.f16.f64));
	// lfs f27,1824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1824);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,2200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2200);
	ctx.f25.f64 = double(temp.f32);
	// stfs f29,2532(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2532, temp.u32);
	// fnmsubs f12,f19,f21,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f21.f64 - ctx.f12.f64)));
	// lfs f19,1492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1492);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f19,f19,f17,f0
	ctx.f19.f64 = double(float(-(ctx.f19.f64 * ctx.f17.f64 - ctx.f0.f64)));
	// lfs f0,4712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4712);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f13,f8,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f8.f64 + ctx.f5.f64));
	// lfs f8,2112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2112);
	ctx.f8.f64 = double(temp.f32);
	// fadds f1,f0,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 + ctx.f1.f64));
	// stfs f8,2476(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2476, temp.u32);
	// fmr f0,f17
	ctx.f0.f64 = ctx.f17.f64;
	// lfs f29,3180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3180);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f4,f26,f8
	ctx.f4.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// lfs f26,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f26,f9
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f9.f64));
	// lfs f8,2016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2016);
	ctx.f8.f64 = double(temp.f32);
	// stfs f25,2008(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2008, temp.u32);
	// stfs f27,1808(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1808, temp.u32);
	// lfs f5,2300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2300);
	ctx.f5.f64 = double(temp.f32);
	// lfs f20,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f20,f20,f5
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f5.f64));
	// fmadds f23,f13,f31,f23
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f31.f64 + ctx.f23.f64));
	// lfs f13,528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 528);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// stfs f1,2540(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2540, temp.u32);
	// fmadds f11,f13,f0,f11
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f0,4704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4704);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f21,f21,f5
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// fmuls f15,f0,f13
	ctx.f15.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f13,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f30,f13,f10
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f10.f64)));
	// lfs f0,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,1384(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1384, temp.u32);
	// lfs f0,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f0.f64 = double(temp.f32);
	// lfs f10,1384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1384);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f26,f10,f19
	ctx.f10.f64 = double(float(-(ctx.f26.f64 * ctx.f10.f64 - ctx.f19.f64)));
	// fadds f30,f12,f13
	ctx.f30.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// lfs f19,3660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3660);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f13,f0,f8
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f8.f64));
	// lfs f12,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f12,f27,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f12.f64));
	// stfs f19,3404(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3404, temp.u32);
	// fmuls f19,f25,f19
	ctx.f19.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// stfs f30,2112(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2112, temp.u32);
	// fmuls f26,f0,f29
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// lfs f27,1392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1392);
	ctx.f27.f64 = double(temp.f32);
	// lfs f17,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f17.f64 = double(temp.f32);
	// lfs f25,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f17,f7,f17
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f17.f64));
	// lfs f30,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// stfs f29,1384(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1384, temp.u32);
	// fmuls f29,f27,f6
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f6.f64));
	// stfs f8,2016(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2016, temp.u32);
	// fmuls f30,f30,f18
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// fmuls f1,f13,f1
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// lfs f13,5096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5096);
	ctx.f13.f64 = double(temp.f32);
	// lfs f8,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f8.f64 = double(temp.f32);
	// stfs f13,1392(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1392, temp.u32);
	// stfs f6,2200(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2200, temp.u32);
	// fmuls f12,f12,f14
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f14.f64));
	// lfs f6,2024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2024);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f8,f8,f24
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f24.f64));
	// stfs f27,2024(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2024, temp.u32);
	// lfs f27,1392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1392);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f24,f27
	ctx.f27.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// stfs f27,1392(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1392, temp.u32);
	// stfs f24,1824(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1824, temp.u32);
	// lfs f13,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f13.f64 = double(temp.f32);
	// lfs f27,1992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1992);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f1,f7,f13,f1
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f13.f64 + ctx.f1.f64));
	// stfs f27,2160(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2160, temp.u32);
	// fmuls f27,f27,f13
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// stfs f12,1792(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1792, temp.u32);
	// fmuls f13,f13,f28
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f28.f64));
	// lfs f12,1784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1784);
	ctx.f12.f64 = double(temp.f32);
	// stfs f6,1376(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1376, temp.u32);
	// fmuls f6,f0,f6
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// stfs f27,3124(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3124, temp.u32);
	// stfs f8,1784(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1784, temp.u32);
	// lfs f8,1768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1768);
	ctx.f8.f64 = double(temp.f32);
	// lfs f27,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f25,f25,f8
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// stfs f29,1768(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1768, temp.u32);
	// fmadds f27,f27,f12,f21
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f12.f64 + ctx.f21.f64));
	// lfs f21,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f21.f64 = double(temp.f32);
	// stfs f14,2104(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2104, temp.u32);
	// fmsubs f11,f20,f21,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f21.f64 - ctx.f11.f64));
	// lfs f14,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f14.f64 = double(temp.f32);
	// fmr f20,f14
	ctx.f20.f64 = ctx.f14.f64;
	// lfs f21,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f4,f4,f14,f23
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f14.f64 - ctx.f23.f64)));
	// lfs f23,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f23.f64 = double(temp.f32);
	// lfs f29,2024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2024);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f23,f17,f23,f15
	ctx.f23.f64 = double(float(ctx.f17.f64 * ctx.f23.f64 - ctx.f15.f64));
	// stfs f29,3396(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3396, temp.u32);
	// fmuls f21,f16,f21
	ctx.f21.f64 = double(float(ctx.f16.f64 * ctx.f21.f64));
	// lfs f29,2200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2200);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,4052(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4052, temp.u32);
	// lfs f29,1392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1392);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,2024(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2024, temp.u32);
	// lfs f29,1824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1824);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,2184(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2184, temp.u32);
	// lfs f29,2016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2016);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f2,f22,f20,f2
	ctx.f2.f64 = double(float(-(ctx.f22.f64 * ctx.f20.f64 - ctx.f2.f64)));
	// stfs f29,3420(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3420, temp.u32);
	// lfs f29,1384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1384);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,3648(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3648, temp.u32);
	// lfs f29,2112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2112);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,5340(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5340, temp.u32);
	// lfs f29,1808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1808);
	ctx.f29.f64 = double(temp.f32);
	// stfs f29,1984(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1984, temp.u32);
	// lfs f17,4352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4352);
	ctx.f17.f64 = double(temp.f32);
	// lfs f22,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f22.f64 = double(temp.f32);
	// lfs f29,2008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2008);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f10,f17,f22,f10
	ctx.f10.f64 = double(float(-(ctx.f17.f64 * ctx.f22.f64 - ctx.f10.f64)));
	// stfs f29,1760(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1760, temp.u32);
	// lfs f29,1376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1376);
	ctx.f29.f64 = double(temp.f32);
	// lfs f16,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f16.f64 = double(temp.f32);
	// stfs f6,2000(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2000, temp.u32);
	// stfs f12,1992(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1992, temp.u32);
	// lfs f24,1268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1268);
	ctx.f24.f64 = double(temp.f32);
	// lfs f6,3572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3572);
	ctx.f6.f64 = double(temp.f32);
	// lfs f20,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f20,f20,f3
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f3.f64));
	// lfs f17,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f22,f9
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f17,f17,f5
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f5.f64));
	// stfs f30,1776(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1776, temp.u32);
	// fmuls f30,f19,f16
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// stfs f29,3616(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3616, temp.u32);
	// fmuls f26,f26,f12
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f12.f64));
	// lfs f29,2160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2160);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f0,f24
	ctx.f19.f64 = double(float(ctx.f0.f64 * ctx.f24.f64));
	// stfs f31,2436(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2436, temp.u32);
	// fmuls f31,f0,f6
	ctx.f31.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// stfs f18,3116(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3116, temp.u32);
	// stfs f29,4836(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4836, temp.u32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f12,f20,f12,f26
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f12.f64 - ctx.f26.f64));
	// lfs f29,1792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1792);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f30,f29,f0,f30
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 - ctx.f30.f64));
	// stfs f6,3660(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3660, temp.u32);
	// lfs f29,2000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2000);
	ctx.f29.f64 = double(temp.f32);
	// lfs f18,1784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1784);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f16.f64 = double(temp.f32);
	// lfs f6,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f4,f18,f16,f4
	ctx.f4.f64 = double(float(-(ctx.f18.f64 * ctx.f16.f64 - ctx.f4.f64)));
	// fnmsubs f6,f29,f6,f1
	ctx.f6.f64 = double(float(-(ctx.f29.f64 * ctx.f6.f64 - ctx.f1.f64)));
	// lfs f1,3164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3164);
	ctx.f1.f64 = double(temp.f32);
	// lfs f18,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f18,f1
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f1.f64));
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,1776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1776);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f10,f15,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f15.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// stfs f2,1392(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1392, temp.u32);
	// fnmsubs f0,f22,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f2,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// lfs f12,1840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1840);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f27,f2,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f2.f64 + ctx.f11.f64));
	// lfs f15,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f15.f64 = double(temp.f32);
	// stfs f21,1840(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1840, temp.u32);
	// lfs f21,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f21.f64 = double(temp.f32);
	// lfs f27,2540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2540);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f6,f17,f15,f6
	ctx.f6.f64 = double(float(ctx.f17.f64 * ctx.f15.f64 + ctx.f6.f64));
	// lfs f26,2104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2104);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f4,f27,f21,f4
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f21.f64 + ctx.f4.f64));
	// lfs f16,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f18,f26
	ctx.f21.f64 = double(float(ctx.f18.f64 * ctx.f26.f64));
	// lfs f29,3124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3124);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f30,f25,f16,f30
	ctx.f30.f64 = double(float(-(ctx.f25.f64 * ctx.f16.f64 - ctx.f30.f64)));
	// lfs f2,1172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f29,f29,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// lfs f27,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// lfs f18,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f10,f2,f27,f10
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f27.f64 - ctx.f10.f64)));
	// lfs f20,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,4664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4664);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f20,f20,f26
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f26.f64));
	// lfs f14,2120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2120);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f27,f18,f16
	ctx.f27.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// lfs f2,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f2.f64 = double(temp.f32);
	// stfs f23,2120(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2120, temp.u32);
	// fmadds f0,f19,f2,f0
	ctx.f0.f64 = double(float(ctx.f19.f64 * ctx.f2.f64 + ctx.f0.f64));
	// lfs f23,1992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1992);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f18,f23
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f23.f64));
	// lfs f18,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f18.f64 = double(temp.f32);
	// lfs f25,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f6,f7,f18,f6
	ctx.f6.f64 = double(float(-(ctx.f7.f64 * ctx.f18.f64 - ctx.f6.f64)));
	// lfs f2,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f25,f25,f12
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f12.f64));
	// lfs f15,1008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1008);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f2,f29,f2,f30
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f2.f64 + ctx.f30.f64));
	// lfs f18,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f18.f64 = double(temp.f32);
	// stfs f12,2008(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2008, temp.u32);
	// fmuls f18,f18,f15
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f15.f64));
	// stfs f8,1808(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1808, temp.u32);
	// stfs f3,1384(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1384, temp.u32);
	// stfs f24,2252(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2252, temp.u32);
	// lfs f24,3252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3252);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,4208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4208);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f22,f24
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// lfs f12,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f20,f20,f17
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f17.f64));
	// lfs f8,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f12,f12,f5
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// lfs f3,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f8,f8,f28
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f28.f64));
	// lfs f30,3412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3412);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f3,f23,f3
	ctx.f3.f64 = double(float(ctx.f23.f64 * ctx.f3.f64));
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// stfs f19,2112(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2112, temp.u32);
	// fmuls f29,f30,f29
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f18,2016(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2016, temp.u32);
	// lfs f19,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,4380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4380);
	ctx.f18.f64 = double(temp.f32);
	// stfs f21,1824(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1824, temp.u32);
	// fmuls f19,f14,f19
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// lfs f21,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// fadds f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// fmadds f25,f18,f21,f25
	ctx.f25.f64 = double(float(ctx.f18.f64 * ctx.f21.f64 + ctx.f25.f64));
	// lfs f21,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f21.f64 = double(temp.f32);
	// stfs f5,3572(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3572, temp.u32);
	// fnmsubs f0,f13,f21,f0
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f21.f64 - ctx.f0.f64)));
	// stfs f7,2160(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2160, temp.u32);
	// fmr f5,f21
	ctx.f5.f64 = ctx.f21.f64;
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f13,f31,f7,f2
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f2.f64)));
	// stfs f4,2200(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2200, temp.u32);
	// lfs f7,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f12,f12,f7,f6
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f7.f64 - ctx.f6.f64)));
	// lfs f2,4572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4572);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f6,f27,f26
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// fmuls f31,f31,f2
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// stfs f28,3180(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3180, temp.u32);
	// fmuls f27,f19,f26
	ctx.f27.f64 = double(float(ctx.f19.f64 * ctx.f26.f64));
	// lfs f28,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f28.f64 = double(temp.f32);
	// stfs f1,1784(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1784, temp.u32);
	// fmuls f1,f29,f26
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// fmadds f0,f8,f5,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f5.f64 + ctx.f0.f64));
	// lfs f4,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f4.f64 = double(temp.f32);
	// lfs f10,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f4,f22,f4
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// fnmsubs f13,f20,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f29,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f29.f64 = double(temp.f32);
	// stfs f24,4284(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4284, temp.u32);
	// fmuls f10,f30,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// fnmsubs f12,f3,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f24,1840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1840);
	ctx.f24.f64 = double(temp.f32);
	// lfs f5,2532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2532);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f9,f9,f29
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f28,2120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2120);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// lfs f29,3992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3992);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f28,f28,f5,f24
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 - ctx.f24.f64));
	// lfs f8,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// lfs f24,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,1392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1392);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f8,f29,f8,f0
	ctx.f8.f64 = double(float(-(ctx.f29.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f3,2024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2024);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,2572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2572);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f3,f3,f24,f22
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f24.f64 - ctx.f22.f64)));
	// stfs f16,1376(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1376, temp.u32);
	// fmuls f10,f10,f26
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// stfs f23,3164(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3164, temp.u32);
	// stfs f14,4380(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4380, temp.u32);
	// lfs f23,1768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1768);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,3116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3116);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,2200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2200);
	ctx.f21.f64 = double(temp.f32);
	// lfs f14,2016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2016);
	ctx.f14.f64 = double(temp.f32);
	// lfs f24,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f24.f64 = double(temp.f32);
	// lfs f0,1824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1824);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f24,f23,f24,f21
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f24.f64 + ctx.f21.f64));
	// lfs f16,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f0,f0,f7,f13
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// stfs f15,2300(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2300, temp.u32);
	// fmadds f16,f16,f22,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f22.f64 + ctx.f14.f64));
	// stfs f31,2572(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2572, temp.u32);
	// lfs f15,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f15.f64 = double(temp.f32);
	// lfs f31,1384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1384);
	ctx.f31.f64 = double(temp.f32);
	// stfs f17,1792(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1792, temp.u32);
	// fmuls f15,f15,f31
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// lfs f13,1848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1848);
	ctx.f13.f64 = double(temp.f32);
	// lfs f23,2620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2620);
	ctx.f23.f64 = double(temp.f32);
	// fadds f20,f13,f29
	ctx.f20.f64 = double(float(ctx.f13.f64 + ctx.f29.f64));
	// lfs f19,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f19,f23
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// lfs f14,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f17,f17,f22
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f22.f64));
	// stfs f15,1848(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1848, temp.u32);
	// fmadds f4,f25,f14,f4
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f14.f64 + ctx.f4.f64));
	// lfs f21,2524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2524);
	ctx.f21.f64 = double(temp.f32);
	// lfs f7,2032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2032);
	ctx.f7.f64 = double(temp.f32);
	// lfs f15,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,2112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2112);
	ctx.f14.f64 = double(temp.f32);
	// lfs f25,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f12,f14,f15,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f15.f64 + ctx.f12.f64));
	// lfs f15,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f6,f6,f25,f0
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// fmuls f25,f21,f15
	ctx.f25.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// lfs f0,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f0.f64 = double(temp.f32);
	// lfs f14,3556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3556);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f9,f9,f0,f8
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f8.f64));
	// stfs f18,2032(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2032, temp.u32);
	// fmuls f18,f14,f7
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f7.f64));
	// stfs f7,3252(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3252, temp.u32);
	// stfs f11,5344(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5344, temp.u32);
	// lfs f7,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f7.f64 = double(temp.f32);
	// stfs f2,1840(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1840, temp.u32);
	// stfs f13,3556(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3556, temp.u32);
	// fnmsubs f0,f19,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f12,3108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3108);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f7,f1,f7,f6
	ctx.f7.f64 = double(float(-(ctx.f1.f64 * ctx.f7.f64 - ctx.f6.f64)));
	// lfs f13,2516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2516);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f11,f25,f26
	ctx.f11.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f6,3984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3984);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f2.f64 = double(temp.f32);
	// stfs f5,2540(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2540, temp.u32);
	// fmadds f9,f6,f2,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f9.f64));
	// stfs f3,5356(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5356, temp.u32);
	// fmuls f5,f13,f12
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// lfs f3,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,2588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2588);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f18,f3
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// lfs f15,1856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1856);
	ctx.f15.f64 = double(temp.f32);
	// stfs f28,5352(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5352, temp.u32);
	// lfs f28,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f7,f27,f28,f7
	ctx.f7.f64 = double(float(-(ctx.f27.f64 * ctx.f28.f64 - ctx.f7.f64)));
	// fmadds f11,f20,f2,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f2.f64 + ctx.f11.f64));
	// stfs f10,1856(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1856, temp.u32);
	// lfs f10,3608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3608);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f1,f26
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f26.f64));
	// fadds f8,f15,f10
	ctx.f8.f64 = double(float(ctx.f15.f64 + ctx.f10.f64));
	// stfs f30,3412(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3412, temp.u32);
	// stfs f29,1768(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1768, temp.u32);
	// lfs f30,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f5,f5,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// fmadds f0,f16,f29,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f29.f64 + ctx.f0.f64));
	// lfs f30,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,4176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4176);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f3,f3,f30
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// lfs f28,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f28.f64 = double(temp.f32);
	// lfs f20,1864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1864);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f9,f29,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// lfs f16,3404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3404);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f7,f4,f26,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f26.f64 + ctx.f7.f64));
	// lfs f25,1808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1808);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f11,f20,f16,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 + ctx.f11.f64));
	// stfs f31,2580(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2580, temp.u32);
	// fmuls f1,f1,f25
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f25.f64));
	// stfs f24,5348(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5348, temp.u32);
	// fmuls f8,f8,f26
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f26.f64));
	// stfs f22,2660(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2660, temp.u32);
	// lfs f6,1212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1212);
	ctx.f6.f64 = double(temp.f32);
	// lfs f31,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,2420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2420);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f6,f31
	ctx.f31.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// lfs f28,2596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2596);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,3428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3428);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,2168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2168);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,4316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4316);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f19,f14,f22
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f22.f64));
	// stfs f23,3608(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3608, temp.u32);
	// fmuls f23,f28,f26
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// stfs f21,2024(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2024, temp.u32);
	// fmuls f21,f13,f24
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f24.f64));
	// stfs f12,2120(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2120, temp.u32);
	// fmuls f12,f30,f16
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f16.f64));
	// lfs f29,3148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3148);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,2508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2508);
	ctx.f27.f64 = double(temp.f32);
	// lfs f18,4552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4552);
	ctx.f18.f64 = double(temp.f32);
	// lfs f4,2040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2040);
	ctx.f4.f64 = double(temp.f32);
	// stfs f1,1864(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1864, temp.u32);
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// fmuls f8,f8,f1
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// lfs f1,1872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1872);
	ctx.f1.f64 = double(temp.f32);
	// stfs f19,1872(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1872, temp.u32);
	// stfs f31,3428(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3428, temp.u32);
	// lfs f19,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f19,f1
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// fmadds f5,f5,f31,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f31.f64 + ctx.f3.f64));
	// lfs f19,4228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4228);
	ctx.f19.f64 = double(temp.f32);
	// lfs f3,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f9,f19,f3,f9
	ctx.f9.f64 = double(float(ctx.f19.f64 * ctx.f3.f64 + ctx.f9.f64));
	// lfs f3,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f3.f64 = double(temp.f32);
	// lfs f19,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f17,f3,f0
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f3.f64 - ctx.f0.f64)));
	// fmuls f3,f19,f18
	ctx.f3.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// stfs f21,2040(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2040, temp.u32);
	// fmuls f21,f29,f4
	ctx.f21.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// stfs f21,2168(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2168, temp.u32);
	// lfs f21,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f27,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f21.f64));
	// stfs f6,1808(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1808, temp.u32);
	// stfs f10,3564(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3564, temp.u32);
	// stfs f2,3140(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3140, temp.u32);
	// lfs f2,1856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1856);
	ctx.f2.f64 = double(temp.f32);
	// lfs f10,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f2,f10,f7
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f10.f64 - ctx.f7.f64)));
	// stfs f30,3404(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3404, temp.u32);
	// stfs f29,1392(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1392, temp.u32);
	// fmuls f6,f3,f26
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f26.f64));
	// lfs f3,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f11,f11,f3
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// lfs f3,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,1848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1848);
	ctx.f29.f64 = double(temp.f32);
	// lfs f7,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f9,f29,f3,f9
	ctx.f9.f64 = double(float(-(ctx.f29.f64 * ctx.f3.f64 - ctx.f9.f64)));
	// lfs f2,2032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2032);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f7,f2,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// stfs f28,2292(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2292, temp.u32);
	// fmuls f30,f21,f30
	ctx.f30.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// lfs f3,4660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4660);
	ctx.f3.f64 = double(temp.f32);
	// lfs f28,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f28.f64 = double(temp.f32);
	// stfs f27,1776(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1776, temp.u32);
	// fmuls f28,f28,f3
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// lfs f27,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f27.f64 = double(temp.f32);
	// stfs f25,2524(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2524, temp.u32);
	// fnmsubs f12,f12,f27,f5
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f27.f64 - ctx.f5.f64)));
	// lfs f25,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f25.f64 = double(temp.f32);
	// lfs f29,2612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2612);
	ctx.f29.f64 = double(temp.f32);
	// fadds f0,f0,f9
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f9.f64));
	// fmuls f27,f29,f25
	ctx.f27.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// lfs f25,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f11,f8,f25,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f25.f64 - ctx.f11.f64));
	// stfs f24,2420(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2420, temp.u32);
	// lfs f5,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f5.f64 = double(temp.f32);
	// lfs f24,2572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2572);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f10,f24,f5,f10
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f5.f64 - ctx.f10.f64)));
	// fmadds f7,f7,f25,f30
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f30.f64));
	// stfs f22,2112(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2112, temp.u32);
	// lfs f31,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f8,4556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4556);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// lfs f5,4732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4732);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,3060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3060);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f5,f8,f5
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f30,4364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4364);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f25,f9,f16
	ctx.f25.f64 = double(float(ctx.f9.f64 * ctx.f16.f64));
	// lfs f22,2128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2128);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f13,f30
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// stfs f20,2200(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2200, temp.u32);
	// fmuls f19,f14,f22
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f22.f64));
	// stfs f18,2532(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2532, temp.u32);
	// lfs f24,3636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3636);
	ctx.f24.f64 = double(temp.f32);
	// lfs f20,3012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3012);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,1880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1880);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,2048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2048);
	ctx.f17.f64 = double(temp.f32);
	// stfs f15,2604(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2604, temp.u32);
	// stfs f28,2588(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2588, temp.u32);
	// fmuls f28,f17,f4
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// stfs f25,3636(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3636, temp.u32);
	// fmuls f27,f27,f26
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// stfs f28,2128(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2128, temp.u32);
	// fmuls f31,f31,f18
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f18.f64));
	// lfs f25,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,1872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1872);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f25
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// lfs f25,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f25.f64 = double(temp.f32);
	// lfs f15,1888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1888);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f1,f25,f24,f1
	ctx.f1.f64 = double(float(ctx.f25.f64 * ctx.f24.f64 + ctx.f1.f64));
	// stfs f14,3148(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3148, temp.u32);
	// lfs f14,5060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5060);
	ctx.f14.f64 = double(temp.f32);
	// stfs f16,1888(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1888, temp.u32);
	// lfs f16,2040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2040);
	ctx.f16.f64 = double(temp.f32);
	// lfs f25,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f25.f64 = double(temp.f32);
	// stfs f21,2596(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2596, temp.u32);
	// fmuls f21,f15,f14
	ctx.f21.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// fnmsubs f12,f16,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f16.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// stfs f21,2048(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2048, temp.u32);
	// lfs f16,1864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1864);
	ctx.f16.f64 = double(temp.f32);
	// lfs f21,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f21.f64 = double(temp.f32);
	// stfs f27,1856(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1856, temp.u32);
	// fnmsubs f10,f16,f21,f10
	ctx.f10.f64 = double(float(-(ctx.f16.f64 * ctx.f21.f64 - ctx.f10.f64)));
	// lfs f27,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// lfs f16,2168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2168);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f11,f16,f25,f11
	ctx.f11.f64 = double(float(ctx.f16.f64 * ctx.f25.f64 + ctx.f11.f64));
	// fmuls f23,f13,f23
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f23.f64));
	// stfs f23,2612(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2612, temp.u32);
	// lfs f23,2492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2492);
	ctx.f23.f64 = double(temp.f32);
	// stfs f31,1880(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1880, temp.u32);
	// fmuls f31,f20,f23
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// stfs f0,5360(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5360, temp.u32);
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// stfs f7,1872(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1872, temp.u32);
	// fmuls f7,f19,f0
	ctx.f7.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// stfs f3,4572(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4572, temp.u32);
	// lfs f3,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f11,f27,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f4,2620(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2620, temp.u32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f1,f26
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f26.f64));
	// stfs f8,2168(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2168, temp.u32);
	// fmr f1,f3
	ctx.f1.f64 = ctx.f3.f64;
	// fmadds f12,f28,f3,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f3.f64 + ctx.f12.f64));
	// lfs f8,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f8.f64 = double(temp.f32);
	// fmr f3,f0
	ctx.f3.f64 = ctx.f0.f64;
	// stfs f29,1824(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1824, temp.u32);
	// fmuls f8,f31,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f8.f64));
	// lfs f31,3428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3428);
	ctx.f31.f64 = double(temp.f32);
	// stfs f5,2040(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2040, temp.u32);
	// lfs f21,3440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3440);
	ctx.f21.f64 = double(temp.f32);
	// lfs f25,2388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2388);
	ctx.f25.f64 = double(temp.f32);
	// lfs f5,3340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3340);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,3628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3628);
	ctx.f29.f64 = double(temp.f32);
	// stfs f2,1384(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1384, temp.u32);
	// fmuls f2,f13,f5
	ctx.f2.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// stfs f9,2032(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2032, temp.u32);
	// fmuls f9,f21,f25
	ctx.f9.f64 = double(float(ctx.f21.f64 * ctx.f25.f64));
	// stfs f24,1848(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1848, temp.u32);
	// fmuls f24,f13,f29
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// fnmsubs f10,f31,f3,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f3.f64 - ctx.f10.f64)));
	// lfs f3,3432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3432);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,3436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3436);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f8,f7,f1,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f1.f64 + ctx.f8.f64));
	// stfs f30,2572(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2572, temp.u32);
	// fmuls f30,f3,f23
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// lfs f27,2192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2192);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f13,f31
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// lfs f7,3044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3044);
	ctx.f7.f64 = double(temp.f32);
	// stfs f22,3012(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3012, temp.u32);
	// stfs f20,1864(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1864, temp.u32);
	// stfs f18,2000(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2000, temp.u32);
	// stfs f17,2016(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2016, temp.u32);
	// stfs f15,3060(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3060, temp.u32);
	// stfs f14,4484(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4484, temp.u32);
	// fmadds f11,f6,f0,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f15,2048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2048);
	ctx.f15.f64 = double(temp.f32);
	// lfs f22,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f12,f15,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f15.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// fnmsubs f8,f2,f16,f8
	ctx.f8.f64 = double(float(-(ctx.f2.f64 * ctx.f16.f64 - ctx.f8.f64)));
	// lfs f1,2484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2484);
	ctx.f1.f64 = double(temp.f32);
	// lfs f22,1880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1880);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f20,f1,f27
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f16,2128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2128);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f10,f22,f0,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f19,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,1896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1896);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,2056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2056);
	ctx.f15.f64 = double(temp.f32);
	// lfs f6,2412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2412);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f0,f3,f15
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f15.f64));
	// fmadds f11,f16,f19,f11
	ctx.f11.f64 = double(float(ctx.f16.f64 * ctx.f19.f64 + ctx.f11.f64));
	// lfs f19,3092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3092);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,1888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1888);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f22,f19
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// fmuls f6,f3,f6
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f6.f64));
	// stfs f19,1888(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1888, temp.u32);
	// stfs f3,2192(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2192, temp.u32);
	// fmuls f17,f7,f18
	ctx.f17.f64 = double(float(ctx.f7.f64 * ctx.f18.f64));
	// lfs f14,1904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1904);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f28,f14,f25,f28
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f25.f64 + ctx.f28.f64));
	// lfs f19,3636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3636);
	ctx.f19.f64 = double(temp.f32);
	// lfs f3,2612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2612);
	ctx.f3.f64 = double(temp.f32);
	// stfs f4,2048(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2048, temp.u32);
	// fmadds f3,f3,f16,f19
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f16.f64 + ctx.f19.f64));
	// fmuls f4,f14,f23
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// lfs f14,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,2596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2596);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f24,f24,f14,f20
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f14.f64 + ctx.f20.f64));
	// stfs f0,1904(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1904, temp.u32);
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fmr f14,f0
	ctx.f14.f64 = ctx.f0.f64;
	// stfs f10,1880(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1880, temp.u32);
	// lfs f10,2380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2380);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f17,f17,f0
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// lfs f19,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f9,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// stfs f7,2128(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2128, temp.u32);
	// fmuls f19,f19,f10
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f10.f64));
	// lfs f2,1976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1976);
	ctx.f2.f64 = double(temp.f32);
	// stfs f30,2056(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2056, temp.u32);
	// fmuls f30,f2,f18
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f18.f64));
	// lfs f12,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f12.f64 = double(temp.f32);
	// stfs f30,1896(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1896, temp.u32);
	// stfs f21,2564(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2564, temp.u32);
	// stfs f2,3124(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3124, temp.u32);
	// fmr f7,f14
	ctx.f7.f64 = ctx.f14.f64;
	// lfs f30,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f8,f6,f14,f8
	ctx.f8.f64 = double(float(-(ctx.f6.f64 * ctx.f14.f64 - ctx.f8.f64)));
	// lfs f6,2008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2008);
	ctx.f6.f64 = double(temp.f32);
	// lfs f21,2588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2588);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f12,f12,f6
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f6.f64));
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f21,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f21.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// stfs f25,3436(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3436, temp.u32);
	// fmadds f0,f3,f2,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f0.f64));
	// stfs f6,2380(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2380, temp.u32);
	// lfs f25,3380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3380);
	ctx.f25.f64 = double(temp.f32);
	// lfs f20,3084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3084);
	ctx.f20.f64 = double(temp.f32);
	// lfs f9,3148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3148);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f30,f20,f10
	ctx.f30.f64 = double(float(ctx.f20.f64 * ctx.f10.f64));
	// lfs f6,2120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2120);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f25,f9,f25
	ctx.f25.f64 = double(float(ctx.f9.f64 * ctx.f25.f64));
	// stfs f5,2612(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2612, temp.u32);
	// fmuls f5,f19,f6
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f6.f64));
	// stfs f31,2492(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2492, temp.u32);
	// fmsubs f7,f24,f7,f17
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f7.f64 - ctx.f17.f64));
	// lfs f31,1872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1872);
	ctx.f31.f64 = double(temp.f32);
	// lfs f3,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f3.f64 = double(temp.f32);
	// stfs f22,4556(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4556, temp.u32);
	// stfs f29,2104(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2104, temp.u32);
	// stfs f27,2508(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2508, temp.u32);
	// stfs f15,4468(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4468, temp.u32);
	// stfs f23,3432(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3432, temp.u32);
	// fmadds f11,f31,f26,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 + ctx.f11.f64));
	// lfs f31,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f20,f31,f12
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f31.f64 + ctx.f12.f64));
	// lfs f17,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f17.f64 = double(temp.f32);
	// fmr f31,f14
	ctx.f31.f64 = ctx.f14.f64;
	// lfs f21,1912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1912);
	ctx.f21.f64 = double(temp.f32);
	// fmr f2,f14
	ctx.f2.f64 = ctx.f14.f64;
	// lfs f23,2460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2460);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f3,f30,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// lfs f30,2652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2652);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,2796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2796);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f23,f18
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f18.f64));
	// stfs f19,1912(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1912, temp.u32);
	// fmuls f29,f9,f29
	ctx.f29.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f19,1856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1856);
	ctx.f19.f64 = double(temp.f32);
	// lfs f15,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f15.f64 = double(temp.f32);
	// stfs f30,1872(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1872, temp.u32);
	// fmuls f15,f13,f15
	ctx.f15.f64 = double(float(ctx.f13.f64 * ctx.f15.f64));
	// stfs f26,2796(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2796, temp.u32);
	// fmuls f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// stfs f20,3636(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3636, temp.u32);
	// fmadds f7,f4,f31,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f31.f64 + ctx.f7.f64));
	// lfs f4,2040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2040);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f8,f28,f2,f8
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f2.f64 + ctx.f8.f64));
	// lfs f31,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f9,f30
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f30.f64));
	// lfs f2,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f4,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// lfs f4,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// fmr f27,f2
	ctx.f27.f64 = ctx.f2.f64;
	// lfs f31,1904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1904);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f0,f16,f2,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f2.f64 - ctx.f0.f64)));
	// lfs f2,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f16,f13,f21
	ctx.f16.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// stfs f23,3148(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3148, temp.u32);
	// stfs f21,3108(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3108, temp.u32);
	// stfs f13,3440(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3440, temp.u32);
	// fnmsubs f7,f25,f4,f7
	ctx.f7.f64 = double(float(-(ctx.f25.f64 * ctx.f4.f64 - ctx.f7.f64)));
	// lfs f25,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f25.f64 = double(temp.f32);
	// lfs f4,3372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3372);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f28,f28,f25
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// lfs f25,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f25,f17,f25
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// fmr f17,f14
	ctx.f17.f64 = ctx.f14.f64;
	// lfs f14,2788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2788);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f8,f31,f27,f8
	ctx.f8.f64 = double(float(-(ctx.f31.f64 * ctx.f27.f64 - ctx.f8.f64)));
	// lfs f31,2136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2136);
	ctx.f31.f64 = double(temp.f32);
	// stfs f16,2136(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2136, temp.u32);
	// fmsubs f5,f3,f2,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 - ctx.f5.f64));
	// lfs f16,2056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2056);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f24,f1,f31
	ctx.f24.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f3,1400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1400);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f4,f9,f4
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lfs f27,2468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2468);
	ctx.f27.f64 = double(temp.f32);
	// lfs f2,3476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3476);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f22,f13,f27
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f27.f64));
	// stfs f31,2516(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2516, temp.u32);
	// fmuls f2,f9,f2
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// stfs f27,3476(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3476, temp.u32);
	// fmadds f0,f16,f17,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f17.f64 + ctx.f0.f64));
	// lfs f17,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f11,f19,f17,f11
	ctx.f11.f64 = double(float(ctx.f19.f64 * ctx.f17.f64 + ctx.f11.f64));
	// lfs f19,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f16,f10,f14
	ctx.f16.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// lfs f14,4700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4700);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f17,f1,f3
	ctx.f17.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// stfs f17,2652(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2652, temp.u32);
	// stfs f16,1400(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1400, temp.u32);
	// fmuls f19,f14,f19
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// lfs f17,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,1896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1896);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f8,f16,f17,f8
	ctx.f8.f64 = double(float(-(ctx.f16.f64 * ctx.f17.f64 - ctx.f8.f64)));
	// fmadds f12,f12,f14,f5
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f14.f64 + ctx.f5.f64));
	// lfs f17,2348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2348);
	ctx.f17.f64 = double(temp.f32);
	// lfs f5,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f17,f9,f17
	ctx.f17.f64 = double(float(ctx.f9.f64 * ctx.f17.f64));
	// stfs f6,1896(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1896, temp.u32);
	// fnmsubs f7,f29,f5,f7
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f5.f64 - ctx.f7.f64)));
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fnmsubs f13,f24,f13,f8
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f13.f64 - ctx.f8.f64)));
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// lfs f30,1888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1888);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f11,f30,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f30.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f31,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f31.f64 = double(temp.f32);
	// lfs f6,2192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2192);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f7,f4,f31,f7
	ctx.f7.f64 = double(float(-(ctx.f4.f64 * ctx.f31.f64 - ctx.f7.f64)));
	// lfs f5,3436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3436);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f30,f25,f18
	ctx.f30.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// lfs f24,1864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1864);
	ctx.f24.f64 = double(temp.f32);
	// stfs f3,4364(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4364, temp.u32);
	// fmuls f3,f6,f5
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// fmuls f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// lfs f4,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f4.f64 = double(temp.f32);
	// lfs f24,2048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2048);
	ctx.f24.f64 = double(temp.f32);
	// lfs f29,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f29.f64 = double(temp.f32);
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f8,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f8.f64 = double(temp.f32);
	// lfs f21,3364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3364);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f11,f24,f4,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 + ctx.f11.f64));
	// lfs f14,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f7,f2,f29,f7
	ctx.f7.f64 = double(float(-(ctx.f2.f64 * ctx.f29.f64 - ctx.f7.f64)));
	// lfs f2,4300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4300);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f21,f9,f21
	ctx.f21.f64 = double(float(ctx.f9.f64 * ctx.f21.f64));
	// stfs f21,1904(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1904, temp.u32);
	// fmuls f15,f15,f14
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// lfs f21,4236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4236);
	ctx.f21.f64 = double(temp.f32);
	// lfs f14,1880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1880);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f2,f9,f2
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f13,1724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1724);
	ctx.f13.f64 = double(temp.f32);
	// lfs f23,1408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1408);
	ctx.f23.f64 = double(temp.f32);
	// stfs f2,1408(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1408, temp.u32);
	// fmuls f24,f1,f23
	ctx.f24.f64 = double(float(ctx.f1.f64 * ctx.f23.f64));
	// fmadds f0,f28,f8,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f8,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f8,f21,f8,f19
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f8.f64 + ctx.f19.f64));
	// lfs f28,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f28.f64 = double(temp.f32);
	// lfs f19,1400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1400);
	ctx.f19.f64 = double(temp.f32);
	// fadds f11,f14,f11
	ctx.f11.f64 = double(float(ctx.f14.f64 + ctx.f11.f64));
	// lfs f14,4172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4172);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f12,f19,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f28,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f19,f13,f14
	ctx.f19.f64 = double(float(ctx.f13.f64 * ctx.f14.f64));
	// lfs f14,3036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3036);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f7,f3,f28,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f28.f64 + ctx.f7.f64));
	// lfs f27,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f14,f13
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f13.f64));
	// lfs f20,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f20.f64 = double(temp.f32);
	// lfs f2,1912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1912);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f22,f22,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f31,1920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1920);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f0,f2,f27,f0
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f27.f64 - ctx.f0.f64)));
	// stfs f30,1920(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1920, temp.u32);
	// stfs f14,2192(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2192, temp.u32);
	// stfs f15,2056(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2056, temp.u32);
	// lfs f3,1416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1416);
	ctx.f3.f64 = double(temp.f32);
	// lfs f26,2176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2176);
	ctx.f26.f64 = double(temp.f32);
	// lfs f4,2340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2340);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f25,f31,f26
	ctx.f25.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// lfs f29,4692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4692);
	ctx.f29.f64 = double(temp.f32);
	// lfs f16,4676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4676);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f29,f9,f29
	ctx.f29.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f20,4668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4668);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f16,f10,f16
	ctx.f16.f64 = double(float(ctx.f10.f64 * ctx.f16.f64));
	// lfs f15,2372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2372);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f20,f10,f20
	ctx.f20.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// lfs f21,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f21.f64 = double(temp.f32);
	// lfs f30,2244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2244);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f21,f15,f21
	ctx.f21.f64 = double(float(ctx.f15.f64 * ctx.f21.f64));
	// lfs f14,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f10,f30
	ctx.f30.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// lfs f2,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f14,f14,f3
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// lfs f27,1424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1424);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f2,f4,f2
	ctx.f2.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// stfs f8,1416(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1416, temp.u32);
	// stfs f24,1424(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1424, temp.u32);
	// lfs f8,3600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3600);
	ctx.f8.f64 = double(temp.f32);
	// lfs f24,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f24.f64 = double(temp.f32);
	// stfs f9,3340(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3340, temp.u32);
	// fnmsubs f9,f17,f24,f0
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f24.f64 - ctx.f0.f64)));
	// stfs f11,5364(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5364, temp.u32);
	// fmuls f8,f8,f27
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// lfs f11,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f7,f29,f24,f7
	ctx.f7.f64 = double(float(-(ctx.f29.f64 * ctx.f24.f64 - ctx.f7.f64)));
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f21,f11
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f11.f64));
	// lfs f27,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f27.f64 = double(temp.f32);
	// stfs f6,2788(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2788, temp.u32);
	// fmuls f6,f19,f0
	ctx.f6.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f25,f27
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f27.f64));
	// stfs f13,1144(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// lfs f13,-19296(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19296);
	ctx.f13.f64 = double(temp.f32);
	// stfs f4,1864(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1864, temp.u32);
	// fmuls f4,f30,f0
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// stfs f1,2340(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2340, temp.u32);
	// fmadds f0,f16,f0,f12
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f0.f64 + ctx.f12.f64));
	// stfs f13,228(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 228, temp.u32);
	// fmuls f13,f14,f27
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// lfs f1,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,2136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2136);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f9,f30,f1,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f1.f64 + ctx.f9.f64));
	// lfs f1,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f1.f64 = double(temp.f32);
	// lfs f12,3668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3668);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f30,f15,f10
	ctx.f30.f64 = double(float(ctx.f15.f64 * ctx.f10.f64));
	// fmsubs f12,f12,f1,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f1.f64 - ctx.f11.f64));
	// stfs f31,2040(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2040, temp.u32);
	// stfs f5,2176(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2176, temp.u32);
	// lfs f31,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f6,f25,f31,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 + ctx.f6.f64));
	// fmuls f5,f28,f5
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f5.f64));
	// lfs f31,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,3928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3928);
	ctx.f28.f64 = double(temp.f32);
	// lfs f11,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f2,f28,f31,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f31.f64 + ctx.f2.f64));
	// lfs f1,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f11,f8,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f11.f64));
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f25,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f25.f64 = double(temp.f32);
	// lfs f31,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f0,f20,f25,f0
	ctx.f0.f64 = double(float(-(ctx.f20.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// fmsubs f12,f12,f10,f4
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 - ctx.f4.f64));
	// lfs f4,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f4.f64 = double(temp.f32);
	// lfs f24,2652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2652);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f7,f22,f4,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f4.f64 + ctx.f7.f64));
	// lfs f19,3708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3708);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f24,f31,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f31.f64 + ctx.f9.f64));
	// stfs f23,4732(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4732, temp.u32);
	// fmuls f22,f10,f19
	ctx.f22.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// lfs f23,4108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4108);
	ctx.f23.f64 = double(temp.f32);
	// lfs f8,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f29,f10,f23
	ctx.f29.f64 = double(float(ctx.f10.f64 * ctx.f23.f64));
	// lfs f21,3620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3620);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f8,f5,f8,f6
	ctx.f8.f64 = double(float(-(ctx.f5.f64 * ctx.f8.f64 - ctx.f6.f64)));
	// stfs f18,3600(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3600, temp.u32);
	// fmuls f20,f21,f10
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f10.f64));
	// lfs f14,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f11,f2,f26,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f26.f64 + ctx.f11.f64));
	// lfs f25,5072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5072);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f1,f1,f27
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f31,3548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3548);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f23,f25,f26
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f24,2364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2364);
	ctx.f24.f64 = double(temp.f32);
	// lfs f4,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f4.f64 = double(temp.f32);
	// lfs f19,5028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5028);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f4,f4,f31
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// lfs f18,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f10,f19
	ctx.f19.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// lfs f17,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f14,f18
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f18.f64));
	// stfs f7,1400(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1400, temp.u32);
	// fmuls f17,f17,f24
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// lfs f6,1376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1376);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,1440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1440);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// lfs f16,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f16.f64 = double(temp.f32);
	// lfs f7,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f7.f64 = double(temp.f32);
	// stfs f20,1144(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// fnmsubs f13,f13,f16,f8
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f16.f64 - ctx.f8.f64)));
	// stfs f3,1440(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1440, temp.u32);
	// lfs f3,1424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1424);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f12,f29,f7,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f12.f64));
	// fmadds f9,f3,f2,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 + ctx.f9.f64));
	// lfs f2,1416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1416);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f0,f2,f10,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f10.f64 + ctx.f0.f64));
	// lfs f2,5024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5024);
	ctx.f2.f64 = double(temp.f32);
	// lfs f3,4620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4620);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f29,f23,f27
	ctx.f29.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// fadds f3,f2,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 + ctx.f3.f64));
	// stfs f6,3116(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3116, temp.u32);
	// fmuls f2,f6,f14
	ctx.f2.f64 = double(float(ctx.f6.f64 * ctx.f14.f64));
	// lfs f6,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f6.f64 = double(temp.f32);
	// stfs f25,3428(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3428, temp.u32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f25,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// stfs f31,1376(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1376, temp.u32);
	// fmuls f31,f5,f26
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// fmadds f13,f11,f27,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f27.f64 + ctx.f13.f64));
	// lfs f11,1408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1408);
	ctx.f11.f64 = double(temp.f32);
	// stfs f5,2048(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2048, temp.u32);
	// fnmsubs f11,f11,f6,f9
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f6.f64 - ctx.f9.f64)));
	// lfs f9,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f5.f64 = double(temp.f32);
	// lfs f20,4100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4100);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f12,f22,f5,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f5.f64 - ctx.f12.f64)));
	// stfs f15,1912(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1912, temp.u32);
	// fmuls f7,f10,f20
	ctx.f7.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// lfs f15,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f22,f17,f14
	ctx.f22.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// lfs f16,4768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4768);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f30,f30,f15
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// lfs f15,1904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1904);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f14,f16
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// stfs f16,1424(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1424, temp.u32);
	// fmr f5,f9
	ctx.f5.f64 = ctx.f9.f64;
	// fnmsubs f13,f1,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f25,1920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1920);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,2356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2356);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f31,f31,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// fmadds f11,f25,f9,f11
	ctx.f11.f64 = double(float(ctx.f25.f64 * ctx.f9.f64 + ctx.f11.f64));
	// stfs f24,1888(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1888, temp.u32);
	// lfs f24,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f3,f3,f10
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f10.f64));
	// fnmsubs f12,f19,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f23,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f23.f64 = double(temp.f32);
	// lfs f19,3212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3212);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f2,f2,f23
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// stfs f28,1880(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1880, temp.u32);
	// fmuls f19,f14,f19
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// lfs f8,4816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4816);
	ctx.f8.f64 = double(temp.f32);
	// lfs f28,2080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2080);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f8,f10,f8
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f8.f64));
	// lfs f26,4808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4808);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,2168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2168);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f28,f26
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f20,4800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4800);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f11,f15,f6,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f6.f64 - ctx.f11.f64)));
	// lfs f6,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f6.f64 = double(temp.f32);
	// lfs f15,2644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2644);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f6,f16,f6
	ctx.f6.f64 = double(float(ctx.f16.f64 * ctx.f6.f64));
	// lfs f16,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f15,1408(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1408, temp.u32);
	// fnmsubs f7,f7,f16,f0
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// lfs f16,1440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1440);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f20,f10,f20
	ctx.f20.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// lfs f17,4776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4776);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f10,f17
	ctx.f17.f64 = double(float(ctx.f10.f64 * ctx.f17.f64));
	// stfs f21,3628(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3628, temp.u32);
	// fmuls f21,f18,f25
	ctx.f21.f64 = double(float(ctx.f18.f64 * ctx.f25.f64));
	// lfs f1,2176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2176);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f15,f15,f16
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// lfs f9,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f9.f64 = double(temp.f32);
	// lfs f23,4960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4960);
	ctx.f23.f64 = double(temp.f32);
	// lfs f24,2032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2032);
	ctx.f24.f64 = double(temp.f32);
	// lfs f0,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f0.f64 = double(temp.f32);
	// stfs f19,1440(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1440, temp.u32);
	// stfs f6,2080(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2080, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f6,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f19,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f19.f64 = double(temp.f32);
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f29,f19,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f19.f64 + ctx.f13.f64));
	// fmadds f12,f8,f0,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,4760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4760);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lfs f0,1400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1400);
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// stfs f10,2356(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2356, temp.u32);
	// lfs f10,2056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2056);
	ctx.f10.f64 = double(temp.f32);
	// fmr f11,f19
	ctx.f11.f64 = ctx.f19.f64;
	// lfs f19,1952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1952);
	ctx.f19.f64 = double(temp.f32);
	// stfs f6,1952(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1952, temp.u32);
	// lfs f8,4696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4696);
	ctx.f8.f64 = double(temp.f32);
	// lfs f6,-19300(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19300);
	ctx.f6.f64 = double(temp.f32);
	// stfs f8,1144(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// fmuls f8,f8,f23
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// stfs f6,884(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 884, temp.u32);
	// stfs f2,1920(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1920, temp.u32);
	// lfs f2,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f0,f10,f5,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f5.f64 + ctx.f0.f64));
	// lfs f10,4912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4912);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f26,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// stfs f31,2652(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2652, temp.u32);
	// fnmsubs f13,f4,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f4,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f24,f19
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// lfs f31,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f31.f64 = double(temp.f32);
	// stfs f28,2168(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2168, temp.u32);
	// lfs f5,-19304(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19304);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f8,f8,f4
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfs f23,3092(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3092, temp.u32);
	// stfs f5,328(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 328, temp.u32);
	// lfs f5,3028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3028);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,1960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1960);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f2,f5,f19
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f19.f64));
	// fmadds f0,f1,f9,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f9.f64 + ctx.f0.f64));
	// lfs f9,2208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2208);
	ctx.f9.f64 = double(temp.f32);
	// stfs f0,5368(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5368, temp.u32);
	// fmuls f6,f10,f9
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f15,f31,f11
	ctx.f11.f64 = double(float(ctx.f15.f64 * ctx.f31.f64 + ctx.f11.f64));
	// fnmsubs f12,f20,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f20.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f1,4752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4752);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f0,f1,f0,f29
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f29.f64));
	// lfs f29,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f21,f31,f13
	ctx.f13.f64 = double(float(-(ctx.f21.f64 * ctx.f31.f64 - ctx.f13.f64)));
	// lfs f31,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f7,f3,f31,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f31.f64 + ctx.f7.f64));
	// lfs f21,1968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1968);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f8,f26,f28,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f28.f64 + ctx.f8.f64));
	// lfs f1,4872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4872);
	ctx.f1.f64 = double(temp.f32);
	// lfs f28,1736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1736);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f1,f1,f4
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// fmuls f6,f6,f29
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f29.f64));
	// lfs f29,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f29.f64 = double(temp.f32);
	// lfs f3,2128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2128);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f12,f30,f29,f12
	ctx.f12.f64 = double(float(-(ctx.f30.f64 * ctx.f29.f64 - ctx.f12.f64)));
	// lfs f30,3496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3496);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,3488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3488);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f15,f30,f21
	ctx.f15.f64 = double(float(ctx.f30.f64 * ctx.f21.f64));
	// lfs f23,1896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1896);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f0,f0,f14
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f25,3436(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3436, temp.u32);
	// fmuls f31,f28,f29
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// stfs f18,2596(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2596, temp.u32);
	// fmuls f25,f3,f19
	ctx.f25.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// stfs f24,2120(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2120, temp.u32);
	// fmuls f20,f14,f23
	ctx.f20.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// stfs f15,1416(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1416, temp.u32);
	// lfs f26,1848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1848);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,2088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2088);
	ctx.f24.f64 = double(temp.f32);
	// lfs f18,1720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1720);
	ctx.f18.f64 = double(temp.f32);
	// lfs f15,3480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3480);
	ctx.f15.f64 = double(temp.f32);
	// stfs f20,2208(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2208, temp.u32);
	// fmuls f20,f30,f18
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// stfs f19,1968(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1968, temp.u32);
	// fmadds f0,f11,f27,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f27.f64 + ctx.f0.f64));
	// stfs f20,2176(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2176, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f19,2192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2192);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f20.f64 = double(temp.f32);
	// stfs f31,2088(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2088, temp.u32);
	// fmuls f20,f19,f20
	ctx.f20.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lfs f31,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f31.f64 = double(temp.f32);
	// lfs f19,3472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3472);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// stfs f31,1960(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1960, temp.u32);
	// fmuls f19,f19,f24
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f24.f64));
	// lfs f31,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f31.f64 = double(temp.f32);
	// stfs f19,1400(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1400, temp.u32);
	// fmsubs f6,f1,f31,f6
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f31.f64 - ctx.f6.f64));
	// lfs f19,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f31,f19,f16
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f16.f64));
	// lfs f16,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f12,f17,f16,f12
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f16.f64 - ctx.f12.f64)));
	// fmadds f13,f22,f19,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f19.f64 + ctx.f13.f64));
	// lfs f19,1456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1456);
	ctx.f19.f64 = double(temp.f32);
	// stfs f7,1456(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1456, temp.u32);
	// fmr f7,f16
	ctx.f7.f64 = ctx.f16.f64;
	// stfs f9,2372(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2372, temp.u32);
	// fmuls f9,f10,f19
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// stfs f5,1856(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1856, temp.u32);
	// stfs f10,5072(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5072, temp.u32);
	// lfs f10,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,2080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2080);
	ctx.f5.f64 = double(temp.f32);
	// lfs f22,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f22.f64 = double(temp.f32);
	// lfs f1,1448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1448);
	ctx.f1.f64 = double(temp.f32);
	// fmsubs f11,f2,f22,f8
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f22.f64 - ctx.f8.f64));
	// fnmsubs f12,f5,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f10,1440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1440);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,1464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1464);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f8,f31,f27
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// fnmsubs f13,f10,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f7,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f25,f7,f6
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f7.f64 + ctx.f6.f64));
	// stfs f4,1448(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1448, temp.u32);
	// fmr f6,f22
	ctx.f6.f64 = ctx.f22.f64;
	// lfs f10,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f10.f64 = double(temp.f32);
	// fmr f4,f16
	ctx.f4.f64 = ctx.f16.f64;
	// lfs f31,1952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1952);
	ctx.f31.f64 = double(temp.f32);
	// stfs f3,2008(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2008, temp.u32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f29,2136(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2136, temp.u32);
	// stfs f26,2056(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2056, temp.u32);
	// stfs f23,1904(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1904, temp.u32);
	// fmadds f12,f31,f10,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f10.f64 + ctx.f12.f64));
	// lfs f31,1424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1424);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f29.f64 = double(temp.f32);
	// lfs f3,3464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3464);
	ctx.f3.f64 = double(temp.f32);
	// lfs f26,1472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1472);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f3,f14,f3
	ctx.f3.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// fmadds f6,f5,f6,f20
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f20.f64));
	// lfs f25,1968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1968);
	ctx.f25.f64 = double(temp.f32);
	// fmr f5,f16
	ctx.f5.f64 = ctx.f16.f64;
	// lfs f23,3020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3020);
	ctx.f23.f64 = double(temp.f32);
	// stfs f28,1896(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1896, temp.u32);
	// fnmsubs f0,f15,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// stfs f24,2128(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2128, temp.u32);
	// fmuls f22,f29,f26
	ctx.f22.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// stfs f21,1992(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1992, temp.u32);
	// fmuls f20,f23,f19
	ctx.f20.f64 = double(float(ctx.f23.f64 * ctx.f19.f64));
	// stfs f18,2192(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2192, temp.u32);
	// lfs f4,1456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1456);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,1840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1840);
	ctx.f2.f64 = double(temp.f32);
	// lfs f10,3988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3988);
	ctx.f10.f64 = double(temp.f32);
	// lfs f21,2332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2332);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f6,f6,f30
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// fmadds f13,f31,f5,f13
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f5.f64 + ctx.f13.f64));
	// lfs f5,4812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4812);
	ctx.f5.f64 = double(temp.f32);
	// lfs f31,2500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2500);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f29,f5
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// fmuls f24,f31,f25
	ctx.f24.f64 = double(float(ctx.f31.f64 * ctx.f25.f64));
	// stfs f8,1952(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1952, temp.u32);
	// fadds f12,f4,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 + ctx.f12.f64));
	// lfs f8,2088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2088);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f18,f2,f18
	ctx.f18.f64 = double(float(ctx.f2.f64 * ctx.f18.f64));
	// lfs f16,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f16.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fnmsubs f11,f8,f16,f11
	ctx.f11.f64 = double(float(-(ctx.f8.f64 * ctx.f16.f64 - ctx.f11.f64)));
	// lfs f15,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,1960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1960);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f16,f15
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f15.f64));
	// lfs f15,3456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3456);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f15,1968(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1968, temp.u32);
	// lfs f17,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f17.f64 = double(temp.f32);
	// lfs f4,4632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4632);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f17,f10,f17
	ctx.f17.f64 = double(float(ctx.f10.f64 * ctx.f17.f64));
	// lfs f15,4688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4688);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f4,f14,f4
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f4.f64));
	// stfs f20,1960(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1960, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// stfs f22,1456(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1456, temp.u32);
	// lfs f20,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,2208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2208);
	ctx.f22.f64 = double(temp.f32);
	// stfs f15,2080(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2080, temp.u32);
	// fmuls f22,f22,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// stfs f4,1464(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1464, temp.u32);
	// stfs f17,1472(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1472, temp.u32);
	// lfs f17,1408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1408);
	ctx.f17.f64 = double(temp.f32);
	// lfs f20,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f15.f64 = double(temp.f32);
	// lfs f4,1416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1416);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f17,f15,f13
	ctx.f13.f64 = double(float(-(ctx.f17.f64 * ctx.f15.f64 - ctx.f13.f64)));
	// fmuls f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f20.f64));
	// lfs f15,2176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2176);
	ctx.f15.f64 = double(temp.f32);
	// lfs f20,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f20.f64 = double(temp.f32);
	// stfs f2,2332(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2332, temp.u32);
	// fmuls f20,f15,f20
	ctx.f20.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// lfs f2,1400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1400);
	ctx.f2.f64 = double(temp.f32);
	// lfs f8,4640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4640);
	ctx.f8.f64 = double(temp.f32);
	// lfs f15,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f8,f14,f8
	ctx.f8.f64 = double(float(ctx.f14.f64 * ctx.f8.f64));
	// stfs f6,2088(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2088, temp.u32);
	// fmadds f0,f2,f15,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f15.f64 + ctx.f0.f64));
	// lfs f6,2144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2144);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f14,f6
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// stfs f6,2208(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2208, temp.u32);
	// lfs f6,4680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4680);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f14,f6
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// stfs f5,1840(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1840, temp.u32);
	// stfs f6,1440(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1440, temp.u32);
	// lfs f6,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f9,f9,f6,f7
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f6.f64 - ctx.f7.f64)));
	// stfs f12,5372(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5372, temp.u32);
	// lfs f7,1920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1920);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f0,f8,f5,f0
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// lfs f5,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f5.f64 = double(temp.f32);
	// lfs f12,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f13,f7,f5,f13
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f5.f64 + ctx.f13.f64));
	// fnmsubs f12,f1,f12,f11
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f12.f64 - ctx.f11.f64)));
	// lfs f17,1756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1756);
	ctx.f17.f64 = double(temp.f32);
	// lfs f11,1764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1764);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f6,f30,f17
	ctx.f6.f64 = double(float(ctx.f30.f64 * ctx.f17.f64));
	// lfs f1,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f7.f64 = double(temp.f32);
	// stfs f10,2588(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2588, temp.u32);
	// fmuls f10,f21,f25
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f25.f64));
	// stfs f3,1144(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// fmuls f3,f30,f11
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f11.f64));
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f7,f1,f7
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f8,1912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1912);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,4608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4608);
	ctx.f5.f64 = double(temp.f32);
	// stfs f27,1416(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1416, temp.u32);
	// stfs f31,1848(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1848, temp.u32);
	// stfs f26,3084(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3084, temp.u32);
	// stfs f14,1424(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1424, temp.u32);
	// stfs f19,1408(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1408, temp.u32);
	// stfs f23,1720(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1720, temp.u32);
	// stfs f21,2032(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2032, temp.u32);
	// stfs f25,2484(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2484, temp.u32);
	// fmr f27,f15
	ctx.f27.f64 = ctx.f15.f64;
	// lfs f1,1448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1448);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,4672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4672);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f28,f2,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f2.f64 + ctx.f12.f64));
	// lfs f2,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f5,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fmuls f31,f29,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// lfs f28,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f9,f4,f2,f9
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 + ctx.f9.f64));
	// stfs f1,1448(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1448, temp.u32);
	// lfs f26,2136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2136);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f2,f8,f28
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f28.f64));
	// lfs f25,4864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4864);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f7,f7,f26
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// lfs f23,4840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4840);
	ctx.f23.f64 = double(temp.f32);
	// lfs f1,1472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1472);
	ctx.f1.f64 = double(temp.f32);
	// lfs f14,2652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2652);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f0,f22,f27,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f27.f64 - ctx.f0.f64)));
	// lfs f27,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f27,f18,f27,f16
	ctx.f27.f64 = double(float(ctx.f18.f64 * ctx.f27.f64 + ctx.f16.f64));
	// lfs f16,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f16.f64 = double(temp.f32);
	// lfs f18,3868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3868);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f1,f23,f16,f1
	ctx.f1.f64 = double(float(ctx.f23.f64 * ctx.f16.f64 + ctx.f1.f64));
	// fmuls f22,f25,f26
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// stfs f22,1756(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1756, temp.u32);
	// lfs f16,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f22,f18,f26
	ctx.f22.f64 = double(float(ctx.f18.f64 * ctx.f26.f64));
	// fmadds f13,f14,f16,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f16.f64 + ctx.f13.f64));
	// stfs f22,1764(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1764, temp.u32);
	// lfs f22,4592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4592);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f5,f29,f22,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f22.f64 + ctx.f5.f64));
	// fmuls f31,f31,f16
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f16.f64));
	// lfs f22,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f9,f20,f22,f9
	ctx.f9.f64 = double(float(-(ctx.f20.f64 * ctx.f22.f64 - ctx.f9.f64)));
	// fmuls f2,f2,f16
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// lfs f16,1464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1464);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f22.f64 = double(temp.f32);
	// lfs f28,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f0,f16,f22,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f22.f64 - ctx.f0.f64)));
	// lfs f4,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f12,f24,f28,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f28.f64 + ctx.f12.f64));
	// fmuls f4,f3,f4
	ctx.f4.f64 = double(float(ctx.f3.f64 * ctx.f4.f64));
	// stfs f11,1912(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1912, temp.u32);
	// stfs f8,1920(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1920, temp.u32);
	// lfs f11,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f11.f64 = double(temp.f32);
	// lfs f8,1968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1968);
	ctx.f8.f64 = double(temp.f32);
	// lfs f20,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,1456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1456);
	ctx.f16.f64 = double(temp.f32);
	// stfs f13,1464(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1464, temp.u32);
	// fmadds f0,f8,f11,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f11.f64 + ctx.f0.f64));
	// lfs f11,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f12,f16,f22,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f22.f64 + ctx.f12.f64));
	// lfs f21,4600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4600);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f20.f64));
	// lfs f28,3532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3532);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,2452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2452);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f11,f5,f11,f31
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f31.f64));
	// lfs f14,3636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3636);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f19,f29,f21
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f21.f64));
	// lfs f20,3956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3956);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f28,f26
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f22,5156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5156);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f15,f24,f26
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// lfs f13,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f20,f20,f22
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// lfs f16,-19308(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19308);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f13,f13,f14
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f14.f64));
	// lfs f8,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f5.f64 = double(temp.f32);
	// stfs f25,3636(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3636, temp.u32);
	// stfs f23,2136(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2136, temp.u32);
	// stfs f27,1472(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1472, temp.u32);
	// stfs f16,380(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 380, temp.u32);
	// stfs f21,1400(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1400, temp.u32);
	// stfs f28,2176(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2176, temp.u32);
	// stfs f3,2652(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2652, temp.u32);
	// stfs f24,2500(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2500, temp.u32);
	// stfs f18,2468(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2468, temp.u32);
	// lfs f3,2088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2088);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f7,f7,f5,f4
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f5.f64 + ctx.f4.f64));
	// fmadds f9,f3,f8,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f8,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,1960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1960);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f12,f4,f8,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f8.f64 - ctx.f12.f64)));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f3,2724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2724);
	ctx.f3.f64 = double(temp.f32);
	// lfs f4,3460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3460);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// fadds f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f3.f64));
	// lfs f3,2208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2208);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f5,f3,f5,f0
	ctx.f5.f64 = double(float(-(ctx.f3.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// lfs f0,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f0.f64 = double(temp.f32);
	// lfs f3,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f3.f64 = double(temp.f32);
	// fmsubs f7,f19,f0,f7
	ctx.f7.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 - ctx.f7.f64));
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f9,f1,f29,f9
	ctx.f9.f64 = double(float(ctx.f1.f64 * ctx.f29.f64 + ctx.f9.f64));
	// fmr f21,f0
	ctx.f21.f64 = ctx.f0.f64;
	// lfs f24,1952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1952);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f12,f10,f3,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f3.f64 + ctx.f12.f64));
	// lfs f10,2024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2024);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f11,f17,f2,f11
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f2.f64 + ctx.f11.f64));
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// lfs f23,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,1756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1756);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f23,f10,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f10.f64 + ctx.f13.f64));
	// lfs f25,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// fnmsubs f5,f24,f2,f5
	ctx.f5.f64 = double(float(-(ctx.f24.f64 * ctx.f2.f64 - ctx.f5.f64)));
	// lfs f1,2732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2732);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f25,f20,f25
	ctx.f25.f64 = double(float(ctx.f20.f64 * ctx.f25.f64));
	// lfs f31,3220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3220);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f7,f15,f0,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f2,4584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4584);
	ctx.f2.f64 = double(temp.f32);
	// lfs f24,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f3,f1,f26
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f26.f64));
	// lfs f23,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f9,f6,f21,f9
	ctx.f9.f64 = double(float(-(ctx.f6.f64 * ctx.f21.f64 - ctx.f9.f64)));
	// fnmsubs f12,f18,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f20,1764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1764);
	ctx.f20.f64 = double(temp.f32);
	// lfs f0,2612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2612);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f11,f20,f23,f11
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f23.f64 - ctx.f11.f64)));
	// lfs f18,1780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1780);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f8,f2,f24,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f24.f64 - ctx.f8.f64));
	// fmuls f15,f31,f0
	ctx.f15.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// stfs f9,1780(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1780, temp.u32);
	// stfs f1,1456(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1456, temp.u32);
	// fmuls f18,f30,f18
	ctx.f18.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// stfs f10,2460(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2460, temp.u32);
	// lfs f28,2740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2740);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,3644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3644);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f30,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// lfs f2,2748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2748);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f27,f30,f27
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// lfs f24,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,3228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3228);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f24,f2
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f2.f64));
	// lfs f6,4260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4260);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f23,f30,f23
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// lfs f21,4576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4576);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f6,f6,f22
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f22.f64));
	// lfs f20,1772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1772);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f21,f26
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f19,4568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4568);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f30,f20
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f20.f64));
	// lfs f17,4536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4536);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f22,f19
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// lfs f16,4524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4524);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f22,f17
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f17.f64));
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f31,f16
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f16.f64));
	// lfs f9,3816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3816);
	ctx.f9.f64 = double(temp.f32);
	// lfs f1,2080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2080);
	ctx.f1.f64 = double(temp.f32);
	// lfs f10,1448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1448);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f0,f1,f0,f5
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f5.f64)));
	// stfs f14,2208(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2208, temp.u32);
	// fmuls f9,f9,f10
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// lfs f5,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f13,f13,f22
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// lfs f1,1440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1440);
	ctx.f1.f64 = double(temp.f32);
	// fmsubs f8,f8,f22,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f22.f64 - ctx.f25.f64));
	// fmadds f0,f1,f5,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f0.f64));
	// lfs f14,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,3808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3808);
	ctx.f16.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fmuls f16,f16,f14
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// lfs f1,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f5.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f13,3228(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3228, temp.u32);
	// lfs f14,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f14.f64 = double(temp.f32);
	// lfs f13,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f11,f27,f14,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f14.f64 - ctx.f11.f64)));
	// stfs f30,1756(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1756, temp.u32);
	// fnmsubs f13,f28,f13,f7
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f13.f64 - ctx.f7.f64)));
	// lfs f30,1904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1904);
	ctx.f30.f64 = double(temp.f32);
	// lfs f25,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f5,f5,f1,f0
	ctx.f5.f64 = double(float(-(ctx.f5.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// lfs f27,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f27.f64 = double(temp.f32);
	// fmr f0,f14
	ctx.f0.f64 = ctx.f14.f64;
	// lfs f28,1472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1472);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f25,f30
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// lfs f7,1424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1424);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f8,f6,f27,f8
	ctx.f8.f64 = double(float(-(ctx.f6.f64 * ctx.f27.f64 - ctx.f8.f64)));
	// lfs f14,3932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3932);
	ctx.f14.f64 = double(temp.f32);
	// lfs f6,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f6.f64 = double(temp.f32);
	// lfs f27,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f6,f14,f6
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// lfs f1,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f11,f23,f1,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f1.f64 + ctx.f11.f64));
	// lfs f1,3076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3076);
	ctx.f1.f64 = double(temp.f32);
	// stfs f9,1772(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1772, temp.u32);
	// lfs f9,1464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1464);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f5,f28,f7,f5
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f7.f64 + ctx.f5.f64));
	// lfs f28,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f13,f4,f0,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f4,3732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3732);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f27,f25,f27,f16
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f27.f64 + ctx.f16.f64));
	// lfs f16,1416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1416);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f29,f4
	ctx.f14.f64 = double(float(ctx.f29.f64 * ctx.f4.f64));
	// stfs f14,3220(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3220, temp.u32);
	// fnmsubs f12,f3,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f3,1788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1788);
	ctx.f3.f64 = double(temp.f32);
	// lfs f14,4512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4512);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f28,f24,f16
	ctx.f28.f64 = double(float(ctx.f24.f64 * ctx.f16.f64));
	// stfs f28,2732(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2732, temp.u32);
	// fmuls f14,f14,f10
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f10.f64));
	// fmuls f28,f1,f3
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// stfs f14,2724(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2724, temp.u32);
	// stfs f28,1764(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1764, temp.u32);
	// lfs f25,4788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4788);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,4384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4384);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f23,f25,f26
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f28,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f14,f22,f14
	ctx.f14.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// stfs f2,3460(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3460, temp.u32);
	// fmuls f28,f28,f2
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f2,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f2.f64 = double(temp.f32);
	// fadds f9,f9,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f5.f64));
	// stfs f23,2740(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2740, temp.u32);
	// fmadds f13,f21,f2,f13
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f2.f64 + ctx.f13.f64));
	// stfs f3,3644(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3644, temp.u32);
	// stfs f14,2748(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2748, temp.u32);
	// lfs f0,-19312(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19312);
	ctx.f0.f64 = double(temp.f32);
	// lfs f24,4748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4748);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,4804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4804);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f22,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// lfs f3,4084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4084);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f23,f23,f0
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// lfs f14,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f3,f22,f3
	ctx.f3.f64 = double(float(ctx.f22.f64 * ctx.f3.f64));
	// lfs f5,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f11,f20,f14,f11
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f14.f64 - ctx.f11.f64)));
	// stfs f31,1788(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1788, temp.u32);
	// fmadds f8,f19,f5,f8
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f5.f64 + ctx.f8.f64));
	// lfs f31,4916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4916);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f2.f64 = double(temp.f32);
	// lfs f21,-19316(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19316);
	ctx.f21.f64 = double(temp.f32);
	// stfs f7,1968(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1968, temp.u32);
	// fmadds f6,f31,f2,f6
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f2.f64 + ctx.f6.f64));
	// stfs f0,772(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 772, temp.u32);
	// fmr f7,f14
	ctx.f7.f64 = ctx.f14.f64;
	// fmuls f2,f27,f22
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// lfs f5,3736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3736);
	ctx.f5.f64 = double(temp.f32);
	// fmr f0,f21
	ctx.f0.f64 = ctx.f21.f64;
	// stfs f10,1448(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1448, temp.u32);
	// fmuls f10,f22,f5
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// lfs f5,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f5.f64 = double(temp.f32);
	// stfs f9,5376(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5376, temp.u32);
	// lfs f9,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f9.f64 = double(temp.f32);
	// stfs f1,1952(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1952, temp.u32);
	// fnmsubs f13,f15,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f15.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f1,1780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1780);
	ctx.f1.f64 = double(temp.f32);
	// fadds f12,f1,f12
	ctx.f12.f64 = double(float(ctx.f1.f64 + ctx.f12.f64));
	// lfs f9,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f9,f17,f9,f8
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f9.f64 - ctx.f8.f64)));
	// stfs f29,1464(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1464, temp.u32);
	// fmadds f11,f18,f7,f11
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f7.f64 + ctx.f11.f64));
	// stfs f30,2088(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2088, temp.u32);
	// fmsubs f7,f6,f22,f2
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f22.f64 - ctx.f2.f64));
	// lfs f6,4876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4876);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f2,4644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4644);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f0,f24,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f30,4952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4952);
	ctx.f30.f64 = double(temp.f32);
	// fmr f5,f21
	ctx.f5.f64 = ctx.f21.f64;
	// lfs f29,1788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1788);
	ctx.f29.f64 = double(temp.f32);
	// stfs f21,712(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 712, temp.u32);
	// lfs f20,4756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4756);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,4420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4420);
	ctx.f21.f64 = double(temp.f32);
	// fadds f21,f20,f21
	ctx.f21.f64 = double(float(ctx.f20.f64 + ctx.f21.f64));
	// lfs f19,4412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4412);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f20.f64 = double(temp.f32);
	// lfs f1,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f3,f3,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// lfs f1,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f1.f64 = double(temp.f32);
	// lfs f18,5004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5004);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f6,f19,f20,f6
	ctx.f6.f64 = double(float(ctx.f19.f64 * ctx.f20.f64 + ctx.f6.f64));
	// fmsubs f0,f10,f1,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 - ctx.f0.f64));
	// lfs f1,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f1.f64 = double(temp.f32);
	// lfs f10,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f5,f2,f5,f23
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 + ctx.f23.f64));
	// fnmsubs f13,f29,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// lfs f29,1772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1772);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f10,f30,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f30,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f11,f29,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f1,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,3228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3228);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f22,f18
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// fmadds f9,f29,f1,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f9.f64));
	// lfs f30,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f30.f64 = double(temp.f32);
	// lfs f1,2748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2748);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f7,f1,f30,f7
	ctx.f7.f64 = double(float(-(ctx.f1.f64 * ctx.f30.f64 - ctx.f7.f64)));
	// stfs f25,1904(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1904, temp.u32);
	// lfs f8,2444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2444);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,2756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2756);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f5,f5,f22,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f22.f64 - ctx.f3.f64));
	// lfs f1,3388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3388);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,2572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2572);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f8,f2
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// lfs f29,3644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3644);
	ctx.f29.f64 = double(temp.f32);
	// lfs f3,3468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3468);
	ctx.f3.f64 = double(temp.f32);
	// lfs f25,2764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2764);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,4068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4068);
	ctx.f24.f64 = double(temp.f32);
	// fadds f23,f25,f3
	ctx.f23.f64 = double(float(ctx.f25.f64 + ctx.f3.f64));
	// lfs f20,4404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4404);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f24,f22,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// lfs f18,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f22,f20
	ctx.f20.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// lfs f15,2740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2740);
	ctx.f15.f64 = double(temp.f32);
	// stfs f4,1960(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1960, temp.u32);
	// fmuls f4,f28,f16
	ctx.f4.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// stfs f26,1472(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1472, temp.u32);
	// fmuls f28,f1,f29
	ctx.f28.f64 = double(float(ctx.f1.f64 * ctx.f29.f64));
	// lfs f27,1936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1936);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f22,f30
	ctx.f26.f64 = double(float(ctx.f22.f64 * ctx.f30.f64));
	// lfs f17,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f13,f15,f18,f13
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f18.f64 + ctx.f13.f64));
	// lfs f15,3220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3220);
	ctx.f15.f64 = double(temp.f32);
	// fadds f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fnmsubs f11,f15,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// lfs f18,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f18.f64 = double(temp.f32);
	// lfs f15,2732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2732);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f6,f6,f22,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f22.f64 + ctx.f5.f64));
	// fmadds f9,f15,f18,f9
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f18.f64 + ctx.f9.f64));
	// lfs f18,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f7,f4,f18,f7
	ctx.f7.f64 = double(float(-(ctx.f4.f64 * ctx.f18.f64 - ctx.f7.f64)));
	// stfs f8,2612(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2612, temp.u32);
	// lfs f8,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f8.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f1,2748(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2748, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f5,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,1764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1764);
	ctx.f1.f64 = double(temp.f32);
	// lfs f4,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f4.f64 = double(temp.f32);
	// lfs f15,3460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3460);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f0,f31,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f31.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// lfs f17,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f17.f64 = double(temp.f32);
	// lfs f14,3276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3276);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f17,f17,f15
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f15.f64));
	// fnmsubs f9,f1,f5,f9
	ctx.f9.f64 = double(float(-(ctx.f1.f64 * ctx.f5.f64 - ctx.f9.f64)));
	// lfs f1,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f24,f8,f7
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f8.f64 - ctx.f7.f64)));
	// lfs f5,4740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4740);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f5,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// lfs f1,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f10,f14,f4,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f4.f64 + ctx.f10.f64));
	// stfs f3,3460(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3460, temp.u32);
	// lfs f8,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,2724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2724);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,3924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3924);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f11,f4,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// fadds f3,f21,f3
	ctx.f3.f64 = double(float(ctx.f21.f64 + ctx.f3.f64));
	// stfs f25,1440(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1440, temp.u32);
	// lfs f8,2772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2772);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f25,f17,f16
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// lfs f14,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f12,f20,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f20.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f20,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f9,f19,f20,f9
	ctx.f9.f64 = double(float(-(ctx.f19.f64 * ctx.f20.f64 - ctx.f9.f64)));
	// lfs f20,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f26,f26,f20
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f20.f64));
	// lfs f20,4580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4580);
	ctx.f20.f64 = double(temp.f32);
	// stfs f30,3228(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3228, temp.u32);
	// fmadds f10,f20,f14,f10
	ctx.f10.f64 = double(float(ctx.f20.f64 * ctx.f14.f64 + ctx.f10.f64));
	// lfs f30,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f30.f64 = double(temp.f32);
	// lfs f17,3968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3968);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f0,f28,f30,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// stfs f9,2772(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2772, temp.u32);
	// fmuls f19,f22,f17
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f17.f64));
	// lfs f9,4544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4544);
	ctx.f9.f64 = double(temp.f32);
	// fadds f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// lfs f15,3236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3236);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f9,f22,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// lfs f14,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f18,f27,f15
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f15.f64));
	// stfs f29,2572(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2572, temp.u32);
	// fmadds f5,f3,f14,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f14.f64 + ctx.f5.f64));
	// lfs f7,1392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1392);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f31,f8,f15
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f15.f64));
	// lfs f30,3588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3588);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f7,f2
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f2.f64));
	// lfs f1,744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f28,f30,f15
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// lfs f29,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f29.f64 = double(temp.f32);
	// lfs f24,2844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2844);
	ctx.f24.f64 = double(temp.f32);
	// fadds f21,f29,f1
	ctx.f21.f64 = double(float(ctx.f29.f64 + ctx.f1.f64));
	// lfs f17,4388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4388);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f24,f22,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64));
	// lfs f20,3100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3100);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f17,f17,f22
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f22.f64));
	// stfs f27,3220(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3220, temp.u32);
	// fmuls f23,f23,f20
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f20.f64));
	// stfs f9,3236(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3236, temp.u32);
	// lfs f11,2780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2780);
	ctx.f11.f64 = double(temp.f32);
	// lfs f27,4188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4188);
	ctx.f27.f64 = double(temp.f32);
	// lfs f9,3936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3936);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f3.f64 = double(temp.f32);
	// lfs f14,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f6,f18,f3,f6
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f3.f64 + ctx.f6.f64));
	// fmadds f0,f4,f14,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f14.f64 + ctx.f0.f64));
	// stfs f13,5380(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5380, temp.u32);
	// fmr f4,f14
	ctx.f4.f64 = ctx.f14.f64;
	// lfs f13,-19324(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19324);
	ctx.f13.f64 = double(temp.f32);
	// lfs f3,2476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2476);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f9,f22,f9
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// stfs f13,956(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 956, temp.u32);
	// fmuls f18,f3,f11
	ctx.f18.f64 = double(float(ctx.f3.f64 * ctx.f11.f64));
	// stfs f11,1144(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f1,2756(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2756, temp.u32);
	// lfs f1,4708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4708);
	ctx.f1.f64 = double(temp.f32);
	// lfs f14,3396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3396);
	ctx.f14.f64 = double(temp.f32);
	// stfs f30,2740(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2740, temp.u32);
	// fmadds f13,f21,f14,f23
	ctx.f13.f64 = double(float(ctx.f21.f64 * ctx.f14.f64 + ctx.f23.f64));
	// stfs f29,3396(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3396, temp.u32);
	// lfs f30,3132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3132);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f31,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f4,2184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2184);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f31,f4
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// stfs f4,3588(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3588, temp.u32);
	// lfs f4,1796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1796);
	ctx.f4.f64 = double(temp.f32);
	// stfs f31,1796(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1796, temp.u32);
	// lfs f31,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f11,f4,f31
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// lfs f31,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f31,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f10,f30,f31,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f31.f64 + ctx.f10.f64));
	// fnmsubs f6,f17,f29,f6
	ctx.f6.f64 = double(float(-(ctx.f17.f64 * ctx.f29.f64 - ctx.f6.f64)));
	// stfs f8,3644(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3644, temp.u32);
	// lfs f8,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f8.f64 = double(temp.f32);
	// stfs f7,1780(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1780, temp.u32);
	// fmadds f0,f24,f8,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f7,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f7.f64 = double(temp.f32);
	// stfs f9,2764(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2764, temp.u32);
	// fmuls f8,f18,f7
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f7.f64));
	// stfs f19,2780(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2780, temp.u32);
	// fmuls f9,f14,f27
	ctx.f9.f64 = double(float(ctx.f14.f64 * ctx.f27.f64));
	// lfs f19,-19320(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19320);
	ctx.f19.f64 = double(temp.f32);
	// fmr f29,f19
	ctx.f29.f64 = ctx.f19.f64;
	// lfs f30,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,5044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5044);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f31,f30
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// lfs f30,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f10,f10,f22,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f22.f64 + ctx.f6.f64));
	// lfs f6,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f12,f28,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// lfs f28,4180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4180);
	ctx.f28.f64 = double(temp.f32);
	// lfs f6,4988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4988);
	ctx.f6.f64 = double(temp.f32);
	// fadds f6,f28,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 + ctx.f6.f64));
	// lfs f28,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f28.f64 = double(temp.f32);
	// stfs f19,976(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 976, temp.u32);
	// fmsubs f9,f9,f30,f8
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f30.f64 - ctx.f8.f64));
	// lfs f8,3244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3244);
	ctx.f8.f64 = double(temp.f32);
	// stfs f16,1772(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1772, temp.u32);
	// fmadds f11,f8,f28,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f28.f64 + ctx.f11.f64));
	// stfs f27,1788(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1788, temp.u32);
	// fnmsubs f0,f25,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f7,2436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2436);
	ctx.f7.f64 = double(temp.f32);
	// lfs f30,3068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3068);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f5,f5,f7
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// lfs f27,532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 532);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f25,f30,f2
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// lfs f21,1480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1480);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f23,f27,f3
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// lfs f19,2700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2700);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f18,f20,f21
	ctx.f18.f64 = double(float(ctx.f20.f64 * ctx.f21.f64));
	// lfs f17,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f7,f19
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f19.f64));
	// lfs f16,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f17,f7
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// lfs f29,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f1,f16
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// lfs f24,1804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1804);
	ctx.f24.f64 = double(temp.f32);
	// lfs f28,1488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1488);
	ctx.f28.f64 = double(temp.f32);
	// stfs f15,2024(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2024, temp.u32);
	// fmadds f12,f26,f29,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f12.f64));
	// stfs f7,1804(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1804, temp.u32);
	// lfs f26,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f15,f20,f28
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f28.f64));
	// lfs f7,2780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2780);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f0,f7,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// lfs f16,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f13,f16,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f16.f64 + ctx.f5.f64));
	// stfs f22,3468(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3468, temp.u32);
	// lfs f22,4276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4276);
	ctx.f22.f64 = double(temp.f32);
	// lfs f5,4636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4636);
	ctx.f5.f64 = double(temp.f32);
	// stfs f8,3244(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3244, temp.u32);
	// fadds f5,f22,f5
	ctx.f5.f64 = double(float(ctx.f22.f64 + ctx.f5.f64));
	// lfs f8,2772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2772);
	ctx.f8.f64 = double(temp.f32);
	// lfs f26,1796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1796);
	ctx.f26.f64 = double(temp.f32);
	// lfs f22,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f22.f64 = double(temp.f32);
	// fadds f12,f8,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// stfs f4,3548(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3548, temp.u32);
	// fnmsubs f9,f26,f22,f9
	ctx.f9.f64 = double(float(-(ctx.f26.f64 * ctx.f22.f64 - ctx.f9.f64)));
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,3236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3236);
	ctx.f4.f64 = double(temp.f32);
	// lfs f16,2160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2160);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f0,f4,f8,f0
	ctx.f0.f64 = double(float(-(ctx.f4.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f29,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,1496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1496);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f16,f29
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// stfs f29,1496(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1496, temp.u32);
	// stfs f20,1480(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1480, temp.u32);
	// lfs f20,1504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1504);
	ctx.f20.f64 = double(temp.f32);
	// lfs f29,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f29.f64 = double(temp.f32);
	// fadds f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f10.f64));
	// stfs f26,1488(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1488, temp.u32);
	// fadds f26,f24,f26
	ctx.f26.f64 = double(float(ctx.f24.f64 + ctx.f26.f64));
	// stfs f24,3388(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3388, temp.u32);
	// fmsubs f31,f20,f29,f31
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f29.f64 - ctx.f31.f64));
	// lfs f24,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f24.f64 = double(temp.f32);
	// stfs f5,1504(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1504, temp.u32);
	// fmsubs f13,f23,f24,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f24.f64 - ctx.f13.f64));
	// lfs f5,-31048(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -31048);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f4.f64 = double(temp.f32);
	// stfs f5,984(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 984, temp.u32);
	// fmadds f10,f19,f4,f9
	ctx.f10.f64 = double(float(ctx.f19.f64 * ctx.f4.f64 + ctx.f9.f64));
	// lfs f23,2764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2764);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f5,f17,f16
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// lfs f22,3588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3588);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f0,f23,f8,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f7,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f7.f64 = double(temp.f32);
	// lfs f23,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f7,f7,f22
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f22.f64));
	// lfs f8,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f8,f8,f23
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// stfs f21,2080(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2080, temp.u32);
	// lfs f21,3452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3452);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f23.f64 = double(temp.f32);
	// stfs f2,1392(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1392, temp.u32);
	// fmadds f2,f26,f14,f18
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f14.f64 + ctx.f18.f64));
	// fnmsubs f31,f21,f23,f31
	ctx.f31.f64 = double(float(-(ctx.f21.f64 * ctx.f23.f64 - ctx.f31.f64)));
	// lfs f26,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f26.f64 = double(temp.f32);
	// lfs f21,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f6,f6,f26,f1
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64 + ctx.f1.f64));
	// lfs f19,4012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4012);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f10,f5,f21,f10
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f21.f64 - ctx.f10.f64)));
	// lfs f23,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f23.f64 = double(temp.f32);
	// stfs f27,2732(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2732, temp.u32);
	// fmuls f23,f19,f23
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// stfs f30,1796(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1796, temp.u32);
	// fmuls f7,f7,f21
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f21.f64));
	// lfs f4,572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,1844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1844);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f29,f4,f3,f15
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f3.f64 + ctx.f15.f64));
	// lfs f27,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f27.f64 = double(temp.f32);
	// lfs f1,2428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2428);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f27,f11,f27
	ctx.f27.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// lfs f26,1852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1852);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f24,f1,f28
	ctx.f24.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f5,2996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2996);
	ctx.f5.f64 = double(temp.f32);
	// stfs f22,3100(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3100, temp.u32);
	// fadds f22,f30,f26
	ctx.f22.f64 = double(float(ctx.f30.f64 + ctx.f26.f64));
	// lfs f9,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f19,f14,f5
	ctx.f19.f64 = double(float(ctx.f14.f64 * ctx.f5.f64));
	// lfs f18,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f25,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f25,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f25.f64 = double(temp.f32);
	// fmr f9,f21
	ctx.f9.f64 = ctx.f21.f64;
	// lfs f21,1504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1504);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f27,f21,f18,f27
	ctx.f27.f64 = double(float(ctx.f21.f64 * ctx.f18.f64 + ctx.f27.f64));
	// lfs f18,1496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1496);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f29,f25,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f25.f64 + ctx.f13.f64));
	// lfs f29,4508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4508);
	ctx.f29.f64 = double(temp.f32);
	// lfs f15,2556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2556);
	ctx.f15.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f25,1488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1488);
	ctx.f25.f64 = double(temp.f32);
	// stfs f14,1488(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1488, temp.u32);
	// lfs f14,4684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4684);
	ctx.f14.f64 = double(temp.f32);
	// stfs f13,1852(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1852, temp.u32);
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// stfs f8,2444(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2444, temp.u32);
	// fmadds f9,f18,f9,f6
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f9.f64 + ctx.f6.f64));
	// lfs f6,4884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4884);
	ctx.f6.f64 = double(temp.f32);
	// fadds f6,f29,f6
	ctx.f6.f64 = double(float(ctx.f29.f64 + ctx.f6.f64));
	// lfs f18,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f18.f64 = double(temp.f32);
	// lfs f29,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f13,f2,f13,f10
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 + ctx.f10.f64));
	// fnmsubs f29,f18,f29,f8
	ctx.f29.f64 = double(float(-(ctx.f18.f64 * ctx.f29.f64 - ctx.f8.f64)));
	// lfs f18,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f31,f15,f18,f31
	ctx.f31.f64 = double(float(-(ctx.f15.f64 * ctx.f18.f64 - ctx.f31.f64)));
	// lfs f18,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f14,f18
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f18.f64));
	// stfs f0,5384(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5384, temp.u32);
	// lfs f2,1804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1804);
	ctx.f2.f64 = double(temp.f32);
	// lfs f8,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f8.f64 = double(temp.f32);
	// lfs f0,4940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4940);
	ctx.f0.f64 = double(temp.f32);
	// lfs f17,1480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1480);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f0,f0,f8,f27
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 + ctx.f27.f64));
	// lfs f15,1868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1868);
	ctx.f15.f64 = double(temp.f32);
	// fmsubs f9,f9,f2,f7
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 - ctx.f7.f64));
	// lfs f12,1860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1860);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f22,f22,f17
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f17.f64));
	// fmuls f12,f12,f15
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f15.f64));
	// lfs f10,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f10.f64 = double(temp.f32);
	// lfs f21,3664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3664);
	ctx.f21.f64 = double(temp.f32);
	// stfs f30,2724(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2724, temp.u32);
	// fmadds f24,f21,f25,f24
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f25.f64 + ctx.f24.f64));
	// fmadds f30,f29,f10,f18
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f10.f64 + ctx.f18.f64));
	// lfs f29,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f27.f64 = double(temp.f32);
	// stfs f21,1844(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1844, temp.u32);
	// fnmsubs f31,f29,f27,f31
	ctx.f31.f64 = double(float(-(ctx.f29.f64 * ctx.f27.f64 - ctx.f31.f64)));
	// stfs f11,1504(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1504, temp.u32);
	// fmuls f27,f1,f26
	ctx.f27.f64 = double(float(ctx.f1.f64 * ctx.f26.f64));
	// lfs f21,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f21.f64 = double(temp.f32);
	// lfs f11,4268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4268);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f11,f11,f21,f23
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f21.f64 + ctx.f23.f64));
	// lfs f8,4956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4956);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f7.f64 = double(temp.f32);
	// lfs f21,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f0,f8,f7,f0
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// stfs f4,1764(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1764, temp.u32);
	// fnmsubs f9,f19,f21,f9
	ctx.f9.f64 = double(float(-(ctx.f19.f64 * ctx.f21.f64 - ctx.f9.f64)));
	// stfs f27,1480(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1480, temp.u32);
	// lfs f29,2636(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2636);
	ctx.f29.f64 = double(temp.f32);
	// lfs f14,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f14.f64 = double(temp.f32);
	// lfs f23,5080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5080);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f14,f16,f14
	ctx.f14.f64 = double(float(ctx.f16.f64 * ctx.f14.f64));
	// lfs f4,920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 920);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f23,f2,f23
	ctx.f23.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// lfs f18,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f13,f22,f18,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f18.f64 + ctx.f13.f64));
	// stfs f28,3588(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3588, temp.u32);
	// fmuls f28,f4,f3
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// stfs f5,3076(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3076, temp.u32);
	// fmuls f5,f15,f20
	ctx.f5.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// stfs f25,2476(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2476, temp.u32);
	// fmuls f25,f2,f10
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f29,592(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// fmuls f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f21,2284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2284);
	ctx.f21.f64 = double(temp.f32);
	// lfs f8,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,1876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1876);
	ctx.f7.f64 = double(temp.f32);
	// lfs f29,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f0,f21,f8,f0
	ctx.f0.f64 = double(float(-(ctx.f21.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// stfs f20,1860(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1860, temp.u32);
	// lfs f18,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f12,f30,f15,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f15.f64 - ctx.f12.f64));
	// lfs f20,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f27,f17,f7
	ctx.f27.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// fnmsubs f31,f18,f20,f31
	ctx.f31.f64 = double(float(-(ctx.f18.f64 * ctx.f20.f64 - ctx.f31.f64)));
	// stfs f1,1868(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1868, temp.u32);
	// lfs f1,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f19,f29,f3
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// fmadds f9,f24,f1,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f1.f64 + ctx.f9.f64));
	// lfs f1,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f1.f64 = double(temp.f32);
	// lfs f22,3244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3244);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f13,f28,f1,f13
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f1.f64 + ctx.f13.f64));
	// lfs f20,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f24,f5,f20
	ctx.f24.f64 = double(float(ctx.f5.f64 * ctx.f20.f64));
	// lfs f8,3348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3348);
	ctx.f8.f64 = double(temp.f32);
	// lfs f30,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f8,f8,f21,f0
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f21.f64 - ctx.f0.f64)));
	// stfs f17,1496(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1496, temp.u32);
	// fmuls f30,f23,f30
	ctx.f30.f64 = double(float(ctx.f23.f64 * ctx.f30.f64));
	// fmuls f17,f22,f15
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// lfs f23,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f23.f64 = double(temp.f32);
	// lfs f1,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f6,f6,f23,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f23.f64 + ctx.f31.f64));
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f1,f16,f1
	ctx.f1.f64 = double(float(ctx.f16.f64 * ctx.f1.f64));
	// stfs f0,1876(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1876, temp.u32);
	// fmadds f11,f11,f2,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + ctx.f9.f64));
	// lfs f9,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f19,f9,f13
	ctx.f13.f64 = double(float(ctx.f19.f64 * ctx.f9.f64 + ctx.f13.f64));
	// lfs f31,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f31.f64 = double(temp.f32);
	// lfs f21,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f9,f14,f0,f8
	ctx.f9.f64 = double(float(ctx.f14.f64 * ctx.f0.f64 + ctx.f8.f64));
	// stfs f7,3068(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3068, temp.u32);
	// fnmsubs f12,f24,f21,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f21.f64 - ctx.f12.f64)));
	// lfs f7,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f31,f17,f31
	ctx.f31.f64 = double(float(ctx.f17.f64 * ctx.f31.f64));
	// stfs f5,1424(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1424, temp.u32);
	// lfs f8,1868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1868);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f0,f1,f0,f6
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f6.f64));
	// stfs f8,2452(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2452, temp.u32);
	// lfs f8,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f11,f30,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f30.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f1,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f1.f64 = double(temp.f32);
	// lfs f8,1860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1860);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,1852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1852);
	ctx.f5.f64 = double(temp.f32);
	// stfs f10,1804(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1804, temp.u32);
	// fadds f13,f5,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 + ctx.f13.f64));
	// fmadds f9,f8,f7,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f9.f64));
	// lfs f7,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f7.f64 = double(temp.f32);
	// stfs f3,2160(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2160, temp.u32);
	// fnmsubs f12,f31,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f31,1504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1504);
	ctx.f31.f64 = double(temp.f32);
	// lfs f23,3468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3468);
	ctx.f23.f64 = double(temp.f32);
	// lfs f28,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f0,f31,f7,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f0.f64));
	// lfs f20,5180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5180);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f28,f25,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f18,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f20,f23
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// lfs f10,1876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1876);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f27,f27,f18
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f18.f64));
	// lfs f3,2756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2756);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f10,f25,f10
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// lfs f30,1844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1844);
	ctx.f30.f64 = double(temp.f32);
	// stfs f4,3244(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3244, temp.u32);
	// stfs f29,2764(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2764, temp.u32);
	// fmuls f29,f3,f30
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfs f26,2772(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2772, temp.u32);
	// lfs f6,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,2276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2276);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f4.f64 = double(temp.f32);
	// lfs f26,388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 388);
	ctx.f26.f64 = double(temp.f32);
	// lfs f1,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f1.f64 = double(temp.f32);
	// lfs f7,2988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2988);
	ctx.f7.f64 = double(temp.f32);
	// stfs f16,3236(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3236, temp.u32);
	// stfs f22,2436(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2436, temp.u32);
	// stfs f15,3468(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3468, temp.u32);
	// lfs f18,3260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3260);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f0,f26,f1,f0
	ctx.f0.f64 = double(float(ctx.f26.f64 * ctx.f1.f64 + ctx.f0.f64));
	// lfs f16,1496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1496);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f11,f28,f6,f11
	ctx.f11.f64 = double(float(-(ctx.f28.f64 * ctx.f6.f64 - ctx.f11.f64)));
	// fmuls f15,f16,f18
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// stfs f15,1844(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1844, temp.u32);
	// lfs f26,1488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1488);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f9,f5,f4,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f4.f64 - ctx.f9.f64)));
	// lfs f14,1792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1792);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f7,f2,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// lfs f15,5152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5152);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f14,f26,f14
	ctx.f14.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// lfs f1,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f15,f1
	ctx.f1.f64 = double(float(ctx.f15.f64 * ctx.f1.f64));
	// stfs f14,3260(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3260, temp.u32);
	// lfs f31,2812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2812);
	ctx.f31.f64 = double(temp.f32);
	// lfs f14,1480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1480);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f15.f64 = double(temp.f32);
	// stfs f26,2812(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2812, temp.u32);
	// fmsubs f27,f14,f15,f27
	ctx.f27.f64 = double(float(ctx.f14.f64 * ctx.f15.f64 - ctx.f27.f64));
	// lfs f26,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f26.f64 = double(temp.f32);
	// lfs f15,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f26,f20,f26
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f26.f64));
	// lfs f24,1780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1780);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f15,f25,f15
	ctx.f15.f64 = double(float(ctx.f25.f64 * ctx.f15.f64));
	// lfs f21,1824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1824);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f19,f24,f31
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f31.f64));
	// lfs f5,1760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1760);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,2200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2200);
	ctx.f4.f64 = double(temp.f32);
	// lfs f20,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f20.f64 = double(temp.f32);
	// fadds f22,f5,f4
	ctx.f22.f64 = double(float(ctx.f5.f64 + ctx.f4.f64));
	// lfs f14,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f20,f20,f21
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f21.f64));
	// stfs f15,1868(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1868, temp.u32);
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// lfs f15,4908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4908);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f2,f15
	ctx.f15.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// stfs f15,1876(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1876, temp.u32);
	// fmadds f11,f10,f14,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f14.f64 + ctx.f11.f64));
	// lfs f15,2268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2268);
	ctx.f15.f64 = double(temp.f32);
	// lfs f10,4596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4596);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f15,f2,f15
	ctx.f15.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// fmuls f10,f2,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f10.f64));
	// stfs f11,1860(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1860, temp.u32);
	// stfs f15,1852(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1852, temp.u32);
	// lfs f15,392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	ctx.f15.f64 = double(temp.f32);
	// lfs f11,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f11.f64 = double(temp.f32);
	// stfs f10,1504(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1504, temp.u32);
	// fnmsubs f11,f15,f11,f9
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f11.f64 - ctx.f9.f64)));
	// stfs f8,2780(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2780, temp.u32);
	// stfs f13,5388(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5388, temp.u32);
	// stfs f0,5392(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5392, temp.u32);
	// lfs f6,1788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1788);
	ctx.f6.f64 = double(temp.f32);
	// lfs f17,5112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5112);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f28,f30,f6
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f6.f64));
	// lfs f14,3780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3780);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f17,f17,f23
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f23.f64));
	// lfs f10,5172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5172);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f14,f14,f23
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// lfs f13,2820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2820);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f10,f23
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f23.f64));
	// lfs f0,4436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4436);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f9,f22,f13,f19
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f13.f64 + ctx.f19.f64));
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// stfs f12,5396(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5396, temp.u32);
	// fmuls f12,f20,f23
	ctx.f12.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// stfs f7,1480(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1480, temp.u32);
	// fmsubs f8,f0,f8,f1
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f8.f64 - ctx.f1.f64));
	// lfs f7,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f7.f64 = double(temp.f32);
	// stfs f6,1792(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1792, temp.u32);
	// stfs f4,1788(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1788, temp.u32);
	// stfs f5,1780(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1780, temp.u32);
	// stfs f31,1496(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1496, temp.u32);
	// stfs f24,2388(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2388, temp.u32);
	// stfs f21,2756(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2756, temp.u32);
	// stfs f16,1824(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1824, temp.u32);
	// stfs f18,3664(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3664, temp.u32);
	// stfs f29,1488(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1488, temp.u32);
	// stfs f3,2184(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2184, temp.u32);
	// stfs f30,2428(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2428, temp.u32);
	// lfs f0,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f7,f17,f7,f26
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f7.f64 - ctx.f26.f64));
	// fmadds f5,f28,f0,f27
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f27.f64));
	// lfs f0,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f0.f64 = double(temp.f32);
	// lfs f6,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f6.f64 = double(temp.f32);
	// lfs f30,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f6,f6,f0
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lfs f31,4308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4308);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f31,f30
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f30.f64));
	// lfs f4,3436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3436);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f1,f1,f4
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f7,f14,f28,f7
	ctx.f7.f64 = double(float(-(ctx.f14.f64 * ctx.f28.f64 - ctx.f7.f64)));
	// lfs f28,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f5,f25,f30,f5
	ctx.f5.f64 = double(float(-(ctx.f25.f64 * ctx.f30.f64 - ctx.f5.f64)));
	// lfs f30,5164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5164);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f8,f30,f28,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f28.f64 + ctx.f8.f64));
	// lfs f27,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64));
	// lfs f3,3484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3484);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f6,f27,f30,f6
	ctx.f6.f64 = double(float(ctx.f27.f64 * ctx.f30.f64 + ctx.f6.f64));
	// lfs f29,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f29,f3
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// lfs f20,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,3260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3260);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f1,f1,f0,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f31.f64));
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f2,f20
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f20.f64));
	// lfs f20,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f20.f64 = double(temp.f32);
	// lfs f14,1876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1876);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f16,f23,f0
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// lfs f25,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f14,f20,f5
	ctx.f0.f64 = double(float(-(ctx.f14.f64 * ctx.f20.f64 - ctx.f5.f64)));
	// lfs f28,2828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2828);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f10,f10,f17,f7
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f17.f64 - ctx.f7.f64)));
	// lfs f15,1772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1772);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// stfs f4,1772(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1772, temp.u32);
	// fmadds f9,f8,f23,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f23.f64 + ctx.f9.f64));
	// stfs f31,2828(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2828, temp.u32);
	// fmuls f29,f29,f15
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// lfs f30,1888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1888);
	ctx.f30.f64 = double(temp.f32);
	// lfs f18,2812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2812);
	ctx.f18.f64 = double(temp.f32);
	// lfs f31,1860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1860);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,4004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4004);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,3524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3524);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f2,f27
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f27.f64));
	// lfs f24,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f26,f26,f23
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f22,408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 408);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f2,f24
	ctx.f24.f64 = double(float(ctx.f2.f64 * ctx.f24.f64));
	// lfs f21,5160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5160);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f2,f22
	ctx.f22.f64 = double(float(ctx.f2.f64 * ctx.f22.f64));
	// lfs f19,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f21,f23
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// lfs f5,2260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2260);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f7,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f5,f5,f23
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f23.f64));
	// lfs f20,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f7,f30,f7
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f7.f64));
	// lfs f8,3268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3268);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f20,f20,f3
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f3.f64));
	// lfs f17,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f17.f64 = double(temp.f32);
	// lfs f4,1868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1868);
	ctx.f4.f64 = double(temp.f32);
	// stfs f13,1760(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1760, temp.u32);
	// fmuls f13,f6,f2
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// stfs f11,5400(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5400, temp.u32);
	// fmuls f11,f28,f8
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// lfs f14,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f4,f4,f17,f31
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f17.f64 - ctx.f31.f64)));
	// lfs f6,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f6.f64 = double(temp.f32);
	// stfs f18,1888(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1888, temp.u32);
	// stfs f30,1416(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1416, temp.u32);
	// stfs f28,1860(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1860, temp.u32);
	// stfs f2,2200(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2200, temp.u32);
	// lfs f2,1852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1852);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f10,f26,f6,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f6.f64 + ctx.f10.f64));
	// fmadds f2,f2,f14,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64 + ctx.f0.f64));
	// lfs f18,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f18.f64 = double(temp.f32);
	// fmr f0,f6
	ctx.f0.f64 = ctx.f6.f64;
	// lfs f30,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f30.f64 = double(temp.f32);
	// lfs f6,5128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5128);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f31,f20,f15
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f15.f64));
	// fmuls f6,f6,f23
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f23.f64));
	// lfs f28,1756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1756);
	ctx.f28.f64 = double(temp.f32);
	// lfs f17,1884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1884);
	ctx.f17.f64 = double(temp.f32);
	// lfs f26,1260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1260);
	ctx.f26.f64 = double(temp.f32);
	// stfs f22,3268(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3268, temp.u32);
	// fmuls f20,f26,f8
	ctx.f20.f64 = double(float(ctx.f26.f64 * ctx.f8.f64));
	// lfs f22,1504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1504);
	ctx.f22.f64 = double(temp.f32);
	// stfs f8,1884(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1884, temp.u32);
	// lfs f8,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f9,f21,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f21.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f21,4528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4528);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f21,f18
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// lfs f0,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,1844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1844);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f2,f18,f0,f2
	ctx.f2.f64 = double(float(-(ctx.f18.f64 * ctx.f0.f64 - ctx.f2.f64)));
	// lfs f18,564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 564);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f12,f0,f10
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f12,3760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3760);
	ctx.f12.f64 = double(temp.f32);
	// fadds f12,f18,f12
	ctx.f12.f64 = double(float(ctx.f18.f64 + ctx.f12.f64));
	// lfs f18,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f11,f11,f18
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// lfs f18,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f10,f5,f30,f9
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f30.f64 - ctx.f9.f64)));
	// lfs f5,4520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4520);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f30,f28,f17
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f17.f64));
	// lfs f17,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f5,f5,f17
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f17.f64));
	// lfs f9,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f9.f64 = double(temp.f32);
	// fmr f17,f14
	ctx.f17.f64 = ctx.f14.f64;
	// lfs f14,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// lfs f7,3920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3920);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f0,f1,f23,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f23.f64 + ctx.f0.f64));
	// lfs f1,2016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2016);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f7,f7,f23
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f23.f64));
	// fmuls f3,f3,f14
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// fmadds f10,f6,f18,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f18.f64 + ctx.f10.f64));
	// lfs f18,1804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1804);
	ctx.f18.f64 = double(temp.f32);
	// fmr f6,f14
	ctx.f6.f64 = ctx.f14.f64;
	// fmadds f13,f13,f18,f4
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f18.f64 + ctx.f4.f64));
	// stfs f13,2820(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2820, temp.u32);
	// fnmsubs f2,f22,f17,f2
	ctx.f2.f64 = double(float(-(ctx.f22.f64 * ctx.f17.f64 - ctx.f2.f64)));
	// lfs f13,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f13.f64 = double(temp.f32);
	// lfs f4,1892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1892);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f13,f30,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f13.f64));
	// stfs f24,1892(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1892, temp.u32);
	// fmadds f9,f9,f8,f21
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f21.f64));
	// lfs f30,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f28,f4
	ctx.f4.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// lfs f24,1488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1488);
	ctx.f24.f64 = double(temp.f32);
	// lfs f8,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f8,f29,f8,f0
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f22,2056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2056);
	ctx.f22.f64 = double(temp.f32);
	// lfs f17,1496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1496);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f14,f16,f22
	ctx.f14.f64 = double(float(ctx.f16.f64 * ctx.f22.f64));
	// fmuls f6,f19,f6
	ctx.f6.f64 = double(float(ctx.f19.f64 * ctx.f6.f64));
	// stfs f6,3484(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3484, temp.u32);
	// lfs f6,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f18,f1,f17
	ctx.f18.f64 = double(float(ctx.f1.f64 * ctx.f17.f64));
	// fmuls f6,f20,f6
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f6.f64));
	// lfs f20,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f2,f24,f30,f2
	ctx.f2.f64 = double(float(-(ctx.f24.f64 * ctx.f30.f64 - ctx.f2.f64)));
	// lfs f30,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f12,f20,f5
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f20.f64 + ctx.f5.f64));
	// lfs f19,4844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4844);
	ctx.f19.f64 = double(temp.f32);
	// lfs f5,2836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2836);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f31,f30,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f30.f64 + ctx.f11.f64));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f19,f19,f23
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// lfs f29,4504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4504);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f5,f28,f5
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f5.f64));
	// lfs f31,3800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3800);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f10,f7,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// fmsubs f9,f9,f23,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f23.f64 - ctx.f6.f64));
	// lfs f0,1480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1480);
	ctx.f0.f64 = double(temp.f32);
	// lfs f6,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f7,f29,f23
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// fnmsubs f6,f0,f6,f2
	ctx.f6.f64 = double(float(-(ctx.f0.f64 * ctx.f6.f64 - ctx.f2.f64)));
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// fmr f24,f0
	ctx.f24.f64 = ctx.f0.f64;
	// lfs f29,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f4,f29
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// stfs f26,1868(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1868, temp.u32);
	// fmr f29,f20
	ctx.f29.f64 = ctx.f20.f64;
	// fmr f26,f2
	ctx.f26.f64 = ctx.f2.f64;
	// stfs f22,2836(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2836, temp.u32);
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// lfs f22,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f22.f64 = double(temp.f32);
	// stfs f1,3044(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3044, temp.u32);
	// fmr f1,f2
	ctx.f1.f64 = ctx.f2.f64;
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f3,f3,f15
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f15.f64));
	// fmuls f5,f5,f21
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f21.f64));
	// stfs f16,1976(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1976, temp.u32);
	// fmuls f31,f31,f23
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f23.f64));
	// stfs f15,3260(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3260, temp.u32);
	// fmadds f6,f25,f2,f6
	ctx.f6.f64 = double(float(ctx.f25.f64 * ctx.f2.f64 + ctx.f6.f64));
	// lfs f2,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f8,f14,f24,f8
	ctx.f8.f64 = double(float(-(ctx.f14.f64 * ctx.f24.f64 - ctx.f8.f64)));
	// lfs f24,3784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3784);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f12,f24,f22,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f22.f64 + ctx.f12.f64));
	// lfs f22,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f9,f7,f22,f9
	ctx.f9.f64 = double(float(-(ctx.f7.f64 * ctx.f22.f64 - ctx.f9.f64)));
	// lfs f24,1900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1900);
	ctx.f24.f64 = double(temp.f32);
	// fmr f7,f20
	ctx.f7.f64 = ctx.f20.f64;
	// lfs f20,556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 556);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f11,f19,f29,f11
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// lfs f29,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f13,f13,f26
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// lfs f26,1384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1384);
	ctx.f26.f64 = double(temp.f32);
	// fmr f21,f30
	ctx.f21.f64 = ctx.f30.f64;
	// lfs f19,740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 740);
	ctx.f19.f64 = double(temp.f32);
	// fmr f22,f0
	ctx.f22.f64 = ctx.f0.f64;
	// lfs f25,3596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3596);
	ctx.f25.f64 = double(temp.f32);
	// fmr f16,f29
	ctx.f16.f64 = ctx.f29.f64;
	// stfs f26,1852(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1852, temp.u32);
	// fnmsubs f6,f27,f1,f6
	ctx.f6.f64 = double(float(-(ctx.f27.f64 * ctx.f1.f64 - ctx.f6.f64)));
	// lfs f27,4944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4944);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f2,f26,f2
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// fmuls f20,f20,f23
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// stfs f20,1900(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1900, temp.u32);
	// lfs f20,1892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1892);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f28,f25
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// fmuls f24,f28,f24
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fnmsubs f11,f3,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f3.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f7,1796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1796);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,3768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3768);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f1,f7,f17
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f17.f64));
	// fmsubs f13,f4,f21,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f21.f64 - ctx.f13.f64));
	// lfs f4,3776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3776);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f8,f18,f22,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f22.f64 + ctx.f8.f64));
	// lfs f18,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f14,f4,f19
	ctx.f14.f64 = double(float(ctx.f4.f64 * ctx.f19.f64));
	// lfs f21,4496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4496);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f6,f20,f0,f6
	ctx.f6.f64 = double(float(-(ctx.f20.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// lfs f20,1512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1512);
	ctx.f20.f64 = double(temp.f32);
	// stfs f19,1512(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1512, temp.u32);
	// fnmsubs f9,f31,f18,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f18.f64 - ctx.f9.f64)));
	// lfs f22,4548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4548);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f3,f3,f23
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// lfs f19,4376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4376);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f27,f27,f23
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f23.f64));
	// lfs f31,1472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1472);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f2,f2,f23
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// stfs f7,2056(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2056, temp.u32);
	// fmadds f12,f12,f23,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f7,1884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1884);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f21,f21,f23
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f23.f64));
	// fmadds f13,f5,f16,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f16.f64 + ctx.f13.f64));
	// lfs f5,1916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1916);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f22,f22,f23
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// fmuls f11,f28,f20
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// fmuls f26,f19,f31
	ctx.f26.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// stfs f4,3436(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3436, temp.u32);
	// fnmsubs f10,f21,f18,f10
	ctx.f10.f64 = double(float(-(ctx.f21.f64 * ctx.f18.f64 - ctx.f10.f64)));
	// lfs f4,3268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3268);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f20,f5,f7
	ctx.f20.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// fmadds f6,f4,f0,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 + ctx.f6.f64));
	// lfs f0,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f25,f0
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f21,1632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1632);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f24,f0
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f0.f64));
	// lfs f24,2828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2828);
	ctx.f24.f64 = double(temp.f32);
	// lfs f16,1408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1408);
	ctx.f16.f64 = double(temp.f32);
	// stfs f17,2016(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2016, temp.u32);
	// lfs f17,1464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1464);
	ctx.f17.f64 = double(temp.f32);
	// fadds f10,f8,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 + ctx.f10.f64));
	// lfs f8,2316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2316);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f8,f8,f23
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// fnmsubs f0,f24,f30,f6
	ctx.f0.f64 = double(float(-(ctx.f24.f64 * ctx.f30.f64 - ctx.f6.f64)));
	// lfs f6,3484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3484);
	ctx.f6.f64 = double(temp.f32);
	// lfs f24,1940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1940);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f0,f6,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f6,2820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2820);
	ctx.f6.f64 = double(temp.f32);
	// lfs f29,3976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3976);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f19,f28,f29
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f29.f64));
	// fadds f0,f6,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 + ctx.f0.f64));
	// stfs f0,5404(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5404, temp.u32);
	// fmr f6,f18
	ctx.f6.f64 = ctx.f18.f64;
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f4,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f4,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f0,f1,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lfs f30,1932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1932);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f28,f30
	ctx.f30.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// fmadds f9,f3,f6,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f9.f64));
	// lfs f6,1924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1924);
	ctx.f6.f64 = double(temp.f32);
	// fmr f3,f18
	ctx.f3.f64 = ctx.f18.f64;
	// stfs f19,1924(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1924, temp.u32);
	// fmuls f12,f6,f4
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f18,1456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1456);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f25,f1,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f1.f64 + ctx.f13.f64));
	// lfs f1,1948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1948);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f15,f18,f4
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f4.f64));
	// fmuls f25,f17,f24
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// fnmsubs f9,f27,f3,f9
	ctx.f9.f64 = double(float(-(ctx.f27.f64 * ctx.f3.f64 - ctx.f9.f64)));
	// lfs f3,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f1,f3
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f27,1640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1640);
	ctx.f27.f64 = double(temp.f32);
	// lfs f1,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f1.f64 = double(temp.f32);
	// fadds f27,f21,f27
	ctx.f27.f64 = double(float(ctx.f21.f64 + ctx.f27.f64));
	// fmadds f1,f22,f1,f0
	ctx.f1.f64 = double(float(ctx.f22.f64 * ctx.f1.f64 + ctx.f0.f64));
	// lfs f21,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f21.f64 = double(temp.f32);
	// lfs f0,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f22,f29,f21
	ctx.f22.f64 = double(float(ctx.f29.f64 * ctx.f21.f64));
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f0,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f0.f64 = double(temp.f32);
	// lfs f21,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f13,f14,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f14.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f30,f30,f21
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f21.f64));
	// lfs f14,1512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1512);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,1956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1956);
	ctx.f21.f64 = double(temp.f32);
	// stfs f14,1940(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1940, temp.u32);
	// fmuls f19,f21,f14
	ctx.f19.f64 = double(float(ctx.f21.f64 * ctx.f14.f64));
	// lfs f14,4928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4928);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f9,f2,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// stfs f4,1956(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1956, temp.u32);
	// fmuls f14,f14,f23
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// lfs f4,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// stfs f14,1948(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1948, temp.u32);
	// fmadds f11,f11,f4,f26
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f4.f64 + ctx.f26.f64));
	// lfs f14,1624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1624);
	ctx.f14.f64 = double(temp.f32);
	// lfs f4,1964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1964);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f28,f14
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f14.f64));
	// lfs f2,3876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3876);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f14.f64 = double(temp.f32);
	// stfs f15,1964(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1964, temp.u32);
	// fmuls f15,f2,f16
	ctx.f15.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// stfs f15,1916(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1916, temp.u32);
	// fmsubs f4,f4,f14,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f14.f64 - ctx.f3.f64));
	// lfs f0,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f0.f64 = double(temp.f32);
	// fmr f15,f14
	ctx.f15.f64 = ctx.f14.f64;
	// lfs f3,1616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1616);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f1,f20,f15,f1
	ctx.f1.f64 = double(float(ctx.f20.f64 * ctx.f15.f64 + ctx.f1.f64));
	// stfs f6,1384(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1384, temp.u32);
	// fmuls f22,f22,f0
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f0.f64));
	// lfs f6,1900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1900);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f0,f25,f0,f12
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f0.f64 - ctx.f12.f64));
	// lfs f12,2048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2048);
	ctx.f12.f64 = double(temp.f32);
	// stfs f7,2048(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2048, temp.u32);
	// fadds f3,f27,f3
	ctx.f3.f64 = double(float(ctx.f27.f64 + ctx.f3.f64));
	// lfs f7,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fmadds f9,f6,f7,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f7.f64 + ctx.f9.f64));
	// lfs f6,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f13,f11,f6,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f6.f64 + ctx.f13.f64));
	// stfs f29,1736(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1736, temp.u32);
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f20,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f29,f26,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// lfs f27,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f27.f64 = double(temp.f32);
	// fadds f10,f10,f1
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f1.f64));
	// stfs f10,1932(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1932, temp.u32);
	// lfs f1,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f27,f20,f27
	ctx.f27.f64 = double(float(ctx.f20.f64 * ctx.f27.f64));
	// fmr f26,f1
	ctx.f26.f64 = ctx.f1.f64;
	// stfs f24,3620(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3620, temp.u32);
	// lfs f24,1964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1964);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f10,f28,f12
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// lfs f15,1360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1360);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f9,f8,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f8.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// stfs f18,2364(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2364, temp.u32);
	// fnmsubs f13,f24,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// lfs f1,1864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1864);
	ctx.f1.f64 = double(temp.f32);
	// lfs f8,2040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2040);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f18,f28,f1
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// stfs f5,1844(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1844, temp.u32);
	// fmuls f5,f28,f15
	ctx.f5.f64 = double(float(ctx.f28.f64 * ctx.f15.f64));
	// stfs f23,1456(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1456, temp.u32);
	// fmuls f23,f28,f8
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// stfs f18,2812(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2812, temp.u32);
	// stfs f21,3036(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3036, temp.u32);
	// lfs f15,1956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1956);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f0,f19,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// lfs f21,1896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1896);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,3356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3356);
	ctx.f18.f64 = double(temp.f32);
	// lfs f7,4920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4920);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f18,f18,f31
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f31.f64));
	// stfs f23,3268(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3268, temp.u32);
	// fmuls f23,f21,f15
	ctx.f23.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// stfs f2,3028(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3028, temp.u32);
	// fmuls f2,f27,f31
	ctx.f2.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// lfs f24,2176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2176);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f20,f28,f7
	ctx.f20.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// stfs f16,1408(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1408, temp.u32);
	// fmuls f14,f24,f15
	ctx.f14.f64 = double(float(ctx.f24.f64 * ctx.f15.f64));
	// stfs f31,3596(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3596, temp.u32);
	// lfs f16,3428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3428);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,4356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4356);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,1940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1940);
	ctx.f31.f64 = double(temp.f32);
	// stfs f23,1892(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1892, temp.u32);
	// lfs f23,1948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1948);
	ctx.f23.f64 = double(temp.f32);
	// lfs f11,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f11.f64 = double(temp.f32);
	// stfs f20,3484(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3484, temp.u32);
	// fmuls f20,f28,f16
	ctx.f20.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// stfs f15,1900(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1900, temp.u32);
	// fmuls f15,f19,f31
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f31.f64));
	// lfs f6,2324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2324);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f11,f23,f11,f9
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f11.f64 - ctx.f9.f64)));
	// lfs f27,1872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1872);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,1880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1880);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f28,f27
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// stfs f30,1512(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1512, temp.u32);
	// fmuls f30,f17,f6
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f6.f64));
	// stfs f20,1876(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1876, temp.u32);
	// fmuls f25,f28,f26
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// stfs f16,2828(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2828, temp.u32);
	// stfs f15,1884(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1884, temp.u32);
	// lfs f20,4832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4832);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,1448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1448);
	ctx.f16.f64 = double(temp.f32);
	// lfs f9,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f9.f64 = double(temp.f32);
	// lfs f23,1352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1352);
	ctx.f23.f64 = double(temp.f32);
	// lfs f15,1932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1932);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f20,f20,f16
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// stfs f16,1956(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1956, temp.u32);
	// stfs f20,1964(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1964, temp.u32);
	// fadds f11,f15,f11
	ctx.f11.f64 = double(float(ctx.f15.f64 + ctx.f11.f64));
	// lfs f16,1924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1924);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f23,f28,f23
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f20,1916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1916);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f9,f16,f9,f20
	ctx.f9.f64 = double(float(ctx.f16.f64 * ctx.f9.f64 + ctx.f20.f64));
	// stfs f9,1924(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1924, temp.u32);
	// lfs f16,3996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3996);
	ctx.f16.f64 = double(temp.f32);
	// lfs f9,2308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2308);
	ctx.f9.f64 = double(temp.f32);
	// fadds f9,f16,f9
	ctx.f9.f64 = double(float(ctx.f16.f64 + ctx.f9.f64));
	// lfs f16,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f16.f64 = double(temp.f32);
	// lfs f20,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f30,f30,f16,f0
	ctx.f30.f64 = double(float(-(ctx.f30.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// lfs f15,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f3,f3,f20,f22
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f20.f64 + ctx.f22.f64));
	// fmsubs f0,f4,f15,f29
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f15.f64 - ctx.f29.f64));
	// lfs f22,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f5,f5,f22
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// lfs f20,1512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1512);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f13,f20,f22,f13
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f22.f64 - ctx.f13.f64)));
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// stfs f10,1940(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1940, temp.u32);
	// stfs f12,1512(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1512, temp.u32);
	// fmuls f12,f27,f22
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// stfs f14,1948(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1948, temp.u32);
	// stfs f1,1496(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1496, temp.u32);
	// lfs f1,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f1.f64 = double(temp.f32);
	// stfs f7,2820(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2820, temp.u32);
	// fmadds f10,f3,f28,f0
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f3,3004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3004);
	ctx.f3.f64 = double(temp.f32);
	// fadds f9,f9,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f3.f64));
	// lfs f7,4372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4372);
	ctx.f7.f64 = double(temp.f32);
	// fmr f3,f16
	ctx.f3.f64 = ctx.f16.f64;
	// stfs f11,5408(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5408, temp.u32);
	// fmadds f5,f2,f1,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f5.f64));
	// lfs f2,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f17,f7
	ctx.f4.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// lfs f11,3744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3744);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f13,f12,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f12,3752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3752);
	ctx.f12.f64 = double(temp.f32);
	// stfs f8,1932(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1932, temp.u32);
	// fmr f0,f15
	ctx.f0.f64 = ctx.f15.f64;
	// fmr f1,f16
	ctx.f1.f64 = ctx.f16.f64;
	// stfs f26,1504(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1504, temp.u32);
	// fmuls f8,f28,f11
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// lfs f29,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f12,f28,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f12.f64));
	// lfs f26,1964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1964);
	ctx.f26.f64 = double(temp.f32);
	// lfs f2,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f10,f26,f29,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f10.f64));
	// lfs f29,1956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1956);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f3,f18,f3,f30
	ctx.f3.f64 = double(float(-(ctx.f18.f64 * ctx.f3.f64 - ctx.f30.f64)));
	// stfs f6,1896(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1896, temp.u32);
	// lfs f6,-19328(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19328);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f9,f9,f29
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// fmadds f5,f4,f2,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 + ctx.f5.f64));
	// stfs f6,448(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// lfs f14,2112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2112);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,1948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1948);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f14,f28,f14
	ctx.f14.f64 = double(float(ctx.f28.f64 * ctx.f14.f64));
	// lfs f6,2852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2852);
	ctx.f6.f64 = double(temp.f32);
	// lfs f29,3492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3492);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f8,f8,f0
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f26,1900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1900);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// stfs f25,1916(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1916, temp.u32);
	// fmuls f30,f6,f31
	ctx.f30.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// stfs f24,1464(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1464, temp.u32);
	// fmuls f25,f29,f26
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// fmadds f3,f2,f1,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f3.f64));
	// lfs f27,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f27.f64 = double(temp.f32);
	// lfs f4,2836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2836);
	ctx.f4.f64 = double(temp.f32);
	// fmr f0,f22
	ctx.f0.f64 = ctx.f22.f64;
	// lfs f2,1940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1940);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,2860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2860);
	ctx.f1.f64 = double(temp.f32);
	// lfs f24,1296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1296);
	ctx.f24.f64 = double(temp.f32);
	// stfs f21,1448(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1448, temp.u32);
	// stfs f19,2176(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2176, temp.u32);
	// fmuls f21,f14,f0
	ctx.f21.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f0,1280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1280);
	ctx.f0.f64 = double(temp.f32);
	// lfs f20,1968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1968);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f0,f28,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f14,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f14.f64 = double(temp.f32);
	// fmr f19,f15
	ctx.f19.f64 = ctx.f15.f64;
	// stfs f0,3492(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3492, temp.u32);
	// fmsubs f9,f9,f14,f5
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f14.f64 - ctx.f5.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f15,f20,f4
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f4.f64));
	// stfs f25,2852(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2852, temp.u32);
	// fnmsubs f10,f23,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f14,1892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1892);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f13,f8,f27,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f27.f64 + ctx.f13.f64));
	// lfs f25,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f28,f24
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// fnmsubs f3,f14,f25,f3
	ctx.f3.f64 = double(float(-(ctx.f14.f64 * ctx.f25.f64 - ctx.f3.f64)));
	// lfs f0,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f0.f64 = double(temp.f32);
	// stfs f6,1756(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1756, temp.u32);
	// stfs f11,1900(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1900, temp.u32);
	// fmuls f11,f1,f0
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f7,1968(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1968, temp.u32);
	// lfs f18,3596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3596);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f2,f19
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f19.f64));
	// lfs f6,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f6.f64 = double(temp.f32);
	// lfs f22,1288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1288);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f7,f15,f0
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// stfs f4,1804(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1804, temp.u32);
	// fnmsubs f10,f21,f6,f10
	ctx.f10.f64 = double(float(-(ctx.f21.f64 * ctx.f6.f64 - ctx.f10.f64)));
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f22,f22,f18
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f18.f64));
	// lfs f8,1820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1820);
	ctx.f8.f64 = double(temp.f32);
	// fmr f6,f0
	ctx.f6.f64 = ctx.f0.f64;
	// lfs f4,1884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1884);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f8,f28,f8
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// fnmsubs f4,f4,f0,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// lfs f16,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f16.f64 = double(temp.f32);
	// lfs f25,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f16,f1,f16
	ctx.f16.f64 = double(float(ctx.f1.f64 * ctx.f16.f64));
	// lfs f23,5084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5084);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f13,f30,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f14,4724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4724);
	ctx.f14.f64 = double(temp.f32);
	// fadds f30,f23,f14
	ctx.f30.f64 = double(float(ctx.f23.f64 + ctx.f14.f64));
	// stfs f26,1480(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1480, temp.u32);
	// lfs f23,4348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4348);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,3792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3792);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f26,f23
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f3,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f3.f64 = double(temp.f32);
	// lfs f23,1924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1924);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f0,f22,f0,f9
	ctx.f0.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f27,2072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2072);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f10,f8,f3,f10
	ctx.f10.f64 = double(float(-(ctx.f8.f64 * ctx.f3.f64 - ctx.f10.f64)));
	// lfs f22,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f6,f23,f6,f4
	ctx.f6.f64 = double(float(ctx.f23.f64 * ctx.f6.f64 + ctx.f4.f64));
	// stfs f28,1892(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1892, temp.u32);
	// fmuls f27,f28,f27
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// stfs f12,2860(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2860, temp.u32);
	// fmuls f23,f1,f22
	ctx.f23.f64 = double(float(ctx.f1.f64 * ctx.f22.f64));
	// stfs f2,1872(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1872, temp.u32);
	// lfs f9,3260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3260);
	ctx.f9.f64 = double(temp.f32);
	// lfs f12,1932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1932);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f11,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f9.f64));
	// lfs f5,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,1920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1920);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f12,f5
	ctx.f5.f64 = double(float(ctx.f12.f64 * ctx.f5.f64));
	// lfs f28,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f28.f64 = double(temp.f32);
	// lfs f25,3268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3268);
	ctx.f25.f64 = double(temp.f32);
	// lfs f4,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f28,f25,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f8,2192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2192);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// stfs f31,1472(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1472, temp.u32);
	// fmuls f8,f8,f20
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f20.f64));
	// stfs f29,1880(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1880, temp.u32);
	// fmuls f29,f16,f9
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f9.f64));
	// lfs f31,2168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2168);
	ctx.f31.f64 = double(temp.f32);
	// lfs f3,4324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4324);
	ctx.f3.f64 = double(temp.f32);
	// lfs f22,2980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2980);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f21.f64 = double(temp.f32);
	// stfs f17,1796(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1796, temp.u32);
	// stfs f18,1488(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1488, temp.u32);
	// fadds f3,f22,f3
	ctx.f3.f64 = double(float(ctx.f22.f64 + ctx.f3.f64));
	// lfs f22,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f22.f64 = double(temp.f32);
	// lfs f18,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f19,f22,f0
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f22.f64 - ctx.f0.f64)));
	// fmuls f30,f30,f18
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// lfs f17,2828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2828);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f5,f17,f21,f5
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f21.f64 + ctx.f5.f64));
	// fnmsubs f6,f24,f18,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f18.f64 - ctx.f6.f64)));
	// lfs f21,3728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3728);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f21,f31
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// lfs f22,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f7,f7,f16
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f16.f64));
	// lfs f16,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f10,f27,f22,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f22.f64 + ctx.f10.f64));
	// lfs f14,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f20.f64));
	// lfs f27,2088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2088);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f8,f8,f14
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f14.f64));
	// stfs f12,3744(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3744, temp.u32);
	// fmuls f26,f26,f9
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f9.f64));
	// fnmsubs f0,f28,f16,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// lfs f18,3284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3284);
	ctx.f18.f64 = double(temp.f32);
	// lfs f12,1512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1512);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f23,f23,f9
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// stfs f2,2088(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2088, temp.u32);
	// fmuls f14,f18,f12
	ctx.f14.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// lfs f28,4996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4996);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f5,f5,f18
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f18.f64));
	// stfs f8,3752(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3752, temp.u32);
	// fmuls f28,f28,f20
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lfs f2,2860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2860);
	ctx.f2.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f8,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f8.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// stfs f12,1940(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1940, temp.u32);
	// fmadds f8,f2,f8,f6
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f6.f64));
	// lfs f12,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f12.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f28,3284(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3284, temp.u32);
	// fmadds f12,f3,f12,f30
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 + ctx.f30.f64));
	// lfs f28,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f3,f21,f9
	ctx.f3.f64 = double(float(ctx.f21.f64 * ctx.f9.f64));
	// stfs f4,3792(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3792, temp.u32);
	// fmuls f11,f11,f28
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f28.f64));
	// lfs f4,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f4.f64 = double(temp.f32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// lfs f6,3492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3492);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f10,f6,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f6,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f7,f29,f6,f7
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f6.f64 + ctx.f7.f64));
	// lfs f6,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,2852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2852);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f8,f2,f6,f8
	ctx.f8.f64 = double(float(-(ctx.f2.f64 * ctx.f6.f64 - ctx.f8.f64)));
	// stfs f31,2836(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2836, temp.u32);
	// lfs f6,2716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2716);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f6,f2,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f12.f64));
	// lfs f24,2596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2596);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f11,f3,f31,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f31.f64 - ctx.f11.f64));
	// lfs f22,1916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1916);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f19,f24,f27
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f27.f64));
	// lfs f15,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f22,f15
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// lfs f6,1808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1808);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f16,f16,f20
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f20.f64));
	// lfs f3,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f6,f3
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// lfs f4,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// lfs f31,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f31.f64 = double(temp.f32);
	// stfs f25,3260(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3260, temp.u32);
	// stfs f17,1920(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1920, temp.u32);
	// stfs f22,1884(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1884, temp.u32);
	// stfs f19,2860(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2860, temp.u32);
	// stfs f24,1932(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1932, temp.u32);
	// stfs f27,2192(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2192, temp.u32);
	// fmsubs f7,f26,f4,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f4.f64 - ctx.f7.f64));
	// lfs f1,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f1.f64 = double(temp.f32);
	// fadds f13,f13,f8
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f8.f64));
	// lfs f4,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f0,f15,f31,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f31.f64 - ctx.f0.f64)));
	// lfs f31,3484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3484);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f31,f1
	ctx.f1.f64 = double(float(ctx.f31.f64 * ctx.f1.f64));
	// lfs f24,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f12,f12,f20,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f20.f64 + ctx.f11.f64));
	// lfs f19,2812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2812);
	ctx.f19.f64 = double(temp.f32);
	// lfs f8,2000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2000);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f2,f2,f9
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f9.f64));
	// lfs f17,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f26,f16,f8
	ctx.f26.f64 = double(float(ctx.f16.f64 * ctx.f8.f64));
	// lfs f11,4340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4340);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f28,f14,f9
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f9.f64));
	// fmuls f11,f11,f20
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// stfs f11,2852(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2852, temp.u32);
	// lfs f11,1876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1876);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f3,f3,f20
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// fnmsubs f7,f23,f4,f7
	ctx.f7.f64 = double(float(-(ctx.f23.f64 * ctx.f4.f64 - ctx.f7.f64)));
	// lfs f23,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f23.f64 = double(temp.f32);
	// fadds f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f10.f64));
	// lfs f10,2708(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2708);
	ctx.f10.f64 = double(temp.f32);
	// fmr f4,f24
	ctx.f4.f64 = ctx.f24.f64;
	// stfs f2,1964(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1964, temp.u32);
	// fmuls f10,f10,f20
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// stfs f10,1948(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1948, temp.u32);
	// fnmsubs f0,f1,f23,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f23.f64 - ctx.f0.f64)));
	// lfs f10,3284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3284);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f24,f19,f24
	ctx.f24.f64 = double(float(ctx.f19.f64 * ctx.f24.f64));
	// stfs f11,2828(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2828, temp.u32);
	// fnmsubs f12,f10,f17,f12
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f17.f64 - ctx.f12.f64)));
	// lfs f2,2692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2692);
	ctx.f2.f64 = double(temp.f32);
	// lfs f27,3520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3520);
	ctx.f27.f64 = double(temp.f32);
	// stfs f6,1924(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1924, temp.u32);
	// fmuls f6,f2,f20
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f20.f64));
	// lfs f22,1476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1476);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f27,f27,f20
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// fmadds f7,f5,f9,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f7.f64));
	// lfs f5,-19344(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19344);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,868(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 868, temp.u32);
	// fmuls f22,f22,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// fmuls f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// lfs f11,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,3792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3792);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f0,f24,f11,f0
	ctx.f0.f64 = double(float(-(ctx.f24.f64 * ctx.f11.f64 - ctx.f0.f64)));
	// lfs f11,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,2820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2820);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f26,f11,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfs f29,1984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1984);
	ctx.f29.f64 = double(temp.f32);
	// lfs f1,-19336(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19336);
	ctx.f1.f64 = double(temp.f32);
	// lfs f25,3676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3676);
	ctx.f25.f64 = double(temp.f32);
	// lfs f21,3580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3580);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f7,f2,f5,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f5.f64 + ctx.f7.f64));
	// lfs f16,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f25,f25,f20
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f20.f64));
	// lfs f15,3204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3204);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f21,f21,f20
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f20.f64));
	// lfs f14,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f10,f16,f29
	ctx.f10.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// stfs f27,3492(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3492, temp.u32);
	// lfs f27,-19332(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19332);
	ctx.f27.f64 = double(temp.f32);
	// lfs f23,-19340(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19340);
	ctx.f23.f64 = double(temp.f32);
	// stfs f22,1956(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1956, temp.u32);
	// fmuls f22,f18,f30
	ctx.f22.f64 = double(float(ctx.f18.f64 * ctx.f30.f64));
	// stfs f8,3284(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3284, temp.u32);
	// fmuls f8,f15,f14
	ctx.f8.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// stfs f1,852(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 852, temp.u32);
	// lfs f11,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,5120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5120);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,5184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5184);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,5144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5144);
	ctx.f1.f64 = double(temp.f32);
	// stfs f27,588(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 588, temp.u32);
	// stfs f23,620(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 620, temp.u32);
	// stfs f31,3268(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3268, temp.u32);
	// stfs f19,3484(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3484, temp.u32);
	// stfs f18,3596(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3596, temp.u32);
	// stfs f30,1512(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1512, temp.u32);
	// stfs f29,1916(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1916, temp.u32);
	// lfs f27,3832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3832);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f4,f11,f0
	ctx.f0.f64 = double(float(-(ctx.f4.f64 * ctx.f11.f64 - ctx.f0.f64)));
	// lfs f24,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f30,f1,f5
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfs f23,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f4,f27,f24
	ctx.f4.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// fmuls f24,f22,f23
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f22,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f3,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// lfs f3,3912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3912);
	ctx.f3.f64 = double(temp.f32);
	// lfs f16,2876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2876);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f3,f5
	ctx.f17.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// lfs f14,3292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3292);
	ctx.f14.f64 = double(temp.f32);
	// lfs f29,3824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3824);
	ctx.f29.f64 = double(temp.f32);
	// stfs f17,3292(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3292, temp.u32);
	// fmuls f17,f16,f5
	ctx.f17.f64 = double(float(ctx.f16.f64 * ctx.f5.f64));
	// lfs f22,2868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2868);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f26,f29,f2
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f2.f64));
	// stfs f17,2868(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2868, temp.u32);
	// fmuls f15,f22,f5
	ctx.f15.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// lfs f17,2892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2892);
	ctx.f17.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f9,2892(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2892, temp.u32);
	// lfs f18,3652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3652);
	ctx.f18.f64 = double(temp.f32);
	// lfs f23,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f23.f64 = double(temp.f32);
	// lfs f19,3752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3752);
	ctx.f19.f64 = double(temp.f32);
	// lfs f9,2228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2228);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f7,f19,f23,f7
	ctx.f7.f64 = double(float(-(ctx.f19.f64 * ctx.f23.f64 - ctx.f7.f64)));
	// lfs f31,5168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5168);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f27.f64 = double(temp.f32);
	// stfs f15,2876(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2876, temp.u32);
	// fmuls f15,f18,f5
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f5.f64));
	// stfs f6,2228(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2228, temp.u32);
	// fmuls f6,f14,f5
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f5.f64));
	// lfs f23,3904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3904);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f31,f27
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// stfs f15,3652(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3652, temp.u32);
	// fmuls f19,f23,f5
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f5.f64));
	// stfs f6,3912(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3912, temp.u32);
	// fmuls f26,f26,f17
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f17.f64));
	// lfs f15,3500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3500);
	ctx.f15.f64 = double(temp.f32);
	// lfs f6,3300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3300);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,3840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3840);
	ctx.f11.f64 = double(temp.f32);
	// stfs f5,3300(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3300, temp.u32);
	// fmuls f5,f15,f5
	ctx.f5.f64 = double(float(ctx.f15.f64 * ctx.f5.f64));
	// stfs f5,3904(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3904, temp.u32);
	// fmuls f11,f11,f2
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// lfs f5,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f5.f64 = double(temp.f32);
	// stfs f26,3500(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3500, temp.u32);
	// fmadds f10,f10,f5,f8
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64 + ctx.f8.f64));
	// lfs f26,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f5,f4,f26,f24
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f26.f64 + ctx.f24.f64));
	// lfs f13,2884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2884);
	ctx.f13.f64 = double(temp.f32);
	// lfs f8,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f26,f9,f2
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f4,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f8,f30,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f8.f64));
	// fmadds f4,f13,f4,f27
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f4.f64 + ctx.f27.f64));
	// lfs f30,2860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2860);
	ctx.f30.f64 = double(temp.f32);
	// lfs f27,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f27.f64 = double(temp.f32);
	// stfs f20,2884(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2884, temp.u32);
	// fnmsubs f7,f30,f27,f7
	ctx.f7.f64 = double(float(-(ctx.f30.f64 * ctx.f27.f64 - ctx.f7.f64)));
	// lfs f20,3744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3744);
	ctx.f20.f64 = double(temp.f32);
	// lfs f24,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f11,f11,f20
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f20.f64));
	// stfs f1,3428(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3428, temp.u32);
	// fnmsubs f12,f25,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f25.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f30,4216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4216);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f6,f2
	ctx.f1.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// stfs f31,2860(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2860, temp.u32);
	// stfs f23,3020(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3020, temp.u32);
	// stfs f29,2000(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2000, temp.u32);
	// stfs f20,3840(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3840, temp.u32);
	// stfs f3,1876(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1876, temp.u32);
	// stfs f22,2596(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2596, temp.u32);
	// stfs f18,2812(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2812, temp.u32);
	// stfs f16,2112(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2112, temp.u32);
	// stfs f14,1984(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1984, temp.u32);
	// stfs f13,1864(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1864, temp.u32);
	// stfs f0,5412(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5412, temp.u32);
	// stfs f15,5144(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5144, temp.u32);
	// lfs f0,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fmadds f13,f28,f0,f7
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f0,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f0.f64 = double(temp.f32);
	// stfs f6,3752(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3752, temp.u32);
	// fmr f6,f0
	ctx.f6.f64 = ctx.f0.f64;
	// lfs f29,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f29.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f31,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f8,f19,f29,f8
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f29.f64 - ctx.f8.f64));
	// stfs f9,5168(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5168, temp.u32);
	// fmuls f11,f11,f31
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f31.f64));
	// lfs f9,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f9.f64 = double(temp.f32);
	// fmr f31,f27
	ctx.f31.f64 = ctx.f27.f64;
	// lfs f29,3492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3492);
	ctx.f29.f64 = double(temp.f32);
	// fmr f7,f9
	ctx.f7.f64 = ctx.f9.f64;
	// lfs f3,-19348(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19348);
	ctx.f3.f64 = double(temp.f32);
	// lis r9,-32254
	ctx.r9.s64 = -2113798144;
	// stfs f3,844(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 844, temp.u32);
	// lis r8,-32254
	ctx.r8.s64 = -2113798144;
	// stfs f17,3832(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3832, temp.u32);
	// fnmsubs f13,f29,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f17,2900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2900);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f12,f21,f6,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f6.f64 - ctx.f12.f64)));
	// lfs f21,2228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2228);
	ctx.f21.f64 = double(temp.f32);
	// stfs f10,2900(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2900, temp.u32);
	// fmuls f6,f30,f2
	ctx.f6.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// lfs f10,2852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2852);
	ctx.f10.f64 = double(temp.f32);
	// lfs f28,3300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3300);
	ctx.f28.f64 = double(temp.f32);
	// fmsubs f11,f4,f28,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f28.f64 - ctx.f11.f64));
	// lfs f9,4224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4224);
	ctx.f9.f64 = double(temp.f32);
	// lfs f4,2908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2908);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f27,f9,f28
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f28.f64));
	// stfs f6,2908(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2908, temp.u32);
	// lfs f6,2892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2892);
	ctx.f6.f64 = double(temp.f32);
	// stfs f9,2852(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2852, temp.u32);
	// fnmsubs f13,f10,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f9,1964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1964);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f12,f21,f31,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f31.f64 - ctx.f12.f64)));
	// lfs f21,4336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4336);
	ctx.f21.f64 = double(temp.f32);
	// lfs f15,3604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3604);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f21,f2
	ctx.f16.f64 = double(float(ctx.f21.f64 * ctx.f2.f64));
	// lfs f29,4232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4232);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,4240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4240);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f24,f29,f2
	ctx.f24.f64 = double(float(ctx.f29.f64 * ctx.f2.f64));
	// lfs f23,4248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4248);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f22,f25,f28
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f3,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f20,f23,f2
	ctx.f20.f64 = double(float(ctx.f23.f64 * ctx.f2.f64));
	// lfs f31,4256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4256);
	ctx.f31.f64 = double(temp.f32);
	// stfs f1,4336(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4336, temp.u32);
	// fmuls f18,f31,f2
	ctx.f18.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f30,2820(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2820, temp.u32);
	// lfs f19,4344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4344);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f13,f9,f0,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f13.f64));
	// fmadds f12,f5,f6,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f12.f64));
	// lfs f1,1972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1972);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f14,f19,f28
	ctx.f14.f64 = double(float(ctx.f19.f64 * ctx.f28.f64));
	// lfs f30,3500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3500);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f1,f1,f2
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// stfs f3,3604(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3604, temp.u32);
	// fmuls f3,f15,f28
	ctx.f3.f64 = double(float(ctx.f15.f64 * ctx.f28.f64));
	// lfs f9,2884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2884);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f30,f5,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f8.f64));
	// stfs f26,4344(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4344, temp.u32);
	// fmuls f26,f17,f28
	ctx.f26.f64 = double(float(ctx.f17.f64 * ctx.f28.f64));
	// stfs f15,1972(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1972, temp.u32);
	// lfs f10,-19352(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19352);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,-19356(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -19356);
	ctx.f7.f64 = double(temp.f32);
	// lfs f15,-19360(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -19360);
	ctx.f15.f64 = double(temp.f32);
	// stfs f27,4256(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4256, temp.u32);
	// stfs f24,4248(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4248, temp.u32);
	// stfs f29,2040(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2040, temp.u32);
	// stfs f22,4240(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4240, temp.u32);
	// stfs f25,3744(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3744, temp.u32);
	// stfs f6,3824(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3824, temp.u32);
	// stfs f9,2884(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2884, temp.u32);
	// stfs f20,4232(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4232, temp.u32);
	// stfs f23,5184(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5184, temp.u32);
	// stfs f18,4224(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4224, temp.u32);
	// lfs f6,1956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1956);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f0,f6,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f10,964(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 964, temp.u32);
	// lfs f13,1972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1972);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,1808(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1808, temp.u32);
	// lfs f10,3604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3604);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,1948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1948);
	ctx.f13.f64 = double(temp.f32);
	// stfs f7,760(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 760, temp.u32);
	// lfs f7,3292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3292);
	ctx.f7.f64 = double(temp.f32);
	// stfs f26,2228(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2228, temp.u32);
	// lfs f24,3652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3652);
	ctx.f24.f64 = double(temp.f32);
	// lfs f27,2128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2128);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f13,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f13.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f13,2900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2900);
	ctx.f13.f64 = double(temp.f32);
	// lfs f30,3196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3196);
	ctx.f30.f64 = double(temp.f32);
	// lfs f25,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f30,f30,f27
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// lfs f20,4472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4472);
	ctx.f20.f64 = double(temp.f32);
	// fmr f27,f15
	ctx.f27.f64 = ctx.f15.f64;
	// lfs f22,4464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4464);
	ctx.f22.f64 = double(temp.f32);
	// stfs f15,996(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 996, temp.u32);
	// fmuls f15,f20,f28
	ctx.f15.f64 = double(float(ctx.f20.f64 * ctx.f28.f64));
	// stfs f17,5120(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5120, temp.u32);
	// fmuls f17,f22,f28
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f28.f64));
	// lfs f5,1752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1752);
	ctx.f5.f64 = double(temp.f32);
	// stfs f19,1964(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1964, temp.u32);
	// stfs f31,3492(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3492, temp.u32);
	// fmadds f0,f13,f9,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f9.f64 + ctx.f0.f64));
	// stfs f0,1972(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1972, temp.u32);
	// lfs f0,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f0,f7,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f9,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f9.f64 = double(temp.f32);
	// lfs f11,2876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2876);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f7,f4,f28
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// fnmsubs f11,f11,f9,f8
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f9.f64 - ctx.f8.f64)));
	// lfs f26,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f26.f64 = double(temp.f32);
	// lfs f9,4400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4400);
	ctx.f9.f64 = double(temp.f32);
	// stfs f2,4400(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4400, temp.u32);
	// fmuls f31,f9,f5
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f5.f64));
	// stfs f3,2892(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2892, temp.u32);
	// stfs f17,4464(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4464, temp.u32);
	// lfs f3,4424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4424);
	ctx.f3.f64 = double(temp.f32);
	// lfs f17,2916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2916);
	ctx.f17.f64 = double(temp.f32);
	// stfs f21,2168(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2168, temp.u32);
	// fnmsubs f0,f24,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f24.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// lfs f24,2868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2868);
	ctx.f24.f64 = double(temp.f32);
	// lfs f26,4440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4440);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f11,f24,f25,f11
	ctx.f11.f64 = double(float(-(ctx.f24.f64 * ctx.f25.f64 - ctx.f11.f64)));
	// lfs f24,4456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4456);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f26,f2
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f2.f64));
	// stfs f15,4456(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4456, temp.u32);
	// lfs f2,3308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3308);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f19,f24,f28
	ctx.f19.f64 = double(float(ctx.f24.f64 * ctx.f28.f64));
	// lfs f25,4448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4448);
	ctx.f25.f64 = double(temp.f32);
	// lfs f15,1400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1400);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f21,f25,f28
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// stfs f5,3604(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3604, temp.u32);
	// stfs f19,4472(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4472, temp.u32);
	// fmuls f19,f2,f15
	ctx.f19.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f5,4480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4480);
	ctx.f5.f64 = double(temp.f32);
	// stfs f2,4424(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4424, temp.u32);
	// fmuls f2,f17,f2
	ctx.f2.f64 = double(float(ctx.f17.f64 * ctx.f2.f64));
	// stfs f21,4480(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4480, temp.u32);
	// lfs f21,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f21.f64 = double(temp.f32);
	// stfs f2,4448(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4448, temp.u32);
	// fmuls f1,f1,f21
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f21.f64));
	// lfs f13,1940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1940);
	ctx.f13.f64 = double(temp.f32);
	// lfs f8,4408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4408);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f10,f28,f13
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f13.f64));
	// lfs f6,4416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4416);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f29,f8,f3
	ctx.f29.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// lfs f18,3228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3228);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f27,f6,f27
	ctx.f27.f64 = double(float(ctx.f6.f64 * ctx.f27.f64));
	// lfs f2,1932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1932);
	ctx.f2.f64 = double(temp.f32);
	// stfs f16,4216(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4216, temp.u32);
	// fmuls f2,f2,f18
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f18.f64));
	// stfs f14,3300(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3300, temp.u32);
	// lfs f16,3148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3148);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,5076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5076);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,3508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3508);
	ctx.f21.f64 = double(temp.f32);
	// stfs f23,3308(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3308, temp.u32);
	// stfs f1,2916(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2916, temp.u32);
	// lfs f1,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f30,f1,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f1.f64 + ctx.f12.f64));
	// stfs f28,3508(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3508, temp.u32);
	// lfs f28,3912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3912);
	ctx.f28.f64 = double(temp.f32);
	// lfs f23,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f23.f64 = double(temp.f32);
	// lfs f1,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f31,f31,f23,f29
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f23.f64 + ctx.f29.f64));
	// fmadds f1,f28,f1,f0
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f1.f64 + ctx.f0.f64));
	// lfs f23,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f23.f64 = double(temp.f32);
	// lfs f0,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f28,f5,f23,f27
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f23.f64 + ctx.f27.f64));
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f27,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f27.f64 = double(temp.f32);
	// stfs f13,3500(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3500, temp.u32);
	// fmuls f27,f9,f27
	ctx.f27.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// stfs f10,4440(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4440, temp.u32);
	// lfs f10,3904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3904);
	ctx.f10.f64 = double(temp.f32);
	// lfs f23,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f23.f64 = double(temp.f32);
	// lfs f13,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f11,f10,f23,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f23.f64 + ctx.f11.f64));
	// stfs f9,2900(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2900, temp.u32);
	// fmr f9,f13
	ctx.f9.f64 = ctx.f13.f64;
	// stfs f8,3532(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3532, temp.u32);
	// lfs f8,4344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4344);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f8,f13,f1
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f13.f64 - ctx.f1.f64)));
	// lfs f8,4336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4336);
	ctx.f8.f64 = double(temp.f32);
	// lfs f29,1980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1980);
	ctx.f29.f64 = double(temp.f32);
	// stfs f7,1980(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1980, temp.u32);
	// fmuls f30,f29,f14
	ctx.f30.f64 = double(float(ctx.f29.f64 * ctx.f14.f64));
	// stfs f6,3912(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3912, temp.u32);
	// lfs f6,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,4256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4256);
	ctx.f7.f64 = double(temp.f32);
	// fmr f1,f6
	ctx.f1.f64 = ctx.f6.f64;
	// lfs f0,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f8,f9,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f9.f64 + ctx.f11.f64));
	// lfs f8,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,2908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2908);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f31,f0
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// stfs f21,4408(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4408, temp.u32);
	// fmadds f12,f2,f0,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f12.f64));
	// fnmsubs f13,f9,f8,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f8.f64 - ctx.f13.f64)));
	// lfs f2,4240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4240);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f21,f21,f16
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// stfs f4,3904(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3904, temp.u32);
	// fmr f0,f8
	ctx.f0.f64 = ctx.f8.f64;
	// stfs f5,3792(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3792, temp.u32);
	// lfs f4,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,4248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4248);
	ctx.f5.f64 = double(temp.f32);
	// stfs f26,1940(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1940, temp.u32);
	// stfs f25,2128(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2128, temp.u32);
	// fnmsubs f11,f7,f6,f11
	ctx.f11.f64 = double(float(-(ctx.f7.f64 * ctx.f6.f64 - ctx.f11.f64)));
	// stfs f24,1956(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1956, temp.u32);
	// stfs f22,3148(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3148, temp.u32);
	// fmr f6,f4
	ctx.f6.f64 = ctx.f4.f64;
	// stfs f20,1948(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1948, temp.u32);
	// fmadds f13,f5,f4,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 + ctx.f13.f64));
	// lfs f5,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f5.f64 = double(temp.f32);
	// stfs f18,2868(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2868, temp.u32);
	// fmr f20,f5
	ctx.f20.f64 = ctx.f5.f64;
	// lfs f9,4232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4232);
	ctx.f9.f64 = double(temp.f32);
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// lfs f8,4224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4224);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,4216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4216);
	ctx.f7.f64 = double(temp.f32);
	// lfs f31,3300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3300);
	ctx.f31.f64 = double(temp.f32);
	// lfs f26,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f11,f2,f1,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f11.f64));
	// lfs f1,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f1.f64 = double(temp.f32);
	// lfs f2,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f10,f21,f1,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f1.f64 - ctx.f10.f64));
	// lfs f25,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f25.f64 = double(temp.f32);
	// fmr f1,f2
	ctx.f1.f64 = ctx.f2.f64;
	// lfs f24,2228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2228);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,2892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2892);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,1980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1980);
	ctx.f22.f64 = double(temp.f32);
	// lfs f21,1972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1972);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,2924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2924);
	ctx.f18.f64 = double(temp.f32);
	// stfs f16,1932(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1932, temp.u32);
	// stfs f14,3652(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3652, temp.u32);
	// stfs f15,3292(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3292, temp.u32);
	// stfs f17,2876(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2876, temp.u32);
	// fnmsubs f13,f9,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f9,2684(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2684);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f11,f8,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f9,2908(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2908, temp.u32);
	// fmuls f15,f29,f9
	ctx.f15.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// lfs f9,2916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2916);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f0.f64 = double(temp.f32);
	// fadds f12,f21,f12
	ctx.f12.f64 = double(float(ctx.f21.f64 + ctx.f12.f64));
	// lfs f21,1204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	ctx.f21.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f8,2932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2932);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f16,f18,f21
	ctx.f16.f64 = double(float(ctx.f18.f64 * ctx.f21.f64));
	// lfs f17,3316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3316);
	ctx.f17.f64 = double(temp.f32);
	// stfs f15,3316(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3316, temp.u32);
	// fmuls f15,f8,f21
	ctx.f15.f64 = double(float(ctx.f8.f64 * ctx.f21.f64));
	// stfs f29,2932(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2932, temp.u32);
	// stfs f15,1980(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1980, temp.u32);
	// fnmsubs f13,f7,f6,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f6.f64 - ctx.f13.f64)));
	// lfs f15,1912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1912);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f11,f31,f26,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 + ctx.f11.f64));
	// lfs f29,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f15,f29
	ctx.f29.f64 = double(float(ctx.f15.f64 * ctx.f29.f64));
	// lfs f15,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f30,f30,f15
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// lfs f14,3704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3704);
	ctx.f14.f64 = double(temp.f32);
	// fmr f15,f5
	ctx.f15.f64 = ctx.f5.f64;
	// stfs f21,3704(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3704, temp.u32);
	// stfs f29,4416(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4416, temp.u32);
	// lfs f29,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f29.f64 = double(temp.f32);
	// stfs f14,2924(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2924, temp.u32);
	// fmuls f14,f14,f3
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// stfs f8,4256(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4256, temp.u32);
	// fnmsubs f10,f19,f29,f10
	ctx.f10.f64 = double(float(-(ctx.f19.f64 * ctx.f29.f64 - ctx.f10.f64)));
	// stfs f12,5416(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5416, temp.u32);
	// fmadds f13,f9,f25,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f25.f64 + ctx.f13.f64));
	// lfs f12,3508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3508);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f11,f24,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f7,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,4448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4448);
	ctx.f6.f64 = double(temp.f32);
	// stfs f12,3508(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3508, temp.u32);
	// lfs f21,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f21.f64 = double(temp.f32);
	// lfs f31,4424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4424);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f17,f21
	ctx.f21.f64 = double(float(ctx.f17.f64 * ctx.f21.f64));
	// stfs f3,1972(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1972, temp.u32);
	// lfs f29,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f29.f64 = double(temp.f32);
	// stfs f18,4336(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4336, temp.u32);
	// stfs f17,4344(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4344, temp.u32);
	// fmadds f0,f23,f0,f13
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f13.f64));
	// fnmsubs f13,f22,f5,f11
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// lfs f11,3308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3308);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,-19364(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19364);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,524(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 524, temp.u32);
	// fnmsubs f0,f11,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f11.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// lfs f11,4480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4480);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f13,f11,f2,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f2.f64 + ctx.f13.f64));
	// lfs f11,4472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4472);
	ctx.f11.f64 = double(temp.f32);
	// lfs f4,4440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4440);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,2588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2588);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f0,f11,f20,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f20.f64 + ctx.f0.f64));
	// lfs f11,4464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4464);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f13,f11,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// lfs f11,4456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4456);
	ctx.f11.f64 = double(temp.f32);
	// lfs f1,1784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1784);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f11,f11,f15,f0
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f15.f64 - ctx.f0.f64)));
	// lfs f0,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f0.f64 = double(temp.f32);
	// fmr f8,f0
	ctx.f8.f64 = ctx.f0.f64;
	// fmuls f9,f14,f0
	ctx.f9.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// fmadds f13,f28,f12,f13
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f12.f64 + ctx.f13.f64));
	// lfs f12,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f6,f0,f30
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f30.f64));
	// lfs f6,4648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4648);
	ctx.f6.f64 = double(temp.f32);
	// lfs f28,2932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2932);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f12,f27,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f12.f64));
	// fmuls f30,f31,f2
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// fnmsubs f11,f4,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f7,3704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3704);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f10,f16,f8,f10
	ctx.f10.f64 = double(float(ctx.f16.f64 * ctx.f8.f64 + ctx.f10.f64));
	// lfs f8,4656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4656);
	ctx.f8.f64 = double(temp.f32);
	// lfs f4,2940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2940);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f5,f6,f7
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// fmuls f3,f8,f7
	ctx.f3.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f25,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f25.f64 = double(temp.f32);
	// fadds f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// fmr f24,f25
	ctx.f24.f64 = ctx.f25.f64;
	// lfs f26,1856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1856);
	ctx.f26.f64 = double(temp.f32);
	// lfs f23,3316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3316);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f26,f25
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f25.f64));
	// lfs f25,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f29,f4,f29
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// fnmsubs f0,f12,f25,f0
	ctx.f0.f64 = double(float(-(ctx.f12.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// lfs f20,2096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2096);
	ctx.f20.f64 = double(temp.f32);
	// lfs f12,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f17,f20,f28
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f28.f64));
	// lfs f11,3324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3324);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f27,f28,f1
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// lfs f16,2948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2948);
	ctx.f16.f64 = double(temp.f32);
	// stfs f17,3324(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3324, temp.u32);
	// fmuls f17,f16,f7
	ctx.f17.f64 = double(float(ctx.f16.f64 * ctx.f7.f64));
	// stfs f17,2948(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2948, temp.u32);
	// lfs f17,2956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2956);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f9,f23,f24,f9
	ctx.f9.f64 = double(float(ctx.f23.f64 * ctx.f24.f64 - ctx.f9.f64));
	// lfs f23,1980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1980);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f23,f12,f10
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f12.f64 - ctx.f10.f64)));
	// lfs f10,2136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2136);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f22,f31,f10
	ctx.f22.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// lfs f14,3516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3516);
	ctx.f14.f64 = double(temp.f32);
	// stfs f22,2956(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2956, temp.u32);
	// lfs f22,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f22.f64 = double(temp.f32);
	// stfs f31,4656(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4656, temp.u32);
	// fmuls f31,f14,f31
	ctx.f31.f64 = double(float(ctx.f14.f64 * ctx.f31.f64));
	// lfs f25,2924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2924);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,4820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4820);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,5092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5092);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f28,f23
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f24,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f15,f28,f18
	ctx.f15.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// fmadds f9,f5,f22,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f22.f64 + ctx.f9.f64));
	// stfs f31,2940(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2940, temp.u32);
	// lfs f31,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f24,f25,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// stfs f28,3516(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3516, temp.u32);
	// fmr f5,f22
	ctx.f5.f64 = ctx.f22.f64;
	// fmuls f28,f17,f7
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// stfs f28,4648(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4648, temp.u32);
	// fmadds f31,f11,f31,f29
	ctx.f31.f64 = double(float(ctx.f11.f64 * ctx.f31.f64 + ctx.f29.f64));
	// lfs f28,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,4416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4416);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f29,f29,f28,f21
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f28.f64 + ctx.f21.f64));
	// lfs f28,2032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2032);
	ctx.f28.f64 = double(temp.f32);
	// lfs f21,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f28,f28,f21,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f21.f64 + ctx.f26.f64));
	// lfs f26,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f30,f30,f26
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// stfs f11,4440(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4440, temp.u32);
	// lfs f11,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f24,f24,f7
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// stfs f10,1980(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1980, temp.u32);
	// fnmsubs f12,f3,f5,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f5.f64 - ctx.f12.f64)));
	// stfs f6,4248(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4248, temp.u32);
	// fmadds f11,f19,f11,f9
	ctx.f11.f64 = double(float(ctx.f19.f64 * ctx.f11.f64 + ctx.f9.f64));
	// lfs f26,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f26.f64 = double(temp.f32);
	// lfs f10,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,1848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1848);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,4408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4408);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f0,f29,f7,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f7.f64 + ctx.f0.f64));
	// stfs f13,5420(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5420, temp.u32);
	// fmuls f13,f27,f26
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// stfs f8,4448(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4448, temp.u32);
	// fmadds f9,f6,f9,f15
	ctx.f9.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 + ctx.f15.f64));
	// lfs f8,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f8.f64 = double(temp.f32);
	// stfs f4,4416(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4416, temp.u32);
	// stfs f23,2916(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2916, temp.u32);
	// fmadds f10,f24,f10,f30
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f10.f64 + ctx.f30.f64));
	// stfs f25,4472(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4472, temp.u32);
	// fmadds f12,f31,f7,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f12.f64));
	// stfs f2,2932(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2932, temp.u32);
	// stfs f1,3704(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3704, temp.u32);
	// stfs f20,3308(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3308, temp.u32);
	// stfs f18,3316(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3316, temp.u32);
	// stfs f16,4424(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4424, temp.u32);
	// stfs f14,2924(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2924, temp.u32);
	// stfs f17,4480(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4480, temp.u32);
	// fmr f5,f8
	ctx.f5.f64 = ctx.f8.f64;
	// lfs f4,2956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2956);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f8,f4,f8,f0
	ctx.f8.f64 = double(float(-(ctx.f4.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f26,3324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3324);
	ctx.f26.f64 = double(temp.f32);
	// lfs f0,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,3516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3516);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,4784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4784);
	ctx.f31.f64 = double(temp.f32);
	// lfs f3,4744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4744);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,1744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1744);
	ctx.f30.f64 = double(temp.f32);
	// fadds f29,f31,f3
	ctx.f29.f64 = double(float(ctx.f31.f64 + ctx.f3.f64));
	// lfs f27,2948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2948);
	ctx.f27.f64 = double(temp.f32);
	// lfs f22,4856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4856);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f12,f27,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f21,3332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3332);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f11,f26,f5,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// lfs f26,2940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2940);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f10,f26,f0,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f23,4824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4824);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f2,f30
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// lfs f27,4792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4792);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f8,f28,f6,f8
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f6.f64 + ctx.f8.f64));
	// lfs f28,4848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4848);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f16,f21,f22
	ctx.f16.f64 = double(float(ctx.f21.f64 * ctx.f22.f64));
	// stfs f16,4856(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4856, temp.u32);
	// lfs f16,4656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4656);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f18,f28,f23
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f17,1960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1960);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f24,f27,f7
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f7.f64));
	// stfs f18,3332(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3332, temp.u32);
	// fmuls f18,f16,f17
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f17.f64));
	// lfs f15,3124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3124);
	ctx.f15.f64 = double(temp.f32);
	// lfs f5,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f5.f64 = double(temp.f32);
	// stfs f18,4848(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4848, temp.u32);
	// fmuls f5,f2,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// lfs f4,4736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4736);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f11,f9,f0,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f11.f64));
	// fmadds f29,f29,f23,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f23.f64 + ctx.f26.f64));
	// lfs f18,2964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2964);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f23,f6,f15
	ctx.f23.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// stfs f23,2964(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2964, temp.u32);
	// fnmsubs f13,f13,f0,f8
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f23,628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	ctx.f23.f64 = double(temp.f32);
	// lfs f8,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f1,f4,f2
	ctx.f1.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f26,3612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3612);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f8,f8,f23
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// stfs f23,3612(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3612, temp.u32);
	// fmadds f10,f24,f0,f10
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f23,2972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2972);
	ctx.f23.f64 = double(temp.f32);
	// stfs f29,2972(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2972, temp.u32);
	// lfs f29,4648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4648);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,2120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2120);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f29,f0,f12
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f9,1840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1840);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f20,f6,f25
	ctx.f20.f64 = double(float(ctx.f6.f64 * ctx.f25.f64));
	// lfs f24,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f16,f9
	ctx.f29.f64 = double(float(ctx.f16.f64 * ctx.f9.f64));
	// stfs f7,4824(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4824, temp.u32);
	// fmuls f5,f5,f24
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f24.f64));
	// stfs f25,4656(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4656, temp.u32);
	// lfs f19,2008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2008);
	ctx.f19.f64 = double(temp.f32);
	// lfs f7,2404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2404);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f14,f6,f19
	ctx.f14.f64 = double(float(ctx.f6.f64 * ctx.f19.f64));
	// lfs f12,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f12.f64 = double(temp.f32);
	// lfs f16,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f12,f12,f7
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// lfs f25,4400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4400);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f1,f16,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f16.f64 - ctx.f11.f64)));
	// stfs f4,4736(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4736, temp.u32);
	// fmuls f24,f23,f25
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// stfs f3,2956(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2956, temp.u32);
	// fadds f4,f26,f18
	ctx.f4.f64 = double(float(ctx.f26.f64 + ctx.f18.f64));
	// stfs f31,3324(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3324, temp.u32);
	// stfs f30,2228(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2228, temp.u32);
	// stfs f27,4232(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4232, temp.u32);
	// stfs f28,2948(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2948, temp.u32);
	// stfs f22,4792(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4792, temp.u32);
	// stfs f21,4464(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4464, temp.u32);
	// stfs f19,4648(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4648, temp.u32);
	// stfs f2,4784(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4784, temp.u32);
	// stfs f17,2940(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2940, temp.u32);
	// stfs f7,2032(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2032, temp.u32);
	// fmr f23,f16
	ctx.f23.f64 = ctx.f16.f64;
	// lfs f7,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f7.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f6,4744(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4744, temp.u32);
	// fmsubs f10,f14,f7,f10
	ctx.f10.f64 = double(float(ctx.f14.f64 * ctx.f7.f64 - ctx.f10.f64));
	// stfs f9,3516(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3516, temp.u32);
	// lfs f9,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f13,f20,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// fmr f7,f6
	ctx.f7.f64 = ctx.f6.f64;
	// lfs f3,3332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3332);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f6,f3,f6,f0
	ctx.f6.f64 = double(float(ctx.f3.f64 * ctx.f6.f64 + ctx.f0.f64));
	// lfs f3,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,2972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2972);
	ctx.f2.f64 = double(temp.f32);
	// lfs f28,4856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4856);
	ctx.f28.f64 = double(temp.f32);
	// stfs f26,3228(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3228, temp.u32);
	// lfs f26,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f26.f64 = double(temp.f32);
	// lfs f30,4092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4092);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f10,f5,f26,f10
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f26.f64 - ctx.f10.f64)));
	// lfs f9,2104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2104);
	ctx.f9.f64 = double(temp.f32);
	// lfs f26,4848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4848);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f8,f8,f9
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// fnmsubs f13,f28,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f3,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f11,f2,f7,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f11.f64));
	// lfs f28,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,3604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3604);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f3,f30,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// lfs f1,1376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1376);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f28,f28,f27
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// lfs f30,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f1,f30
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f7,4896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4896);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f2,4904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4904);
	ctx.f2.f64 = double(temp.f32);
	// lfs f22,3612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3612);
	ctx.f22.f64 = double(temp.f32);
	// fadds f31,f2,f7
	ctx.f31.f64 = double(float(ctx.f2.f64 + ctx.f7.f64));
	// lfs f20,4968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4968);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f0,f8,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fadds f13,f6,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// stfs f18,1960(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1960, temp.u32);
	// fmadds f11,f26,f23,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f17,4976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4976);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f20,f22
	ctx.f18.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// stfs f18,4976(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4976, temp.u32);
	// lfs f14,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f3,f3,f22
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f22.f64));
	// lfs f18,2964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2964);
	ctx.f18.f64 = double(temp.f32);
	// lfs f5,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f10,f18,f14,f10
	ctx.f10.f64 = double(float(ctx.f18.f64 * ctx.f14.f64 + ctx.f10.f64));
	// fmuls f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// lfs f26,4936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4936);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,2748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2748);
	ctx.f24.f64 = double(temp.f32);
	// lfs f18,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f8,f24,f26
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// lfs f21,2396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2396);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f31,f31,f22
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f22.f64));
	// lfs f14,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f28,f18,f21,f28
	ctx.f28.f64 = double(float(ctx.f18.f64 * ctx.f21.f64 + ctx.f28.f64));
	// fadds f13,f13,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f11.f64));
	// stfs f25,4936(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4936, temp.u32);
	// lfs f24,4060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4060);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f30,f30,f14
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f23,2208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2208);
	ctx.f23.f64 = double(temp.f32);
	// lfs f6,3052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3052);
	ctx.f6.f64 = double(temp.f32);
	// lfs f19,1776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1776);
	ctx.f19.f64 = double(temp.f32);
	// lfs f16,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f16.f64 = double(temp.f32);
	// lfs f25,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f16,f23,f16
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f16.f64));
	// lfs f18,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f25,f6,f25
	ctx.f25.f64 = double(float(ctx.f6.f64 * ctx.f25.f64));
	// lfs f11,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f18,f19,f18
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// stfs f15,4456(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4456, temp.u32);
	// fmuls f15,f17,f24
	ctx.f15.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// stfs f13,4968(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4968, temp.u32);
	// fmadds f12,f4,f11,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfs f14,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f14.f64 = double(temp.f32);
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,-19368(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19368);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f0,f5,f14,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f14.f64 - ctx.f0.f64));
	// stfs f7,4240(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4240, temp.u32);
	// fmuls f13,f8,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// stfs f11,968(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 968, temp.u32);
	// lfs f5,4984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4984);
	ctx.f5.f64 = double(temp.f32);
	// lfs f11,3540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3540);
	ctx.f11.f64 = double(temp.f32);
	// lfs f8,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f5,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f8.f64));
	// stfs f6,3332(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3332, temp.u32);
	// fmadds f7,f11,f7,f25
	ctx.f7.f64 = double(float(ctx.f11.f64 * ctx.f7.f64 + ctx.f25.f64));
	// lfs f6,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f6,f28,f6,f30
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f6.f64 - ctx.f30.f64));
	// fmadds f12,f16,f4,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// stfs f24,4984(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4984, temp.u32);
	// fnmsubs f0,f15,f25,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// lfs f24,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f10,f29,f24,f10
	ctx.f10.f64 = double(float(-(ctx.f29.f64 * ctx.f24.f64 - ctx.f10.f64)));
	// lfs f28,5008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5008);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f24.f64 = double(temp.f32);
	// stfs f27,2096(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2096, temp.u32);
	// fmadds f8,f7,f25,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f8.f64));
	// lfs f27,5032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5032);
	ctx.f27.f64 = double(temp.f32);
	// lfs f7,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f7.f64 = double(temp.f32);
	// stfs f19,1752(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1752, temp.u32);
	// fmadds f6,f28,f24,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f24.f64 + ctx.f6.f64));
	// lfs f19,5056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5056);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f12,f27,f7,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f24,4888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4888);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f15,f19,f22
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f22.f64));
	// stfs f20,2208(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2208, temp.u32);
	// fmuls f20,f17,f24
	ctx.f20.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// lfs f25,5040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5040);
	ctx.f25.f64 = double(temp.f32);
	// lfs f7,3644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3644);
	ctx.f7.f64 = double(temp.f32);
	// lfs f16,2740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2740);
	ctx.f16.f64 = double(temp.f32);
	// stfs f9,2136(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2136, temp.u32);
	// fmuls f9,f18,f22
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f22.f64));
	// stfs f23,4896(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4896, temp.u32);
	// fmuls f23,f25,f7
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// stfs f15,5008(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5008, temp.u32);
	// fmuls f15,f25,f16
	ctx.f15.f64 = double(float(ctx.f25.f64 * ctx.f16.f64));
	// stfs f1,2588(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2588, temp.u32);
	// stfs f21,1744(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1744, temp.u32);
	// fmadds f13,f12,f22,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f22.f64 + ctx.f13.f64));
	// stfs f20,5040(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5040, temp.u32);
	// lfs f1,4824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4824);
	ctx.f1.f64 = double(temp.f32);
	// lfs f21,5048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5048);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,2236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2236);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f18,f22,f21
	ctx.f18.f64 = double(float(ctx.f22.f64 * ctx.f21.f64));
	// lfs f4,4992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4992);
	ctx.f4.f64 = double(temp.f32);
	// stfs f23,2236(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2236, temp.u32);
	// fmuls f23,f20,f1
	ctx.f23.f64 = double(float(ctx.f20.f64 * ctx.f1.f64));
	// stfs f15,5032(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5032, temp.u32);
	// fmuls f30,f4,f1
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// lfs f14,3220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3220);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,2232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2232);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// stfs f18,5048(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5048, temp.u32);
	// stfs f23,2232(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2232, temp.u32);
	// lfs f18,1992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1992);
	ctx.f18.f64 = double(temp.f32);
	// lfs f23,2240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2240);
	ctx.f23.f64 = double(temp.f32);
	// stfs f25,2240(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2240, temp.u32);
	// fmuls f25,f18,f1
	ctx.f25.f64 = double(float(ctx.f18.f64 * ctx.f1.f64));
	// stfs f25,5056(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5056, temp.u32);
	// lfs f25,2248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2248);
	ctx.f25.f64 = double(temp.f32);
	// lfs f12,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f12.f64 = double(temp.f32);
	// stfs f22,2248(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2248, temp.u32);
	// fmuls f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f12.f64));
	// lfs f22,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f22.f64 = double(temp.f32);
	// stfs f2,4408(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4408, temp.u32);
	// fmadds f0,f31,f22,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f22.f64 + ctx.f0.f64));
	// lfs f2,5000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5000);
	ctx.f2.f64 = double(temp.f32);
	// fmr f31,f22
	ctx.f31.f64 = ctx.f22.f64;
	// lfs f29,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f29.f64 = double(temp.f32);
	// lfs f9,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f29,f2,f29
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f29.f64));
	// fmadds f10,f30,f9,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f9.f64 + ctx.f10.f64));
	// stfs f7,3612(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3612, temp.u32);
	// fmr f7,f22
	ctx.f7.f64 = ctx.f22.f64;
	// lfs f30,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f8,f15,f31,f8
	ctx.f8.f64 = double(float(ctx.f15.f64 * ctx.f31.f64 + ctx.f8.f64));
	// fnmsubs f0,f3,f30,f0
	ctx.f0.f64 = double(float(-(ctx.f3.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// lfs f3,2240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2240);
	ctx.f3.f64 = double(temp.f32);
	// stfs f11,1912(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1912, temp.u32);
	// fmadds f31,f25,f31,f29
	ctx.f31.f64 = double(float(ctx.f25.f64 * ctx.f31.f64 + ctx.f29.f64));
	// lfs f9,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f9.f64 = double(temp.f32);
	// fmr f29,f22
	ctx.f29.f64 = ctx.f22.f64;
	// lfs f11,1768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1768);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f9,f23,f9,f6
	ctx.f9.f64 = double(float(ctx.f23.f64 * ctx.f9.f64 + ctx.f6.f64));
	// stfs f4,4856(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4856, temp.u32);
	// lfs f4,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f4.f64 = double(temp.f32);
	// stfs f5,4216(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4216, temp.u32);
	// fmuls f5,f11,f17
	ctx.f5.f64 = double(float(ctx.f11.f64 * ctx.f17.f64));
	// stfs f2,3604(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3604, temp.u32);
	// fmuls f7,f3,f7
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// lfs f3,2236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2236);
	ctx.f3.f64 = double(temp.f32);
	// fmsubs f13,f3,f4,f13
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f4.f64 - ctx.f13.f64));
	// lfs f4,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,2232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2232);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f10,f2,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f2,5056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5056);
	ctx.f2.f64 = double(temp.f32);
	// lfs f3,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f2,f3
	ctx.f3.f64 = double(float(ctx.f2.f64 * ctx.f3.f64));
	// stfs f1,5000(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5000, temp.u32);
	// lfs f1,2248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2248);
	ctx.f1.f64 = double(temp.f32);
	// fmsubs f12,f9,f1,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f1.f64 - ctx.f12.f64));
	// stfs f27,3644(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3644, temp.u32);
	// lfs f2,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f2.f64 = double(temp.f32);
	// lfs f9,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f9.f64 = double(temp.f32);
	// fmsubs f7,f5,f29,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f29.f64 - ctx.f7.f64));
	// lfs f27,4976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4976);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,5048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5048);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f0,f27,f2,f0
	ctx.f0.f64 = double(float(-(ctx.f27.f64 * ctx.f2.f64 - ctx.f0.f64)));
	// lfs f4,4428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4428);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f29,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// fmuls f30,f4,f1
	ctx.f30.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// lfs f27,4924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4924);
	ctx.f27.f64 = double(temp.f32);
	// lfs f9,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f9.f64 = double(temp.f32);
	// fmr f29,f22
	ctx.f29.f64 = ctx.f22.f64;
	// stfs f28,4224(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4224, temp.u32);
	// fmadds f9,f27,f9,f8
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f9.f64 + ctx.f8.f64));
	// lfs f28,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f10,f3,f28,f10
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f28.f64 - ctx.f10.f64)));
	// lfs f8,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f8.f64 = double(temp.f32);
	// lfs f3,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f3.f64 = double(temp.f32);
	// lfs f22,5040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5040);
	ctx.f22.f64 = double(temp.f32);
	// lfs f6,1952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1952);
	ctx.f6.f64 = double(temp.f32);
	// stfs f21,2748(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2748, temp.u32);
	// fmuls f6,f6,f26
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f26.f64));
	// lfs f2,4836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4836);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f8,f30,f8,f7
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f8.f64 + ctx.f7.f64));
	// lfs f30,5032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5032);
	ctx.f30.f64 = double(temp.f32);
	// lfs f5,3740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3740);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f0,f22,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f21,4968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4968);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f13,f30,f3,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f3.f64 + ctx.f13.f64));
	// stfs f24,4904(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4904, temp.u32);
	// fmadds f12,f9,f1,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 * ctx.f1.f64 + ctx.f12.f64));
	// stfs f23,3220(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3220, temp.u32);
	// fmuls f26,f2,f1
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// stfs f25,4824(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4824, temp.u32);
	// fmuls f28,f5,f1
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// lfs f25,1652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1652);
	ctx.f25.f64 = double(temp.f32);
	// fadds f10,f21,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 + ctx.f10.f64));
	// lfs f24,3244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3244);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,2780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2780);
	ctx.f23.f64 = double(temp.f32);
	// lfs f29,3756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3756);
	ctx.f29.f64 = double(temp.f32);
	// lfs f22,3748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3748);
	ctx.f22.f64 = double(temp.f32);
	// lfs f7,3588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3588);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,2256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2256);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,2264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2264);
	ctx.f30.f64 = double(temp.f32);
	// lfs f9,2772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2772);
	ctx.f9.f64 = double(temp.f32);
	// stfs f19,4400(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4400, temp.u32);
	// stfs f16,2972(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2972, temp.u32);
	// stfs f14,2964(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2964, temp.u32);
	// stfs f15,2740(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2740, temp.u32);
	// stfs f20,4848(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4848, temp.u32);
	// stfs f18,4992(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4992, temp.u32);
	// fadds f18,f30,f3
	ctx.f18.f64 = double(float(ctx.f30.f64 + ctx.f3.f64));
	// stfs f2,1768(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1768, temp.u32);
	// stfs f18,3756(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3756, temp.u32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// lfs f18,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f7,f22
	ctx.f20.f64 = double(float(ctx.f7.f64 * ctx.f22.f64));
	// lfs f2,5008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5008);
	ctx.f2.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// fnmsubs f0,f2,f18,f0
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f18.f64 - ctx.f0.f64)));
	// lfs f19,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f26,f26,f19
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f19.f64));
	// lfs f19,2272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2272);
	ctx.f19.f64 = double(temp.f32);
	// lfs f14,1412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1412);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f24,f19
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// stfs f26,2272(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2272, temp.u32);
	// fmuls f26,f9,f22
	ctx.f26.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// stfs f14,3740(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3740, temp.u32);
	// fmuls f14,f14,f29
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f29.f64));
	// lfs f16,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f23,f16
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f16.f64));
	// stfs f16,2256(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2256, temp.u32);
	// stfs f9,2248(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2248, temp.u32);
	// lfs f16,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f16.f64 = double(temp.f32);
	// lfs f9,3764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3764);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f0,f31,f1,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f1.f64 + ctx.f0.f64));
	// stfs f4,5032(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5032, temp.u32);
	// stfs f11,4428(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4428, temp.u32);
	// fnmsubs f11,f28,f16,f8
	ctx.f11.f64 = double(float(-(ctx.f28.f64 * ctx.f16.f64 - ctx.f8.f64)));
	// stfs f26,3748(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3748, temp.u32);
	// fmuls f4,f9,f1
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// lfs f26,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f26.f64 = double(temp.f32);
	// stfs f7,2236(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2236, temp.u32);
	// fnmsubs f13,f6,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// stfs f10,5424(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5424, temp.u32);
	// lfs f31,-19372(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19372);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f17.f64 = double(temp.f32);
	// lfs f10,2732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2732);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f17,f25,f17
	ctx.f17.f64 = double(float(ctx.f25.f64 * ctx.f17.f64));
	// lfs f8,4764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4764);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f6,f10,f19,f14
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f19.f64 + ctx.f14.f64));
	// fadds f12,f0,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// lfs f7,3772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3772);
	ctx.f7.f64 = double(temp.f32);
	// fmr f0,f18
	ctx.f0.f64 = ctx.f18.f64;
	// stfs f31,992(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 992, temp.u32);
	// fadds f2,f7,f8
	ctx.f2.f64 = double(float(ctx.f7.f64 + ctx.f8.f64));
	// stfs f30,1400(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1400, temp.u32);
	// fmr f31,f18
	ctx.f31.f64 = ctx.f18.f64;
	// lfs f30,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f30.f64 = double(temp.f32);
	// stfs f3,1952(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1952, temp.u32);
	// fmuls f30,f20,f30
	ctx.f30.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// lfs f3,-19376(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19376);
	ctx.f3.f64 = double(temp.f32);
	// lis r11,-32252
	ctx.r11.s64 = -2113667072;
	// lfs f21,1564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1564);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f20.f64 = double(temp.f32);
	// stfs f3,444(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// stfs f27,5056(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5056, temp.u32);
	// stfs f5,5048(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5048, temp.u32);
	// fnmsubs f11,f4,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f4,2272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2272);
	ctx.f4.f64 = double(temp.f32);
	// stfs f24,1848(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1848, temp.u32);
	// fnmsubs f13,f4,f20,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f20.f64 - ctx.f13.f64)));
	// stfs f23,4924(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4924, temp.u32);
	// fmuls f31,f15,f31
	ctx.f31.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// lfs f28,2540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2540);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f5,1396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1396);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f3.f64 = double(temp.f32);
	// lfs f27,2280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2280);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f3,f5,f3,f17
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f3.f64 + ctx.f17.f64));
	// lfs f24,3236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3236);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,3460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3460);
	ctx.f23.f64 = double(temp.f32);
	// stfs f29,2264(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2264, temp.u32);
	// fmuls f29,f21,f29
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// stfs f25,2240(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2240, temp.u32);
	// fmuls f25,f28,f27
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// stfs f21,2232(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2232, temp.u32);
	// fmuls f21,f28,f24
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// lfs f26,2288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2288);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f6,f23,f22,f6
	ctx.f6.f64 = double(float(ctx.f23.f64 * ctx.f22.f64 + ctx.f6.f64));
	// lfs f4,2296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2296);
	ctx.f4.f64 = double(temp.f32);
	// lfs f20,2764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2764);
	ctx.f20.f64 = double(temp.f32);
	// lfs f17,2264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2264);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f0,f29,f0,f30
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f0.f64 - ctx.f30.f64));
	// fmadds f3,f3,f17,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f17.f64 + ctx.f31.f64));
	// lfs f29,2724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2724);
	ctx.f29.f64 = double(temp.f32);
	// stfs f5,3764(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3764, temp.u32);
	// fadds f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// lfs f5,1764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1764);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f16,f20,f19
	ctx.f16.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// stfs f17,2288(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2288, temp.u32);
	// fmuls f17,f29,f22
	ctx.f17.f64 = double(float(ctx.f29.f64 * ctx.f22.f64));
	// lfs f31,2304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2304);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f25,f5,f19,f25
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f19.f64 + ctx.f25.f64));
	// stfs f5,4764(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4764, temp.u32);
	// fmuls f18,f28,f4
	ctx.f18.f64 = double(float(ctx.f28.f64 * ctx.f4.f64));
	// stfs f17,2304(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2304, temp.u32);
	// lfs f17,2256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2256);
	ctx.f17.f64 = double(temp.f32);
	// lfs f5,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f17,f5
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f5.f64));
	// stfs f13,3772(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3772, temp.u32);
	// lfs f17,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f17.f64 = double(temp.f32);
	// lfs f13,3748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3748);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f13,f13,f17,f3
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f17.f64 - ctx.f3.f64)));
	// lfs f3,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f0,f6,f12,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f12.f64 + ctx.f0.f64));
	// fmadds f12,f2,f3,f11
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f3.f64 + ctx.f11.f64));
	// lfs f11,3756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3756);
	ctx.f11.f64 = double(temp.f32);
	// lfs f15,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f15.f64 = double(temp.f32);
	// stfs f19,2280(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2280, temp.u32);
	// lfs f19,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f26,f19
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f19.f64));
	// lfs f30,2312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2312);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f11,f11,f15,f5
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f15.f64 + ctx.f5.f64));
	// stfs f8,5008(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5008, temp.u32);
	// fmuls f14,f30,f1
	ctx.f14.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f9,4976(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4976, temp.u32);
	// lfs f8,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,4444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4444);
	ctx.f9.f64 = double(temp.f32);
	// stfs f28,2296(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2296, temp.u32);
	// lfs f28,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f28.f64 = double(temp.f32);
	// lfs f3,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f3.f64 = double(temp.f32);
	// stfs f10,1840(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1840, temp.u32);
	// fnmsubs f13,f18,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f18.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// stfs f31,2732(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2732, temp.u32);
	// fmadds f10,f31,f28,f19
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f28.f64 + ctx.f19.f64));
	// lfs f31,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f11,f9,f8,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f8.f64 + ctx.f11.f64));
	// lfs f3,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f3.f64 = double(temp.f32);
	// stfs f25,2312(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2312, temp.u32);
	// fmadds f0,f16,f31,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f31.f64 + ctx.f0.f64));
	// stfs f7,3588(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3588, temp.u32);
	// fnmsubs f12,f14,f3,f12
	ctx.f12.f64 = double(float(-(ctx.f14.f64 * ctx.f3.f64 - ctx.f12.f64)));
	// stfs f4,2772(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2772, temp.u32);
	// stfs f27,3236(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3236, temp.u32);
	// stfs f24,2256(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2256, temp.u32);
	// lfs f8,3468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3468);
	ctx.f8.f64 = double(temp.f32);
	// lfs f25,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f25.f64 = double(temp.f32);
	// lfs f7,3796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3796);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f25,f21,f25
	ctx.f25.f64 = double(float(ctx.f21.f64 * ctx.f25.f64));
	// lfs f6,4772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4772);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f7,f1
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f1.f64));
	// lfs f4,3804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3804);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f11,f11,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f8.f64));
	// lfs f3,4452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4452);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f4,f6
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f6.f64));
	// lfs f31,2080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2080);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,4932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4932);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f31,f22
	ctx.f28.f64 = double(float(ctx.f31.f64 * ctx.f22.f64));
	// lfs f24,4936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4936);
	ctx.f24.f64 = double(temp.f32);
	// lfs f17,-16940(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -16940);
	ctx.f17.f64 = double(temp.f32);
	// stfs f26,2892(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2892, temp.u32);
	// stfs f23,2264(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2264, temp.u32);
	// fmuls f23,f27,f24
	ctx.f23.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// stfs f20,1856(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1856, temp.u32);
	// stfs f30,3756(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3756, temp.u32);
	// fmuls f30,f8,f3
	ctx.f30.f64 = double(float(ctx.f8.f64 * ctx.f3.f64));
	// stfs f29,2272(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2272, temp.u32);
	// lfs f29,3812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3812);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,3820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3820);
	ctx.f26.f64 = double(temp.f32);
	// lfs f20,4292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4292);
	ctx.f20.f64 = double(temp.f32);
	// stfs f17,508(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 508, temp.u32);
	// lfs f15,2312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2312);
	ctx.f15.f64 = double(temp.f32);
	// fmsubs f11,f10,f8,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f8.f64 - ctx.f11.f64));
	// lfs f17,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f18,f8,f20
	ctx.f18.f64 = double(float(ctx.f8.f64 * ctx.f20.f64));
	// fmadds f13,f15,f17,f13
	ctx.f13.f64 = double(float(ctx.f15.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f15,2304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2304);
	ctx.f15.f64 = double(temp.f32);
	// lfs f16,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f16.f64 = double(temp.f32);
	// fadds f2,f2,f26
	ctx.f2.f64 = double(float(ctx.f2.f64 + ctx.f26.f64));
	// lfs f10,3836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3836);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f0,f15,f16,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// stfs f24,2304(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2304, temp.u32);
	// fmuls f24,f10,f24
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f24.f64));
	// lfs f16,4460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4460);
	ctx.f16.f64 = double(temp.f32);
	// stfs f24,4460(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4460, temp.u32);
	// fmuls f15,f8,f16
	ctx.f15.f64 = double(float(ctx.f8.f64 * ctx.f16.f64));
	// lfs f24,2320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2320);
	ctx.f24.f64 = double(temp.f32);
	// lfs f14,1440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1440);
	ctx.f14.f64 = double(temp.f32);
	// stfs f14,2320(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2320, temp.u32);
	// fmuls f22,f14,f22
	ctx.f22.f64 = double(float(ctx.f14.f64 * ctx.f22.f64));
	// lfs f19,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f19.f64 = double(temp.f32);
	// lfs f14,2288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2288);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f19,f29,f19
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// stfs f14,3804(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3804, temp.u32);
	// fmuls f14,f24,f14
	ctx.f14.f64 = double(float(ctx.f24.f64 * ctx.f14.f64));
	// lfs f17,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f17.f64 = double(temp.f32);
	// stfs f14,3836(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3836, temp.u32);
	// fnmsubs f12,f5,f17,f12
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f17.f64 - ctx.f12.f64)));
	// lfs f17,4780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4780);
	ctx.f17.f64 = double(temp.f32);
	// stfs f15,4780(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4780, temp.u32);
	// stfs f24,3812(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3812, temp.u32);
	// stfs f22,4452(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4452, temp.u32);
	// lfs f14,2280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2280);
	ctx.f14.f64 = double(temp.f32);
	// lfs f22,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f22.f64 = double(temp.f32);
	// lfs f24,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f24.f64 = double(temp.f32);
	// stfs f22,3796(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3796, temp.u32);
	// fmadds f24,f17,f24,f19
	ctx.f24.f64 = double(float(ctx.f17.f64 * ctx.f24.f64 + ctx.f19.f64));
	// stfs f10,3748(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3748, temp.u32);
	// fmuls f22,f22,f14
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f14.f64));
	// stfs f14,4772(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4772, temp.u32);
	// lfs f14,524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 524);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f13,f25,f14,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f14.f64 + ctx.f13.f64));
	// stfs f21,4444(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4444, temp.u32);
	// fnmsubs f0,f28,f19,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f19.f64 - ctx.f0.f64)));
	// lfs f5,3828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3828);
	ctx.f5.f64 = double(temp.f32);
	// lfs f10,2320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2320);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,2320(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2320, temp.u32);
	// fmr f10,f19
	ctx.f10.f64 = ctx.f19.f64;
	// lfs f21,2296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2296);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f15,f21,f5
	ctx.f15.f64 = double(float(ctx.f21.f64 * ctx.f5.f64));
	// stfs f6,3300(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3300, temp.u32);
	// stfs f15,3828(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3828, temp.u32);
	// fmuls f2,f2,f21
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f6,3836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3836);
	ctx.f6.f64 = double(temp.f32);
	// stfs f24,4932(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4932, temp.u32);
	// lfs f15,4052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4052);
	ctx.f15.f64 = double(temp.f32);
	// lfs f24,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f12,f23,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// stfs f8,3820(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3820, temp.u32);
	// fnmsubs f13,f6,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// stfs f9,2764(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2764, temp.u32);
	// fmuls f8,f8,f15
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f15.f64));
	// stfs f7,2288(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2288, temp.u32);
	// fmuls f9,f18,f24
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f24.f64));
	// lfs f7,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f11,f30,f14,f11
	ctx.f11.f64 = double(float(-(ctx.f30.f64 * ctx.f14.f64 - ctx.f11.f64)));
	// lfs f10,4780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4780);
	ctx.f10.f64 = double(temp.f32);
	// stfs f3,4936(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4936, temp.u32);
	// stfs f1,2312(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2312, temp.u32);
	// stfs f31,2296(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2296, temp.u32);
	// stfs f4,5040(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5040, temp.u32);
	// stfs f16,3124(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3124, temp.u32);
	// stfs f29,3468(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3468, temp.u32);
	// stfs f27,2280(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2280, temp.u32);
	// stfs f5,4968(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4968, temp.u32);
	// stfs f26,2780(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2780, temp.u32);
	// stfs f20,2104(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2104, temp.u32);
	// stfs f17,3244(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3244, temp.u32);
	// stfs f15,1144(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1144, temp.u32);
	// fnmsubs f11,f10,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f10,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f10.f64 = double(temp.f32);
	// fmr f4,f25
	ctx.f4.f64 = ctx.f25.f64;
	// lfs f6,3828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3828);
	ctx.f6.f64 = double(temp.f32);
	// fmr f7,f19
	ctx.f7.f64 = ctx.f19.f64;
	// lfs f1,4460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4460);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f0,f6,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f10,5136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5136);
	ctx.f10.f64 = double(temp.f32);
	// lfs f6,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f10,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f3,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,3820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3820);
	ctx.f31.f64 = double(temp.f32);
	// lfs f30,4932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4932);
	ctx.f30.f64 = double(temp.f32);
	// lfs f26,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,5140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5140);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f11,f30,f31,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f30,5132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5132);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f1,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f17,4452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4452);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f13,f22,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f7,5124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5124);
	ctx.f7.f64 = double(temp.f32);
	// fmr f4,f3
	ctx.f4.f64 = ctx.f3.f64;
	// lfs f5,332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	ctx.f5.f64 = double(temp.f32);
	// fmr f1,f14
	ctx.f1.f64 = ctx.f14.f64;
	// lfs f24,5012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5012);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f6,f7,f26,f6
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f26.f64 - ctx.f6.f64));
	// lfs f26,3812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3812);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f0,f2,f3,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f3,5148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5148);
	ctx.f3.f64 = double(temp.f32);
	// fadds f2,f29,f30
	ctx.f2.f64 = double(float(ctx.f29.f64 + ctx.f30.f64));
	// lfs f28,3844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3844);
	ctx.f28.f64 = double(temp.f32);
	// lfs f19,3772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3772);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f5,f8,f5
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// fmr f22,f14
	ctx.f22.f64 = ctx.f14.f64;
	// lfs f20,3852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3852);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f31,f28
	ctx.f25.f64 = double(float(ctx.f31.f64 * ctx.f28.f64));
	// lfs f16,2344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2344);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f27,f21,f3
	ctx.f27.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// lfs f14,2352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2352);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f23,f24,f26
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// stfs f10,5136(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5136, temp.u32);
	// fmadds f13,f17,f4,f13
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f4.f64 + ctx.f13.f64));
	// lfs f4,3860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3860);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f11,f9,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f9.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// stfs f8,3860(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3860, temp.u32);
	// fadds f12,f19,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 + ctx.f12.f64));
	// lfs f19,4476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4476);
	ctx.f19.f64 = double(temp.f32);
	// lfs f9,2328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2328);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f17,f31,f19
	ctx.f17.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// lfs f8,508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f15,f21,f4
	ctx.f15.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// stfs f18,4476(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4476, temp.u32);
	// fnmsubs f8,f20,f8,f6
	ctx.f8.f64 = double(float(-(ctx.f20.f64 * ctx.f8.f64 - ctx.f6.f64)));
	// fmuls f18,f31,f9
	ctx.f18.f64 = double(float(ctx.f31.f64 * ctx.f9.f64));
	// stfs f31,2328(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2328, temp.u32);
	// lfs f1,2336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2336);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f21,f16
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// lfs f6,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f2,f2,f21
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// stfs f31,3852(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3852, temp.u32);
	// fmuls f6,f1,f6
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f6.f64));
	// lfs f31,2360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2360);
	ctx.f31.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f8,5012(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5012, temp.u32);
	// fnmsubs f13,f5,f22,f11
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f22.f64 - ctx.f11.f64)));
	// stfs f15,2352(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2352, temp.u32);
	// stfs f6,2360(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2360, temp.u32);
	// stfs f21,2336(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2336, temp.u32);
	// lfs f8,4764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4764);
	ctx.f8.f64 = double(temp.f32);
	// lfs f15,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f15.f64 = double(temp.f32);
	// lfs f6,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f11,f27,f15
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f15.f64));
	// lfs f21,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f21.f64 = double(temp.f32);
	// stfs f18,2344(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2344, temp.u32);
	// fmuls f18,f8,f31
	ctx.f18.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// stfs f12,5428(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5428, temp.u32);
	// fmuls f12,f25,f6
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// stfs f7,1776(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1776, temp.u32);
	// fmuls f10,f23,f21
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// stfs f20,3812(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3812, temp.u32);
	// stfs f0,5432(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5432, temp.u32);
	// stfs f3,2008(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2008, temp.u32);
	// stfs f1,5124(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5124, temp.u32);
	// lfs f1,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f3.f64 = double(temp.f32);
	// stfs f13,5436(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5436, temp.u32);
	// fmuls f3,f3,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fmr f1,f15
	ctx.f1.f64 = ctx.f15.f64;
	// fmsubs f13,f2,f13,f11
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f13.f64 - ctx.f11.f64));
	// lfs f6,992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 992);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f12,f17,f6,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f6.f64 - ctx.f12.f64));
	// lfs f7,3804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3804);
	ctx.f7.f64 = double(temp.f32);
	// lfs f11,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f0,f14,f7
	ctx.f0.f64 = double(float(ctx.f14.f64 * ctx.f7.f64));
	// fmsubs f11,f18,f11,f10
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f11.f64 - ctx.f10.f64));
	// stfs f28,4932(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4932, temp.u32);
	// stfs f26,3844(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3844, temp.u32);
	// stfs f29,3820(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3820, temp.u32);
	// lfs f28,2360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2360);
	ctx.f28.f64 = double(temp.f32);
	// lfs f6,3884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3884);
	ctx.f6.f64 = double(temp.f32);
	// lfs f26,2352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2352);
	ctx.f26.f64 = double(temp.f32);
	// lfs f2,3796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3796);
	ctx.f2.f64 = double(temp.f32);
	// lfs f29,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f2,f2,f31
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fnmsubs f13,f26,f1,f13
	ctx.f13.f64 = double(float(-(ctx.f26.f64 * ctx.f1.f64 - ctx.f13.f64)));
	// stfs f9,4460(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4460, temp.u32);
	// fmadds f29,f6,f29,f28
	ctx.f29.f64 = double(float(ctx.f6.f64 * ctx.f29.f64 + ctx.f28.f64));
	// lfs f28,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,2344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2344);
	ctx.f26.f64 = double(temp.f32);
	// lfs f9,3764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3764);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f12,f26,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f26.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f31,3740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3740);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,4772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4772);
	ctx.f5.f64 = double(temp.f32);
	// lfs f10,1692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1692);
	ctx.f10.f64 = double(temp.f32);
	// stfs f30,5148(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5148, temp.u32);
	// fmuls f30,f31,f24
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f24.f64));
	// stfs f8,1440(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1440, temp.u32);
	// fmuls f8,f9,f24
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f24.f64));
	// lfs f23,2328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2328);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,3908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3908);
	ctx.f21.f64 = double(temp.f32);
	// stfs f4,2080(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2080, temp.u32);
	// fmuls f4,f10,f5
	ctx.f4.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// lfs f26,2336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2336);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// lfs f27,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,4500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4500);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f0,f0,f27,f11
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f27.f64 - ctx.f11.f64)));
	// stfs f14,2540(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2540, temp.u32);
	// fmuls f14,f23,f21
	ctx.f14.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// stfs f19,5140(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5140, temp.u32);
	// lfs f19,5052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5052);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,3916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3916);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f15.f64 = double(temp.f32);
	// stfs f16,3772(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3772, temp.u32);
	// fmuls f16,f26,f24
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// lfs f3,2368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2368);
	ctx.f3.f64 = double(temp.f32);
	// stfs f2,5052(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5052, temp.u32);
	// fmuls f2,f26,f19
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f19.f64));
	// stfs f14,3916(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3916, temp.u32);
	// fmuls f14,f26,f17
	ctx.f14.f64 = double(float(ctx.f26.f64 * ctx.f17.f64));
	// stfs f7,2360(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2360, temp.u32);
	// fmuls f7,f15,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f7.f64));
	// lfs f1,4492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4492);
	ctx.f1.f64 = double(temp.f32);
	// lfs f11,4796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4796);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f25,f26,f1
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// lfs f27,3900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3900);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f20,f26,f11
	ctx.f20.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// lfs f28,3892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3892);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f18,f23,f27
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// stfs f16,2368(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2368, temp.u32);
	// fmuls f16,f26,f3
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f3.f64));
	// stfs f30,4492(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4492, temp.u32);
	// fmuls f22,f23,f28
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f28.f64));
	// stfs f2,3908(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3908, temp.u32);
	// stfs f14,4796(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4796, temp.u32);
	// stfs f7,3900(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3900, temp.u32);
	// lfs f30,2376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2376);
	ctx.f30.f64 = double(temp.f32);
	// lfs f2,2384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2384);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,1424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1424);
	ctx.f14.f64 = double(temp.f32);
	// lfs f7,2392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2392);
	ctx.f7.f64 = double(temp.f32);
	// stfs f16,3884(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3884, temp.u32);
	// stfs f18,2392(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2392, temp.u32);
	// fmadds f12,f29,f23,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f23.f64 + ctx.f12.f64));
	// lfs f16,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f16.f64 = double(temp.f32);
	// lfs f18,3860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3860);
	ctx.f18.f64 = double(temp.f32);
	// stfs f8,2384(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2384, temp.u32);
	// fadds f8,f2,f30
	ctx.f8.f64 = double(float(ctx.f2.f64 + ctx.f30.f64));
	// fmuls f18,f18,f16
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f16.f64));
	// stfs f8,2376(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2376, temp.u32);
	// stfs f18,4500(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4500, temp.u32);
	// lfs f18,4476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4476);
	ctx.f18.f64 = double(temp.f32);
	// lfs f8,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f8,f18,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f8.f64));
	// stfs f8,3892(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3892, temp.u32);
	// lfs f8,3852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3852);
	ctx.f8.f64 = double(temp.f32);
	// lfs f16,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f8,f16,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f16.f64 + ctx.f13.f64));
	// stfs f11,3828(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3828, temp.u32);
	// lfs f11,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f11.f64 = double(temp.f32);
	// lfs f18,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f4,f18,f0
	ctx.f0.f64 = double(float(-(ctx.f4.f64 * ctx.f18.f64 - ctx.f0.f64)));
	// stfs f9,2352(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2352, temp.u32);
	// lfs f9,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f9.f64 = double(temp.f32);
	// lfs f29,4516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4516);
	ctx.f29.f64 = double(temp.f32);
	// stfs f31,4516(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4516, temp.u32);
	// stfs f1,3836(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3836, temp.u32);
	// stfs f19,3852(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3852, temp.u32);
	// lfs f31,2392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2392);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f25,f11,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f11.f64 - ctx.f13.f64)));
	// lfs f11,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f12,f22,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// lfs f19,2368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2368);
	ctx.f19.f64 = double(temp.f32);
	// fmr f11,f18
	ctx.f11.f64 = ctx.f18.f64;
	// stfs f27,3860(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3860, temp.u32);
	// fmadds f0,f5,f9,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f0.f64));
	// lfs f9,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f9.f64 = double(temp.f32);
	// fmr f5,f18
	ctx.f5.f64 = ctx.f18.f64;
	// stfs f21,2328(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2328, temp.u32);
	// fmr f4,f9
	ctx.f4.f64 = ctx.f9.f64;
	// lfs f22,5012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5012);
	ctx.f22.f64 = double(temp.f32);
	// fmr f1,f9
	ctx.f1.f64 = ctx.f9.f64;
	// lfs f21,2376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2376);
	ctx.f21.f64 = double(temp.f32);
	// lfs f27,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f27.f64 = double(temp.f32);
	// stfs f10,2120(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2120, temp.u32);
	// fmuls f10,f26,f7
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// stfs f7,3460(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3460, temp.u32);
	// fmadds f27,f21,f27,f22
	ctx.f27.f64 = double(float(ctx.f21.f64 * ctx.f27.f64 + ctx.f22.f64));
	// lfs f7,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f12,f31,f9,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f9.f64 - ctx.f12.f64)));
	// lfs f31,2384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2384);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f13,f20,f11,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f11.f64 + ctx.f13.f64));
	// stfs f28,1992(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1992, temp.u32);
	// lfs f8,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f0,f31,f7,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f0.f64));
	// lfs f28,3940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3940);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f8,f14,f8
	ctx.f8.f64 = double(float(ctx.f14.f64 * ctx.f8.f64));
	// lfs f22,4948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4948);
	ctx.f22.f64 = double(temp.f32);
	// fmr f31,f18
	ctx.f31.f64 = ctx.f18.f64;
	// stfs f6,4476(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4476, temp.u32);
	// fmuls f6,f26,f29
	ctx.f6.f64 = double(float(ctx.f26.f64 * ctx.f29.f64));
	// stfs f24,2724(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2724, temp.u32);
	// fmr f7,f1
	ctx.f7.f64 = ctx.f1.f64;
	// stfs f3,1424(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1424, temp.u32);
	// fmuls f25,f23,f28
	ctx.f25.f64 = double(float(ctx.f23.f64 * ctx.f28.f64));
	// stfs f2,5132(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5132, temp.u32);
	// fmr f2,f18
	ctx.f2.f64 = ctx.f18.f64;
	// stfs f30,4780(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4780, temp.u32);
	// fmuls f20,f26,f22
	ctx.f20.f64 = double(float(ctx.f26.f64 * ctx.f22.f64));
	// lfs f3,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f13,f19,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// lfs f19,3916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3916);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f12,f19,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// lfs f30,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f30.f64 = double(temp.f32);
	// fmr f4,f16
	ctx.f4.f64 = ctx.f16.f64;
	// lfs f24,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f24.f64 = double(temp.f32);
	// lfs f21,3948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3948);
	ctx.f21.f64 = double(temp.f32);
	// lfs f5,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f5.f64 = double(temp.f32);
	// lfs f19,5052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5052);
	ctx.f19.f64 = double(temp.f32);
	// stfs f17,2336(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2336, temp.u32);
	// stfs f15,1376(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1376, temp.u32);
	// stfs f14,2344(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2344, temp.u32);
	// fnmsubs f0,f19,f3,f0
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f3.f64 - ctx.f0.f64)));
	// lfs f3,4924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4924);
	ctx.f3.f64 = double(temp.f32);
	// lfs f18,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f26,f21
	ctx.f19.f64 = double(float(ctx.f26.f64 * ctx.f21.f64));
	// fmadds f27,f3,f18,f27
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f18.f64 + ctx.f27.f64));
	// lfs f18,2416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2416);
	ctx.f18.f64 = double(temp.f32);
	// stfs f20,2416(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2416, temp.u32);
	// lfs f20,3908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3908);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f13,f20,f2,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f2.f64 + ctx.f13.f64));
	// lfs f20,4500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4500);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f12,f20,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f20.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f20,3900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3900);
	ctx.f20.f64 = double(temp.f32);
	// stfs f28,2368(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2368, temp.u32);
	// fmr f1,f9
	ctx.f1.f64 = ctx.f9.f64;
	// lfs f28,4796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4796);
	ctx.f28.f64 = double(temp.f32);
	// lfs f16,2400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2400);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f11,f20,f11,f0
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f11.f64 - ctx.f0.f64)));
	// lfs f15,2408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2408);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f14,f26,f16
	ctx.f14.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// stfs f26,2400(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2400, temp.u32);
	// fmuls f26,f26,f15
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f15.f64));
	// fmr f20,f4
	ctx.f20.f64 = ctx.f4.f64;
	// stfs f26,3948(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3948, temp.u32);
	// stfs f20,2408(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2408, temp.u32);
	// fnmsubs f13,f28,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// lfs f9,3892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3892);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f12,f9,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f9,4492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4492);
	ctx.f9.f64 = double(temp.f32);
	// lfs f20,4444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4444);
	ctx.f20.f64 = double(temp.f32);
	// lfs f26,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f26,f20,f26
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f26.f64));
	// stfs f26,4948(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4948, temp.u32);
	// fmadds f11,f9,f31,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f9,3884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3884);
	ctx.f9.f64 = double(temp.f32);
	// stfs f14,3940(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3940, temp.u32);
	// stfs f27,5440(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5440, temp.u32);
	// lfs f14,3284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3284);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f13,f9,f30,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f30.f64 + ctx.f13.f64));
	// lfs f0,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f12,f8,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f26,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f26.f64 = double(temp.f32);
	// lfs f9,4532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4532);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f2,f20,f2
	ctx.f2.f64 = double(float(ctx.f20.f64 * ctx.f2.f64));
	// lfs f7,3972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3972);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f26,f18,f26
	ctx.f26.f64 = double(float(ctx.f18.f64 * ctx.f26.f64));
	// lfs f28,4540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4540);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f11,f6,f5,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// lfs f6,3412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3412);
	ctx.f6.f64 = double(temp.f32);
	// lfs f27,1868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1868);
	ctx.f27.f64 = double(temp.f32);
	// fadds f5,f7,f9
	ctx.f5.f64 = double(float(ctx.f7.f64 + ctx.f9.f64));
	// stfs f29,3804(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3804, temp.u32);
	// fmuls f29,f0,f14
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f14.f64));
	// stfs f23,2384(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2384, temp.u32);
	// fmuls f24,f27,f28
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f28.f64));
	// stfs f3,2392(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2392, temp.u32);
	// fnmsubs f13,f10,f4,f13
	ctx.f13.f64 = double(float(-(ctx.f10.f64 * ctx.f4.f64 - ctx.f13.f64)));
	// lfs f3,5020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5020);
	ctx.f3.f64 = double(temp.f32);
	// fmr f10,f4
	ctx.f10.f64 = ctx.f4.f64;
	// lfs f4,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f12,f25,f17,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f17.f64 + ctx.f12.f64));
	// stfs f12,5444(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5444, temp.u32);
	// lfs f31,2532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2532);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f6,f4
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f25,2424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2424);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f19,f1,f11
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f11,2416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2416);
	ctx.f11.f64 = double(temp.f32);
	// lfs f1,3980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3980);
	ctx.f1.f64 = double(temp.f32);
	// lfs f23,1924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1924);
	ctx.f23.f64 = double(temp.f32);
	// stfs f22,3740(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3740, temp.u32);
	// stfs f21,5052(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5052, temp.u32);
	// stfs f16,4924(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4924, temp.u32);
	// fmadds f13,f11,f10,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f13.f64));
	// lfs f11,2312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2312);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,3964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3964);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f30,f11,f1
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f1.f64));
	// stfs f15,1784(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1784, temp.u32);
	// fmuls f8,f11,f10
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f14,2376(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2376, temp.u32);
	// stfs f18,3908(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3908, temp.u32);
	// lfs f21,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f0,f23,f0
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f0.f64));
	// lfs f22,4380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4380);
	ctx.f22.f64 = double(temp.f32);
	// fadds f5,f5,f25
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f25.f64));
	// fmuls f22,f22,f21
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f21.f64));
	// lfs f21,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f21,f31
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// lfs f19,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f29,f29,f19,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f19.f64 + ctx.f26.f64));
	// fmuls f20,f3,f20
	ctx.f20.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// lfs f18,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f18.f64 = double(temp.f32);
	// lfs f26,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f8,f8,f18
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f18.f64));
	// fmuls f4,f4,f26
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// lfs f26,1860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1860);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f18,f26,f28
	ctx.f18.f64 = double(float(ctx.f26.f64 * ctx.f28.f64));
	// stfs f28,3980(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3980, temp.u32);
	// lfs f28,2456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2456);
	ctx.f28.f64 = double(temp.f32);
	// lfs f19,2432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2432);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f11.f64));
	// lfs f14,2448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2448);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f16,f11,f19
	ctx.f16.f64 = double(float(ctx.f11.f64 * ctx.f19.f64));
	// stfs f23,2432(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2432, temp.u32);
	// fmuls f21,f21,f11
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f11.f64));
	// stfs f21,2456(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2456, temp.u32);
	// lfs f21,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f29,f29,f11
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f11.f64));
	// stfs f2,2448(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2448, temp.u32);
	// fmadds f22,f22,f21,f20
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f21.f64 + ctx.f20.f64));
	// lfs f17,2440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2440);
	ctx.f17.f64 = double(temp.f32);
	// lfs f2,2464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2464);
	ctx.f2.f64 = double(temp.f32);
	// lfs f23,2400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2400);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f21.f64 = double(temp.f32);
	// stfs f16,2440(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2440, temp.u32);
	// fmuls f16,f23,f14
	ctx.f16.f64 = double(float(ctx.f23.f64 * ctx.f14.f64));
	// stfs f18,2464(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2464, temp.u32);
	// fmsubs f8,f30,f21,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f21.f64 - ctx.f8.f64));
	// lfs f18,2472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2472);
	ctx.f18.f64 = double(temp.f32);
	// stfs f16,2472(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2472, temp.u32);
	// lfs f20,2304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2304);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f16.f64 = double(temp.f32);
	// lfs f30,2480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2480);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f16,f20,f16
	ctx.f16.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// lfs f15,4428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4428);
	ctx.f15.f64 = double(temp.f32);
	// stfs f20,4540(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4540, temp.u32);
	// fmadds f24,f15,f30,f24
	ctx.f24.f64 = double(float(ctx.f15.f64 * ctx.f30.f64 + ctx.f24.f64));
	// lfs f20,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f20.f64 = double(temp.f32);
	// stfs f24,2480(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2480, temp.u32);
	// fmuls f21,f17,f20
	ctx.f21.f64 = double(float(ctx.f17.f64 * ctx.f20.f64));
	// lfs f24,3948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3948);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f8,f22,f11,f8
	ctx.f8.f64 = double(float(ctx.f22.f64 * ctx.f11.f64 + ctx.f8.f64));
	// lfs f20,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f20.f64 = double(temp.f32);
	// stfs f9,1924(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1924, temp.u32);
	// fmadds f12,f24,f20,f12
	ctx.f12.f64 = double(float(ctx.f24.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f9,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f9.f64 = double(temp.f32);
	// fadds f20,f2,f28
	ctx.f20.f64 = double(float(ctx.f2.f64 + ctx.f28.f64));
	// stfs f30,2424(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2424, temp.u32);
	// fmsubs f0,f0,f9,f29
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f9.f64 - ctx.f29.f64));
	// lfs f30,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,4020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4020);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f5,f5,f30,f4
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f30.f64 + ctx.f4.f64));
	// stfs f10,3948(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3948, temp.u32);
	// fmuls f10,f11,f18
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// stfs f7,1868(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1868, temp.u32);
	// lfs f9,4828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4828);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,1852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1852);
	ctx.f7.f64 = double(temp.f32);
	// stfs f3,2416(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2416, temp.u32);
	// stfs f1,3916(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3916, temp.u32);
	// stfs f31,3284(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3284, temp.u32);
	// stfs f6,1764(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1764, temp.u32);
	// stfs f26,4020(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4020, temp.u32);
	// stfs f27,5020(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5020, temp.u32);
	// stfs f19,4532(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4532, temp.u32);
	// stfs f25,3764(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3764, temp.u32);
	// stfs f17,4796(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4796, temp.u32);
	// stfs f15,3972(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3972, temp.u32);
	// stfs f14,4452(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4452, temp.u32);
	// stfs f2,3892(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3892, temp.u32);
	// stfs f28,5012(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5012, temp.u32);
	// stfs f18,2408(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2408, temp.u32);
	// lfs f29,4948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4948);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f6,f20,f23
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f23.f64));
	// lfs f31,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f1,f16,f24
	ctx.f1.f64 = double(float(ctx.f16.f64 * ctx.f24.f64));
	// fnmsubs f13,f29,f31,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f13.f64)));
	// lfs f29,2472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2472);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f29,f30,f12
	ctx.f12.f64 = double(float(-(ctx.f29.f64 * ctx.f30.f64 - ctx.f12.f64)));
	// lfs f4,4028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4028);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f5,f4,f31,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f31.f64 + ctx.f5.f64));
	// lfs f29,2464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2464);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f31.f64 = double(temp.f32);
	// lfs f26,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f8,f29,f31,f8
	ctx.f8.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f8.f64)));
	// lfs f22,2448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2448);
	ctx.f22.f64 = double(temp.f32);
	// lfs f25,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f25.f64 = double(temp.f32);
	// lfs f30,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f13,f22,f26,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f26.f64 + ctx.f13.f64));
	// lfs f26,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,2456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2456);
	ctx.f29.f64 = double(temp.f32);
	// lfs f3,2360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2360);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f12,f6,f25,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f25.f64 + ctx.f12.f64));
	// lfs f2,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f2.f64 = double(temp.f32);
	// fmr f6,f26
	ctx.f6.f64 = ctx.f26.f64;
	// fmadds f2,f9,f2,f21
	ctx.f2.f64 = double(float(ctx.f9.f64 * ctx.f2.f64 + ctx.f21.f64));
	// lfs f27,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f29,f30,f0
	ctx.f0.f64 = double(float(-(ctx.f29.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// lfs f30,2756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2756);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f3,f7,f3
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// lfs f29,4564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4564);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f5,f29,f26,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f26.f64 + ctx.f5.f64));
	// lfs f24,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f27,f30,f27
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// lfs f26,2440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2440);
	ctx.f26.f64 = double(temp.f32);
	// lfs f31,3116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3116);
	ctx.f31.f64 = double(temp.f32);
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f31
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f16,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,1788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1788);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f2,f2,f11
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// lfs f18,1780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1780);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f26,f6,f0
	ctx.f0.f64 = double(float(-(ctx.f26.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// lfs f26,2480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2480);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f3,f3,f24
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f24.f64));
	// lfs f6,4036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4036);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f8,f26,f25,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f25.f64 + ctx.f8.f64));
	// lfs f15,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f27,f27,f16
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// lfs f16,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,3940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3940);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f19,f19,f6
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f6.f64));
	// fmuls f6,f18,f6
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f6.f64));
	// lfs f26,5088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5088);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f13,f14,f15,f13
	ctx.f13.f64 = double(float(-(ctx.f14.f64 * ctx.f15.f64 - ctx.f13.f64)));
	// lfs f25,4284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4284);
	ctx.f25.f64 = double(temp.f32);
	// lfs f22,4044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4044);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f11,f26
	ctx.f24.f64 = double(float(ctx.f11.f64 * ctx.f26.f64));
	// lfs f21,2524(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2524);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f28,f28,f11
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// fmsubs f5,f5,f11,f2
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f11.f64 - ctx.f2.f64));
	// lfs f17,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f11,f22
	ctx.f20.f64 = double(float(ctx.f11.f64 * ctx.f22.f64));
	// fmadds f12,f3,f16,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f16.f64 + ctx.f12.f64));
	// lfs f15,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f15.f64 = double(temp.f32);
	// lfs f2,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f17,f17,f25
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// lfs f3,2488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2488);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f18,f18,f21
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f21.f64));
	// stfs f9,4948(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4948, temp.u32);
	// fnmsubs f0,f1,f15,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f15.f64 - ctx.f0.f64)));
	// stfs f31,1852(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1852, temp.u32);
	// fnmsubs f10,f10,f2,f8
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f2.f64 - ctx.f8.f64)));
	// stfs f23,3964(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3964, temp.u32);
	// fmuls f9,f11,f3
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// stfs f7,2472(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2472, temp.u32);
	// stfs f4,2312(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2312, temp.u32);
	// stfs f30,2464(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2464, temp.u32);
	// stfs f29,2304(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2304, temp.u32);
	// lfs f8,1916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1916);
	ctx.f8.f64 = double(temp.f32);
	// fadds f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f4,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f7,f17,f11
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f11.f64));
	// fmuls f12,f8,f4
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// stfs f21,1860(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1860, temp.u32);
	// fmr f4,f2
	ctx.f4.f64 = ctx.f2.f64;
	// lfs f21,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f21.f64 = double(temp.f32);
	// lfs f2,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f2.f64 = double(temp.f32);
	// fmr f16,f21
	ctx.f16.f64 = ctx.f21.f64;
	// fmadds f10,f28,f2,f10
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f2.f64 + ctx.f10.f64));
	// lfs f17,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f27,f18,f21,f27
	ctx.f27.f64 = double(float(ctx.f18.f64 * ctx.f21.f64 + ctx.f27.f64));
	// lfs f2,2900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2900);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f5,f20,f17,f5
	ctx.f5.f64 = double(float(-(ctx.f20.f64 * ctx.f17.f64 - ctx.f5.f64)));
	// lfs f29,2496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2496);
	ctx.f29.f64 = double(temp.f32);
	// lfs f21,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f28,f11,f29
	ctx.f28.f64 = double(float(ctx.f11.f64 * ctx.f29.f64));
	// lfs f20,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f20.f64 = double(temp.f32);
	// stfs f26,2448(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2448, temp.u32);
	// stfs f25,3412(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3412, temp.u32);
	// fmuls f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// lfs f1,5000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5000);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f4,f19,f4,f0
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f4.f64 + ctx.f0.f64));
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f26,2504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2504);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f10,f6,f0,f10
	ctx.f10.f64 = double(float(ctx.f6.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f25,3704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3704);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f6,f2,f21
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f21.f64));
	// lfs f31,2432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2432);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f23,f26,f1
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// stfs f22,4772(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4772, temp.u32);
	// fmadds f5,f27,f11,f5
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f11.f64 + ctx.f5.f64));
	// lfs f27,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f27.f64 = double(temp.f32);
	// lfs f22,2512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2512);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f31,f2
	ctx.f30.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// lfs f18,2528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2528);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f22,f1
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f1.f64));
	// stfs f30,2504(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2504, temp.u32);
	// fmuls f14,f11,f18
	ctx.f14.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// fmuls f12,f12,f20
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f20.f64));
	// lfs f21,2520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2520);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f4,f24,f16,f4
	ctx.f4.f64 = double(float(-(ctx.f24.f64 * ctx.f16.f64 - ctx.f4.f64)));
	// lfs f24,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f24,f24,f25
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// lfs f30,3556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3556);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f10,f7,f27,f10
	ctx.f10.f64 = double(float(-(ctx.f7.f64 * ctx.f27.f64 - ctx.f10.f64)));
	// stfs f10,4044(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4044, temp.u32);
	// lfs f10,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f16,f11,f21
	ctx.f16.f64 = double(float(ctx.f11.f64 * ctx.f21.f64));
	// stfs f14,2496(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2496, temp.u32);
	// lfs f17,4792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4792);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 452);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,3688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3688);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,1900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1900);
	ctx.f20.f64 = double(temp.f32);
	// lfs f7,1972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1972);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f12,f28,f10,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f10.f64 - ctx.f12.f64));
	// fnmsubs f0,f9,f0,f4
	ctx.f0.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// stfs f0,2512(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2512, temp.u32);
	// lfs f0,2424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2424);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f7,f15,f7
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f7.f64));
	// lfs f4,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f10,f24,f11
	ctx.f10.f64 = double(float(ctx.f24.f64 * ctx.f11.f64));
	// stfs f0,2488(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2488, temp.u32);
	// fmuls f0,f30,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f9,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f27,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f9,f17,f9
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f9.f64));
	// stfs f2,4036(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4036, temp.u32);
	// fmuls f2,f2,f14
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f14.f64));
	// stfs f16,2520(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2520, temp.u32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// stfs f5,5088(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5088, temp.u32);
	// fmuls f5,f20,f1
	ctx.f5.f64 = double(float(ctx.f20.f64 * ctx.f1.f64));
	// stfs f0,2528(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2528, temp.u32);
	// lfs f16,2908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2908);
	ctx.f16.f64 = double(temp.f32);
	// lfs f0,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f0.f64 = double(temp.f32);
	// lfs f4,600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 600);
	ctx.f4.f64 = double(temp.f32);
	// stfs f3,2432(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2432, temp.u32);
	// stfs f8,4564(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4564, temp.u32);
	// stfs f31,1916(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1916, temp.u32);
	// fmuls f0,f19,f0
	ctx.f0.f64 = double(float(ctx.f19.f64 * ctx.f0.f64));
	// lfs f3,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f8,f17,f16
	ctx.f8.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// lfs f31,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f9,f9,f3,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f3.f64 + ctx.f6.f64));
	// fmadds f7,f7,f31,f27
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f31.f64 + ctx.f27.f64));
	// lfs f6,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f12,f10,f6,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f6.f64 + ctx.f12.f64));
	// lfs f27,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f27.f64 = double(temp.f32);
	// stfs f30,2480(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2480, temp.u32);
	// stfs f26,4428(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4428, temp.u32);
	// fmr f26,f27
	ctx.f26.f64 = ctx.f27.f64;
	// lfs f30,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f5,f5,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// lfs f30,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f30.f64 = double(temp.f32);
	// stfs f13,5448(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5448, temp.u32);
	// lfs f28,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f28.f64 = double(temp.f32);
	// lfs f13,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f9,f28,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f28,2520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2520);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f7,f2,f30,f7
	ctx.f7.f64 = double(float(-(ctx.f2.f64 * ctx.f30.f64 - ctx.f7.f64)));
	// lfs f30,2528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2528);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f30,f27,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f12.f64));
	// lfs f27,2512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2512);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f13,f4,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f13.f64));
	// lfs f24,2804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2804);
	ctx.f24.f64 = double(temp.f32);
	// lfs f30,2536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2536);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f28,f28,f26,f27
	ctx.f28.f64 = double(float(-(ctx.f28.f64 * ctx.f26.f64 - ctx.f27.f64)));
	// stfs f14,4764(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4764, temp.u32);
	// fmuls f14,f17,f24
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// lfs f26,2552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2552);
	ctx.f26.f64 = double(temp.f32);
	// stfs f21,2440(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2440, temp.u32);
	// fmuls f21,f30,f1
	ctx.f21.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f18,2360(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2360, temp.u32);
	// stfs f22,4444(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4444, temp.u32);
	// stfs f20,2456(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2456, temp.u32);
	// lfs f22,2560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2560);
	ctx.f22.f64 = double(temp.f32);
	// lfs f20,2568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2568);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,2576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2576);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f13,f13,f1
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// stfs f16,4828(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4828, temp.u32);
	// fmuls f16,f26,f1
	ctx.f16.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// stfs f14,2536(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2536, temp.u32);
	// lfs f14,4540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4540);
	ctx.f14.f64 = double(temp.f32);
	// lfs f27,2544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2544);
	ctx.f27.f64 = double(temp.f32);
	// stfs f21,2576(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2576, temp.u32);
	// fmuls f19,f11,f27
	ctx.f19.f64 = double(float(ctx.f11.f64 * ctx.f27.f64));
	// stfs f17,2560(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2560, temp.u32);
	// fmuls f17,f20,f1
	ctx.f17.f64 = double(float(ctx.f20.f64 * ctx.f1.f64));
	// stfs f15,4792(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4792, temp.u32);
	// lfs f9,4076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4076);
	ctx.f9.f64 = double(temp.f32);
	// lfs f15,1844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1844);
	ctx.f15.f64 = double(temp.f32);
	// lfs f21,3980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3980);
	ctx.f21.f64 = double(temp.f32);
	// stfs f16,2544(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2544, temp.u32);
	// fmuls f16,f14,f22
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f22.f64));
	// stfs f14,2568(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2568, temp.u32);
	// fmuls f14,f18,f1
	ctx.f14.f64 = double(float(ctx.f18.f64 * ctx.f1.f64));
	// stfs f17,4076(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4076, temp.u32);
	// fmuls f21,f15,f21
	ctx.f21.f64 = double(float(ctx.f15.f64 * ctx.f21.f64));
	// lfs f10,4964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4964);
	ctx.f10.f64 = double(temp.f32);
	// lfs f2,4588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4588);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f11,f10
	ctx.f3.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f6,2300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2300);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f23,f11,f2
	ctx.f23.f64 = double(float(ctx.f11.f64 * ctx.f2.f64));
	// lfs f31,4784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4784);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,2584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2584);
	ctx.f17.f64 = double(temp.f32);
	// stfs f29,2400(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2400, temp.u32);
	// fmuls f29,f31,f6
	ctx.f29.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// stfs f25,4028(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4028, temp.u32);
	// fmuls f25,f9,f1
	ctx.f25.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// stfs f4,2528(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2528, temp.u32);
	// stfs f16,2552(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2552, temp.u32);
	// stfs f14,4964(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4964, temp.u32);
	// stfs f21,4588(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4588, temp.u32);
	// stfs f19,2584(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2584, temp.u32);
	// lfs f4,2516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2516);
	ctx.f4.f64 = double(temp.f32);
	// lfs f16,4984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4984);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f14.f64 = double(temp.f32);
	// lfs f21,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,2504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2504);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f19,f14
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f14.f64));
	// lfs f14,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f14.f64 = double(temp.f32);
	// fmsubs f0,f8,f14,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f14.f64 - ctx.f0.f64));
	// lfs f8,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f13,f29,f21,f13
	ctx.f13.f64 = double(float(ctx.f29.f64 * ctx.f21.f64 + ctx.f13.f64));
	// lfs f29,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f3,f8,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f8.f64 - ctx.f12.f64)));
	// stfs f10,2424(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2424, temp.u32);
	// fnmsubs f7,f5,f29,f7
	ctx.f7.f64 = double(float(-(ctx.f5.f64 * ctx.f29.f64 - ctx.f7.f64)));
	// lfs f29,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f29.f64 = double(temp.f32);
	// lfs f10,2496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2496);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f3,f31,f4
	ctx.f3.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// fmadds f10,f10,f29,f28
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f29.f64 + ctx.f28.f64));
	// stfs f11,2908(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2908, temp.u32);
	// lfs f11,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f11.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// stfs f4,4540(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4540, temp.u32);
	// lfs f4,4044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4044);
	ctx.f4.f64 = double(temp.f32);
	// stfs f18,3796(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3796, temp.u32);
	// stfs f15,2512(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2512, temp.u32);
	// lfs f18,4736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4736);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f12,f23,f11,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f11.f64 + ctx.f12.f64));
	// lfs f15,2560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2560);
	ctx.f15.f64 = double(temp.f32);
	// fmr f11,f14
	ctx.f11.f64 = ctx.f14.f64;
	// stfs f9,3980(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3980, temp.u32);
	// lfs f9,2488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2488);
	ctx.f9.f64 = double(temp.f32);
	// fadds f10,f10,f4
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f4.f64));
	// lfs f4,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f4,f25,f4,f0
	ctx.f4.f64 = double(float(-(ctx.f25.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// lfs f25,5088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5088);
	ctx.f25.f64 = double(temp.f32);
	// fmr f0,f14
	ctx.f0.f64 = ctx.f14.f64;
	// stfs f24,4492(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4492, temp.u32);
	// lfs f24,2568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2568);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f9,f16,f9
	ctx.f9.f64 = double(float(ctx.f16.f64 * ctx.f9.f64));
	// stfs f2,2496(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2496, temp.u32);
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// stfs f16,2520(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2520, temp.u32);
	// stfs f30,3900(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3900, temp.u32);
	// fnmsubs f11,f19,f11,f7
	ctx.f11.f64 = double(float(-(ctx.f19.f64 * ctx.f11.f64 - ctx.f7.f64)));
	// lfs f7,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f7.f64 = double(temp.f32);
	// lfs f19,4852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4852);
	ctx.f19.f64 = double(temp.f32);
	// fadds f10,f10,f25
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f25.f64));
	// lfs f25,2584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2584);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f25,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f25.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f25,2576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2576);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f4,f25,f0,f4
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f0.f64 + ctx.f4.f64));
	// lfs f7,4604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4604);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,3108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3108);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f23,f24,f7
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// lfs f8,4744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4744);
	ctx.f8.f64 = double(temp.f32);
	// lfs f30,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f5,f8,f5
	ctx.f5.f64 = double(float(ctx.f8.f64 * ctx.f5.f64));
	// lfs f29,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f29.f64 = double(temp.f32);
	// lfs f16,2508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2508);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f0,f13,f0,f11
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f13,4116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4116);
	ctx.f13.f64 = double(temp.f32);
	// fmr f11,f14
	ctx.f11.f64 = ctx.f14.f64;
	// stfs f6,2504(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2504, temp.u32);
	// fmuls f14,f18,f15
	ctx.f14.f64 = double(float(ctx.f18.f64 * ctx.f15.f64));
	// stfs f14,4852(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4852, temp.u32);
	// lfs f14,2552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2552);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f6,f17,f1
	ctx.f6.f64 = double(float(ctx.f17.f64 * ctx.f1.f64));
	// stfs f20,3940(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3940, temp.u32);
	// fmadds f12,f14,f2,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f2.f64 + ctx.f12.f64));
	// stfs f24,2568(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2568, temp.u32);
	// fmr f20,f2
	ctx.f20.f64 = ctx.f2.f64;
	// stfs f27,2488(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2488, temp.u32);
	// fmr f24,f2
	ctx.f24.f64 = ctx.f2.f64;
	// stfs f26,4500(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4500, temp.u32);
	// fmr f26,f30
	ctx.f26.f64 = ctx.f30.f64;
	// stfs f22,3884(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3884, temp.u32);
	// fmr f22,f29
	ctx.f22.f64 = ctx.f29.f64;
	// stfs f17,2532(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2532, temp.u32);
	// fmuls f21,f13,f1
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// lfs f28,968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f17,f19,f1
	ctx.f17.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// lfs f27,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f16,f31,f16
	ctx.f16.f64 = double(float(ctx.f31.f64 * ctx.f16.f64));
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// fmr f2,f11
	ctx.f2.f64 = ctx.f11.f64;
	// lfs f14,2544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2544);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f4,f14,f30,f4
	ctx.f4.f64 = double(float(-(ctx.f14.f64 * ctx.f30.f64 - ctx.f4.f64)));
	// lfs f14,2536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2536);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f0,f14,f29,f0
	ctx.f0.f64 = double(float(ctx.f14.f64 * ctx.f29.f64 + ctx.f0.f64));
	// lfs f14,2420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2420);
	ctx.f14.f64 = double(temp.f32);
	// stfs f8,2544(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2544, temp.u32);
	// fmuls f8,f8,f14
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f14.f64));
	// lfs f29,4124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4124);
	ctx.f29.f64 = double(temp.f32);
	// stfs f8,4604(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4604, temp.u32);
	// stfs f17,4124(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4124, temp.u32);
	// lfs f30,3404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3404);
	ctx.f30.f64 = double(temp.f32);
	// lfs f17,4036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4036);
	ctx.f17.f64 = double(temp.f32);
	// lfs f8,4732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4732);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f17,f17,f30
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f30.f64));
	// fmuls f8,f31,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f8.f64));
	// stfs f8,2576(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2576, temp.u32);
	// stfs f17,2552(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2552, temp.u32);
	// fmr f8,f11
	ctx.f8.f64 = ctx.f11.f64;
	// lfs f17,4612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4612);
	ctx.f17.f64 = double(temp.f32);
	// stfs f8,4612(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4612, temp.u32);
	// lfs f8,4588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4588);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f12,f8,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f28,4076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4076);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f4,f28,f27,f4
	ctx.f4.f64 = double(float(-(ctx.f28.f64 * ctx.f27.f64 - ctx.f4.f64)));
	// stfs f1,4116(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4116, temp.u32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// stfs f1,2560(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2560, temp.u32);
	// lfs f1,4132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4132);
	ctx.f1.f64 = double(temp.f32);
	// stfs f10,4132(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4132, temp.u32);
	// fmr f10,f27
	ctx.f10.f64 = ctx.f27.f64;
	// stfs f31,2584(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2584, temp.u32);
	// fmr f31,f11
	ctx.f31.f64 = ctx.f11.f64;
	// lfs f27,4964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4964);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f27,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f27.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// stfs f13,4588(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4588, temp.u32);
	// lfs f28,3508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3508);
	ctx.f28.f64 = double(temp.f32);
	// stfs f7,4964(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4964, temp.u32);
	// fmuls f7,f28,f1
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// fnmsubs f12,f9,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// stfs f1,5000(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5000, temp.u32);
	// fnmsubs f11,f5,f11,f4
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f11.f64 - ctx.f4.f64)));
	// lfs f1,4604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4604);
	ctx.f1.f64 = double(temp.f32);
	// fmr f4,f24
	ctx.f4.f64 = ctx.f24.f64;
	// lfs f27,4616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4616);
	ctx.f27.f64 = double(temp.f32);
	// lfs f8,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f8.f64 = double(temp.f32);
	// stfs f30,4076(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4076, temp.u32);
	// fmuls f8,f17,f8
	ctx.f8.f64 = double(float(ctx.f17.f64 * ctx.f8.f64));
	// fmuls f13,f16,f31
	ctx.f13.f64 = double(float(ctx.f16.f64 * ctx.f31.f64));
	// stfs f29,5088(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5088, temp.u32);
	// lfs f30,4972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4972);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f31,f15,f27
	ctx.f31.f64 = double(float(ctx.f15.f64 * ctx.f27.f64));
	// fmadds f0,f3,f22,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f22.f64 + ctx.f0.f64));
	// lfs f3,2528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2528);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,4156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4156);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,4628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4628);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f12,f23,f20,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f25,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f6,f24,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f24.f64 - ctx.f11.f64)));
	// lfs f6,4116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4116);
	ctx.f6.f64 = double(temp.f32);
	// stfs f19,2536(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2536, temp.u32);
	// stfs f18,4616(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4616, temp.u32);
	// lfs f9,4132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4132);
	ctx.f9.f64 = double(temp.f32);
	// stfs f14,4044(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4044, temp.u32);
	// stfs f17,4036(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4036, temp.u32);
	// fmadds f0,f21,f2,f0
	ctx.f0.f64 = double(float(ctx.f21.f64 * ctx.f2.f64 + ctx.f0.f64));
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// stfs f12,5452(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5452, temp.u32);
	// lfs f12,4124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4124);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f12,f10,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64 + ctx.f11.f64));
	// lfs f11,4852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4852);
	ctx.f11.f64 = double(temp.f32);
	// fmr f10,f2
	ctx.f10.f64 = ctx.f2.f64;
	// lfs f9,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f9.f64 = double(temp.f32);
	// fmr f2,f22
	ctx.f2.f64 = ctx.f22.f64;
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// fnmsubs f0,f11,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f11.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f11,5176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5176);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,4140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4140);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f13,f1,f2,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f2.f64 - ctx.f13.f64));
	// lfs f2,4624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4624);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f10,f6
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f1,4148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4148);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// lfs f23,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f1,f24
	ctx.f24.f64 = double(float(ctx.f1.f64 * ctx.f24.f64));
	// lfs f22,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f8,f30,f23,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f23.f64 - ctx.f8.f64));
	// fmuls f7,f7,f22
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f22.f64));
	// lfs f21,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f4,f29,f21,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f21.f64 + ctx.f4.f64));
	// fmuls f5,f5,f20
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f20.f64));
	// lfs f20,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f20.f64 = double(temp.f32);
	// lfs f23,3840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3840);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f13,f31,f20,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f20.f64 - ctx.f13.f64)));
	// lfs f22,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f25,f2,f25
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f25.f64));
	// lfs f31,4164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4164);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f22,f23,f22
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f14,2608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2608);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f21,f26,f21
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f21.f64));
	// stfs f23,4164(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4164, temp.u32);
	// fmuls f23,f28,f14
	ctx.f23.f64 = double(float(ctx.f28.f64 * ctx.f14.f64));
	// lfs f18,2592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2592);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f9,f9,f6
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f6.f64));
	// lfs f17,2600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2600);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f16,f28,f18
	ctx.f16.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// fmsubs f8,f8,f28,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f28.f64 - ctx.f7.f64));
	// lfs f7,2616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2616);
	ctx.f7.f64 = double(temp.f32);
	// stfs f0,2616(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2616, temp.u32);
	// fmuls f0,f7,f6
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f6.f64));
	// stfs f0,2600(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2600, temp.u32);
	// fmsubs f5,f4,f6,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f6.f64 - ctx.f5.f64));
	// stfs f23,2608(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2608, temp.u32);
	// fmuls f19,f31,f6
	ctx.f19.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// lfs f0,2632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2632);
	ctx.f0.f64 = double(temp.f32);
	// lfs f23,2624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2624);
	ctx.f23.f64 = double(temp.f32);
	// stfs f16,2632(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2632, temp.u32);
	// fmuls f16,f28,f23
	ctx.f16.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f4,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f4.f64 = double(temp.f32);
	// stfs f16,2592(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2592, temp.u32);
	// fmadds f4,f17,f4,f24
	ctx.f4.f64 = double(float(ctx.f17.f64 * ctx.f4.f64 + ctx.f24.f64));
	// lfs f20,4860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4860);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f16.f64 = double(temp.f32);
	// lfs f24,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f25,f20,f16,f25
	ctx.f25.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 + ctx.f25.f64));
	// lfs f16,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f24,f22,f24,f21
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f24.f64 + ctx.f21.f64));
	// fmuls f16,f15,f16
	ctx.f16.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// stfs f4,2624(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2624, temp.u32);
	// lfs f4,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f21,f0,f6
	ctx.f21.f64 = double(float(ctx.f0.f64 * ctx.f6.f64));
	// fmadds f13,f9,f4,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f4.f64 + ctx.f13.f64));
	// stfs f7,4156(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4156, temp.u32);
	// stfs f11,1780(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1780, temp.u32);
	// stfs f10,4140(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4140, temp.u32);
	// stfs f0,4148(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4148, temp.u32);
	// lfs f22,4364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4364);
	ctx.f22.f64 = double(temp.f32);
	// lfs f9,2584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2584);
	ctx.f9.f64 = double(temp.f32);
	// lfs f11,4196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4196);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f0,f25,f28,f8
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f28.f64 + ctx.f8.f64));
	// lfs f10,4868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4868);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f8,f24,f6,f5
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f6.f64 + ctx.f5.f64));
	// lfs f7,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f9,f22
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// stfs f21,4860(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4860, temp.u32);
	// fadds f5,f10,f11
	ctx.f5.f64 = double(float(ctx.f10.f64 + ctx.f11.f64));
	// lfs f21,-19380(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19380);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f7,f16,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 * ctx.f7.f64));
	// lfs f4,3500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3500);
	ctx.f4.f64 = double(temp.f32);
	// stfs f27,4628(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4628, temp.u32);
	// stfs f3,4132(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4132, temp.u32);
	// stfs f2,4852(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4852, temp.u32);
	// stfs f21,444(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 444, temp.u32);
	// stfs f31,1900(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1900, temp.u32);
	// stfs f1,1788(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1788, temp.u32);
	// stfs f30,1844(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1844, temp.u32);
	// stfs f20,4604(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4604, temp.u32);
	// stfs f29,4624(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4624, temp.u32);
	// stfs f26,4972(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4972, temp.u32);
	// stfs f18,4124(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4124, temp.u32);
	// stfs f17,5176(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5176, temp.u32);
	// stfs f14,4612(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4612, temp.u32);
	// stfs f23,2528(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2528, temp.u32);
	// lfs f29,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f29.f64 = double(temp.f32);
	// lfs f1,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f8,f7,f29,f8
	ctx.f8.f64 = double(float(-(ctx.f7.f64 * ctx.f29.f64 - ctx.f8.f64)));
	// lfs f31,2576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2576);
	ctx.f31.f64 = double(temp.f32);
	// fmr f7,f21
	ctx.f7.f64 = ctx.f21.f64;
	// fnmsubs f12,f31,f1,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f1.f64 - ctx.f12.f64)));
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f13,f19,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// lfs f30,2632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2632);
	ctx.f30.f64 = double(temp.f32);
	// lfs f2,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f2.f64 = double(temp.f32);
	// lfs f3,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f0,f30,f2,f0
	ctx.f0.f64 = double(float(-(ctx.f30.f64 * ctx.f2.f64 - ctx.f0.f64)));
	// lfs f23,2616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2616);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f3,f4,f3
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// lfs f27,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,2560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2560);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f29,f29,f27,f23
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f27.f64 + ctx.f23.f64));
	// lfs f23,2552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2552);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,2568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2568);
	ctx.f26.f64 = double(temp.f32);
	// lfs f31,4652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4652);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f23,f7,f12
	ctx.f12.f64 = double(float(ctx.f23.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f23,2640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2640);
	ctx.f23.f64 = double(temp.f32);
	// lfs f2,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f14,f23,f6
	ctx.f14.f64 = double(float(ctx.f23.f64 * ctx.f6.f64));
	// lfs f16,2656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2656);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f2,f15,f2
	ctx.f2.f64 = double(float(ctx.f15.f64 * ctx.f2.f64));
	// lfs f24,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f26,f31
	ctx.f25.f64 = double(float(ctx.f26.f64 * ctx.f31.f64));
	// lfs f20,2608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2608);
	ctx.f20.f64 = double(temp.f32);
	// stfs f14,2656(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2656, temp.u32);
	// fmadds f0,f20,f24,f0
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f24.f64 + ctx.f0.f64));
	// lfs f30,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,2664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2664);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f3,f3,f30
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// lfs f1,4204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4204);
	ctx.f1.f64 = double(temp.f32);
	// stfs f4,2664(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2664, temp.u32);
	// fadds f5,f5,f1
	ctx.f5.f64 = double(float(ctx.f5.f64 + ctx.f1.f64));
	// lfs f24,2492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2492);
	ctx.f24.f64 = double(temp.f32);
	// fadds f12,f12,f29
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f29.f64));
	// lfs f4,2544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2544);
	ctx.f4.f64 = double(temp.f32);
	// lfs f30,2624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2624);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f4,f4,f24
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// lfs f20,3060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3060);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f13,f30,f6,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f6.f64 + ctx.f13.f64));
	// stfs f4,2640(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2640, temp.u32);
	// fmuls f15,f15,f20
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// lfs f4,2672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2672);
	ctx.f4.f64 = double(temp.f32);
	// stfs f15,2672(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2672, temp.u32);
	// lfs f18,2648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2648);
	ctx.f18.f64 = double(temp.f32);
	// lfs f15,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f25,f25,f18
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// fmuls f2,f2,f15
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f15,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f15.f64 = double(temp.f32);
	// stfs f25,2648(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2648, temp.u32);
	// fnmsubs f9,f9,f15,f8
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f15.f64 - ctx.f8.f64)));
	// lfs f25,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f25.f64 = double(temp.f32);
	// lfs f8,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f5,f5,f25,f3
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f25.f64 + ctx.f3.f64));
	// lfs f15,2600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2600);
	ctx.f15.f64 = double(temp.f32);
	// stfs f22,4204(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4204, temp.u32);
	// fnmsubs f13,f15,f8,f13
	ctx.f13.f64 = double(float(-(ctx.f15.f64 * ctx.f8.f64 - ctx.f13.f64)));
	// lfs f30,4212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4212);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f15,f26,f14
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// lfs f27,4980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4980);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f21,f28,f30
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f30.f64));
	// lfs f7,4220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4220);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f19,f27,f6
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f6.f64));
	// lfs f3,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f17,f26,f7
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// lfs f25,964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f3,f16,f3
	ctx.f3.f64 = double(float(ctx.f16.f64 * ctx.f3.f64));
	// lfs f22,2592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2592);
	ctx.f22.f64 = double(temp.f32);
	// stfs f11,2624(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2624, temp.u32);
	// fnmsubs f0,f22,f25,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// lfs f8,4244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4244);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f11,f4,f6
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// stfs f10,2632(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2632, temp.u32);
	// stfs f30,2756(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2756, temp.u32);
	// stfs f27,4212(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4212, temp.u32);
	// stfs f7,2584(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2584, temp.u32);
	// stfs f1,2616(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2616, temp.u32);
	// stfs f4,4868(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4868, temp.u32);
	// fmuls f7,f28,f8
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// lfs f10,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,4860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4860);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f4,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f4,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f0,f21,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f21.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// lfs f10,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f2,f10,f9
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// lfs f9,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f9.f64 = double(temp.f32);
	// lfs f27,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f27.f64 = double(temp.f32);
	// stfs f20,4980(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4980, temp.u32);
	// lfs f2,5036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5036);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,2696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2696);
	ctx.f20.f64 = double(temp.f32);
	// stfs f14,2608(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2608, temp.u32);
	// fmuls f14,f28,f20
	ctx.f14.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// stfs f16,4116(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4116, temp.u32);
	// lfs f16,2712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2712);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f19,f9,f13
	ctx.f13.f64 = double(float(ctx.f19.f64 * ctx.f9.f64 + ctx.f13.f64));
	// lfs f9,2688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2688);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f0,f17,f27,f0
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f27.f64 - ctx.f0.f64)));
	// lfs f27,2664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2664);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f9,f9,f27
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f27.f64));
	// stfs f27,5036(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5036, temp.u32);
	// fmadds f10,f5,f6,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f27,2720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2720);
	ctx.f27.f64 = double(temp.f32);
	// stfs f14,2712(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2712, temp.u32);
	// fmuls f14,f26,f27
	ctx.f14.f64 = double(float(ctx.f26.f64 * ctx.f27.f64));
	// stfs f31,3116(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3116, temp.u32);
	// lfs f31,2680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2680);
	ctx.f31.f64 = double(temp.f32);
	// stfs f24,4196(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4196, temp.u32);
	// stfs f14,2680(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2680, temp.u32);
	// lfs f24,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f24.f64 = double(temp.f32);
	// lfs f14,2656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2656);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,2704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2704);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f13,f14,f24,f13
	ctx.f13.f64 = double(float(-(ctx.f14.f64 * ctx.f24.f64 - ctx.f13.f64)));
	// lfs f14,2736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2736);
	ctx.f14.f64 = double(temp.f32);
	// stfs f12,2704(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2704, temp.u32);
	// fmuls f12,f26,f16
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// stfs f11,2736(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2736, temp.u32);
	// lfs f21,444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	ctx.f21.f64 = double(temp.f32);
	// lfs f11,2672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2672);
	ctx.f11.f64 = double(temp.f32);
	// stfs f12,2696(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2696, temp.u32);
	// fmadds f11,f11,f21,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f21.f64 + ctx.f10.f64));
	// lfs f29,2884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2884);
	ctx.f29.f64 = double(temp.f32);
	// lfs f12,2728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2728);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f25,f29,f31
	ctx.f25.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// lfs f10,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f21,f29,f12
	ctx.f21.f64 = double(float(ctx.f29.f64 * ctx.f12.f64));
	// stfs f21,2688(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2688, temp.u32);
	// fmr f21,f10
	ctx.f21.f64 = ctx.f10.f64;
	// lfs f1,4252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4252);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f10,f17,f10,f3
	ctx.f10.f64 = double(float(ctx.f17.f64 * ctx.f10.f64 + ctx.f3.f64));
	// stfs f10,4252(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4252, temp.u32);
	// fmuls f30,f26,f1
	ctx.f30.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// lfs f10,2648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2648);
	ctx.f10.f64 = double(temp.f32);
	// stfs f23,4652(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4652, temp.u32);
	// stfs f26,2720(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2720, temp.u32);
	// lfs f4,1512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1512);
	ctx.f4.f64 = double(temp.f32);
	// lfs f23,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f23.f64 = double(temp.f32);
	// lfs f5,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f23,f4,f23
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f23.f64));
	// lfs f22,3832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3832);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f5,f2,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// fmadds f0,f10,f21,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f21.f64 + ctx.f0.f64));
	// lfs f10,2744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2744);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f21,f26,f14
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// stfs f21,4244(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4244, temp.u32);
	// lfs f21,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f10,f10,f21
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f21.f64));
	// lfs f19,4164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4164);
	ctx.f19.f64 = double(temp.f32);
	// lfs f26,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f26.f64 = double(temp.f32);
	// lfs f21,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f2,f26
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f26.f64));
	// stfs f18,4220(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4220, temp.u32);
	// fmuls f18,f19,f22
	ctx.f18.f64 = double(float(ctx.f19.f64 * ctx.f22.f64));
	// stfs f29,2728(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2728, temp.u32);
	// fmuls f9,f9,f21
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f21.f64));
	// lfs f24,1504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1504);
	ctx.f24.f64 = double(temp.f32);
	// lfs f3,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f3.f64 = double(temp.f32);
	// lfs f29,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f29.f64 = double(temp.f32);
	// stfs f7,2744(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2744, temp.u32);
	// lfs f7,2640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2640);
	ctx.f7.f64 = double(temp.f32);
	// lfs f21,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f13,f7,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// fmadds f3,f24,f29,f23
	ctx.f3.f64 = double(float(ctx.f24.f64 * ctx.f29.f64 + ctx.f23.f64));
	// lfs f7,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f5,f5,f21
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f21.f64));
	// stfs f12,2592(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2592, temp.u32);
	// lfs f12,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f7,f30,f7
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f7.f64));
	// lfs f30,2752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2752);
	ctx.f30.f64 = double(temp.f32);
	// fmsubs f12,f10,f12,f9
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 - ctx.f9.f64));
	// lfs f9,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f9.f64 = double(temp.f32);
	// stfs f11,2752(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2752, temp.u32);
	// fnmsubs f0,f15,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f21,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f21.f64 = double(temp.f32);
	// stfs f8,2568(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2568, temp.u32);
	// fmuls f8,f18,f21
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f21.f64));
	// lfs f29,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f25,f29
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// stfs f4,3508(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3508, temp.u32);
	// lfs f4,4564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4564);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f11,f3,f22,f5
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f22.f64 + ctx.f5.f64));
	// lfs f25,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f25.f64 = double(temp.f32);
	// fmr f5,f9
	ctx.f5.f64 = ctx.f9.f64;
	// lfs f10,4892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4892);
	ctx.f10.f64 = double(temp.f32);
	// stfs f27,2656(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2656, temp.u32);
	// fmuls f25,f25,f4
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// stfs f23,2672(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2672, temp.u32);
	// stfs f1,2600(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2600, temp.u32);
	// lfs f1,2768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2768);
	ctx.f1.f64 = double(temp.f32);
	// lfs f27,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f27.f64 = double(temp.f32);
	// lfs f23,3596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3596);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f2,f27
	ctx.f27.f64 = double(float(ctx.f2.f64 * ctx.f27.f64));
	// stfs f24,2900(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2900, temp.u32);
	// fmuls f23,f23,f1
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// lfs f3,2760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2760);
	ctx.f3.f64 = double(temp.f32);
	// lfs f24,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f12,f10,f5,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f5.f64 + ctx.f12.f64));
	// lfs f5,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f5.f64 = double(temp.f32);
	// stfs f20,3832(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3832, temp.u32);
	// fmadds f8,f26,f5,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f5.f64 + ctx.f8.f64));
	// lfs f9,3824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3824);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f26,f3,f24
	ctx.f26.f64 = double(float(ctx.f3.f64 * ctx.f24.f64));
	// lfs f5,588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 588);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f11,f11,f9,f29
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f9.f64 - ctx.f29.f64));
	// lfs f20,2744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2744);
	ctx.f20.f64 = double(temp.f32);
	// lfs f24,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f0,f20,f5,f0
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f5.f64 + ctx.f0.f64));
	// lfs f21,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f29,f2,f24
	ctx.f29.f64 = double(float(ctx.f2.f64 * ctx.f24.f64));
	// lfs f20,2736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2736);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f27,f27,f9
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f9.f64));
	// lfs f18,2728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2728);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f13,f20,f21,f13
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f21.f64 - ctx.f13.f64)));
	// lfs f24,2784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2784);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f25,f18
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// stfs f16,2648(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2648, temp.u32);
	// fmuls f23,f23,f9
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f9.f64));
	// lfs f16,2720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2720);
	ctx.f16.f64 = double(temp.f32);
	// stfs f31,4860(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4860, temp.u32);
	// fmsubs f12,f12,f16,f7
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f16.f64 - ctx.f7.f64));
	// stfs f14,2664(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2664, temp.u32);
	// fmuls f14,f28,f24
	ctx.f14.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// lfs f31,996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,2776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2776);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// lfs f21,2792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2792);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,2800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2800);
	ctx.f20.f64 = double(temp.f32);
	// stfs f17,2640(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2640, temp.u32);
	// fmuls f17,f18,f5
	ctx.f17.f64 = double(float(ctx.f18.f64 * ctx.f5.f64));
	// stfs f6,2884(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2884, temp.u32);
	// fmuls f6,f28,f19
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f19.f64));
	// stfs f2,2760(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2760, temp.u32);
	// stfs f14,2792(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2792, temp.u32);
	// stfs f25,2800(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2800, temp.u32);
	// lfs f7,1496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1496);
	ctx.f7.f64 = double(temp.f32);
	// lfs f15,2808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2808);
	ctx.f15.f64 = double(temp.f32);
	// lfs f2,3660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3660);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,2816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2816);
	ctx.f14.f64 = double(temp.f32);
	// lfs f25,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f25.f64 = double(temp.f32);
	// stfs f23,2784(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2784, temp.u32);
	// fmr f23,f25
	ctx.f23.f64 = ctx.f25.f64;
	// fmadds f11,f8,f9,f11
	ctx.f11.f64 = double(float(ctx.f8.f64 * ctx.f9.f64 + ctx.f11.f64));
	// stfs f4,4892(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4892, temp.u32);
	// fmadds f31,f21,f25,f31
	ctx.f31.f64 = double(float(ctx.f21.f64 * ctx.f25.f64 + ctx.f31.f64));
	// stfs f9,2768(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2768, temp.u32);
	// lfs f4,2696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2696);
	ctx.f4.f64 = double(temp.f32);
	// lfs f9,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f9.f64 = double(temp.f32);
	// lfs f25,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f4,f9,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f9.f64 - ctx.f12.f64)));
	// fmuls f25,f18,f25
	ctx.f25.f64 = double(float(ctx.f18.f64 * ctx.f25.f64));
	// stfs f18,2808(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2808, temp.u32);
	// lfs f18,2712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2712);
	ctx.f18.f64 = double(temp.f32);
	// stfs f10,2712(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2712, temp.u32);
	// lfs f10,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,2688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2688);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f26,f20,f23,f26
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f23.f64 + ctx.f26.f64));
	// fnmsubs f11,f4,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f23,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f0,f18,f23,f0
	ctx.f0.f64 = double(float(-(ctx.f18.f64 * ctx.f23.f64 - ctx.f0.f64)));
	// stfs f7,1972(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1972, temp.u32);
	// fmuls f18,f7,f22
	ctx.f18.f64 = double(float(ctx.f7.f64 * ctx.f22.f64));
	// stfs f26,2776(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2776, temp.u32);
	// lfs f26,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f26.f64 = double(temp.f32);
	// lfs f7,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f29,f29,f26
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// lfs f10,2680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2680);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f12,f10,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// stfs f31,2816(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2816, temp.u32);
	// lfs f31,2704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2704);
	ctx.f31.f64 = double(temp.f32);
	// lfs f23,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f23.f64 = double(temp.f32);
	// fadds f13,f31,f13
	ctx.f13.f64 = double(float(ctx.f31.f64 + ctx.f13.f64));
	// lfs f10,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f31,f28,f15
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f15.f64));
	// stfs f5,4164(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4164, temp.u32);
	// fmuls f5,f2,f23
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// fnmsubs f11,f17,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f17.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f23,1416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1416);
	ctx.f23.f64 = double(temp.f32);
	// lfs f10,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f10.f64 = double(temp.f32);
	// stfs f21,3824(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3824, temp.u32);
	// fmuls f10,f10,f23
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f23.f64));
	// lfs f7,760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 760);
	ctx.f7.f64 = double(temp.f32);
	// lfs f21,4244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4244);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f21,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f26,1772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1772);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f29,f18,f23,f29
	ctx.f29.f64 = double(float(ctx.f18.f64 * ctx.f23.f64 - ctx.f29.f64));
	// stfs f3,3840(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3840, temp.u32);
	// fmuls f9,f25,f26
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f4,4252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4252);
	ctx.f4.f64 = double(temp.f32);
	// lfs f23,2752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2752);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f0,f4,f28,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f3,5036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5036);
	ctx.f3.f64 = double(temp.f32);
	// fadds f13,f13,f23
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f23.f64));
	// lfs f7,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f7.f64 = double(temp.f32);
	// stfs f24,2576(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2576, temp.u32);
	// fmuls f7,f6,f7
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// stfs f20,1504(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1504, temp.u32);
	// lfs f8,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f8.f64 = double(temp.f32);
	// lfs f26,4900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4900);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f8,f14,f8
	ctx.f8.f64 = double(float(ctx.f14.f64 * ctx.f8.f64));
	// lfs f25,4332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4332);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,2836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2836);
	ctx.f24.f64 = double(temp.f32);
	// lfs f21,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,4716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4716);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f26,f21
	ctx.f21.f64 = double(float(ctx.f26.f64 * ctx.f21.f64));
	// stfs f2,3500(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3500, temp.u32);
	// fmuls f2,f3,f22
	ctx.f2.f64 = double(float(ctx.f3.f64 * ctx.f22.f64));
	// stfs f30,2728(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2728, temp.u32);
	// fmuls f22,f25,f24
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// stfs f1,2696(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2696, temp.u32);
	// stfs f19,2744(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2744, temp.u32);
	// fmuls f19,f16,f20
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f20.f64));
	// lfs f4,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f30.f64 = double(temp.f32);
	// lfs f6,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f6.f64 = double(temp.f32);
	// lfs f23,2824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2824);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,2832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2832);
	ctx.f18.f64 = double(temp.f32);
	// stfs f15,3596(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3596, temp.u32);
	// stfs f14,2720(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2720, temp.u32);
	// fnmsubs f11,f27,f4,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f4.f64 - ctx.f11.f64)));
	// lfs f4,2848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2848);
	ctx.f4.f64 = double(temp.f32);
	// lfs f16,2840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2840);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f28,f18
	ctx.f15.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// stfs f15,2848(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2848, temp.u32);
	// fmuls f15,f28,f16
	ctx.f15.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f27,2816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2816);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f17,f28,f23
	ctx.f17.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// stfs f15,2832(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2832, temp.u32);
	// fmadds f12,f27,f28,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f28.f64 + ctx.f12.f64));
	// lfs f15,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f8,f5,f15,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f15.f64 + ctx.f8.f64));
	// lfs f5,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f10,f10,f5,f21
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64 + ctx.f21.f64));
	// lfs f21,2800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2800);
	ctx.f21.f64 = double(temp.f32);
	// lfs f27,2856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2856);
	ctx.f27.f64 = double(temp.f32);
	// fmr f5,f1
	ctx.f5.f64 = ctx.f1.f64;
	// stfs f17,2856(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2856, temp.u32);
	// fnmsubs f11,f21,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f21.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f21,2792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2792);
	ctx.f21.f64 = double(temp.f32);
	// lfs f14,2864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2864);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,2808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2808);
	ctx.f17.f64 = double(temp.f32);
	// stfs f19,2864(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2864, temp.u32);
	// fmuls f19,f17,f4
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// fnmsubs f12,f21,f30,f12
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f30.f64 - ctx.f12.f64)));
	// stfs f19,2840(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2840, temp.u32);
	// lfs f19,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f19.f64 = double(temp.f32);
	// stfs f13,5456(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5456, temp.u32);
	// fnmsubs f0,f7,f19,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f19.f64 - ctx.f0.f64)));
	// lfs f13,2784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2784);
	ctx.f13.f64 = double(temp.f32);
	// lfs f7,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f22,f7,f29
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64 + ctx.f29.f64));
	// lfs f22,3656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3656);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f13,f13,f6,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f11.f64));
	// lfs f11,2776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2776);
	ctx.f11.f64 = double(temp.f32);
	// fmr f29,f30
	ctx.f29.f64 = ctx.f30.f64;
	// lfs f1,1892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1892);
	ctx.f1.f64 = double(temp.f32);
	// lfs f15,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f19,f17,f22
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f22.f64));
	// stfs f22,2824(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2824, temp.u32);
	// fmuls f22,f1,f27
	ctx.f22.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// fmadds f12,f11,f28,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f28.f64 + ctx.f12.f64));
	// stfs f3,2816(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2816, temp.u32);
	// fmr f28,f15
	ctx.f28.f64 = ctx.f15.f64;
	// lfs f3,4992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4992);
	ctx.f3.f64 = double(temp.f32);
	// stfs f27,1772(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1772, temp.u32);
	// lfs f27,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f27.f64 = double(temp.f32);
	// stfs f24,4332(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4332, temp.u32);
	// stfs f25,2792(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2792, temp.u32);
	// fnmsubs f13,f9,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// stfs f4,2704(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2704, temp.u32);
	// lfs f30,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f30.f64 = double(temp.f32);
	// lfs f21,2872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2872);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f30,f14,f30
	ctx.f30.f64 = double(float(ctx.f14.f64 * ctx.f30.f64));
	// lfs f11,2880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2880);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f27,f22,f27
	ctx.f27.f64 = double(float(ctx.f22.f64 * ctx.f27.f64));
	// fnmsubs f12,f31,f29,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f29.f64 - ctx.f12.f64)));
	// lfs f29,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// lfs f29,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f29.f64 = double(temp.f32);
	// lfs f9,3292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3292);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f29,f19,f29
	ctx.f29.f64 = double(float(ctx.f19.f64 * ctx.f29.f64));
	// lfs f5,2888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2888);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f1,f11
	ctx.f6.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// lfs f4,1488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1488);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f28,f5,f28
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f28.f64));
	// lfs f25,1480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1480);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f31,f9,f4
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// lfs f24,2956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2956);
	ctx.f24.f64 = double(temp.f32);
	// stfs f26,2688(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2688, temp.u32);
	// fmuls f26,f21,f15
	ctx.f26.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// stfs f20,2752(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2752, temp.u32);
	// fmuls f22,f24,f25
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// stfs f23,2736(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2736, temp.u32);
	// stfs f18,1512(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1512, temp.u32);
	// fmr f18,f15
	ctx.f18.f64 = ctx.f15.f64;
	// lfs f23,2896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2896);
	ctx.f23.f64 = double(temp.f32);
	// lfs f20,2904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2904);
	ctx.f20.f64 = double(temp.f32);
	// lfs f19,3516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3516);
	ctx.f19.f64 = double(temp.f32);
	// stfs f16,2784(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2784, temp.u32);
	// stfs f14,4252(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4252, temp.u32);
	// fmadds f7,f2,f18,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f18.f64 + ctx.f7.f64));
	// lfs f2,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f2.f64 = double(temp.f32);
	// fmr f18,f2
	ctx.f18.f64 = ctx.f2.f64;
	// lfs f16,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f30,f23,f16,f30
	ctx.f30.f64 = double(float(ctx.f23.f64 * ctx.f16.f64 + ctx.f30.f64));
	// lfs f16,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f16.f64 = double(temp.f32);
	// fmsubs f2,f20,f2,f26
	ctx.f2.f64 = double(float(ctx.f20.f64 * ctx.f2.f64 - ctx.f26.f64));
	// fmadds f3,f3,f15,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f15.f64 + ctx.f28.f64));
	// lfs f28,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f31,f31,f28,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f28.f64 - ctx.f27.f64));
	// lfs f28,2948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2948);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f26,f19,f26
	ctx.f26.f64 = double(float(ctx.f19.f64 * ctx.f26.f64));
	// stfs f0,2880(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2880, temp.u32);
	// fmadds f13,f8,f17,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f0,2928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2928);
	ctx.f0.f64 = double(temp.f32);
	// stfs f25,2888(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2888, temp.u32);
	// lfs f15,2912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2912);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f6,f6,f18
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f18.f64));
	// lfs f18,1804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1804);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// lfs f16,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f29,f29,f16
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f16.f64));
	// lfs f27,3628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3628);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f16,f28,f25
	ctx.f16.f64 = double(float(ctx.f28.f64 * ctx.f25.f64));
	// stfs f1,2904(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2904, temp.u32);
	// fmuls f25,f17,f0
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f0.f64));
	// stfs f25,2896(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2896, temp.u32);
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f14,f1,f27
	ctx.f14.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// fmadds f30,f15,f25,f30
	ctx.f30.f64 = double(float(ctx.f15.f64 * ctx.f25.f64 + ctx.f30.f64));
	// lfs f25,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f25.f64 = double(temp.f32);
	// stfs f10,2872(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2872, temp.u32);
	// stfs f14,2928(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2928, temp.u32);
	// lfs f10,2920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2920);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f6,f2,f1,f6
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 - ctx.f6.f64));
	// lfs f1,2676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2676);
	ctx.f1.f64 = double(temp.f32);
	// lfs f14,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f25,f1,f25
	ctx.f25.f64 = double(float(ctx.f1.f64 * ctx.f25.f64));
	// stfs f1,2920(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2920, temp.u32);
	// fmadds f26,f10,f14,f26
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f14.f64 + ctx.f26.f64));
	// stfs f25,2912(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2912, temp.u32);
	// lfs f1,2864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2864);
	ctx.f1.f64 = double(temp.f32);
	// lfs f25,868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	ctx.f25.f64 = double(temp.f32);
	// lfs f2,4028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4028);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f12,f1,f25,f12
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f25.f64 + ctx.f12.f64));
	// lfs f14,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f25,f2,f14,f18
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f14.f64 + ctx.f18.f64));
	// lfs f14,2768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2768);
	ctx.f14.f64 = double(temp.f32);
	// lfs f1,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f1.f64 = double(temp.f32);
	// fmsubs f7,f7,f14,f29
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f14.f64 - ctx.f29.f64));
	// fnmsubs f1,f22,f1,f31
	ctx.f1.f64 = double(float(-(ctx.f22.f64 * ctx.f1.f64 - ctx.f31.f64)));
	// stfs f9,4716(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4716, temp.u32);
	// stfs f0,3292(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3292, temp.u32);
	// lfs f9,2856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2856);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	ctx.f0.f64 = double(temp.f32);
	// stfs f10,4244(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4244, temp.u32);
	// lfs f10,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f10.f64 = double(temp.f32);
	// lfs f18,4396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4396);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f0,f9,f0,f12
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f29,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f16,f10,f6
	ctx.f12.f64 = double(float(-(ctx.f16.f64 * ctx.f10.f64 - ctx.f6.f64)));
	// stfs f11,1496(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1496, temp.u32);
	// fmadds f3,f18,f29,f3
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f29.f64 + ctx.f3.f64));
	// fmadds f11,f30,f17,f7
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f17.f64 + ctx.f7.f64));
	// lfs f10,852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,2936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2936);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f7,f26,f4,f1
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f4.f64 + ctx.f1.f64));
	// lfs f6,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f6.f64 = double(temp.f32);
	// lfs f8,2944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2944);
	ctx.f8.f64 = double(temp.f32);
	// stfs f21,1488(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1488, temp.u32);
	// stfs f5,1892(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1892, temp.u32);
	// stfs f24,2864(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2864, temp.u32);
	// stfs f23,2776(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2776, temp.u32);
	// stfs f20,1480(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1480, temp.u32);
	// stfs f19,2800(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2800, temp.u32);
	// stfs f28,2808(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2808, temp.u32);
	// stfs f27,4900(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4900, temp.u32);
	// stfs f15,2680(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2680, temp.u32);
	// stfs f2,2836(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2836, temp.u32);
	// lfs f5,2928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2928);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f29,f25,f17
	ctx.f29.f64 = double(float(ctx.f25.f64 * ctx.f17.f64));
	// fmuls f6,f5,f6
	ctx.f6.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// lfs f26,2848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2848);
	ctx.f26.f64 = double(temp.f32);
	// lfs f5,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f0,f26,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f26.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f31,2912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2912);
	ctx.f31.f64 = double(temp.f32);
	// lfs f26,2904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2904);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f8,f8,f5,f31
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64 + ctx.f31.f64));
	// lfs f1,1796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1796);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f3,f26,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f26.f64 + ctx.f12.f64));
	// fmuls f5,f1,f9
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f9.f64));
	// lfs f2,2920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2920);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f2,f17
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f17.f64));
	// lfs f23,2896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2896);
	ctx.f23.f64 = double(temp.f32);
	// lfs f3,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f3.f64 = double(temp.f32);
	// lfs f25,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f23,f3,f11
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f3.f64 - ctx.f11.f64)));
	// lfs f10,2952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2952);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f7,f6,f25,f7
	ctx.f7.f64 = double(float(-(ctx.f6.f64 * ctx.f25.f64 - ctx.f7.f64)));
	// lfs f6,2888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2888);
	ctx.f6.f64 = double(temp.f32);
	// lfs f15,2832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2832);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f10,f26,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f10.f64));
	// lfs f31,620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 620);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,3324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3324);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f8,f8,f26
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f26.f64));
	// lfs f22,2876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2876);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f0,f15,f31,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f31.f64 - ctx.f0.f64)));
	// lfs f3,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f25,f28,f6
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f6.f64));
	// lfs f30,1756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1756);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f5,f3,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f3.f64 + ctx.f12.f64));
	// lfs f24,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f16,f22,f4
	ctx.f16.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// lfs f21,2840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2840);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f27,f30,f4
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// lfs f31,1988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1988);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f21,f24,f13
	ctx.f13.f64 = double(float(-(ctx.f21.f64 * ctx.f24.f64 - ctx.f13.f64)));
	// lfs f5,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f5.f64 = double(temp.f32);
	// stfs f26,2936(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2936, temp.u32);
	// fmuls f5,f2,f5
	ctx.f5.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// stfs f16,1988(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1988, temp.u32);
	// lfs f16,2760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2760);
	ctx.f16.f64 = double(temp.f32);
	// lfs f26,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f26.f64 = double(temp.f32);
	// lfs f2,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f26,f16,f26
	ctx.f26.f64 = double(float(ctx.f16.f64 * ctx.f26.f64));
	// lfs f3,2652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2652);
	ctx.f3.f64 = double(temp.f32);
	// lfs f16,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f3,f3,f2
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// lfs f2,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f11,f29,f16,f11
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f16.f64 + ctx.f11.f64));
	// lfs f29,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f8,f8,f2,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f2.f64 + ctx.f7.f64));
	// fmuls f10,f10,f2
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// stfs f9,5036(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5036, temp.u32);
	// fmuls f2,f25,f29
	ctx.f2.f64 = double(float(ctx.f25.f64 * ctx.f29.f64));
	// lfs f9,2872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2872);
	ctx.f9.f64 = double(temp.f32);
	// lfs f7,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f13,f9,f17,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f29,2880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2880);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f27,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f23,2960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2960);
	ctx.f23.f64 = double(temp.f32);
	// fadds f0,f29,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 + ctx.f0.f64));
	// lfs f24,2932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2932);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f20,f17,f23
	ctx.f20.f64 = double(float(ctx.f17.f64 * ctx.f23.f64));
	// lfs f21,2968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2968);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f15,f24,f4
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// lfs f19,2976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2976);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f9,f17,f21
	ctx.f9.f64 = double(float(ctx.f17.f64 * ctx.f21.f64));
	// stfs f6,2920(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2920, temp.u32);
	// fmuls f7,f17,f19
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// stfs f18,2876(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2876, temp.u32);
	// fmuls f6,f17,f31
	ctx.f6.f64 = double(float(ctx.f17.f64 * ctx.f31.f64));
	// stfs f30,2904(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2904, temp.u32);
	// stfs f28,2872(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2872, temp.u32);
	// stfs f23,2768(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2768, temp.u32);
	// stfs f22,2560(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2560, temp.u32);
	// stfs f24,2896(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2896, temp.u32);
	// stfs f21,2848(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2848, temp.u32);
	// stfs f0,5460(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5460, temp.u32);
	// stfs f19,2832(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2832, temp.u32);
	// lfs f0,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f0,f5,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f25,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f8,f3,f25,f8
	ctx.f8.f64 = double(float(-(ctx.f3.f64 * ctx.f25.f64 - ctx.f8.f64)));
	// lfs f3,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f3.f64 = double(temp.f32);
	// lfs f24,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,2824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2824);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f2,f15,f24,f2
	ctx.f2.f64 = double(float(ctx.f15.f64 * ctx.f24.f64 - ctx.f2.f64));
	// lfs f18,1980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1980);
	ctx.f18.f64 = double(temp.f32);
	// stfs f31,2856(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2856, temp.u32);
	// fmuls f31,f26,f14
	ctx.f31.f64 = double(float(ctx.f26.f64 * ctx.f14.f64));
	// lfs f11,3636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3636);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f15,f18,f4
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f4.f64));
	// stfs f14,2968(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2968, temp.u32);
	// fmuls f30,f1,f11
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f11.f64));
	// lfs f28,2992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2992);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,3000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3000);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f20,f3,f0
	ctx.f0.f64 = double(float(-(ctx.f20.f64 * ctx.f3.f64 - ctx.f0.f64)));
	// lfs f3,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f3.f64 = double(temp.f32);
	// lfs f14,1464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1464);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f3,f22,f3
	ctx.f3.f64 = double(float(ctx.f22.f64 * ctx.f3.f64));
	// lfs f22,2924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2924);
	ctx.f22.f64 = double(temp.f32);
	// stfs f15,3000(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3000, temp.u32);
	// fmuls f15,f1,f14
	ctx.f15.f64 = double(float(ctx.f1.f64 * ctx.f14.f64));
	// stfs f13,4396(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4396, temp.u32);
	// fmuls f19,f22,f4
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f4.f64));
	// stfs f6,2992(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2992, temp.u32);
	// lfs f16,3008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3008);
	ctx.f16.f64 = double(temp.f32);
	// lfs f13,3016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3016);
	ctx.f13.f64 = double(temp.f32);
	// lfs f5,3316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3316);
	ctx.f5.f64 = double(temp.f32);
	// lfs f23,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,4616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4616);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f23,f5,f23
	ctx.f23.f64 = double(float(ctx.f5.f64 * ctx.f23.f64));
	// lfs f6,2484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2484);
	ctx.f6.f64 = double(temp.f32);
	// stfs f4,3016(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3016, temp.u32);
	// stfs f7,3008(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3008, temp.u32);
	// fmuls f7,f21,f6
	ctx.f7.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// stfs f15,2976(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2976, temp.u32);
	// lfs f4,1988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1988);
	ctx.f4.f64 = double(temp.f32);
	// lfs f15,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f15.f64 = double(temp.f32);
	// stfs f7,2952(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2952, temp.u32);
	// fnmsubs f12,f4,f15,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f15.f64 - ctx.f12.f64)));
	// lfs f24,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f24.f64 = double(temp.f32);
	// lfs f7,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f7.f64 = double(temp.f32);
	// lfs f25,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f10,f10,f7,f8
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f7.f64 - ctx.f8.f64)));
	// lfs f4,1904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1904);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f25,f25,f24
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// fmuls f7,f1,f4
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// stfs f10,1988(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1988, temp.u32);
	// lfs f4,2056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2056);
	ctx.f4.f64 = double(temp.f32);
	// lfs f8,2612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2612);
	ctx.f8.f64 = double(temp.f32);
	// lfs f10,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f10.f64 = double(temp.f32);
	// fadds f8,f4,f8
	ctx.f8.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
	// fnmsubs f10,f30,f10,f2
	ctx.f10.f64 = double(float(-(ctx.f30.f64 * ctx.f10.f64 - ctx.f2.f64)));
	// lfs f2,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f30.f64 = double(temp.f32);
	// lfs f20,2500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2500);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f3,f3,f30
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// lfs f4,3652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3652);
	ctx.f4.f64 = double(temp.f32);
	// stfs f17,2888(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2888, temp.u32);
	// fmadds f4,f4,f2,f23
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 + ctx.f23.f64));
	// lfs f29,2984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2984);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f17,f1,f20
	ctx.f17.f64 = double(float(ctx.f1.f64 * ctx.f20.f64));
	// lfs f2,2572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2572);
	ctx.f2.f64 = double(temp.f32);
	// fadds f26,f28,f29
	ctx.f26.f64 = double(float(ctx.f28.f64 + ctx.f29.f64));
	// lfs f30,4904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4904);
	ctx.f30.f64 = double(temp.f32);
	// stfs f17,2944(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2944, temp.u32);
	// fmuls f30,f30,f2
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// stfs f19,2984(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2984, temp.u32);
	// lfs f24,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f24.f64 = double(temp.f32);
	// lfs f17,1472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1472);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f24,f27,f24
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f24.f64));
	// lfs f19,3188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3188);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f25,f25,f17
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f17.f64));
	// stfs f31,2960(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2960, temp.u32);
	// fadds f31,f13,f16
	ctx.f31.f64 = double(float(ctx.f13.f64 + ctx.f16.f64));
	// stfs f2,2928(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2928, temp.u32);
	// fmuls f15,f1,f19
	ctx.f15.f64 = double(float(ctx.f1.f64 * ctx.f19.f64));
	// lfs f23,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f23.f64 = double(temp.f32);
	// lfs f2,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f0,f9,f23,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f23.f64 + ctx.f0.f64));
	// stfs f11,4564(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4564, temp.u32);
	// fmadds f11,f4,f6,f10
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f6.f64 + ctx.f10.f64));
	// lfs f4,2024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2024);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f8,f8,f4,f30
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f4.f64 + ctx.f30.f64));
	// stfs f29,3704(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3704, temp.u32);
	// lfs f29,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f2,f26,f2,f24
	ctx.f2.f64 = double(float(ctx.f26.f64 * ctx.f2.f64 + ctx.f24.f64));
	// lfs f30,3008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3008);
	ctx.f30.f64 = double(temp.f32);
	// stfs f13,4984(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4984, temp.u32);
	// lfs f13,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f13.f64 = double(temp.f32);
	// lfs f10,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f13,f25,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f13.f64));
	// fmadds f10,f31,f10,f3
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f10.f64 + ctx.f3.f64));
	// lfs f24,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f24.f64 = double(temp.f32);
	// stfs f20,2840(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2840, temp.u32);
	// fnmsubs f0,f30,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f30.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f30,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f30.f64 = double(temp.f32);
	// stfs f22,4616(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4616, temp.u32);
	// fmadds f11,f7,f30,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f30.f64 + ctx.f11.f64));
	// lfs f20,2992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2992);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f8,f8,f24
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f24.f64));
	// lfs f22,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f22.f64 = double(temp.f32);
	// stfs f5,2552(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2552, temp.u32);
	// lfs f24,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f24.f64 = double(temp.f32);
	// lfs f5,3024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3024);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f4.f64 = double(temp.f32);
	// lfs f29,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f29.f64 = double(temp.f32);
	// lfs f30,3000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3000);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f10,f5,f29,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f29.f64 + ctx.f10.f64));
	// fnmsubs f22,f20,f22,f0
	ctx.f22.f64 = double(float(-(ctx.f20.f64 * ctx.f22.f64 - ctx.f0.f64)));
	// lfs f29,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f13,f30,f4,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 - ctx.f13.f64));
	// lfs f30,3048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3048);
	ctx.f30.f64 = double(temp.f32);
	// fmr f20,f24
	ctx.f20.f64 = ctx.f24.f64;
	// lfs f0,2984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2984);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f30,f29
	ctx.f29.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// stfs f21,4992(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4992, temp.u32);
	// fmadds f12,f0,f24,f12
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f24.f64 + ctx.f12.f64));
	// lfs f21,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f21.f64 = double(temp.f32);
	// lfs f0,2976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2976);
	ctx.f0.f64 = double(temp.f32);
	// lfs f7,3032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3032);
	ctx.f7.f64 = double(temp.f32);
	// lfs f25,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f25.f64 = double(temp.f32);
	// stfs f18,2880(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2880, temp.u32);
	// fmadds f2,f7,f25,f2
	ctx.f2.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 + ctx.f2.f64));
	// lfs f3,3016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3016);
	ctx.f3.f64 = double(temp.f32);
	// lfs f18,3084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3084);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f0,f21,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f21.f64 + ctx.f13.f64));
	// lfs f24,2468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2468);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f11,f15,f20,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f20.f64 - ctx.f11.f64)));
	// lfs f15,2968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2968);
	ctx.f15.f64 = double(temp.f32);
	// stfs f15,3024(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3024, temp.u32);
	// fmuls f29,f29,f15
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// stfs f16,4736(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4736, temp.u32);
	// fmr f0,f21
	ctx.f0.f64 = ctx.f21.f64;
	// stfs f28,4744(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4744, temp.u32);
	// fmuls f15,f18,f3
	ctx.f15.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// stfs f27,4028(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4028, temp.u32);
	// lfs f28,1456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1456);
	ctx.f28.f64 = double(temp.f32);
	// lfs f9,3436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3436);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f10,f10,f28
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// lfs f27,3092(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3092);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f9,f3
	ctx.f31.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// lfs f4,3040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3040);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f27,f3
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f3.f64));
	// lfs f25,3448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3448);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f23,f4,f28
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// lfs f21,3056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3056);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f25,f28
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// lfs f20,1408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1408);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,2940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2940);
	ctx.f16.f64 = double(temp.f32);
	// stfs f19,2824(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2824, temp.u32);
	// fmuls f19,f1,f24
	ctx.f19.f64 = double(float(ctx.f1.f64 * ctx.f24.f64));
	// stfs f17,2912(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2912, temp.u32);
	// fmuls f17,f21,f20
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f20.f64));
	// stfs f14,2760(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2760, temp.u32);
	// stfs f1,3032(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3032, temp.u32);
	// stfs f15,3048(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3048, temp.u32);
	// fmuls f15,f16,f3
	ctx.f15.f64 = double(float(ctx.f16.f64 * ctx.f3.f64));
	// lfs f14,1996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1996);
	ctx.f14.f64 = double(temp.f32);
	// lfs f1,2004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2004);
	ctx.f1.f64 = double(temp.f32);
	// stfs f15,3040(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3040, temp.u32);
	// fmadds f8,f2,f28,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f28.f64 + ctx.f8.f64));
	// lfs f15,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f15.f64 = double(temp.f32);
	// stfs f30,2004(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2004, temp.u32);
	// fmuls f25,f25,f15
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f15.f64));
	// lfs f30,2960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2960);
	ctx.f30.f64 = double(temp.f32);
	// fmr f15,f0
	ctx.f15.f64 = ctx.f0.f64;
	// lfs f2,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f2,f30,f2,f22
	ctx.f2.f64 = double(float(-(ctx.f30.f64 * ctx.f2.f64 - ctx.f22.f64)));
	// stfs f9,2992(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2992, temp.u32);
	// stfs f25,1996(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1996, temp.u32);
	// fmuls f25,f14,f28
	ctx.f25.f64 = double(float(ctx.f14.f64 * ctx.f28.f64));
	// lfs f9,2952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2952);
	ctx.f9.f64 = double(temp.f32);
	// lfs f30,1988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1988);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f9,f9,f0,f30
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f30.f64)));
	// stfs f25,3056(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3056, temp.u32);
	// lfs f30,2944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2944);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f12,f30,f0,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f0.f64 + ctx.f12.f64));
	// stfs f5,2544(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2544, temp.u32);
	// lfs f5,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f0,f31,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f3,3016(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3016, temp.u32);
	// fmadds f8,f29,f5,f8
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f5.f64 + ctx.f8.f64));
	// lfs f3,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f11,f26,f15,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f15.f64 - ctx.f11.f64)));
	// fmr f5,f3
	ctx.f5.f64 = ctx.f3.f64;
	// lfs f25,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f25.f64 = double(temp.f32);
	// stfs f27,3000(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3000, temp.u32);
	// fmsubs f10,f23,f25,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f25.f64 - ctx.f10.f64));
	// fmr f27,f15
	ctx.f27.f64 = ctx.f15.f64;
	// stfs f24,2960(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2960, temp.u32);
	// lfs f25,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f25.f64 = double(temp.f32);
	// lfs f13,2936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2936);
	ctx.f13.f64 = double(temp.f32);
	// stfs f4,2948(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2948, temp.u32);
	// lfs f4,4396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4396);
	ctx.f4.f64 = double(temp.f32);
	// fadds f12,f9,f12
	ctx.f12.f64 = double(float(ctx.f9.f64 + ctx.f12.f64));
	// fnmsubs f0,f19,f3,f0
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f3.f64 - ctx.f0.f64)));
	// stfs f7,4784(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4784, temp.u32);
	// stfs f21,2952(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2952, temp.u32);
	// fmuls f7,f13,f1
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// stfs f1,1980(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1980, temp.u32);
	// fadds f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f2.f64));
	// fmadds f11,f17,f5,f11
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f5.f64 + ctx.f11.f64));
	// lfs f5,2916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2916);
	ctx.f5.f64 = double(temp.f32);
	// lfs f24,1996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1996);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f2,f5,f6
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// fnmsubs f10,f24,f27,f10
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f27.f64 - ctx.f10.f64)));
	// lfs f27,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f27.f64 = double(temp.f32);
	// lfs f24,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f24.f64 = double(temp.f32);
	// lfs f23,3056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3056);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f8,f23,f25,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f25.f64 - ctx.f8.f64));
	// lfs f25,3048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3048);
	ctx.f25.f64 = double(temp.f32);
	// lfs f3,3064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3064);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f0,f25,f27,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f27.f64 + ctx.f0.f64));
	// lfs f25,3040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3040);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,4020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4020);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f31,f3,f28
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// lfs f30,1392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1392);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f11,f25,f24,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f24.f64 - ctx.f11.f64)));
	// lfs f25,4828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4828);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,4628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4628);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f1,f30
	ctx.f29.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f9,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f9.f64 = double(temp.f32);
	// lfs f26,2004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2004);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,1448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1448);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f9,f26,f9
	ctx.f9.f64 = double(float(ctx.f26.f64 * ctx.f9.f64));
	// lfs f23,3032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3032);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,3072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3072);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f22,f23,f27
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// lfs f19,3308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3308);
	ctx.f19.f64 = double(temp.f32);
	// stfs f20,3008(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3008, temp.u32);
	// fmuls f20,f25,f6
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f6.f64));
	// stfs f18,2984(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2984, temp.u32);
	// fmuls f18,f24,f6
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// stfs f16,2944(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2944, temp.u32);
	// fmuls f16,f13,f21
	ctx.f16.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// stfs f14,4904(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4904, temp.u32);
	// fmuls f6,f19,f6
	ctx.f6.f64 = double(float(ctx.f19.f64 * ctx.f6.f64));
	// lfs f17,3080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3080);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,3088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3088);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,3096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3096);
	ctx.f14.f64 = double(temp.f32);
	// stfs f16,3080(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3080, temp.u32);
	// lfs f16,3024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3024);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f9,f9,f16
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f16.f64));
	// stfs f9,3088(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3088, temp.u32);
	// lfs f9,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f10,f31,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// stfs f12,3072(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3072, temp.u32);
	// lfs f9,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f12,f15,f28
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f28.f64));
	// lfs f31,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f0,f7,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// stfs f12,3056(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3056, temp.u32);
	// fmadds f11,f2,f31,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f12,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f31,f14,f28
	ctx.f31.f64 = double(float(ctx.f14.f64 * ctx.f28.f64));
	// lfs f2,2928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2928);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f12,f26,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f12.f64));
	// lfs f9,2520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2520);
	ctx.f9.f64 = double(temp.f32);
	// lfs f26,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f9,f9,f2
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f2.f64));
	// lfs f7,4896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4896);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f8,f29,f26,f8
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f26.f64 + ctx.f8.f64));
	// lfs f2,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f2.f64 = double(temp.f32);
	// stfs f8,3048(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3048, temp.u32);
	// fmuls f7,f2,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// lfs f2,2160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2160);
	ctx.f2.f64 = double(temp.f32);
	// lfs f8,3844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3844);
	ctx.f8.f64 = double(temp.f32);
	// stfs f6,2004(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2004, temp.u32);
	// fmuls f8,f2,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f8.f64));
	// lfs f6,2248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2248);
	ctx.f6.f64 = double(temp.f32);
	// lfs f29,1888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1888);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f12,f12,f16
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f16.f64));
	// stfs f11,3064(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3064, temp.u32);
	// fmuls f26,f29,f6
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f6.f64));
	// stfs f18,3096(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3096, temp.u32);
	// stfs f13,3024(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3024, temp.u32);
	// lfs f18,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f18.f64 = double(temp.f32);
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f7,f7,f28
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f28.f64));
	// lfs f11,2668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2668);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f0,f22,f13,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// fmuls f11,f11,f18
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f18.f64));
	// stfs f27,4020(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4020, temp.u32);
	// lfs f27,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f27.f64 = double(temp.f32);
	// stfs f5,2976(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2976, temp.u32);
	// fmuls f8,f8,f27
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// lfs f5,2240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2240);
	ctx.f5.f64 = double(temp.f32);
	// stfs f3,2956(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2956, temp.u32);
	// stfs f1,3032(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3032, temp.u32);
	// fmadds f1,f5,f2,f26
	ctx.f1.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f26.f64));
	// lfs f3,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f3.f64 = double(temp.f32);
	// lfs f27,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f9,f9,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f3.f64));
	// stfs f25,2936(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2936, temp.u32);
	// stfs f24,2968(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2968, temp.u32);
	// fmadds f0,f20,f27,f0
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f27.f64 + ctx.f0.f64));
	// stfs f6,3040(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3040, temp.u32);
	// fmuls f11,f11,f3
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f3.f64));
	// lfs f13,3848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3848);
	ctx.f13.f64 = double(temp.f32);
	// lfs f6,3856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3856);
	ctx.f6.f64 = double(temp.f32);
	// lfs f26,3864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3864);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,3872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3872);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,2272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2272);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,3888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3888);
	ctx.f22.f64 = double(temp.f32);
	// stfs f30,1996(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1996, temp.u32);
	// fmuls f30,f17,f28
	ctx.f30.f64 = double(float(ctx.f17.f64 * ctx.f28.f64));
	// stfs f4,5464(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5464, temp.u32);
	// fadds f4,f6,f13
	ctx.f4.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// stfs f23,2932(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2932, temp.u32);
	// fadds f23,f25,f26
	ctx.f23.f64 = double(float(ctx.f25.f64 + ctx.f26.f64));
	// stfs f21,2924(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2924, temp.u32);
	// fmuls f21,f29,f24
	ctx.f21.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// stfs f19,1988(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1988, temp.u32);
	// fmuls f19,f22,f28
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f28.f64));
	// lfs f3,2200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2200);
	ctx.f3.f64 = double(temp.f32);
	// lfs f27,3880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3880);
	ctx.f27.f64 = double(temp.f32);
	// lfs f20,3896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3896);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f18.f64 = double(temp.f32);
	// stfs f17,3516(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3516, temp.u32);
	// stfs f15,4896(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4896, temp.u32);
	// stfs f14,4396(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4396, temp.u32);
	// fnmsubs f10,f30,f18,f10
	ctx.f10.f64 = double(float(-(ctx.f30.f64 * ctx.f18.f64 - ctx.f10.f64)));
	// lfs f15,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f15.f64 = double(temp.f32);
	// lfs f30,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f18,f27,f15
	ctx.f18.f64 = double(float(ctx.f27.f64 * ctx.f15.f64));
	// fmsubs f9,f31,f30,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f30.f64 - ctx.f9.f64));
	// lfs f15,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f15.f64 = double(temp.f32);
	// lfs f31,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f8,f1,f15,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f15.f64 - ctx.f8.f64));
	// fmadds f11,f4,f31,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f1,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f1.f64 = double(temp.f32);
	// lfs f4,3096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3096);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f17,f3,f20
	ctx.f17.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// fnmsubs f4,f4,f1,f0
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// lfs f1,3104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3104);
	ctx.f1.f64 = double(temp.f32);
	// lfs f31,3112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3112);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f1,f28
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// stfs f2,3104(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3104, temp.u32);
	// fmuls f15,f3,f31
	ctx.f15.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f2,3128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3128);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,3120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3120);
	ctx.f14.f64 = double(temp.f32);
	// stfs f3,3120(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3120, temp.u32);
	// fmuls f3,f3,f2
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// stfs f15,3128(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3128, temp.u32);
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,3136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3136);
	ctx.f15.f64 = double(temp.f32);
	// stfs f3,3136(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3136, temp.u32);
	// fmr f3,f0
	ctx.f3.f64 = ctx.f0.f64;
	// stfs f29,3112(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3112, temp.u32);
	// fmuls f29,f14,f0
	ctx.f29.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// fmadds f0,f23,f0,f18
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f18.f64));
	// lfs f18,3088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3088);
	ctx.f18.f64 = double(temp.f32);
	// lfs f23,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f9,f7,f23,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 + ctx.f9.f64));
	// lfs f23,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f8,f21,f23,f8
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f23.f64 + ctx.f8.f64));
	// lfs f23,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f23.f64 = double(temp.f32);
	// stfs f13,3324(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3324, temp.u32);
	// lfs f13,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f13.f64 = double(temp.f32);
	// stfs f2,4628(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4628, temp.u32);
	// lfs f2,2004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2004);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f10,f18,f3,f10
	ctx.f10.f64 = double(float(ctx.f18.f64 * ctx.f3.f64 + ctx.f10.f64));
	// stfs f6,3096(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3096, temp.u32);
	// fmuls f3,f19,f3
	ctx.f3.f64 = double(float(ctx.f19.f64 * ctx.f3.f64));
	// lfs f19,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f11,f15,f19,f11
	ctx.f11.f64 = double(float(ctx.f15.f64 * ctx.f19.f64 + ctx.f11.f64));
	// lfs f19,3080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3080);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f4,f19,f23,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f23.f64 + ctx.f4.f64));
	// stfs f5,3316(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3316, temp.u32);
	// fnmsubs f13,f30,f13,f9
	ctx.f13.f64 = double(float(-(ctx.f30.f64 * ctx.f13.f64 - ctx.f9.f64)));
	// lfs f9,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,3064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3064);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,3072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3072);
	ctx.f6.f64 = double(temp.f32);
	// stfs f27,2520(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2520, temp.u32);
	// fadds f6,f6,f5
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f5.f64));
	// lfs f27,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f27.f64 = double(temp.f32);
	// stfs f1,2940(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2940, temp.u32);
	// lfs f7,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f7.f64 = double(temp.f32);
	// lfs f21,3944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3944);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f7,f17,f7
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// lfs f5,1824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1824);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f11,f11,f28,f3
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f28.f64 - ctx.f3.f64));
	// fnmsubs f9,f2,f9,f4
	ctx.f9.f64 = double(float(-(ctx.f2.f64 * ctx.f9.f64 - ctx.f4.f64)));
	// lfs f4,3952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3952);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,3960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3960);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f29,f4,f27,f29
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f27.f64 + ctx.f29.f64));
	// lfs f30,2476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2476);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f2,f21,f28
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f28.f64));
	// stfs f26,2248(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2248, temp.u32);
	// fmuls f30,f5,f30
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// stfs f31,2272(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2272, temp.u32);
	// fmuls f31,f1,f28
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f28.f64));
	// lfs f3,3100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3100);
	ctx.f3.f64 = double(temp.f32);
	// lfs f26,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f26.f64 = double(temp.f32);
	// lfs f27,2236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2236);
	ctx.f27.f64 = double(temp.f32);
	// stfs f25,3844(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3844, temp.u32);
	// stfs f24,3896(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3896, temp.u32);
	// stfs f22,3856(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3856, temp.u32);
	// stfs f20,2928(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2928, temp.u32);
	// stfs f14,3088(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3088, temp.u32);
	// stfs f16,3880(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3880, temp.u32);
	// stfs f15,4828(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4828, temp.u32);
	// fnmsubs f12,f12,f26,f10
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f26.f64 - ctx.f10.f64)));
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,3128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3128);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f11,f29,f28,f11
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f28.f64 + ctx.f11.f64));
	// lfs f15,3056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3056);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f8,f25,f26,f8
	ctx.f8.f64 = double(float(-(ctx.f25.f64 * ctx.f26.f64 - ctx.f8.f64)));
	// lfs f14,3048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3048);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f0,f0,f28,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f28.f64 + ctx.f13.f64));
	// lfs f29,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f29.f64 = double(temp.f32);
	// fadds f9,f6,f9
	ctx.f9.f64 = double(float(ctx.f6.f64 + ctx.f9.f64));
	// lfs f10,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f29,f15,f29,f14
	ctx.f29.f64 = double(float(-(ctx.f15.f64 * ctx.f29.f64 - ctx.f14.f64)));
	// lfs f15,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f10,f27,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f10.f64));
	// lfs f27,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f27.f64 = double(temp.f32);
	// lfs f25,3136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3136);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f7,f25,f27,f7
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f27.f64 - ctx.f7.f64));
	// fmadds f12,f31,f15,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f15.f64 + ctx.f12.f64));
	// lfs f14,2460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2460);
	ctx.f14.f64 = double(temp.f32);
	// lfs f31,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f27,f26,f3
	ctx.f27.f64 = double(float(ctx.f26.f64 * ctx.f3.f64));
	// fmuls f31,f14,f31
	ctx.f31.f64 = double(float(ctx.f14.f64 * ctx.f31.f64));
	// lfs f13,3612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3612);
	ctx.f13.f64 = double(temp.f32);
	// lfs f24,1996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1996);
	ctx.f24.f64 = double(temp.f32);
	// lfs f26,2972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2972);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f20,f24,f13
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f13.f64));
	// lfs f14,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f23,f24,f26
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// lfs f25,2964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2964);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f8,f30,f14,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f14.f64 + ctx.f8.f64));
	// lfs f19,3160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3160);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f24,f25,f24
	ctx.f24.f64 = double(float(ctx.f25.f64 * ctx.f24.f64));
	// lfs f30,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f30.f64 = double(temp.f32);
	// lfs f18,3168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3168);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f2,f2,f30,f0
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// stfs f24,3160(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3160, temp.u32);
	// fmuls f30,f18,f28
	ctx.f30.f64 = double(float(ctx.f18.f64 * ctx.f28.f64));
	// lfs f24,1792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1792);
	ctx.f24.f64 = double(temp.f32);
	// lfs f22,3152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3152);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f14,f5,f24
	ctx.f14.f64 = double(float(ctx.f5.f64 * ctx.f24.f64));
	// stfs f24,3152(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3152, temp.u32);
	// fmuls f17,f22,f28
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f28.f64));
	// stfs f30,3960(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3960, temp.u32);
	// lfs f24,2296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2296);
	ctx.f24.f64 = double(temp.f32);
	// lfs f30,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f7,f27,f30,f7
	ctx.f7.f64 = double(float(-(ctx.f27.f64 * ctx.f30.f64 - ctx.f7.f64)));
	// fmadds f0,f24,f0,f10
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f10.f64));
	// stfs f10,3888(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3888, temp.u32);
	// lfs f10,3112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3112);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,3168(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3168, temp.u32);
	// stfs f9,5468(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5468, temp.u32);
	// lfs f6,3144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3144);
	ctx.f6.f64 = double(temp.f32);
	// lfs f11,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f11.f64 = double(temp.f32);
	// stfs f13,3864(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3864, temp.u32);
	// fmuls f11,f6,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f11.f64));
	// lfs f16,3396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3396);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,3120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3120);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f16,f16,f5
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f5.f64));
	// lfs f24,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f9,f0,f10,f7
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 + ctx.f7.f64));
	// lfs f13,2184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2184);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f24,f3
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f3.f64));
	// stfs f17,3144(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3144, temp.u32);
	// fmuls f17,f15,f19
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f19.f64));
	// stfs f6,2004(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2004, temp.u32);
	// fmuls f13,f13,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// lfs f7,4516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4516);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f10,f0
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f21,3072(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3072, temp.u32);
	// stfs f4,2240(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2240, temp.u32);
	// stfs f1,2296(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2296, temp.u32);
	// stfs f25,3612(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3612, temp.u32);
	// stfs f22,3064(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3064, temp.u32);
	// stfs f19,3080(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3080, temp.u32);
	// stfs f23,3952(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3952, temp.u32);
	// stfs f26,3872(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3872, temp.u32);
	// stfs f28,3944(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3944, temp.u32);
	// stfs f18,3848(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3848, temp.u32);
	// lfs f27,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// fadds f12,f29,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 + ctx.f12.f64));
	// lfs f29,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f8,f16,f27,f8
	ctx.f8.f64 = double(float(-(ctx.f16.f64 * ctx.f27.f64 - ctx.f8.f64)));
	// lfs f1,3104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3104);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f29,f0,f29
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// lfs f4,3548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3548);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f13,f13,f27,f9
	ctx.f13.f64 = double(float(-(ctx.f13.f64 * ctx.f27.f64 - ctx.f9.f64)));
	// lfs f0,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f28,f7,f1
	ctx.f28.f64 = double(float(ctx.f7.f64 * ctx.f1.f64));
	// fmuls f26,f4,f0
	ctx.f26.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f21,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f21.f64 = double(temp.f32);
	// lfs f24,4008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4008);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f2,f20,f21,f2
	ctx.f2.f64 = double(float(-(ctx.f20.f64 * ctx.f21.f64 - ctx.f2.f64)));
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// lfs f25,4000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4000);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f20,f24,f19
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// lfs f0,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f0.f64 = double(temp.f32);
	// lfs f19,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f23,f25,f0
	ctx.f23.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// fnmsubs f0,f17,f0,f8
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f18,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f18.f64 = double(temp.f32);
	// lfs f8,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f29,f29,f19
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// lfs f19,2392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2392);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f6,f6,f18
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f18.f64));
	// fmadds f13,f28,f8,f13
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f8.f64 + ctx.f13.f64));
	// lfs f8,2352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2352);
	ctx.f8.f64 = double(temp.f32);
	// fadds f18,f19,f26
	ctx.f18.f64 = double(float(ctx.f19.f64 + ctx.f26.f64));
	// stfs f26,3128(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3128, temp.u32);
	// lfs f26,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f14,f8,f1,f14
	ctx.f14.f64 = double(float(ctx.f8.f64 * ctx.f1.f64 + ctx.f14.f64));
	// fmadds f11,f31,f26,f11
	ctx.f11.f64 = double(float(ctx.f31.f64 * ctx.f26.f64 + ctx.f11.f64));
	// stfs f19,3136(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3136, temp.u32);
	// stfs f14,4000(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4000, temp.u32);
	// lfs f14,3160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3160);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f19.f64 = double(temp.f32);
	// lfs f26,3168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3168);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f26,f14,f19,f26
	ctx.f26.f64 = double(float(ctx.f14.f64 * ctx.f19.f64 + ctx.f26.f64));
	// lfs f14,2320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2320);
	ctx.f14.f64 = double(temp.f32);
	// lfs f9,2264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2264);
	ctx.f9.f64 = double(temp.f32);
	// lfs f31,2256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2256);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f22,f10,f9
	ctx.f22.f64 = double(float(ctx.f10.f64 * ctx.f9.f64));
	// lfs f27,4016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4016);
	ctx.f27.f64 = double(temp.f32);
	// lfs f19,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f19.f64 = double(temp.f32);
	// stfs f14,3168(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3168, temp.u32);
	// fmuls f14,f10,f14
	ctx.f14.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// stfs f11,4008(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4008, temp.u32);
	// fmuls f19,f31,f19
	ctx.f19.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// lfs f21,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f21.f64 = double(temp.f32);
	// stfs f10,4016(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4016, temp.u32);
	// fmuls f21,f27,f21
	ctx.f21.f64 = double(float(ctx.f27.f64 * ctx.f21.f64));
	// lfs f10,3152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3152);
	ctx.f10.f64 = double(temp.f32);
	// lfs f11,2452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2452);
	ctx.f11.f64 = double(temp.f32);
	// stfs f31,3120(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3120, temp.u32);
	// fmuls f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f28,4032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4032);
	ctx.f28.f64 = double(temp.f32);
	// lfs f31,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,4024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4024);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f31,f28,f31,f20
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64 - ctx.f20.f64));
	// stfs f11,4032(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4032, temp.u32);
	// fmuls f16,f15,f17
	ctx.f16.f64 = double(float(ctx.f15.f64 * ctx.f17.f64));
	// stfs f12,4024(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4024, temp.u32);
	// lfs f20,2384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2384);
	ctx.f20.f64 = double(temp.f32);
	// lfs f11,3144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3144);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f20.f64));
	// lfs f12,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f12,f11,f12,f2
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f2.f64));
	// stfs f20,3160(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3160, temp.u32);
	// lfs f2,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f6,f6,f2
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f2.f64));
	// fnmsubs f0,f30,f20,f0
	ctx.f0.f64 = double(float(-(ctx.f30.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f11,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f11.f64 = double(temp.f32);
	// lfs f30,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f11,f29,f11,f23
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f11.f64 + ctx.f23.f64));
	// lfs f2,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f13,f22,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// stfs f3,3152(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3152, temp.u32);
	// fmsubs f3,f18,f2,f21
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f2.f64 - ctx.f21.f64));
	// stfs f17,3104(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3104, temp.u32);
	// stfs f7,2320(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2320, temp.u32);
	// fmr f7,f30
	ctx.f7.f64 = ctx.f30.f64;
	// stfs f28,3048(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3048, temp.u32);
	// fmsubs f11,f11,f15,f6
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f15.f64 - ctx.f6.f64));
	// lfs f29,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f29.f64 = double(temp.f32);
	// lfs f28,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f28.f64 = double(temp.f32);
	// lfs f30,2444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2444);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f30,f29,f28,f30
	ctx.f30.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f30.f64)));
	// stfs f27,1996(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1996, temp.u32);
	// stfs f10,3144(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3144, temp.u32);
	// lfs f27,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f29.f64 = double(temp.f32);
	// lfs f10,2232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2232);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f4,f4,f29
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// lfs f6,5116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5116);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f13,f14,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f14.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// stfs f9,2972(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2972, temp.u32);
	// fmuls f28,f6,f27
	ctx.f28.f64 = double(float(ctx.f6.f64 * ctx.f27.f64));
	// lfs f9,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f9.f64 = double(temp.f32);
	// stfs f8,3112(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3112, temp.u32);
	// fmuls f8,f10,f1
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f1.f64));
	// stfs f25,4516(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4516, temp.u32);
	// fmadds f9,f19,f9,f31
	ctx.f9.f64 = double(float(ctx.f19.f64 * ctx.f9.f64 + ctx.f31.f64));
	// lfs f29,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,3960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3960);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,4040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4040);
	ctx.f1.f64 = double(temp.f32);
	// lfs f27,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f3,f1,f29,f3
	ctx.f3.f64 = double(float(-(ctx.f1.f64 * ctx.f29.f64 - ctx.f3.f64)));
	// fnmsubs f27,f25,f27,f26
	ctx.f27.f64 = double(float(-(ctx.f25.f64 * ctx.f27.f64 - ctx.f26.f64)));
	// lfs f26,3952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3952);
	ctx.f26.f64 = double(temp.f32);
	// lfs f29,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f26,f29,f12
	ctx.f12.f64 = double(float(-(ctx.f26.f64 * ctx.f29.f64 - ctx.f12.f64)));
	// lfs f25,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f25.f64 = double(temp.f32);
	// lfs f31,4048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4048);
	ctx.f31.f64 = double(temp.f32);
	// lfs f7,3076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3076);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f9,f31,f25,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f25.f64 - ctx.f9.f64)));
	// lfs f26,3388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3388);
	ctx.f26.f64 = double(temp.f32);
	// lfs f21,4024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4024);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f2,f5,f7
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// lfs f29,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// fnmsubs f0,f16,f29,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f26,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f26.f64 = double(temp.f32);
	// lfs f19,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f13,f8,f26,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f26.f64 + ctx.f13.f64));
	// fmuls f4,f4,f19
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f19.f64));
	// stfs f24,3056(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3056, temp.u32);
	// lfs f19,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f19.f64 = double(temp.f32);
	// fadds f12,f21,f12
	ctx.f12.f64 = double(float(ctx.f21.f64 + ctx.f12.f64));
	// lfs f21,4088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4088);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f30,f30,f21,f28
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f21.f64 + ctx.f28.f64));
	// lfs f28,984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 984);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,4072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4072);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,4080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4080);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f3,f26,f28,f3
	ctx.f3.f64 = double(float(-(ctx.f26.f64 * ctx.f28.f64 - ctx.f3.f64)));
	// fmadds f9,f24,f19,f9
	ctx.f9.f64 = double(float(ctx.f24.f64 * ctx.f19.f64 + ctx.f9.f64));
	// lfs f19,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f19.f64 = double(temp.f32);
	// lfs f28,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f19
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f19.f64));
	// lfs f18,3944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3944);
	ctx.f18.f64 = double(temp.f32);
	// lfs f29,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f29.f64 = double(temp.f32);
	// lfs f25,4032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4032);
	ctx.f25.f64 = double(temp.f32);
	// lfs f17,4008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4008);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f11,f25,f29,f11
	ctx.f11.f64 = double(float(ctx.f25.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f16,4000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4000);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f27,f17,f18,f27
	ctx.f27.f64 = double(float(ctx.f17.f64 * ctx.f18.f64 + ctx.f27.f64));
	// lfs f19,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f19,f16,f19,f0
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f19.f64 + ctx.f0.f64));
	// lfs f8,4056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4056);
	ctx.f8.f64 = double(temp.f32);
	// lfs f29,4064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4064);
	ctx.f29.f64 = double(temp.f32);
	// lfs f22,3068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3068);
	ctx.f22.f64 = double(temp.f32);
	// fadds f25,f29,f8
	ctx.f25.f64 = double(float(ctx.f29.f64 + ctx.f8.f64));
	// lfs f20,4016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4016);
	ctx.f20.f64 = double(temp.f32);
	// lfs f17,2428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2428);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f20,f22
	ctx.f22.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// lfs f16,3168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3168);
	ctx.f16.f64 = double(temp.f32);
	// stfs f18,4088(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4088, temp.u32);
	// fmuls f14,f17,f16
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f16.f64));
	// lfs f23,2436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2436);
	ctx.f23.f64 = double(temp.f32);
	// lfs f0,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f2,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f8,4072(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4072, temp.u32);
	// lfs f8,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f13,f5,f18,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f18.f64 + ctx.f13.f64));
	// fmadds f8,f25,f8,f3
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f8.f64 + ctx.f3.f64));
	// stfs f10,2352(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2352, temp.u32);
	// lfs f5,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f5.f64 = double(temp.f32);
	// fadds f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f27.f64));
	// lfs f3,3160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3160);
	ctx.f3.f64 = double(temp.f32);
	// lfs f10,3136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3136);
	ctx.f10.f64 = double(temp.f32);
	// fmsubs f4,f30,f3,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f3.f64 - ctx.f4.f64));
	// stfs f6,5116(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5116, temp.u32);
	// fmadds f10,f23,f5,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f5.f64 + ctx.f10.f64));
	// lfs f6,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f6.f64 = double(temp.f32);
	// lfs f30,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f30.f64 = double(temp.f32);
	// lfs f5,4096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4096);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f6,f22,f6,f0
	ctx.f6.f64 = double(float(-(ctx.f22.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f9,f5,f30,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f30.f64 + ctx.f9.f64));
	// lfs f11,312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f11.f64 = double(temp.f32);
	// fmr f30,f0
	ctx.f30.f64 = ctx.f0.f64;
	// stfs f1,4056(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4056, temp.u32);
	// stfs f31,4064(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4064, temp.u32);
	// fmuls f11,f23,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f11.f64));
	// lfs f31,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f31.f64 = double(temp.f32);
	// fadds f13,f19,f13
	ctx.f13.f64 = double(float(ctx.f19.f64 + ctx.f13.f64));
	// lfs f1,2344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2344);
	ctx.f1.f64 = double(temp.f32);
	// stfs f26,4048(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4048, temp.u32);
	// fmr f26,f0
	ctx.f26.f64 = ctx.f0.f64;
	// lfs f2,4572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4572);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
	// lfs f31,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f2,f20,f2
	ctx.f2.f64 = double(float(ctx.f20.f64 * ctx.f2.f64));
	// lfs f27,3120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3120);
	ctx.f27.f64 = double(temp.f32);
	// stfs f29,4040(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4040, temp.u32);
	// lfs f29,3128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3128);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f31,f31,f30,f28
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f30.f64 + ctx.f28.f64));
	// lfs f30,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f27,f30
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f27,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f27.f64 = double(temp.f32);
	// fadds f10,f10,f29
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f29.f64));
	// lfs f29,3040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3040);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f11,f11,f27,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f27.f64 + ctx.f8.f64));
	// lfs f18,88(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f8,f14,f26,f6
	ctx.f8.f64 = double(float(-(ctx.f14.f64 * ctx.f26.f64 - ctx.f6.f64)));
	// stfs f7,4016(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4016, temp.u32);
	// fmuls f29,f17,f29
	ctx.f29.f64 = double(float(ctx.f17.f64 * ctx.f29.f64));
	// stfs f16,2264(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2264, temp.u32);
	// fmuls f2,f2,f18
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f18.f64));
	// lfs f18,4144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4144);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f7,f15,f21
	ctx.f7.f64 = double(float(ctx.f15.f64 * ctx.f21.f64));
	// lfs f28,4104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4104);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f3,f18,f3
	ctx.f3.f64 = double(float(ctx.f18.f64 * ctx.f3.f64));
	// lfs f18,824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 824);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,4152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4152);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f4,f1,f18,f4
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f18.f64 - ctx.f4.f64)));
	// fmuls f28,f28,f0
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f19,3664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3664);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f16,f16,f0
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f0.f64));
	// lfs f0,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f20,f20,f19
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// lfs f6,4112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4112);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f31,f31,f15
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f15.f64));
	// fnmsubs f9,f6,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f6.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f27,4120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4120);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f8,f29,f1,f8
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f8.f64));
	// lfs f26,4128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4128);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,4088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4088);
	ctx.f25.f64 = double(temp.f32);
	// lfs f19,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f19.f64 = double(temp.f32);
	// lfs f0,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f19,f7,f19
	ctx.f19.f64 = double(float(ctx.f7.f64 * ctx.f19.f64));
	// lfs f1,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f29,f15,f0
	ctx.f29.f64 = double(float(ctx.f15.f64 * ctx.f0.f64));
	// stfs f24,4080(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4080, temp.u32);
	// fmuls f24,f27,f25
	ctx.f24.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lfs f23,3032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3032);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f1,f26,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// lfs f22,4136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4136);
	ctx.f22.f64 = double(temp.f32);
	// lfs f18,2016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2016);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,1760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1760);
	ctx.f14.f64 = double(temp.f32);
	// stfs f13,5476(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5476, temp.u32);
	// fmuls f0,f23,f18
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f18.f64));
	// lfs f13,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f20,f13,f8
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f13.f64 - ctx.f8.f64)));
	// lfs f8,3176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3176);
	ctx.f8.f64 = double(temp.f32);
	// stfs f0,3176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3176, temp.u32);
	// stfs f18,4120(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4120, temp.u32);
	// lfs f18,2480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2480);
	ctx.f18.f64 = double(temp.f32);
	// stfs f31,4152(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4152, temp.u32);
	// fmuls f18,f18,f14
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f14.f64));
	// lfs f31,592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	ctx.f31.f64 = double(temp.f32);
	// stfs f6,4096(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4096, temp.u32);
	// fmadds f11,f30,f31,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f6,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f6.f64 = double(temp.f32);
	// stfs f3,4128(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4128, temp.u32);
	// fmuls f6,f29,f6
	ctx.f6.f64 = double(float(ctx.f29.f64 * ctx.f6.f64));
	// stfs f14,4112(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4112, temp.u32);
	// stfs f17,4144(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4144, temp.u32);
	// lfs f14,3888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3888);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f17.f64 = double(temp.f32);
	// lfs f3,3896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3896);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f3,f3,f17,f14
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f17.f64 + ctx.f14.f64));
	// stfs f12,5472(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5472, temp.u32);
	// fmuls f30,f24,f30
	ctx.f30.f64 = double(float(ctx.f24.f64 * ctx.f30.f64));
	// lfs f29,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f29.f64 = double(temp.f32);
	// lfs f12,4168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4168);
	ctx.f12.f64 = double(temp.f32);
	// lfs f17,4160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4160);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f1,f12,f29,f1
	ctx.f1.f64 = double(float(ctx.f12.f64 * ctx.f29.f64 + ctx.f1.f64));
	// lfs f31,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f31.f64 = double(temp.f32);
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,4104(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4104, temp.u32);
	// fmadds f5,f17,f31,f28
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f31.f64 + ctx.f28.f64));
	// stfs f27,4024(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4024, temp.u32);
	// fmuls f31,f8,f0
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// lfs f27,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f0,f18,f0
	ctx.f0.f64 = double(float(ctx.f18.f64 * ctx.f0.f64));
	// fmadds f13,f2,f27,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f27.f64 + ctx.f13.f64));
	// lfs f27,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f27.f64 = double(temp.f32);
	// stfs f4,4136(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4136, temp.u32);
	// fmadds f10,f10,f27,f9
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f27.f64 + ctx.f9.f64));
	// stfs f23,4160(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4160, temp.u32);
	// fmuls f4,f22,f25
	ctx.f4.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// stfs f22,4032(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4032, temp.u32);
	// lfs f23,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,3176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3176);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f6,f6,f23
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f23.f64));
	// lfs f9,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f9.f64 = double(temp.f32);
	// lfs f27,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f9,f22,f9,f30
	ctx.f9.f64 = double(float(ctx.f22.f64 * ctx.f9.f64 + ctx.f30.f64));
	// lfs f29,4184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4184);
	ctx.f29.f64 = double(temp.f32);
	// stfs f26,3040(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3040, temp.u32);
	// fnmsubs f1,f29,f27,f1
	ctx.f1.f64 = double(float(-(ctx.f29.f64 * ctx.f27.f64 - ctx.f1.f64)));
	// lfs f23,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f23.f64 = double(temp.f32);
	// lfs f30,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f30.f64 = double(temp.f32);
	// lfs f26,4192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4192);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,4200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4200);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f11,f26,f23,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f27,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f31,f24,f30,f31
	ctx.f31.f64 = double(float(ctx.f24.f64 * ctx.f30.f64 + ctx.f31.f64));
	// lfs f2,3880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3880);
	ctx.f2.f64 = double(temp.f32);
	// lfs f22,3200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3200);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f30,f27,f2
	ctx.f30.f64 = double(float(ctx.f27.f64 * ctx.f2.f64));
	// lfs f14,3216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3216);
	ctx.f14.f64 = double(temp.f32);
	// lfs f23,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f23.f64 = double(temp.f32);
	// stfs f5,3216(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3216, temp.u32);
	// fmuls f5,f15,f22
	ctx.f5.f64 = double(float(ctx.f15.f64 * ctx.f22.f64));
	// lfs f27,3184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3184);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f13,f19,f23,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f23.f64 - ctx.f13.f64)));
	// stfs f5,3184(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3184, temp.u32);
	// fmuls f20,f27,f25
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lfs f19,3208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3208);
	ctx.f19.f64 = double(temp.f32);
	// lfs f5,3224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3224);
	ctx.f5.f64 = double(temp.f32);
	// stfs f15,3224(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3224, temp.u32);
	// fmuls f15,f25,f19
	ctx.f15.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// lfs f23,3192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3192);
	ctx.f23.f64 = double(temp.f32);
	// stfs f15,3200(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3200, temp.u32);
	// fmuls f18,f23,f25
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f25.f64));
	// lfs f15,3232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3232);
	ctx.f15.f64 = double(temp.f32);
	// stfs f18,3232(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3232, temp.u32);
	// fmuls f18,f14,f25
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f25.f64));
	// fmadds f0,f1,f25,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f25.f64 + ctx.f0.f64));
	// lfs f1,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f1.f64 = double(temp.f32);
	// stfs f25,3208(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3208, temp.u32);
	// fmuls f6,f6,f1
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f1.f64));
	// lfs f25,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f25.f64 = double(temp.f32);
	// fadds f31,f31,f16
	ctx.f31.f64 = double(float(ctx.f31.f64 + ctx.f16.f64));
	// fnmsubs f9,f4,f25,f9
	ctx.f9.f64 = double(float(-(ctx.f4.f64 * ctx.f25.f64 - ctx.f9.f64)));
	// lfs f4,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f4.f64 = double(temp.f32);
	// fmr f25,f4
	ctx.f25.f64 = ctx.f4.f64;
	// stfs f12,3952(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3952, temp.u32);
	// lfs f12,4152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4152);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f10,f5,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// fmadds f13,f12,f21,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f21.f64 + ctx.f13.f64));
	// lfs f12,4144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4144);
	ctx.f12.f64 = double(temp.f32);
	// stfs f18,3192(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3192, temp.u32);
	// lfs f18,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f18.f64 = double(temp.f32);
	// stfs f28,4168(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4168, temp.u32);
	// fmuls f18,f7,f18
	ctx.f18.f64 = double(float(ctx.f7.f64 * ctx.f18.f64));
	// stfs f27,3136(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3136, temp.u32);
	// fmsubs f12,f3,f12,f6
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f12.f64 - ctx.f6.f64));
	// lfs f28,4136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4136);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,4128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4128);
	ctx.f27.f64 = double(temp.f32);
	// lfs f3,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f3.f64 = double(temp.f32);
	// stfs f29,3960(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3960, temp.u32);
	// fnmsubs f3,f27,f3,f28
	ctx.f3.f64 = double(float(-(ctx.f27.f64 * ctx.f3.f64 - ctx.f28.f64)));
	// lfs f29,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f15,f25,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f25.f64 - ctx.f11.f64)));
	// fmr f28,f29
	ctx.f28.f64 = ctx.f29.f64;
	// lfs f4,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f4.f64 = double(temp.f32);
	// stfs f5,4088(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4088, temp.u32);
	// fmuls f4,f4,f2
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f2.f64));
	// lfs f6,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,3232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3232);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f0,f5,f6,f0
	ctx.f0.f64 = double(float(-(ctx.f5.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// lfs f6,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f6.f64 = double(temp.f32);
	// stfs f8,3176(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3176, temp.u32);
	// fnmsubs f13,f18,f6,f13
	ctx.f13.f64 = double(float(-(ctx.f18.f64 * ctx.f6.f64 - ctx.f13.f64)));
	// lfs f8,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f8.f64 = double(temp.f32);
	// lfs f27,3224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3224);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f9,f20,f8,f9
	ctx.f9.f64 = double(float(ctx.f20.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f5,4280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4280);
	ctx.f5.f64 = double(temp.f32);
	// lfs f18,4320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4320);
	ctx.f18.f64 = double(temp.f32);
	// lfs f6,4272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4272);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f11,f5,f28,f11
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f28.f64 - ctx.f11.f64)));
	// stfs f15,4152(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4152, temp.u32);
	// fmuls f15,f27,f18
	ctx.f15.f64 = double(float(ctx.f27.f64 * ctx.f18.f64));
	// stfs f16,3168(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3168, temp.u32);
	// fnmsubs f10,f6,f29,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f29.f64 - ctx.f10.f64)));
	// lfs f20,4312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4312);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,4328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4328);
	ctx.f16.f64 = double(temp.f32);
	// lfs f28,3216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3216);
	ctx.f28.f64 = double(temp.f32);
	// stfs f17,4184(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4184, temp.u32);
	// fmuls f17,f27,f20
	ctx.f17.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// stfs f15,4272(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4272, temp.u32);
	// fmuls f15,f27,f16
	ctx.f15.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// stfs f22,4192(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4192, temp.u32);
	// fmadds f12,f28,f27,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f27.f64 + ctx.f12.f64));
	// stfs f24,4008(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4008, temp.u32);
	// stfs f14,3880(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3880, temp.u32);
	// lfs f25,4264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4264);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f30,f30,f25
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f8,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f1,f7,f1
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f1.f64));
	// lfs f29,5020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5020);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f8,f8,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// lfs f24,4120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4120);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f4,f4,f25
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f25.f64));
	// lfs f22,4304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4304);
	ctx.f22.f64 = double(temp.f32);
	// lfs f14,3240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3240);
	ctx.f14.f64 = double(temp.f32);
	// stfs f23,4000(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4000, temp.u32);
	// fmuls f23,f29,f24
	ctx.f23.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// stfs f26,4200(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4200, temp.u32);
	// stfs f19,3944(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3944, temp.u32);
	// fmuls f19,f27,f22
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// stfs f15,3224(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3224, temp.u32);
	// stfs f17,3240(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3240, temp.u32);
	// lfs f28,4288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4288);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,4296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4296);
	ctx.f26.f64 = double(temp.f32);
	// lfs f15,3248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3248);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,3208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3208);
	ctx.f17.f64 = double(temp.f32);
	// stfs f4,3248(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3248, temp.u32);
	// fmuls f4,f14,f17
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f17.f64));
	// fmuls f8,f8,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f25.f64));
	// stfs f4,4288(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4288, temp.u32);
	// lfs f4,3256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3256);
	ctx.f4.f64 = double(temp.f32);
	// lfs f25,3872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3872);
	ctx.f25.f64 = double(temp.f32);
	// stfs f24,3256(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3256, temp.u32);
	// fmuls f24,f24,f25
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// stfs f8,4280(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4280, temp.u32);
	// fmuls f8,f27,f15
	ctx.f8.f64 = double(float(ctx.f27.f64 * ctx.f15.f64));
	// stfs f17,4328(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4328, temp.u32);
	// fmuls f17,f17,f4
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// stfs f24,4264(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4264, temp.u32);
	// stfs f8,3216(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3216, temp.u32);
	// stfs f17,3232(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3232, temp.u32);
	// lfs f17,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f17.f64 = double(temp.f32);
	// lfs f8,2292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2292);
	ctx.f8.f64 = double(temp.f32);
	// lfs f24,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f8,f8,f17
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f17.f64));
	// fmuls f24,f28,f24
	ctx.f24.f64 = double(float(ctx.f28.f64 * ctx.f24.f64));
	// stfs f24,4312(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4312, temp.u32);
	// stfs f8,4320(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4320, temp.u32);
	// lfs f17,4556(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4556);
	ctx.f17.f64 = double(temp.f32);
	// lfs f8,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f8.f64 = double(temp.f32);
	// lfs f24,712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f8,f8,f17
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f17.f64));
	// fmuls f24,f26,f24
	ctx.f24.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// stfs f24,4296(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4296, temp.u32);
	// stfs f8,4304(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4304, temp.u32);
	// lfs f24,3200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3200);
	ctx.f24.f64 = double(temp.f32);
	// lfs f17,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f17.f64 = double(temp.f32);
	// lfs f8,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f9,f24,f17,f9
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f17.f64 - ctx.f9.f64)));
	// fmuls f8,f7,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// lfs f24,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f24.f64 = double(temp.f32);
	// lfs f7,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f7.f64 = double(temp.f32);
	// lfs f17,3192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3192);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f13,f1,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// fnmsubs f0,f17,f24,f0
	ctx.f0.f64 = double(float(-(ctx.f17.f64 * ctx.f24.f64 - ctx.f0.f64)));
	// lfs f1,3184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3184);
	ctx.f1.f64 = double(temp.f32);
	// lfs f24,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f1,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// stfs f4,3120(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3120, temp.u32);
	// lfs f4,976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 976);
	ctx.f4.f64 = double(temp.f32);
	// stfs f2,3208(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3208, temp.u32);
	// lfs f2,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f9,f30,f4,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f4.f64 + ctx.f9.f64));
	// stfs f6,4136(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4136, temp.u32);
	// stfs f10,5488(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5488, temp.u32);
	// stfs f5,4144(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4144, temp.u32);
	// stfs f11,5480(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5480, temp.u32);
	// lfs f7,3152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3152);
	ctx.f7.f64 = double(temp.f32);
	// lfs f1,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f31,f27,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f27.f64 + ctx.f12.f64));
	// lfs f11,4360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4360);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,2048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2048);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f1,f7
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f6,2252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2252);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,3024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3024);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f4.f64 = double(temp.f32);
	// lfs f31,2388(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2388);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f11,f4
	ctx.f4.f64 = double(float(ctx.f11.f64 * ctx.f4.f64));
	// stfs f29,3652(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3652, temp.u32);
	// fmuls f31,f31,f10
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// stfs f28,3128(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3128, temp.u32);
	// stfs f3,5484(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5484, temp.u32);
	// fmuls f3,f5,f6
	ctx.f3.f64 = double(float(ctx.f5.f64 * ctx.f6.f64));
	// lfs f30,4368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4368);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f12,f19,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f29,3264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3264);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f2.f64 = double(temp.f32);
	// lfs f28,3044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3044);
	ctx.f28.f64 = double(temp.f32);
	// stfs f21,3192(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3192, temp.u32);
	// stfs f26,3896(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3896, temp.u32);
	// stfs f22,3184(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3184, temp.u32);
	// stfs f20,4128(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4128, temp.u32);
	// stfs f18,3152(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3152, temp.u32);
	// stfs f16,3160(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3160, temp.u32);
	// stfs f14,3888(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3888, temp.u32);
	// stfs f15,4120(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4120, temp.u32);
	// stfs f25,3200(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3200, temp.u32);
	// fnmsubs f0,f23,f2,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f2.f64 - ctx.f0.f64)));
	// lfs f23,3248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3248);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f10,f28,f10
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f10.f64));
	// lfs f28,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f9,f23,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f23.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// lfs f25,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f4,f29,f24,f4
	ctx.f4.f64 = double(float(ctx.f29.f64 * ctx.f24.f64 - ctx.f4.f64));
	// lfs f23,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,3240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3240);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f3,f23
	ctx.f24.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// lfs f2,3016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3016);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f22,f25,f12
	ctx.f12.f64 = double(float(-(ctx.f22.f64 * ctx.f25.f64 - ctx.f12.f64)));
	// lfs f19,4328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4328);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f26,f2,f30
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// lfs f23,3272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3272);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,3280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3280);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f18,f23,f19
	ctx.f18.f64 = double(float(ctx.f23.f64 * ctx.f19.f64));
	// stfs f2,3272(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3272, temp.u32);
	// fmuls f16,f25,f19
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// lfs f17,3304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3304);
	ctx.f17.f64 = double(temp.f32);
	// lfs f2,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f2.f64 = double(temp.f32);
	// stfs f16,3304(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3304, temp.u32);
	// fmuls f7,f7,f2
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f2.f64));
	// lfs f22,3288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3288);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f16,f17,f19
	ctx.f16.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// lfs f15,3312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3312);
	ctx.f15.f64 = double(temp.f32);
	// stfs f16,3288(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3288, temp.u32);
	// stfs f7,3280(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3280, temp.u32);
	// stfs f18,3312(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3312, temp.u32);
	// lfs f18,4312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4312);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f16.f64 = double(temp.f32);
	// lfs f7,4320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4320);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f7,f7,f16,f18
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f16.f64 + ctx.f18.f64));
	// lfs f20,3296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3296);
	ctx.f20.f64 = double(temp.f32);
	// stfs f7,3296(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3296, temp.u32);
	// fadds f14,f20,f22
	ctx.f14.f64 = double(float(ctx.f20.f64 + ctx.f22.f64));
	// lfs f2,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f2.f64 = double(temp.f32);
	// lfs f16,4304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4304);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f10,f10,f2
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// lfs f18,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f18.f64 = double(temp.f32);
	// lfs f7,4296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4296);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f13,f8,f18,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f18.f64 + ctx.f13.f64));
	// fmadds f7,f16,f2,f7
	ctx.f7.f64 = double(float(ctx.f16.f64 * ctx.f2.f64 + ctx.f7.f64));
	// lfs f8,3172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3172);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,3008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3008);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f18,f5,f15
	ctx.f18.f64 = double(float(ctx.f5.f64 * ctx.f15.f64));
	// fmadds f2,f2,f8,f26
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f26.f64));
	// stfs f8,3264(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3264, temp.u32);
	// lfs f16,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f16.f64 = double(temp.f32);
	// lfs f8,4288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4288);
	ctx.f8.f64 = double(temp.f32);
	// lfs f28,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f0,f8,f16,f0
	ctx.f0.f64 = double(float(ctx.f8.f64 * ctx.f16.f64 + ctx.f0.f64));
	// lfs f16,4280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4280);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f31,f31,f28
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f28.f64));
	// lfs f26,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f26.f64 = double(temp.f32);
	// lfs f8,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f9,f16,f26,f9
	ctx.f9.f64 = double(float(ctx.f16.f64 * ctx.f26.f64 + ctx.f9.f64));
	// lfs f28,2512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2512);
	ctx.f28.f64 = double(temp.f32);
	// fmr f26,f8
	ctx.f26.f64 = ctx.f8.f64;
	// lfs f21,3256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3256);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f28,f21
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f21.f64));
	// lfs f16,4272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4272);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f12,f16,f8,f12
	ctx.f12.f64 = double(float(-(ctx.f16.f64 * ctx.f8.f64 - ctx.f12.f64)));
	// lfs f8,956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	ctx.f8.f64 = double(temp.f32);
	// stfs f11,4312(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4312, temp.u32);
	// stfs f27,4360(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4360, temp.u32);
	// stfs f6,2964(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2964, temp.u32);
	// fmadds f4,f4,f19,f31
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f19.f64 + ctx.f31.f64));
	// stfs f30,4296(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4296, temp.u32);
	// stfs f28,3256(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3256, temp.u32);
	// stfs f23,4304(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4304, temp.u32);
	// stfs f25,4280(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4280, temp.u32);
	// fmuls f31,f24,f26
	ctx.f31.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// stfs f29,3872(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3872, temp.u32);
	// stfs f3,4368(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4368, temp.u32);
	// fmadds f11,f21,f8,f10
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f8.f64 + ctx.f10.f64));
	// stfs f20,3248(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3248, temp.u32);
	// stfs f22,4328(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4328, temp.u32);
	// stfs f17,4320(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4320, temp.u32);
	// stfs f15,1464(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1464, temp.u32);
	// fmr f10,f26
	ctx.f10.f64 = ctx.f26.f64;
	// lfs f6,4264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4264);
	ctx.f6.f64 = double(temp.f32);
	// lfs f8,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f8.f64 = double(temp.f32);
	// lfs f3,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f3.f64 = double(temp.f32);
	// lfs f27,3304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3304);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,3296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3296);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,3224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3224);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,4112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4112);
	ctx.f25.f64 = double(temp.f32);
	// lfs f29,3972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3972);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f29,f25
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f25.f64));
	// lfs f25,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f0,f6,f10,f0
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f10.f64 - ctx.f0.f64)));
	// lfs f6,3232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3232);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f9,f6,f8,f9
	ctx.f9.f64 = double(float(-(ctx.f6.f64 * ctx.f8.f64 - ctx.f9.f64)));
	// fnmsubs f12,f1,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f1,3312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3312);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f8,f1,f8,f4
	ctx.f8.f64 = double(float(-(ctx.f1.f64 * ctx.f8.f64 - ctx.f4.f64)));
	// lfs f10,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f11,f27,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f27,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f4,f2,f3,f31
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f3.f64 + ctx.f31.f64));
	// lfs f3,4432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4432);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f6,f14,f19
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f19.f64));
	// lfs f31,3648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3648);
	ctx.f31.f64 = double(temp.f32);
	// lfs f10,2472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2472);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f3,f19
	ctx.f1.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// fmuls f10,f10,f27
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f27.f64));
	// lfs f2,3320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3320);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f28,f5,f31
	ctx.f28.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// lfs f27,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f0,f7,f19,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f19.f64 + ctx.f0.f64));
	// lfs f7,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f9,f26,f19,f9
	ctx.f9.f64 = double(float(ctx.f26.f64 * ctx.f19.f64 + ctx.f9.f64));
	// lfs f26,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f12,f24,f7,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f7.f64 - ctx.f12.f64)));
	// lfs f7,3288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3288);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f8,f7,f26,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f26.f64 + ctx.f8.f64));
	// lfs f14,3216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3216);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f30,f5,f2
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// lfs f21,2464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2464);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f4,f18,f25,f4
	ctx.f4.f64 = double(float(-(ctx.f18.f64 * ctx.f25.f64 - ctx.f4.f64)));
	// lfs f18,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f11,f6,f27,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f27.f64 + ctx.f11.f64));
	// lfs f6,3352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3352);
	ctx.f6.f64 = double(temp.f32);
	// lfs f27,3360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3360);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f22,f6,f19
	ctx.f22.f64 = double(float(ctx.f6.f64 * ctx.f19.f64));
	// stfs f22,3360(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3360, temp.u32);
	// fmuls f20,f27,f19
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f19.f64));
	// fmuls f28,f28,f18
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// lfs f18,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f18.f64 = double(temp.f32);
	// lfs f22,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f10,f10,f19
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f19.f64));
	// fadds f0,f9,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 + ctx.f0.f64));
	// lfs f9,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f12,f14,f16,f12
	ctx.f12.f64 = double(float(-(ctx.f14.f64 * ctx.f16.f64 - ctx.f12.f64)));
	// lfs f16,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f9,f9,f21
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f21.f64));
	// lfs f21,2456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2456);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f8,f1,f16,f8
	ctx.f8.f64 = double(float(-(ctx.f1.f64 * ctx.f16.f64 - ctx.f8.f64)));
	// lfs f1,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f21,f21,f18
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// lfs f7,3328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3328);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f4,f30,f22,f4
	ctx.f4.f64 = double(float(-(ctx.f30.f64 * ctx.f22.f64 - ctx.f4.f64)));
	// lfs f26,3336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3336);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f11,f29,f1,f11
	ctx.f11.f64 = double(float(ctx.f29.f64 * ctx.f1.f64 + ctx.f11.f64));
	// lfs f25,3344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3344);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,2176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2176);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f7,f19
	ctx.f24.f64 = double(float(ctx.f7.f64 * ctx.f19.f64));
	// lfs f17,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f17,f26,f17
	ctx.f17.f64 = double(float(ctx.f26.f64 * ctx.f17.f64));
	// lfs f15,2920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2920);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f18,f25,f18
	ctx.f18.f64 = double(float(ctx.f25.f64 * ctx.f18.f64));
	// lfs f30,1384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1384);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f14,f23,f15
	ctx.f14.f64 = double(float(ctx.f23.f64 * ctx.f15.f64));
	// lfs f1,2912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2912);
	ctx.f1.f64 = double(temp.f32);
	// stfs f3,3232(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3232, temp.u32);
	// fmuls f3,f30,f1
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// stfs f2,1472(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1472, temp.u32);
	// stfs f0,3336(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3336, temp.u32);
	// fmr f29,f16
	ctx.f29.f64 = ctx.f16.f64;
	// stfs f6,4272(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4272, temp.u32);
	// fmuls f9,f9,f19
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f19.f64));
	// stfs f7,4264(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4264, temp.u32);
	// lfs f7,3368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3368);
	ctx.f7.f64 = double(temp.f32);
	// lfs f2,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,3280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3280);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f12,f30,f2,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f2.f64 + ctx.f12.f64));
	// lfs f6,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f0,f7,f0,f18
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 - ctx.f18.f64));
	// fmuls f6,f14,f6
	ctx.f6.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// stfs f26,1448(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1448, temp.u32);
	// lfs f26,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f4,f28,f26,f4
	ctx.f4.f64 = double(float(ctx.f28.f64 * ctx.f26.f64 + ctx.f4.f64));
	// lfs f26,3392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3392);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f8,f24,f29,f8
	ctx.f8.f64 = double(float(-(ctx.f24.f64 * ctx.f29.f64 - ctx.f8.f64)));
	// lfs f28,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f28.f64 = double(temp.f32);
	// lfs f29,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f29.f64 = double(temp.f32);
	// stfs f23,2512(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2512, temp.u32);
	// fmuls f23,f26,f19
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f19.f64));
	// lfs f18,3408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3408);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f29,f29,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// lfs f24,1896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1896);
	ctx.f24.f64 = double(temp.f32);
	// fadds f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f12,2012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2012);
	ctx.f12.f64 = double(temp.f32);
	// lfs f16,3272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3272);
	ctx.f16.f64 = double(temp.f32);
	// fmsubs f6,f0,f5,f6
	ctx.f6.f64 = double(float(ctx.f0.f64 * ctx.f5.f64 - ctx.f6.f64));
	// lfs f0,3000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3000);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,2012(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2012, temp.u32);
	// fmuls f14,f24,f16
	ctx.f14.f64 = double(float(ctx.f24.f64 * ctx.f16.f64));
	// stfs f23,3392(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3392, temp.u32);
	// fmuls f23,f18,f19
	ctx.f23.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f28,3384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3384);
	ctx.f28.f64 = double(temp.f32);
	// lfs f22,3400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3400);
	ctx.f22.f64 = double(temp.f32);
	// stfs f25,1416(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1416, temp.u32);
	// fmuls f25,f5,f28
	ctx.f25.f64 = double(float(ctx.f5.f64 * ctx.f28.f64));
	// stfs f14,3384(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3384, temp.u32);
	// fmuls f14,f22,f19
	ctx.f14.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// stfs f23,3352(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3352, temp.u32);
	// fmuls f23,f12,f19
	ctx.f23.f64 = double(float(ctx.f12.f64 * ctx.f19.f64));
	// stfs f31,3216(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3216, temp.u32);
	// lfs f2,3376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3376);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,2504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2504);
	ctx.f31.f64 = double(temp.f32);
	// stfs f14,3344(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3344, temp.u32);
	// fmuls f30,f5,f31
	ctx.f30.f64 = double(float(ctx.f5.f64 * ctx.f31.f64));
	// stfs f23,3328(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3328, temp.u32);
	// stfs f25,3408(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3408, temp.u32);
	// lfs f23,3416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3416);
	ctx.f23.f64 = double(temp.f32);
	// lfs f14,3616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3616);
	ctx.f14.f64 = double(temp.f32);
	// stfs f27,3240(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3240, temp.u32);
	// fmuls f27,f5,f2
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f2.f64));
	// lfs f0,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,3368(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3368, temp.u32);
	// fmuls f5,f5,f14
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f14.f64));
	// stfs f20,3416(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3416, temp.u32);
	// fnmsubs f11,f10,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f5,3376(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3376, temp.u32);
	// lfs f25,2012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2012);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f20,f25,f15
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f15.f64));
	// stfs f20,4432(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4432, temp.u32);
	// lfs f5,2380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2380);
	ctx.f5.f64 = double(temp.f32);
	// lfs f20,3424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3424);
	ctx.f20.f64 = double(temp.f32);
	// lfs f10,1976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1976);
	ctx.f10.f64 = double(temp.f32);
	// stfs f11,3424(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3424, temp.u32);
	// fmuls f10,f10,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// fmuls f11,f15,f23
	ctx.f11.f64 = double(float(ctx.f15.f64 * ctx.f23.f64));
	// stfs f11,2012(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2012, temp.u32);
	// stfs f10,3400(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3400, temp.u32);
	// lfs f11,2372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2372);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,3264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3264);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// stfs f5,3312(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3312, temp.u32);
	// lfs f5,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f5.f64 = double(temp.f32);
	// stfs f11,3320(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3320, temp.u32);
	// fmadds f5,f21,f5,f17
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f5.f64 + ctx.f17.f64));
	// lfs f11,3036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3036);
	ctx.f11.f64 = double(temp.f32);
	// lfs f21,4368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4368);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f11,f11,f15
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f15.f64));
	// lfs f17,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f0,f9,f0,f8
	ctx.f0.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// stfs f7,1456(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1456, temp.u32);
	// fmuls f8,f21,f17
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f17.f64));
	// lfs f21,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f4,f3,f21,f4
	ctx.f4.f64 = double(float(-(ctx.f3.f64 * ctx.f21.f64 - ctx.f4.f64)));
	// lfs f3,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f6,f27,f3,f6
	ctx.f6.f64 = double(float(-(ctx.f27.f64 * ctx.f3.f64 - ctx.f6.f64)));
	// stfs f10,3272(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3272, temp.u32);
	// stfs f13,5492(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5492, temp.u32);
	// fmuls f3,f29,f15
	ctx.f3.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// stfs f12,3288(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3288, temp.u32);
	// fmr f9,f17
	ctx.f9.f64 = ctx.f17.f64;
	// lfs f10,3416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3416);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f21,f16,f20
	ctx.f21.f64 = double(float(ctx.f16.f64 * ctx.f20.f64));
	// lfs f7,3424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3424);
	ctx.f7.f64 = double(temp.f32);
	// lfs f13,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f13,f11,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmadds f12,f10,f12,f7
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f12.f64 + ctx.f7.f64));
	// lfs f7,3360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3360);
	ctx.f7.f64 = double(temp.f32);
	// lfs f11,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f11,f7,f11,f0
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f11.f64 + ctx.f0.f64));
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f8,f8,f0,f6
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// lfs f7,3408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3408);
	ctx.f7.f64 = double(temp.f32);
	// lfs f10,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f7,f10,f4
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f4.f64));
	// lfs f4,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f7,f3,f0
	ctx.f7.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f27,2012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2012);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f9,f30,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f9.f64));
	// lfs f17,2364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2364);
	ctx.f17.f64 = double(temp.f32);
	// stfs f28,1796(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1796, temp.u32);
	// fmsubs f13,f27,f4,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f4.f64 - ctx.f13.f64));
	// stfs f2,3308(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3308, temp.u32);
	// fmuls f28,f17,f1
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f1.f64));
	// lfs f2,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f2.f64 = double(temp.f32);
	// lfs f4,3400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3400);
	ctx.f4.f64 = double(temp.f32);
	// stfs f31,3224(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3224, temp.u32);
	// fnmsubs f12,f4,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f31,380(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,3392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3392);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f2,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f2.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// lfs f29,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f9,f9,f0,f8
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f31,3384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3384);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f7,f21,f0,f7
	ctx.f7.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 - ctx.f7.f64));
	// stfs f26,4288(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4288, temp.u32);
	// fnmsubs f10,f31,f29,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f29.f64 - ctx.f10.f64)));
	// lfs f26,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f26.f64 = double(temp.f32);
	// stfs f24,3264(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3264, temp.u32);
	// fnmsubs f13,f28,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// stfs f23,1408(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1408, temp.u32);
	// lfs f21,3368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3368);
	ctx.f21.f64 = double(temp.f32);
	// lfs f6,2864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2864);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,4560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4560);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f6,f16
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f16.f64));
	// lfs f2,2020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2020);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f27,f16,f4
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f4.f64));
	// lfs f31,3608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3608);
	ctx.f31.f64 = double(temp.f32);
	// lfs f8,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f8.f64 = double(temp.f32);
	// lfs f29,3012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3012);
	ctx.f29.f64 = double(temp.f32);
	// lfs f24,772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 772);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f21,f29
	ctx.f29.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// lfs f23,3376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3376);
	ctx.f23.f64 = double(temp.f32);
	// stfs f22,4112(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4112, temp.u32);
	// fmuls f8,f23,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f8.f64));
	// stfs f19,3296(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3296, temp.u32);
	// fmr f22,f24
	ctx.f22.f64 = ctx.f24.f64;
	// stfs f25,3304(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3304, temp.u32);
	// fmuls f25,f16,f2
	ctx.f25.f64 = double(float(ctx.f16.f64 * ctx.f2.f64));
	// stfs f20,3032(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3032, temp.u32);
	// fmuls f20,f21,f31
	ctx.f20.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// lfs f19,2028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2028);
	ctx.f19.f64 = double(temp.f32);
	// lfs f28,2036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2036);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,3352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3352);
	ctx.f26.f64 = double(temp.f32);
	// stfs f18,3280(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3280, temp.u32);
	// stfs f14,4368(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4368, temp.u32);
	// fnmsubs f12,f26,f24,f12
	ctx.f12.f64 = double(float(-(ctx.f26.f64 * ctx.f24.f64 - ctx.f12.f64)));
	// lfs f24,3344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3344);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f9,f3,f0,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f26,2044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2044);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f11,f24,f22,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f22.f64 + ctx.f11.f64));
	// lfs f24,3620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3620);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f7,f27,f0,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f7.f64));
	// lfs f27,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f25,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f13,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f17,f21,f28
	ctx.f17.f64 = double(float(ctx.f21.f64 * ctx.f28.f64));
	// stfs f1,2036(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2036, temp.u32);
	// fmuls f24,f24,f16
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f16.f64));
	// stfs f4,3024(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3024, temp.u32);
	// fmadds f10,f5,f21,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f21.f64 + ctx.f10.f64));
	// lfs f5,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f5,f20,f5
	ctx.f5.f64 = double(float(ctx.f20.f64 * ctx.f5.f64));
	// lfs f20,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f1,f21,f26
	ctx.f1.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f4,3328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3328);
	ctx.f4.f64 = double(temp.f32);
	// lfs f14,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f22,f21,f19
	ctx.f22.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// fnmsubs f9,f8,f27,f9
	ctx.f9.f64 = double(float(-(ctx.f8.f64 * ctx.f27.f64 - ctx.f9.f64)));
	// lfs f8,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f12,f4,f14,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f14.f64 - ctx.f12.f64)));
	// lfs f4,3320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3320);
	ctx.f4.f64 = double(temp.f32);
	// stfs f30,2044(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2044, temp.u32);
	// lfs f30,3252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3252);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f13,f17,f13,f7
	ctx.f13.f64 = double(float(ctx.f17.f64 * ctx.f13.f64 + ctx.f7.f64));
	// lfs f18,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f8,f24,f8,f0
	ctx.f8.f64 = double(float(ctx.f24.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f3,1520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1520);
	ctx.f3.f64 = double(temp.f32);
	// fmr f0,f20
	ctx.f0.f64 = ctx.f20.f64;
	// lfs f19,2904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2904);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f10,f4,f20,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f20.f64 + ctx.f10.f64));
	// stfs f28,1804(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1804, temp.u32);
	// fmuls f4,f21,f30
	ctx.f4.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// stfs f23,2028(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2028, temp.u32);
	// lfs f30,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f30.f64 = double(temp.f32);
	// fmr f28,f18
	ctx.f28.f64 = ctx.f18.f64;
	// lfs f23,3336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3336);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f19,f19,f15
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f15.f64));
	// fmuls f3,f21,f3
	ctx.f3.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// stfs f6,2020(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2020, temp.u32);
	// fadds f11,f23,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 + ctx.f11.f64));
	// stfs f16,1520(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1520, temp.u32);
	// fmuls f6,f29,f18
	ctx.f6.f64 = double(float(ctx.f29.f64 * ctx.f18.f64));
	// lfs f29,3268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3268);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f13,f1,f30,f13
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f30.f64 - ctx.f13.f64)));
	// lfs f30,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f30.f64 = double(temp.f32);
	// fmr f1,f18
	ctx.f1.f64 = ctx.f18.f64;
	// lfs f7,4716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4716);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f9,f5,f0,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f5,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f5.f64 = double(temp.f32);
	// fmr f16,f18
	ctx.f16.f64 = ctx.f18.f64;
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// stfs f2,2920(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2920, temp.u32);
	// fmuls f29,f29,f0
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// stfs f31,3328(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3328, temp.u32);
	// fmuls f31,f7,f15
	ctx.f31.f64 = double(float(ctx.f7.f64 * ctx.f15.f64));
	// fnmsubs f8,f19,f5,f8
	ctx.f8.f64 = double(float(-(ctx.f19.f64 * ctx.f5.f64 - ctx.f8.f64)));
	// lfs f5,2828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2828);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// lfs f28,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f28.f64 = double(temp.f32);
	// fadds f12,f11,f12
	ctx.f12.f64 = double(float(ctx.f11.f64 + ctx.f12.f64));
	// lfs f11,1884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1884);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f5,f5,f30
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f30.f64));
	// lfs f30,2896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2896);
	ctx.f30.f64 = double(temp.f32);
	// stfs f26,1756(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1756, temp.u32);
	// fmuls f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f4,f4,f1
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// lfs f1,2052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2052);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f9,f6,f28,f9
	ctx.f9.f64 = double(float(-(ctx.f6.f64 * ctx.f28.f64 - ctx.f9.f64)));
	// lfs f6,2992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2992);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f22,f16
	ctx.f2.f64 = double(float(ctx.f22.f64 * ctx.f16.f64));
	// lfs f27,1968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1968);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f21,f1
	ctx.f28.f64 = double(float(ctx.f21.f64 * ctx.f1.f64));
	// lfs f25,1528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1528);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f6,f15
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// fmuls f24,f30,f15
	ctx.f24.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// fmr f23,f0
	ctx.f23.f64 = ctx.f0.f64;
	// lfs f18,4432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4432);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f2,f0,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f19,3484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3484);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f8,f31,f0,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 + ctx.f8.f64));
	// lfs f16,3260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3260);
	ctx.f16.f64 = double(temp.f32);
	// stfs f29,2052(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2052, temp.u32);
	// fmadds f0,f3,f0,f9
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f29,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,2028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2028);
	ctx.f14.f64 = double(temp.f32);
	// lfs f22,1520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1520);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f20,f27,f22
	ctx.f20.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// stfs f1,2916(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2916, temp.u32);
	// stfs f20,1528(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1528, temp.u32);
	// fnmsubs f10,f18,f23,f10
	ctx.f10.f64 = double(float(-(ctx.f18.f64 * ctx.f23.f64 - ctx.f10.f64)));
	// lfs f23,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f23,f19,f23
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// lfs f19,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f16,f19
	ctx.f19.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// lfs f18,2044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2044);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,1872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1872);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f2,f18,f2
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f2.f64));
	// fmuls f3,f16,f29
	ctx.f3.f64 = double(float(ctx.f16.f64 * ctx.f29.f64));
	// lfs f29,328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	ctx.f29.f64 = double(temp.f32);
	// lfs f18,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f13,f4,f29,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f29.f64 - ctx.f13.f64)));
	// fmuls f18,f14,f18
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f18.f64));
	// lfs f4,2888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2888);
	ctx.f4.f64 = double(temp.f32);
	// lfs f29,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f29.f64 = double(temp.f32);
	// lfs f16,2376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2376);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f8,f5,f29,f8
	ctx.f8.f64 = double(float(-(ctx.f5.f64 * ctx.f29.f64 - ctx.f8.f64)));
	// stfs f12,5496(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5496, temp.u32);
	// fmuls f16,f4,f16
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// lfs f12,3208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3208);
	ctx.f12.f64 = double(temp.f32);
	// lfs f20,1536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1536);
	ctx.f20.f64 = double(temp.f32);
	// lfs f5,4220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4220);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,4332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4332);
	ctx.f1.f64 = double(temp.f32);
	// stfs f27,3320(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3320, temp.u32);
	// fmuls f1,f1,f12
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f7,1520(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1520, temp.u32);
	// fmuls f7,f5,f20
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f20.f64));
	// lfs f27,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f27.f64 = double(temp.f32);
	// lfs f9,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f9.f64 = double(temp.f32);
	// lfs f29,2088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2088);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f9,f25,f9
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f9.f64));
	// stfs f25,1884(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1884, temp.u32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f25,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f0,f18,f25,f0
	ctx.f0.f64 = double(float(-(ctx.f18.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// fnmsubs f13,f28,f27,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f27.f64 - ctx.f13.f64)));
	// lfs f25,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f25.f64 = double(temp.f32);
	// lfs f28,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f16,f25
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f25.f64));
	// fnmsubs f8,f23,f28,f8
	ctx.f8.f64 = double(float(-(ctx.f23.f64 * ctx.f28.f64 - ctx.f8.f64)));
	// stfs f6,2044(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2044, temp.u32);
	// stfs f22,2256(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2256, temp.u32);
	// fmuls f7,f7,f12
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// stfs f30,1536(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1536, temp.u32);
	// lfs f17,2036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2036);
	ctx.f17.f64 = double(temp.f32);
	// lfs f31,1880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1880);
	ctx.f31.f64 = double(temp.f32);
	// lfs f14,2984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2984);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f31,f31,f17
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// lfs f6,3312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3312);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f14,f14,f15
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f30,2068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2068);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f6,f4,f6
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f6.f64));
	// lfs f25,4900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4900);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f1,f1,f30
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lfs f23,2076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2076);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f25,f4
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// lfs f22,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f23,f23,f12
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f12.f64));
	// stfs f21,2392(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2392, temp.u32);
	// fmuls f22,f4,f22
	ctx.f22.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// lfs f28,1920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1920);
	ctx.f28.f64 = double(temp.f32);
	// lfs f21,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f18.f64 = double(temp.f32);
	// stfs f17,2232(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2232, temp.u32);
	// stfs f15,2236(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2236, temp.u32);
	// fmadds f0,f24,f18,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f18.f64 + ctx.f0.f64));
	// lfs f17,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f29,f29,f21,f9
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f21.f64 + ctx.f9.f64));
	// lfs f21,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f27,f27,f21
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f21.f64));
	// lfs f24,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f13,f2,f17,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f2,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f8,f19,f24,f8
	ctx.f8.f64 = double(float(-(ctx.f19.f64 * ctx.f24.f64 - ctx.f8.f64)));
	// lfs f24,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f24.f64 = double(temp.f32);
	// lfs f19,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f2,f1,f2
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// lfs f18,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f1,f24,f19
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// lfs f17,196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f18,f12
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f12.f64));
	// lfs f15,320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f25,f25,f17
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f17.f64));
	// lfs f18,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f23,f23,f15
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f15.f64));
	// fadds f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// lfs f10,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f10.f64 = double(temp.f32);
	// stfs f0,1152(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// fmadds f10,f7,f10,f27
	ctx.f10.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f27.f64));
	// lfs f21,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f21.f64 = double(temp.f32);
	// lfs f15,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f21,f28,f21
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f21.f64));
	// lfs f0,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f13,f31,f15,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f15.f64 - ctx.f13.f64)));
	// fnmsubs f0,f3,f0,f8
	ctx.f0.f64 = double(float(-(ctx.f3.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f24,2084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2084);
	ctx.f24.f64 = double(temp.f32);
	// lfs f31,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f2,f29,f4,f2
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f2.f64));
	// fmuls f16,f4,f24
	ctx.f16.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// stfs f9,2464(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2464, temp.u32);
	// fmuls f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// lfs f31,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f31.f64 = double(temp.f32);
	// lfs f9,228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f27,f4,f18
	ctx.f27.f64 = double(float(ctx.f4.f64 * ctx.f18.f64));
	// lfs f3,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f3,f3,f31,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f31.f64 + ctx.f1.f64));
	// lfs f7,2816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2816);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f10,f25,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f25.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f31,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f31.f64 = double(temp.f32);
	// lfs f9,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f9.f64 = double(temp.f32);
	// lfs f8,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f26,f31,f13
	ctx.f13.f64 = double(float(-(ctx.f26.f64 * ctx.f31.f64 - ctx.f13.f64)));
	// fmadds f8,f7,f8,f21
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f8.f64 + ctx.f21.f64));
	// stfs f28,4432(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4432, temp.u32);
	// fnmsubs f0,f11,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f11.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f11,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f16,f11,f2
	ctx.f11.f64 = double(float(-(ctx.f16.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// lfs f28,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f28.f64 = double(temp.f32);
	// lfs f2,3332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3332);
	ctx.f2.f64 = double(temp.f32);
	// stfs f30,3016(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3016, temp.u32);
	// fmuls f2,f28,f2
	ctx.f2.f64 = double(float(ctx.f28.f64 * ctx.f2.f64));
	// lfs f30,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f9,f30,f12
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f12.f64));
	// lfs f31,884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f10,f23,f28,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f28.f64 + ctx.f10.f64));
	// lfs f26,2672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2672);
	ctx.f26.f64 = double(temp.f32);
	// lfs f1,1552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1552);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f13,f14,f31,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f17,2868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2868);
	ctx.f17.f64 = double(temp.f32);
	// fadds f8,f8,f26
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f26.f64));
	// lfs f7,4892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4892);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f17,f22,f17
	ctx.f17.f64 = double(float(ctx.f22.f64 * ctx.f17.f64));
	// lfs f30,428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 428);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f7,f7,f4
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// lfs f28,1560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1560);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f19,f1
	ctx.f29.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// stfs f24,3000(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3000, temp.u32);
	// fmuls f3,f3,f1
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f1.f64));
	// lfs f31,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f30,f30,f12
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f12.f64));
	// lfs f26,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f25,f4,f28
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// lfs f24,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f24.f64 = double(temp.f32);
	// stfs f20,2504(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2504, temp.u32);
	// stfs f18,2472(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2472, temp.u32);
	// lfs f19,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f9,f9,f1
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// lfs f16,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f4,f19
	ctx.f15.f64 = double(float(ctx.f4.f64 * ctx.f19.f64));
	// stfs f15,908(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// fmuls f15,f4,f16
	ctx.f15.f64 = double(float(ctx.f4.f64 * ctx.f16.f64));
	// stfs f15,1168(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// fmuls f1,f30,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f1.f64));
	// lfs f14,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f8,f8,f5,f3
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f5.f64 + ctx.f3.f64));
	// lfs f15,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f23,f4,f26
	ctx.f23.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// stfs f12,820(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// fmuls f12,f4,f14
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f14.f64));
	// stfs f12,916(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// fmuls f20,f4,f24
	ctx.f20.f64 = double(float(ctx.f4.f64 * ctx.f24.f64));
	// lfs f12,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f2,f2,f4
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// stfs f29,660(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// fmuls f29,f4,f15
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f15.f64));
	// lfs f21,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f21.f64 = double(temp.f32);
	// stfs f29,1032(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// fmuls f18,f4,f21
	ctx.f18.f64 = double(float(ctx.f4.f64 * ctx.f21.f64));
	// lfs f29,1528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1528);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f13,f29,f31,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f13.f64)));
	// lfs f29,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f29.f64 = double(temp.f32);
	// stfs f21,548(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// lfs f31,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f31.f64 = double(temp.f32);
	// stfs f27,636(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// fmuls f27,f4,f12
	ctx.f27.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// stfs f27,2076(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2076, temp.u32);
	// lfs f27,2744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2744);
	ctx.f27.f64 = double(temp.f32);
	// lfs f30,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f27,f5,f27
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// fnmsubs f11,f6,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// stfs f27,812(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// lfs f6,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f10,f17,f6,f10
	ctx.f10.f64 = double(float(ctx.f17.f64 * ctx.f6.f64 + ctx.f10.f64));
	// stfs f9,1160(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// lfs f6,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f6.f64 = double(temp.f32);
	// lfs f9,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f9.f64 = double(temp.f32);
	// fadds f13,f6,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 + ctx.f13.f64));
	// lfs f21,2052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2052);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f6,f4,f31
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f31.f64));
	// fnmsubs f0,f21,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f21.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// stfs f12,2896(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2896, temp.u32);
	// lfs f27,5168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5168);
	ctx.f27.f64 = double(temp.f32);
	// lfs f9,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f9,f27,f9
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f9.f64));
	// lfs f27,2192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2192);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f30,f22,f27
	ctx.f30.f64 = double(float(ctx.f22.f64 * ctx.f27.f64));
	// lfs f27,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f27.f64 = double(temp.f32);
	// stfs f28,4716(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4716, temp.u32);
	// fmuls f28,f4,f29
	ctx.f28.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// stfs f4,3332(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3332, temp.u32);
	// fmuls f7,f7,f27
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f27.f64));
	// lfs f4,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f4.f64 = double(temp.f32);
	// lfs f12,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,3312(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3312, temp.u32);
	// lfs f5,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f5.f64 = double(temp.f32);
	// fadds f13,f13,f0
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f12,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f12,f5,f12,f11
	ctx.f12.f64 = double(float(-(ctx.f5.f64 * ctx.f12.f64 - ctx.f11.f64)));
	// lfs f0,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// lfs f11,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f11,f0,f10
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f10,2356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2356);
	ctx.f10.f64 = double(temp.f32);
	// lfs f5,2340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2340);
	ctx.f5.f64 = double(temp.f32);
	// lfs f0,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f0.f64 = double(temp.f32);
	// stfs f19,2984(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2984, temp.u32);
	// stfs f16,4332(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4332, temp.u32);
	// stfs f14,2904(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2904, temp.u32);
	// stfs f25,1560(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1560, temp.u32);
	// stfs f15,2912(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2912, temp.u32);
	// stfs f23,1552(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1552, temp.u32);
	// stfs f26,2868(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2868, temp.u32);
	// stfs f20,1152(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// stfs f24,2888(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2888, temp.u32);
	// stfs f18,2084(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2084, temp.u32);
	// stfs f31,4900(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4900, temp.u32);
	// stfs f29,2480(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2480, temp.u32);
	// fmuls f3,f0,f4
	ctx.f3.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f26,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f26.f64 = double(temp.f32);
	// lfs f0,5144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5144);
	ctx.f0.f64 = double(temp.f32);
	// lfs f23,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,3904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3904);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f23,f26,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f26.f64 - ctx.f12.f64)));
	// lfs f29,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f29.f64 = double(temp.f32);
	// fadds f25,f25,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 + ctx.f0.f64));
	// lfs f24,4656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4656);
	ctx.f24.f64 = double(temp.f32);
	// fmr f0,f29
	ctx.f0.f64 = ctx.f29.f64;
	// lfs f23,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f5,f24
	ctx.f24.f64 = double(float(ctx.f5.f64 * ctx.f24.f64));
	// fmadds f11,f23,f29,f11
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f26,5184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5184);
	ctx.f26.f64 = double(temp.f32);
	// lfs f23,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f9,f26,f23,f9
	ctx.f9.f64 = double(float(ctx.f26.f64 * ctx.f23.f64 + ctx.f9.f64));
	// lfs f21,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,4848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4848);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// lfs f21,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f16.f64 = double(temp.f32);
	// lfs f31,3912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3912);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,3440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3440);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f27,f31,f10
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// fmadds f12,f16,f0,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f16,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f24,f24,f21
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f21,1988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1988);
	ctx.f21.f64 = double(temp.f32);
	// fmr f18,f0
	ctx.f18.f64 = ctx.f0.f64;
	// lfs f22,4648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4648);
	ctx.f22.f64 = double(temp.f32);
	// stfs f28,2068(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2068, temp.u32);
	// fnmsubs f11,f16,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f16.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f28,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f21,f17,f21
	ctx.f21.f64 = double(float(ctx.f17.f64 * ctx.f21.f64));
	// stfs f6,812(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// fmuls f22,f5,f22
	ctx.f22.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// stfs f17,636(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// fmuls f25,f25,f10
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f10.f64));
	// lfs f16,5120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5120);
	ctx.f16.f64 = double(temp.f32);
	// lfs f6,3792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3792);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f14,f16,f10
	ctx.f14.f64 = double(float(ctx.f16.f64 * ctx.f10.f64));
	// lfs f17,2860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2860);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f6,f6,f10
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f10.f64));
	// stfs f5,624(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// fmuls f17,f17,f10
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f10.f64));
	// lfs f5,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,4856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4856);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f27,f27,f18
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f18.f64));
	// lfs f20,3340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3340);
	ctx.f20.f64 = double(temp.f32);
	// stfs f10,660(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// fmuls f19,f20,f29
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f29.f64));
	// fmr f10,f5
	ctx.f10.f64 = ctx.f5.f64;
	// lfs f15,4480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4480);
	ctx.f15.f64 = double(temp.f32);
	// fmsubs f5,f15,f5,f23
	ctx.f5.f64 = double(float(ctx.f15.f64 * ctx.f5.f64 - ctx.f23.f64));
	// lfs f23,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f11,f7,f23,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f23,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f23,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// stfs f3,820(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// stfs f2,908(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// fmuls f0,f25,f0
	ctx.f0.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// lfs f18,2880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2880);
	ctx.f18.f64 = double(temp.f32);
	// lfs f3,3444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3444);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,3600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3600);
	ctx.f2.f64 = double(temp.f32);
	// fmsubs f9,f9,f28,f27
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f28.f64 - ctx.f27.f64));
	// lfs f7,3752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3752);
	ctx.f7.f64 = double(temp.f32);
	// lfs f23,3432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3432);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f7,f28,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// fmuls f10,f19,f10
	ctx.f10.f64 = double(float(ctx.f19.f64 * ctx.f10.f64));
	// lfs f19,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f24,f22,f19,f24
	ctx.f24.f64 = double(float(ctx.f22.f64 * ctx.f19.f64 - ctx.f24.f64));
	// lfs f25,4472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4472);
	ctx.f25.f64 = double(temp.f32);
	// lfs f22,4464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4464);
	ctx.f22.f64 = double(temp.f32);
	// stfs f3,548(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// fmuls f3,f3,f18
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f18.f64));
	// stfs f26,2880(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2880, temp.u32);
	// fmuls f26,f25,f23
	ctx.f26.f64 = double(float(ctx.f25.f64 * ctx.f23.f64));
	// stfs f13,5500(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5500, temp.u32);
	// fmadds f13,f22,f2,f21
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f2.f64 + ctx.f21.f64));
	// lfs f27,4344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4344);
	ctx.f27.f64 = double(temp.f32);
	// stfs f31,3208(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3208, temp.u32);
	// fmr f25,f19
	ctx.f25.f64 = ctx.f19.f64;
	// stfs f2,1528(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1528, temp.u32);
	// lfs f2,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f12,f8,f4,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f4.f64 + ctx.f12.f64));
	// fnmsubs f11,f30,f2,f11
	ctx.f11.f64 = double(float(-(ctx.f30.f64 * ctx.f2.f64 - ctx.f11.f64)));
	// lfs f8,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f0,f6,f8,f0
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f8.f64 - ctx.f0.f64));
	// stfs f18,2028(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2028, temp.u32);
	// lfs f23,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f10,f5,f20,f10
	ctx.f10.f64 = double(float(ctx.f5.f64 * ctx.f20.f64 - ctx.f10.f64));
	// lfs f18,2820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2820);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f4,f20,f27
	ctx.f4.f64 = double(float(ctx.f20.f64 * ctx.f27.f64));
	// lfs f21,2976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2976);
	ctx.f21.f64 = double(temp.f32);
	// stfs f16,2864(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2864, temp.u32);
	// stfs f15,3972(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3972, temp.u32);
	// lfs f15,2636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2636);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f3,f3,f25,f24
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f25.f64 + ctx.f24.f64));
	// lfs f25,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f9,f17,f25,f9
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f25.f64 - ctx.f9.f64)));
	// lfs f24,1560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1560);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f12,f1,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// fmadds f11,f24,f25,f11
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f25.f64 + ctx.f11.f64));
	// lfs f25,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f14,f25,f0
	ctx.f0.f64 = double(float(-(ctx.f14.f64 * ctx.f25.f64 - ctx.f0.f64)));
	// lfs f17,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f14,f28,f18
	ctx.f14.f64 = double(float(ctx.f28.f64 * ctx.f18.f64));
	// lfs f25,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,4456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4456);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f16,f17,f21
	ctx.f16.f64 = double(float(ctx.f17.f64 * ctx.f21.f64));
	// stfs f14,1560(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1560, temp.u32);
	// fmadds f13,f25,f1,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f1.f64 + ctx.f13.f64));
	// lfs f6,2852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2852);
	ctx.f6.f64 = double(temp.f32);
	// lfs f2,3744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3744);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,1964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1964);
	ctx.f30.f64 = double(temp.f32);
	// lfs f5,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f5.f64 = double(temp.f32);
	// lfs f14,2808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2808);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f5,f30,f5
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f5.f64));
	// stfs f29,2456(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2456, temp.u32);
	// fadds f29,f2,f6
	ctx.f29.f64 = double(float(ctx.f2.f64 + ctx.f6.f64));
	// stfs f16,1168(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// fmuls f16,f14,f15
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f1,1536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1536);
	ctx.f1.f64 = double(temp.f32);
	// lfs f8,3492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3492);
	ctx.f8.f64 = double(temp.f32);
	// lfs f24,4336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4336);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f28,f8
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// stfs f16,1536(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1536, temp.u32);
	// fmuls f19,f20,f24
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f24.f64));
	// lfs f16,4256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4256);
	ctx.f16.f64 = double(temp.f32);
	// stfs f20,636(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// fmuls f20,f20,f16
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f16.f64));
	// stfs f20,916(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// lfs f20,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f3,f26,f20,f3
	ctx.f3.f64 = double(float(-(ctx.f26.f64 * ctx.f20.f64 - ctx.f3.f64)));
	// stfs f19,624(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// lfs f19,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f19.f64 = double(temp.f32);
	// lfs f26,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f5,f29,f19,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f19.f64 + ctx.f5.f64));
	// fnmsubs f10,f4,f26,f10
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f26.f64 - ctx.f10.f64)));
	// lfs f29,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f29.f64 = double(temp.f32);
	// fmr f26,f19
	ctx.f26.f64 = ctx.f19.f64;
	// lfs f4,1956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1956);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f9,f31,f29,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f29.f64 - ctx.f9.f64)));
	// lfs f31,1948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1948);
	ctx.f31.f64 = double(temp.f32);
	// lfs f23,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f23.f64 = double(temp.f32);
	// fadds f4,f31,f4
	ctx.f4.f64 = double(float(ctx.f31.f64 + ctx.f4.f64));
	// lfs f29,1552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1552);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f22,f23,f1
	ctx.f22.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// lfs f31,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f31.f64 = double(temp.f32);
	// stfs f25,1032(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// fnmsubs f11,f29,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// stfs f23,1160(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// stfs f22,548(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// lfs f23,2020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2020);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,2812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2812);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f13,f23,f15,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 * ctx.f15.f64 + ctx.f13.f64));
	// lfs f22,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f22.f64 = double(temp.f32);
	// lfs f29,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f25,f25,f22
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f22.f64));
	// lfs f31,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f29,f26,f12
	ctx.f12.f64 = double(float(ctx.f29.f64 * ctx.f26.f64 + ctx.f12.f64));
	// stfs f2,3008(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3008, temp.u32);
	// fmadds f7,f7,f31,f0
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f31.f64 + ctx.f0.f64));
	// stfs f1,3384(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3384, temp.u32);
	// fmadds f9,f5,f22,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f22.f64 + ctx.f9.f64));
	// lfs f2,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f1.f64 = double(temp.f32);
	// stfs f8,2992(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2992, temp.u32);
	// fmadds f10,f1,f2,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f2.f64 + ctx.f10.f64));
	// lfs f8,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f8.f64 = double(temp.f32);
	// fmr f2,f8
	ctx.f2.f64 = ctx.f8.f64;
	// stfs f30,1988(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1988, temp.u32);
	// lfs f30,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,1876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1876);
	ctx.f29.f64 = double(temp.f32);
	// lfs f0,2040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2040);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f29,f22
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f22.f64));
	// fmuls f26,f28,f0
	ctx.f26.f64 = double(float(ctx.f28.f64 * ctx.f0.f64));
	// lfs f29,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f29.f64 = double(temp.f32);
	// lfs f0,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f11,f29,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// stfs f27,5020(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5020, temp.u32);
	// lfs f0,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f0.f64 = double(temp.f32);
	// lfs f27,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f7,f25,f0,f7
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f0.f64 + ctx.f7.f64));
	// stfs f23,3344(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3344, temp.u32);
	// fmr f23,f0
	ctx.f23.f64 = ctx.f0.f64;
	// fmadds f12,f30,f2,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f2.f64 + ctx.f12.f64));
	// stfs f16,2816(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2816, temp.u32);
	// fmr f30,f20
	ctx.f30.f64 = ctx.f20.f64;
	// lfs f16,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f16.f64 = double(temp.f32);
	// stfs f6,2672(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2672, temp.u32);
	// fmuls f6,f4,f22
	ctx.f6.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// stfs f17,1552(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1552, temp.u32);
	// fmr f4,f8
	ctx.f4.f64 = ctx.f8.f64;
	// stfs f21,3424(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3424, temp.u32);
	// lfs f21,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,4424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4424);
	ctx.f20.f64 = double(temp.f32);
	// stfs f14,3352(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3352, temp.u32);
	// lfs f29,4448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4448);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f7,f31,f23,f7
	ctx.f7.f64 = double(float(-(ctx.f31.f64 * ctx.f23.f64 - ctx.f7.f64)));
	// lfs f23,1864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1864);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f17,f23,f22
	ctx.f17.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f31,4440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4440);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f3,f27,f30,f3
	ctx.f3.f64 = double(float(-(ctx.f27.f64 * ctx.f30.f64 - ctx.f3.f64)));
	// lfs f30,2076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2076);
	ctx.f30.f64 = double(temp.f32);
	// fmadds f11,f30,f8,f11
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f8.f64 + ctx.f11.f64));
	// lfs f27,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f27.f64 = double(temp.f32);
	// lfs f8,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 * ctx.f29.f64));
	// lfs f14,4248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4248);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f10,f21,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f21.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// stfs f17,916(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// fmuls f17,f27,f20
	ctx.f17.f64 = double(float(ctx.f27.f64 * ctx.f20.f64));
	// fmuls f14,f27,f14
	ctx.f14.f64 = double(float(ctx.f27.f64 * ctx.f14.f64));
	// stfs f24,4892(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4892, temp.u32);
	// stfs f17,908(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// fmuls f19,f27,f31
	ctx.f19.f64 = double(float(ctx.f27.f64 * ctx.f31.f64));
	// stfs f27,636(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// lfs f2,2660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2660);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,2796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2796);
	ctx.f30.f64 = double(temp.f32);
	// lfs f25,2288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2288);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f0,f16,f0,f11
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f16,4416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4416);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f27,f16
	ctx.f16.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// lfs f24,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f24.f64 = double(temp.f32);
	// lfs f8,1940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1940);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f25,f25,f30
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// lfs f11,2168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2168);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f24,f2,f24
	ctx.f24.f64 = double(float(ctx.f2.f64 * ctx.f24.f64));
	// lfs f17,1932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1932);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f28,f8
	ctx.f21.f64 = double(float(ctx.f28.f64 * ctx.f8.f64));
	// lfs f27,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f11,f28,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f11.f64));
	// stfs f18,4220(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4220, temp.u32);
	// fmuls f17,f27,f17
	ctx.f17.f64 = double(float(ctx.f27.f64 * ctx.f17.f64));
	// stfs f6,812(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// stfs f14,1152(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// lfs f5,2084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2084);
	ctx.f5.f64 = double(temp.f32);
	// lfs f1,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f1.f64 = double(temp.f32);
	// lfs f18,1520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1520);
	ctx.f18.f64 = double(temp.f32);
	// lfs f6,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f6.f64 = double(temp.f32);
	// lfs f14,2628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2628);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f0,f5,f4,f0
	ctx.f0.f64 = double(float(-(ctx.f5.f64 * ctx.f4.f64 - ctx.f0.f64)));
	// lfs f5,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f10,f29,f5,f10
	ctx.f10.f64 = double(float(ctx.f29.f64 * ctx.f5.f64 + ctx.f10.f64));
	// lfs f29,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f29.f64 = double(temp.f32);
	// stfs f19,624(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// lfs f19,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f19.f64 = double(temp.f32);
	// stfs f8,2976(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2976, temp.u32);
	// fnmsubs f8,f26,f29,f7
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f29.f64 - ctx.f7.f64)));
	// fnmsubs f6,f19,f6,f3
	ctx.f6.f64 = double(float(-(ctx.f19.f64 * ctx.f6.f64 - ctx.f3.f64)));
	// lfs f7,2068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2068);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f3.f64 = double(temp.f32);
	// lfs f5,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f3,f3,f2
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f2.f64));
	// fmuls f5,f24,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// lfs f19,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f19.f64 = double(temp.f32);
	// stfs f15,548(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// fnmsubs f0,f7,f1,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// lfs f15,3200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3200);
	ctx.f15.f64 = double(temp.f32);
	// stfs f19,660(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// fmuls f19,f19,f18
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f18.f64));
	// stfs f2,2012(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2012, temp.u32);
	// fmuls f15,f14,f15
	ctx.f15.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f2,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f2.f64 = double(temp.f32);
	// stfs f19,820(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// fnmsubs f10,f16,f2,f10
	ctx.f10.f64 = double(float(-(ctx.f16.f64 * ctx.f2.f64 - ctx.f10.f64)));
	// stfs f14,1032(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// lfs f14,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f14.f64 = double(temp.f32);
	// lfs f19,4824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4824);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// lfs f2,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f19,f19,f14
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f14.f64));
	// lfs f14,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f14.f64 = double(temp.f32);
	// fmsubs f5,f3,f2,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f2.f64 - ctx.f5.f64));
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// stfs f0,5504(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5504, temp.u32);
	// fmadds f0,f13,f14,f6
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f14.f64 + ctx.f6.f64));
	// lfs f1,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f1.f64 = double(temp.f32);
	// fmr f3,f14
	ctx.f3.f64 = ctx.f14.f64;
	// lfs f2,1560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1560);
	ctx.f2.f64 = double(temp.f32);
	// lfs f6,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f8,f2,f1,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f8.f64));
	// lfs f7,4408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4408);
	ctx.f7.f64 = double(temp.f32);
	// fmr f26,f14
	ctx.f26.f64 = ctx.f14.f64;
	// lfs f1,1536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1536);
	ctx.f1.f64 = double(temp.f32);
	// stfs f31,2744(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2744, temp.u32);
	// lfs f31,5048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5048);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f7,f7,f6,f19
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 - ctx.f19.f64));
	// lfs f4,2280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2280);
	ctx.f4.f64 = double(temp.f32);
	// lfs f13,3180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3180);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f4,f4,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f28.f64));
	// lfs f12,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f12.f64 = double(temp.f32);
	// lfs f6,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fnmsubs f0,f1,f3,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f3.f64 - ctx.f0.f64)));
	// lfs f1,4400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4400);
	ctx.f1.f64 = double(temp.f32);
	// fadds f1,f1,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f31.f64));
	// lfs f29,3604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3604);
	ctx.f29.f64 = double(temp.f32);
	// lfs f31,3756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3756);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f6,f15,f6,f25
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f6.f64 + ctx.f25.f64));
	// stfs f27,1160(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// fadds f31,f29,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 + ctx.f31.f64));
	// lfs f27,4240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4240);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f10,f17,f26,f10
	ctx.f10.f64 = double(float(ctx.f17.f64 * ctx.f26.f64 + ctx.f10.f64));
	// lfs f29,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f7,f27,f29,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f29.f64 + ctx.f7.f64));
	// lfs f29,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f11,f29,f9
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f29.f64 - ctx.f9.f64)));
	// lfs f9,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f9.f64 = double(temp.f32);
	// lfs f27,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f27.f64 = double(temp.f32);
	// lfs f2,3748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3748);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f12,f9,f5
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f9.f64 - ctx.f5.f64)));
	// lfs f3,5056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5056);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f2,f2,f28
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f28.f64));
	// lfs f9,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f3,f30,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// fnmsubs f6,f4,f27,f6
	ctx.f6.f64 = double(float(-(ctx.f4.f64 * ctx.f27.f64 - ctx.f6.f64)));
	// lfs f4,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f4.f64 = double(temp.f32);
	// stfs f23,3200(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3200, temp.u32);
	// stfs f20,2808(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2808, temp.u32);
	// stfs f22,2084(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2084, temp.u32);
	// stfs f18,3360(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3360, temp.u32);
	// lfs f20,4224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4224);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f8,f21,f4,f8
	ctx.f8.f64 = double(float(-(ctx.f21.f64 * ctx.f4.f64 - ctx.f8.f64)));
	// fadds f1,f1,f20
	ctx.f1.f64 = double(float(ctx.f1.f64 + ctx.f20.f64));
	// lfs f21,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f21.f64 = double(temp.f32);
	// fmr f20,f14
	ctx.f20.f64 = ctx.f14.f64;
	// lfs f19,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f3,f3,f21
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f21.f64));
	// lfs f21,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f21.f64 = double(temp.f32);
	// lfs f5,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f31,f31,f21
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f21.f64));
	// fmadds f0,f5,f9,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f9.f64 + ctx.f0.f64));
	// lfs f21,5008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5008);
	ctx.f21.f64 = double(temp.f32);
	// lfs f22,5032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5032);
	ctx.f22.f64 = double(temp.f32);
	// lfs f9,2872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2872);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f22,f22,f30
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f30.f64));
	// lfs f4,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f4,f9,f4
	ctx.f4.f64 = double(float(ctx.f9.f64 * ctx.f4.f64));
	// stfs f9,3368(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3368, temp.u32);
	// fmr f9,f14
	ctx.f9.f64 = ctx.f14.f64;
	// lfs f5,4232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4232);
	ctx.f5.f64 = double(temp.f32);
	// lfs f27,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f10,f19,f20,f10
	ctx.f10.f64 = double(float(-(ctx.f19.f64 * ctx.f20.f64 - ctx.f10.f64)));
	// lfs f20,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f7,f21,f20,f7
	ctx.f7.f64 = double(float(ctx.f21.f64 * ctx.f20.f64 + ctx.f7.f64));
	// lfs f21,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f26,f27,f5
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// lfs f19,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f11,f20,f21,f11
	ctx.f11.f64 = double(float(ctx.f20.f64 * ctx.f21.f64 + ctx.f11.f64));
	// fmadds f6,f2,f19,f6
	ctx.f6.f64 = double(float(ctx.f2.f64 * ctx.f19.f64 + ctx.f6.f64));
	// lfs f2,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f2.f64 = double(temp.f32);
	// lfs f20,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f8,f20,f2,f8
	ctx.f8.f64 = double(float(-(ctx.f20.f64 * ctx.f2.f64 - ctx.f8.f64)));
	// lfs f20,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f21,f20,f21,f0
	ctx.f21.f64 = double(float(-(ctx.f20.f64 * ctx.f21.f64 - ctx.f0.f64)));
	// lfs f0,4216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4216);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,2872(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2872, temp.u32);
	// fmuls f5,f30,f0
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f10,f4,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f25,5040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5040);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f7,f7,f30,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f30.f64 - ctx.f3.f64));
	// lfs f2,4976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4976);
	ctx.f2.f64 = double(temp.f32);
	// fsubs f12,f12,f25
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f25.f64));
	// stfs f27,908(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// fmuls f2,f2,f30
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// fnmsubs f6,f22,f0,f6
	ctx.f6.f64 = double(float(-(ctx.f22.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// fmr f27,f0
	ctx.f27.f64 = ctx.f0.f64;
	// lfs f29,2800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2800);
	ctx.f29.f64 = double(temp.f32);
	// lfs f24,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f24.f64 = double(temp.f32);
	// fadds f11,f11,f8
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f8.f64));
	// lfs f9,4968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4968);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f23,f24,f29
	ctx.f23.f64 = double(float(ctx.f24.f64 * ctx.f29.f64));
	// lfs f4,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f4.f64 = double(temp.f32);
	// stfs f28,1168(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// fmadds f10,f26,f4,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f4.f64 + ctx.f10.f64));
	// stfs f24,2344(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2344, temp.u32);
	// stfs f25,2384(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2384, temp.u32);
	// fsubs f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f9.f64));
	// lfs f9,3572(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3572);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,4936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4936);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f6,f2,f0,f6
	ctx.f6.f64 = double(float(-(ctx.f2.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// lfs f8,4360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4360);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f1,f1,f27,f31
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f27.f64 + ctx.f31.f64));
	// lfs f28,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f28.f64 = double(temp.f32);
	// lfs f4,2228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2228);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f28,f28,f9
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f9.f64));
	// lfs f26,1552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1552);
	ctx.f26.f64 = double(temp.f32);
	// fmr f27,f14
	ctx.f27.f64 = ctx.f14.f64;
	// lfs f31,3864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3864);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f4,f26,f4
	ctx.f4.f64 = double(float(ctx.f26.f64 * ctx.f4.f64));
	// lfs f25,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,2892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2892);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f31,f25,f31
	ctx.f31.f64 = double(float(ctx.f25.f64 * ctx.f31.f64));
	// lfs f2,3244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3244);
	ctx.f2.f64 = double(temp.f32);
	// fadds f24,f3,f24
	ctx.f24.f64 = double(float(ctx.f3.f64 + ctx.f24.f64));
	// stfs f29,2020(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2020, temp.u32);
	// fmuls f2,f8,f2
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f2.f64));
	// lfs f29,3300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3300);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f7,f5,f0,f7
	ctx.f7.f64 = double(float(-(ctx.f5.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// lfs f0,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f23,f27,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 * ctx.f27.f64 + ctx.f10.f64));
	// lfs f23,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f23,f23,f0,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f21.f64));
	// lfs f0,2748(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2748);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f17,f30,f0
	ctx.f17.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f0,2740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2740);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f14,f30,f0
	ctx.f14.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// lfs f0,2208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2208);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f0,f0,f30
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f30.f64));
	// stfs f0,820(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// fmadds f28,f13,f15,f28
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f15.f64 + ctx.f28.f64));
	// lfs f15,3220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3220);
	ctx.f15.f64 = double(temp.f32);
	// fadds f12,f12,f29
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f29.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f1,f15,f0,f1
	ctx.f1.f64 = double(float(ctx.f15.f64 * ctx.f0.f64 + ctx.f1.f64));
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// lfs f15,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f10,f4,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f5,3228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3228);
	ctx.f5.f64 = double(temp.f32);
	// lfs f19,2336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2336);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f6,f31,f15,f6
	ctx.f6.f64 = double(float(-(ctx.f31.f64 * ctx.f15.f64 - ctx.f6.f64)));
	// lfs f0,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f5,f30,f5
	ctx.f5.f64 = double(float(ctx.f30.f64 * ctx.f5.f64));
	// lfs f4,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f2,f2,f0
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f0.f64));
	// stfs f13,3408(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3408, temp.u32);
	// fmuls f13,f19,f4
	ctx.f13.f64 = double(float(ctx.f19.f64 * ctx.f4.f64));
	// stfs f26,1032(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// lfs f22,3164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3164);
	ctx.f22.f64 = double(temp.f32);
	// lfs f0,2448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2448);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f18.f64 = double(temp.f32);
	// lfs f26,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f18,f18,f22
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f22.f64));
	// lfs f4,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f26,f26,f22
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f22.f64));
	// stfs f3,2376(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2376, temp.u32);
	// fmuls f4,f0,f4
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f4.f64));
	// lfs f3,2440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2440);
	ctx.f3.f64 = double(temp.f32);
	// lfs f20,2780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2780);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f3,f30,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// stfs f22,916(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// fadds f12,f12,f20
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f20.f64));
	// lfs f22,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f22,f22,f9
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f9.f64));
	// stfs f22,812(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// lfs f22,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f6,f5,f22,f6
	ctx.f6.f64 = double(float(-(ctx.f5.f64 * ctx.f22.f64 - ctx.f6.f64)));
	// stfs f19,2280(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2280, temp.u32);
	// stfs f25,3400(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3400, temp.u32);
	// lfs f15,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f15.f64 = double(temp.f32);
	// lfs f5,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f5.f64 = double(temp.f32);
	// lfs f25,5148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5148);
	ctx.f25.f64 = double(temp.f32);
	// lfs f19,4476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4476);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f13,f25,f5,f13
	ctx.f13.f64 = double(float(ctx.f25.f64 * ctx.f5.f64 - ctx.f13.f64));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f19,f19,f15
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f15.f64));
	// lfs f22,2432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2432);
	ctx.f22.f64 = double(temp.f32);
	// stfs f20,2288(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2288, temp.u32);
	// fmsubs f5,f22,f0,f4
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f0.f64 - ctx.f4.f64));
	// stfs f29,2336(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2336, temp.u32);
	// fmuls f0,f3,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f27,3588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3588);
	ctx.f27.f64 = double(temp.f32);
	// lfs f21,3644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3644);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f27,f27,f30
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f16,1960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1960);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f21,f30,f21
	ctx.f21.f64 = double(float(ctx.f30.f64 * ctx.f21.f64));
	// lfs f31,2328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2328);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f16,f30,f16
	ctx.f16.f64 = double(float(ctx.f30.f64 * ctx.f16.f64));
	// lfs f29,3860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3860);
	ctx.f29.f64 = double(temp.f32);
	// lfs f20,5140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5140);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,2772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2772);
	ctx.f15.f64 = double(temp.f32);
	// fadds f20,f31,f20
	ctx.f20.f64 = double(float(ctx.f31.f64 + ctx.f20.f64));
	// stfs f11,5508(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5508, temp.u32);
	// fmuls f11,f24,f8
	ctx.f11.f64 = double(float(ctx.f24.f64 * ctx.f8.f64));
	// lfs f4,1952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1952);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f24,f8,f29
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f29.f64));
	// lfs f3,3236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3236);
	ctx.f3.f64 = double(temp.f32);
	// fadds f12,f12,f15
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f15.f64));
	// stfs f9,660(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// fmuls f15,f8,f4
	ctx.f15.f64 = double(float(ctx.f8.f64 * ctx.f4.f64));
	// lfs f9,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f9.f64 = double(temp.f32);
	// fmsubs f0,f5,f30,f0
	ctx.f0.f64 = double(float(ctx.f5.f64 * ctx.f30.f64 - ctx.f0.f64));
	// stfs f26,636(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// fmsubs f11,f11,f9,f2
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f9.f64 - ctx.f2.f64));
	// lfs f26,4532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4532);
	ctx.f26.f64 = double(temp.f32);
	// fsubs f12,f12,f3
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f3.f64));
	// stfs f16,548(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// fmuls f2,f30,f26
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// lfs f16,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f16.f64 = double(temp.f32);
	// fadds f10,f23,f10
	ctx.f10.f64 = double(float(ctx.f23.f64 + ctx.f10.f64));
	// fmadds f20,f20,f16,f19
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f16.f64 + ctx.f19.f64));
	// lfs f26,2496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2496);
	ctx.f26.f64 = double(temp.f32);
	// lfs f9,2424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2424);
	ctx.f9.f64 = double(temp.f32);
	// fmr f19,f16
	ctx.f19.f64 = ctx.f16.f64;
	// fadds f9,f26,f9
	ctx.f9.f64 = double(float(ctx.f26.f64 + ctx.f9.f64));
	// lfs f26,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f6,f27,f26,f6
	ctx.f6.f64 = double(float(-(ctx.f27.f64 * ctx.f26.f64 - ctx.f6.f64)));
	// lfs f5,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f5.f64 = double(temp.f32);
	// lfs f16,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,3852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3852);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f7,f14,f16,f7
	ctx.f7.f64 = double(float(-(ctx.f14.f64 * ctx.f16.f64 - ctx.f7.f64)));
	// fnmsubs f11,f15,f5,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f5.f64 - ctx.f11.f64)));
	// lfs f5,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f5.f64 = double(temp.f32);
	// lfs f15,5136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5136);
	ctx.f15.f64 = double(temp.f32);
	// fsubs f13,f13,f22
	ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f22.f64));
	// lfs f14,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f12,f18,f16,f12
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f16.f64 - ctx.f12.f64)));
	// lfs f18,3836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3836);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f26,f24,f19
	ctx.f26.f64 = double(float(ctx.f24.f64 * ctx.f19.f64));
	// lfs f19,3468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3468);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f8,f19
	ctx.f19.f64 = double(float(ctx.f8.f64 * ctx.f19.f64));
	// stfs f19,624(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// fnmsubs f6,f17,f5,f6
	ctx.f6.f64 = double(float(-(ctx.f17.f64 * ctx.f5.f64 - ctx.f6.f64)));
	// lfs f16,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f17,f15,f14
	ctx.f17.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// lfs f14,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f2,f2,f14,f0
	ctx.f2.f64 = double(float(-(ctx.f2.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// lfs f0,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f9,f9,f30
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f30.f64));
	// stfs f4,2440(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2440, temp.u32);
	// fmadds f13,f18,f16,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f16.f64 + ctx.f13.f64));
	// lfs f4,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f4.f64 = double(temp.f32);
	// stfs f25,2800(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2800, temp.u32);
	// fmadds f7,f1,f30,f7
	ctx.f7.f64 = double(float(ctx.f1.f64 * ctx.f30.f64 + ctx.f7.f64));
	// lfs f27,5132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5132);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f12,f28,f4,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f5,3964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3964);
	ctx.f5.f64 = double(temp.f32);
	// lfs f28,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f15,f5,f27
	ctx.f15.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// lfs f24,2764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2764);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f26,f20,f8,f26
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f8.f64 - ctx.f26.f64));
	// fnmsubs f6,f21,f0,f6
	ctx.f6.f64 = double(float(-(ctx.f21.f64 * ctx.f0.f64 - ctx.f6.f64)));
	// stfs f27,3964(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3964, temp.u32);
	// fmr f0,f14
	ctx.f0.f64 = ctx.f14.f64;
	// stfs f29,4532(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4532, temp.u32);
	// lfs f19,5124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5124);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,1400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1400);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f19,f8,f19
	ctx.f19.f64 = double(float(ctx.f8.f64 * ctx.f19.f64));
	// lfs f18,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f18.f64 = double(temp.f32);
	// lfs f29,4780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4780);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,2368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2368);
	ctx.f27.f64 = double(temp.f32);
	// stfs f31,2424(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2424, temp.u32);
	// fmuls f31,f8,f24
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f24.f64));
	// stfs f3,2448(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2448, temp.u32);
	// fmuls f3,f20,f18
	ctx.f3.f64 = double(float(ctx.f20.f64 * ctx.f18.f64));
	// lfs f25,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f27,f8,f27
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// fnmsubs f11,f25,f28,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f28.f64 - ctx.f11.f64)));
	// lfs f28,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f9,f9,f0,f2
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f2.f64));
	// lfs f2,3828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3828);
	ctx.f2.f64 = double(temp.f32);
	// stfs f24,2496(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2496, temp.u32);
	// fadds f13,f13,f2
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f2.f64));
	// lfs f1,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f28,f29,f28,f17
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f28.f64 + ctx.f17.f64));
	// lfs f25,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f24.f64 = double(temp.f32);
	// stfs f22,2328(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2328, temp.u32);
	// fmadds f0,f24,f0,f6
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f6.f64));
	// lfs f24,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f24,f4,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f4.f64 - ctx.f12.f64)));
	// lfs f18,4932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4932);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f16,f8,f18
	ctx.f16.f64 = double(float(ctx.f8.f64 * ctx.f18.f64));
	// stfs f16,636(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// lfs f6,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f26,f19,f25,f26
	ctx.f26.f64 = double(float(-(ctx.f19.f64 * ctx.f25.f64 - ctx.f26.f64)));
	// lfs f16,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f6,f15,f6
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f6.f64));
	// lfs f15,2732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2732);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f3,f15,f14,f3
	ctx.f3.f64 = double(float(ctx.f15.f64 * ctx.f14.f64 + ctx.f3.f64));
	// lfs f14,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,2488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2488);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f17,f30,f17
	ctx.f17.f64 = double(float(ctx.f30.f64 * ctx.f17.f64));
	// lfs f22,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f0,f16,f1,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f1.f64 - ctx.f0.f64)));
	// lfs f1,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f11,f31,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f31.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f23,4460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4460);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f12,f14,f4,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f4,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f4.f64 = double(temp.f32);
	// lfs f16,2416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2416);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f19,f8,f23
	ctx.f19.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// fmuls f4,f4,f22
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// stfs f4,820(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// fmuls f31,f30,f16
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f16.f64));
	// stfs f19,624(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// lfs f19,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f6,f28,f5,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f5.f64 - ctx.f6.f64));
	// lfs f16,416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	ctx.f16.f64 = double(temp.f32);
	// lfs f4,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f16,f16,f19
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f19.f64));
	// lfs f24,2620(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2620);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f4,f4,f19
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f19.f64));
	// lfs f14,2408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2408);
	ctx.f14.f64 = double(temp.f32);
	// fadds f0,f7,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 + ctx.f0.f64));
	// lfs f15,3256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3256);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f14,f30,f14
	ctx.f14.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f1,4160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4160);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f15,f24,f15
	ctx.f15.f64 = double(float(ctx.f24.f64 * ctx.f15.f64));
	// lfs f21,3820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3820);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f1,f24,f1
	ctx.f1.f64 = double(float(ctx.f24.f64 * ctx.f1.f64));
	// lfs f7,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f11,f3,f8,f11
	ctx.f11.f64 = double(float(ctx.f3.f64 * ctx.f8.f64 + ctx.f11.f64));
	// stfs f10,5512(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5512, temp.u32);
	// fnmsubs f13,f21,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f21.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// stfs f0,5516(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5516, temp.u32);
	// stfs f16,812(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// stfs f4,916(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// lfs f24,2400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2400);
	ctx.f24.f64 = double(temp.f32);
	// lfs f10,3916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3916);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f24,f30,f24
	ctx.f24.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// lfs f25,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,3812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3812);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f25,f22,f25
	ctx.f25.f64 = double(float(ctx.f22.f64 * ctx.f25.f64));
	// lfs f4,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f4.f64 = double(temp.f32);
	// lfs f0,3948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3948);
	ctx.f0.f64 = double(temp.f32);
	// stfs f14,548(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// fadds f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f10.f64));
	// lfs f14,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f14.f64 = double(temp.f32);
	// lfs f8,3908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3908);
	ctx.f8.f64 = double(temp.f32);
	// lfs f7,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f7.f64 = double(temp.f32);
	// stfs f20,2400(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2400, temp.u32);
	// fmuls f20,f5,f16
	ctx.f20.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// stfs f12,5520(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5520, temp.u32);
	// fnmsubs f12,f17,f4,f9
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f4.f64 - ctx.f9.f64)));
	// stfs f29,2416(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2416, temp.u32);
	// fnmsubs f29,f27,f14,f26
	ctx.f29.f64 = double(float(-(ctx.f27.f64 * ctx.f14.f64 - ctx.f26.f64)));
	// lfs f10,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f8,f8,f7
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f7.f64));
	// lfs f9,5052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5052);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f5.f64 = double(temp.f32);
	// stfs f2,2368(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2368, temp.u32);
	// stfs f25,660(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// stfs f22,3376(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3376, temp.u32);
	// stfs f23,2488(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2488, temp.u32);
	// stfs f19,3416(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3416, temp.u32);
	// stfs f21,2408(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2408, temp.u32);
	// stfs f18,2432(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2432, temp.u32);
	// fmr f7,f4
	ctx.f7.f64 = ctx.f4.f64;
	// lfs f21,3892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3892);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f13,f9,f5,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f5.f64 + ctx.f13.f64));
	// lfs f5,4948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4948);
	ctx.f5.f64 = double(temp.f32);
	// fadds f5,f0,f5
	ctx.f5.f64 = double(float(ctx.f0.f64 + ctx.f5.f64));
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// lfs f0,4500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4500);
	ctx.f0.f64 = double(temp.f32);
	// lfs f28,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f28.f64 = double(temp.f32);
	// lfs f4,4540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4540);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f28,f0,f28
	ctx.f28.f64 = double(float(ctx.f0.f64 * ctx.f28.f64));
	// lfs f2,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f4,f2,f4
	ctx.f4.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f3,2536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2536);
	ctx.f3.f64 = double(temp.f32);
	// lfs f22,4796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4796);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f3,f10,f3
	ctx.f3.f64 = double(float(ctx.f10.f64 * ctx.f3.f64));
	// fmadds f12,f31,f7,f12
	ctx.f12.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f12.f64));
	// lfs f31,3940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3940);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f21,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f21.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// lfs f21,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f8,f5,f21,f8
	ctx.f8.f64 = double(float(ctx.f5.f64 * ctx.f21.f64 + ctx.f8.f64));
	// lfs f7,3980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3980);
	ctx.f7.f64 = double(temp.f32);
	// fmr f5,f0
	ctx.f5.f64 = ctx.f0.f64;
	// lfs f18,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f18.f64 = double(temp.f32);
	// fadds f7,f31,f7
	ctx.f7.f64 = double(float(ctx.f31.f64 + ctx.f7.f64));
	// lfs f31,4156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4156);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f22,f30,f22
	ctx.f22.f64 = double(float(ctx.f30.f64 * ctx.f22.f64));
	// lfs f27,1528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1528);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f10,f31
	ctx.f31.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// lfs f19,4492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4492);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f4,f19,f27,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f27.f64 + ctx.f4.f64));
	// lfs f19,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f12,f15,f18,f12
	ctx.f12.f64 = double(float(ctx.f15.f64 * ctx.f18.f64 + ctx.f12.f64));
	// lfs f26,3900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3900);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,4588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4588);
	ctx.f25.f64 = double(temp.f32);
	// fadds f23,f26,f25
	ctx.f23.f64 = double(float(ctx.f26.f64 + ctx.f25.f64));
	// stfs f9,4156(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4156, temp.u32);
	// fmuls f5,f3,f5
	ctx.f5.f64 = double(float(ctx.f3.f64 * ctx.f5.f64));
	// lfs f3,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f7,f7,f0,f28
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + ctx.f28.f64));
	// stfs f11,5524(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5524, temp.u32);
	// fmuls f22,f22,f19
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// lfs f19,5012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5012);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f13,f19,f17,f13
	ctx.f13.f64 = double(float(ctx.f19.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f19,2360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2360);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f0,f31,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// lfs f17,4964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4964);
	ctx.f17.f64 = double(temp.f32);
	// lfs f11,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f11.f64 = double(temp.f32);
	// lfs f18,4972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4972);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f12,f1,f16,f12
	ctx.f12.f64 = double(float(-(ctx.f1.f64 * ctx.f16.f64 - ctx.f12.f64)));
	// lfs f1,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f8,f19,f1,f8
	ctx.f8.f64 = double(float(-(ctx.f19.f64 * ctx.f1.f64 - ctx.f8.f64)));
	// lfs f1,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f19,f3,f17
	ctx.f19.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// lfs f17,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f5,f4,f1,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f1.f64 + ctx.f5.f64));
	// lfs f4,4980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4980);
	ctx.f4.f64 = double(temp.f32);
	// lfs f1,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f4,f27
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f27.f64));
	// lfs f4,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f1,f1,f17,f29
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f17.f64 + ctx.f29.f64));
	// lfs f28,3884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3884);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f18,f10,f18
	ctx.f18.f64 = double(float(ctx.f10.f64 * ctx.f18.f64));
	// fmsubs f0,f7,f10,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 - ctx.f0.f64));
	// lfs f31,4484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4484);
	ctx.f31.f64 = double(temp.f32);
	// lfs f21,4076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4076);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f28,f3,f28
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// lfs f29,4212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4212);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f21,f21,f31
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f31.f64));
	// lfs f7,4452(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4452);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f12,f4,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f4.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// stfs f26,4972(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4972, temp.u32);
	// fmuls f23,f23,f10
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f10.f64));
	// lfs f11,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f8,f8,f30,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f30.f64 - ctx.f22.f64));
	// lfs f4,4652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4652);
	ctx.f4.f64 = double(temp.f32);
	// fadds f13,f13,f7
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f7.f64));
	// stfs f25,2536(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2536, temp.u32);
	// fmuls f26,f10,f29
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// lfs f16,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f12,f24,f11,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fnmsubs f6,f20,f16,f6
	ctx.f6.f64 = double(float(-(ctx.f20.f64 * ctx.f16.f64 - ctx.f6.f64)));
	// lfs f16,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f16.f64 = double(temp.f32);
	// lfs f24,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f23,f23,f16,f0
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f16.f64 + ctx.f0.f64));
	// fmr f0,f24
	ctx.f0.f64 = ctx.f24.f64;
	// lfs f22,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f5,f21,f22,f5
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f22.f64 - ctx.f5.f64));
	// lfs f25,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f25.f64 = double(temp.f32);
	// lfs f20,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f4,f25
	ctx.f25.f64 = double(float(ctx.f4.f64 * ctx.f25.f64));
	// lfs f14,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,3804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3804);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f1,f20,f14,f1
	ctx.f1.f64 = double(float(-(ctx.f20.f64 * ctx.f14.f64 - ctx.f1.f64)));
	// lfs f22,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f13,f17,f22,f13
	ctx.f13.f64 = double(float(-(ctx.f17.f64 * ctx.f22.f64 - ctx.f13.f64)));
	// lfs f14,4624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4624);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f16,f10,f14
	ctx.f16.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// lfs f14,4140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4140);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f12,f28,f24,f12
	ctx.f12.f64 = double(float(ctx.f28.f64 * ctx.f24.f64 + ctx.f12.f64));
	// lfs f20,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f20.f64 = double(temp.f32);
	// lfs f21,4148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4148);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f8,f19,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f19.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f19,f10,f14
	ctx.f19.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// lfs f14,3796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3796);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f5,f18,f0,f5
	ctx.f5.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f5.f64));
	// lfs f0,4444(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4444);
	ctx.f0.f64 = double(temp.f32);
	// lfs f18,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f24,f10,f14
	ctx.f24.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// fmadds f25,f0,f18,f25
	ctx.f25.f64 = double(float(ctx.f0.f64 * ctx.f18.f64 + ctx.f25.f64));
	// lfs f14,3772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3772);
	ctx.f14.f64 = double(temp.f32);
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f15,f10,f21
	ctx.f15.f64 = double(float(ctx.f10.f64 * ctx.f21.f64));
	// fnmsubs f0,f14,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f14.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f14,4132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4132);
	ctx.f14.f64 = double(temp.f32);
	// lfs f13,4468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4468);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f14,f13
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f13.f64));
	// stfs f14,4560(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4560, temp.u32);
	// stfs f13,1032(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// lfs f14,4044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4044);
	ctx.f14.f64 = double(temp.f32);
	// lfs f13,4764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4764);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f14,f20,f14
	ctx.f14.f64 = double(float(ctx.f20.f64 * ctx.f14.f64));
	// fmuls f13,f13,f31
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f14,2076(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2076, temp.u32);
	// stfs f13,1552(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1552, temp.u32);
	// lfs f14,3764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3764);
	ctx.f14.f64 = double(temp.f32);
	// lfs f13,4196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4196);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f14,f30,f14
	ctx.f14.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f11,4204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4204);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f13,f20,f13
	ctx.f13.f64 = double(float(ctx.f20.f64 * ctx.f13.f64));
	// lfs f22,4868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4868);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f11,f2,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f11.f64));
	// stfs f25,624(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// fmuls f22,f10,f22
	ctx.f22.f64 = double(float(ctx.f10.f64 * ctx.f22.f64));
	// lfs f17,4772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4772);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,5176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5176);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f17,f30,f17
	ctx.f17.f64 = double(float(ctx.f30.f64 * ctx.f17.f64));
	// lfs f18,2312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2312);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f28,f10,f28
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// lfs f25,2304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2304);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f18,f30,f18
	ctx.f18.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// stfs f14,636(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// fmuls f25,f30,f25
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// stfs f13,1536(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1536, temp.u32);
	// lfs f14,3740(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3740);
	ctx.f14.f64 = double(temp.f32);
	// lfs f13,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f13.f64 = double(temp.f32);
	// stfs f29,4868(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4868, temp.u32);
	// stfs f7,4652(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4652, temp.u32);
	// stfs f4,4204(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4204, temp.u32);
	// stfs f6,5528(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5528, temp.u32);
	// stfs f1,5532(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5532, temp.u32);
	// stfs f11,908(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// stfs f2,4212(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4212, temp.u32);
	// stfs f15,548(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// stfs f21,4148(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4148, temp.u32);
	// stfs f12,2052(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2052, temp.u32);
	// stfs f22,1560(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1560, temp.u32);
	// stfs f31,4980(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4980, temp.u32);
	// stfs f20,3864(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3864, temp.u32);
	// fmadds f13,f14,f13,f0
	ctx.f13.f64 = double(float(ctx.f14.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,3460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3460);
	ctx.f12.f64 = double(temp.f32);
	// lfs f4,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f9,f9,f4,f5
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f4.f64 - ctx.f5.f64)));
	// fnmsubs f6,f26,f6,f23
	ctx.f6.f64 = double(float(-(ctx.f26.f64 * ctx.f6.f64 - ctx.f23.f64)));
	// lfs f11,4924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4924);
	ctx.f11.f64 = double(temp.f32);
	// lfs f2,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f8,f17,f2,f8
	ctx.f8.f64 = double(float(ctx.f17.f64 * ctx.f2.f64 + ctx.f8.f64));
	// lfs f4,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f2.f64 = double(temp.f32);
	// lfs f7,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,2792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2792);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f13,f12,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f12,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,3788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3788);
	ctx.f1.f64 = double(temp.f32);
	// lfs f23,2968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2968);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f31,f5,f1
	ctx.f31.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// fmadds f9,f16,f4,f9
	ctx.f9.f64 = double(float(ctx.f16.f64 * ctx.f4.f64 + ctx.f9.f64));
	// lfs f15,1868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1868);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f6,f24,f2,f6
	ctx.f6.f64 = double(float(-(ctx.f24.f64 * ctx.f2.f64 - ctx.f6.f64)));
	// lfs f2,4428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4428);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f21,f23,f27
	ctx.f21.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// lfs f20,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f12,f18,f12,f8
	ctx.f12.f64 = double(float(-(ctx.f18.f64 * ctx.f12.f64 - ctx.f8.f64)));
	// lfs f16,1924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1924);
	ctx.f16.f64 = double(temp.f32);
	// stfs f21,1520(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1520, temp.u32);
	// fmuls f15,f30,f15
	ctx.f15.f64 = double(float(ctx.f30.f64 * ctx.f15.f64));
	// lfs f14,2656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2656);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f18,f10,f2
	ctx.f18.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// lfs f21,2752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2752);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f16,f30,f16
	ctx.f16.f64 = double(float(ctx.f30.f64 * ctx.f16.f64));
	// fmadds f13,f11,f0,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f0,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f0.f64 = double(temp.f32);
	// stfs f15,1152(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// fadds f21,f21,f14
	ctx.f21.f64 = double(float(ctx.f21.f64 + ctx.f14.f64));
	// lfs f26,2632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2632);
	ctx.f26.f64 = double(temp.f32);
	// lfs f11,3284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3284);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f22,f10,f26
	ctx.f22.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// lfs f15,816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f15.f64 = double(temp.f32);
	// stfs f18,1528(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1528, temp.u32);
	// fmuls f15,f11,f15
	ctx.f15.f64 = double(float(ctx.f11.f64 * ctx.f15.f64));
	// stfs f16,660(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// lfs f14,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f18,f14,f18
	ctx.f18.f64 = double(float(ctx.f14.f64 * ctx.f18.f64));
	// fnmsubs f13,f7,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f7,204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,1248(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// fmuls f7,f7,f20
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f20.f64));
	// stfs f7,1160(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// fnmsubs f9,f19,f16,f9
	ctx.f9.f64 = double(float(-(ctx.f19.f64 * ctx.f16.f64 - ctx.f9.f64)));
	// lfs f7,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f7.f64 = double(temp.f32);
	// stfs f22,2068(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2068, temp.u32);
	// fnmsubs f7,f28,f7,f6
	ctx.f7.f64 = double(float(-(ctx.f28.f64 * ctx.f7.f64 - ctx.f6.f64)));
	// stfs f14,2036(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2036, temp.u32);
	// stfs f15,1168(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// lfs f4,2624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2624);
	ctx.f4.f64 = double(temp.f32);
	// lfs f8,2616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2616);
	ctx.f8.f64 = double(temp.f32);
	// lfs f14,2608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2608);
	ctx.f14.f64 = double(temp.f32);
	// fadds f17,f4,f8
	ctx.f17.f64 = double(float(ctx.f4.f64 + ctx.f8.f64));
	// lfs f24,2664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2664);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f14,f3,f14
	ctx.f14.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// lfs f22,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f22,f24,f22
	ctx.f22.f64 = double(float(ctx.f24.f64 * ctx.f22.f64));
	// lfs f19,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f12,f25,f15,f12
	ctx.f12.f64 = double(float(ctx.f25.f64 * ctx.f15.f64 + ctx.f12.f64));
	// lfs f28,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f31,f31,f19
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f19.f64));
	// lfs f29,2724(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2724);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f13,f28,f0,f13
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f6,2736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2736);
	ctx.f6.f64 = double(temp.f32);
	// stfs f26,4196(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4196, temp.u32);
	// stfs f23,3336(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3336, temp.u32);
	// stfs f27,2792(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2792, temp.u32);
	// stfs f2,2624(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2624, temp.u32);
	// stfs f4,2632(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2632, temp.u32);
	// stfs f8,2608(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2608, temp.u32);
	// stfs f11,2664(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2664, temp.u32);
	// lfs f11,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f13,f11,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f11,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f11.f64 = double(temp.f32);
	// stfs f5,2736(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2736, temp.u32);
	// lfs f5,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f8,f8,f5,f7
	ctx.f8.f64 = double(float(-(ctx.f8.f64 * ctx.f5.f64 - ctx.f7.f64)));
	// lfs f7,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f7.f64 = double(temp.f32);
	// stfs f20,4160(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4160, temp.u32);
	// fmuls f5,f17,f10
	ctx.f5.f64 = double(float(ctx.f17.f64 * ctx.f10.f64));
	// lfs f20,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f20.f64 = double(temp.f32);
	// stfs f29,2752(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2752, temp.u32);
	// lfs f2,4612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4612);
	ctx.f2.f64 = double(temp.f32);
	// stfs f24,2616(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2616, temp.u32);
	// fmadds f0,f11,f0,f13
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64 + ctx.f13.f64));
	// lfs f11,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f11.f64 = double(temp.f32);
	// lfs f13,2084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2084);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f11,f11,f10,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f10.f64 + ctx.f9.f64));
	// fmuls f9,f13,f6
	ctx.f9.f64 = double(float(ctx.f13.f64 * ctx.f6.f64));
	// lfs f24,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f31,f18,f24,f31
	ctx.f31.f64 = double(float(ctx.f18.f64 * ctx.f24.f64 - ctx.f31.f64));
	// lfs f18,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f18.f64 = double(temp.f32);
	// lfs f26,2728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2728);
	ctx.f26.f64 = double(temp.f32);
	// lfs f16,2776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2776);
	ctx.f16.f64 = double(temp.f32);
	// lfs f23,1912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1912);
	ctx.f23.f64 = double(temp.f32);
	// lfs f19,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f19.f64 = double(temp.f32);
	// lfs f28,2856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2856);
	ctx.f28.f64 = double(temp.f32);
	// lfs f24,2640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2640);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f28,f1
	ctx.f25.f64 = double(float(ctx.f28.f64 * ctx.f1.f64));
	// fsubs f0,f0,f29
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f29.f64));
	// stfs f0,5536(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5536, temp.u32);
	// lfs f0,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f24,f13,f24
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f24.f64));
	// fmadds f6,f21,f0,f22
	ctx.f6.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 + ctx.f22.f64));
	// lfs f29,2784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2784);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f4,f14,f0
	ctx.f4.f64 = double(float(ctx.f14.f64 * ctx.f0.f64));
	// lfs f21,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f21.f64 = double(temp.f32);
	// fmr f0,f15
	ctx.f0.f64 = ctx.f15.f64;
	// lfs f14,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f14.f64 = double(temp.f32);
	// fadds f27,f2,f29
	ctx.f27.f64 = double(float(ctx.f2.f64 + ctx.f29.f64));
	// lfs f22,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f23,f23,f22
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// stfs f5,1248(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// lfs f5,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f5.f64 = double(temp.f32);
	// lfs f17,2600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2600);
	ctx.f17.f64 = double(temp.f32);
	// stfs f30,3392(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3392, temp.u32);
	// fmuls f6,f6,f3
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f3.f64));
	// fnmsubs f12,f7,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f7.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f8,f20,f0,f8
	ctx.f8.f64 = double(float(ctx.f20.f64 * ctx.f0.f64 + ctx.f8.f64));
	// lfs f7,2648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2648);
	ctx.f7.f64 = double(temp.f32);
	// fmr f20,f15
	ctx.f20.f64 = ctx.f15.f64;
	// fmadds f11,f21,f0,f11
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f0,4124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4124);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f27,f27,f13
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// stfs f27,660(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// fmuls f27,f16,f1
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f1.f64));
	// stfs f27,624(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// lfs f27,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f30,f3,f7
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// fmuls f27,f27,f14
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f14.f64));
	// lfs f21,4852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4852);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f21,f21,f19
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// fmuls f15,f3,f17
	ctx.f15.f64 = double(float(ctx.f3.f64 * ctx.f17.f64));
	// fmadds f12,f18,f20,f12
	ctx.f12.f64 = double(float(ctx.f18.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f20,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f20.f64 = double(temp.f32);
	// fadds f18,f0,f26
	ctx.f18.f64 = double(float(ctx.f0.f64 + ctx.f26.f64));
	// fmr f0,f20
	ctx.f0.f64 = ctx.f20.f64;
	// fmadds f9,f9,f20,f4
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f20.f64 + ctx.f4.f64));
	// lfs f4,5088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5088);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f19,f10,f4
	ctx.f19.f64 = double(float(ctx.f10.f64 * ctx.f4.f64));
	// stfs f19,636(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// lfs f19,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f19.f64 = double(temp.f32);
	// lfs f20,2720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2720);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f5,f5,f19,f31
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f19.f64 + ctx.f31.f64));
	// lfs f31,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f23,f31
	ctx.f31.f64 = double(float(ctx.f23.f64 * ctx.f31.f64));
	// fmr f14,f0
	ctx.f14.f64 = ctx.f0.f64;
	// fmuls f19,f20,f14
	ctx.f19.f64 = double(float(ctx.f20.f64 * ctx.f14.f64));
	// lfs f14,1560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1560);
	ctx.f14.f64 = double(temp.f32);
	// lfs f23,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f25,f0
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f0.f64));
	// fnmsubs f11,f14,f23,f11
	ctx.f11.f64 = double(float(-(ctx.f14.f64 * ctx.f23.f64 - ctx.f11.f64)));
	// stfs f25,548(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// lfs f25,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,1552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1552);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f8,f14,f25,f8
	ctx.f8.f64 = double(float(-(ctx.f14.f64 * ctx.f25.f64 - ctx.f8.f64)));
	// lfs f23,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f23.f64 = double(temp.f32);
	// lfs f14,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f14.f64 = double(temp.f32);
	// stfs f10,2856(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2856, temp.u32);
	// fnmsubs f12,f14,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f14.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// stfs f7,2720(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2720, temp.u32);
	// stfs f4,2600(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2600, temp.u32);
	// fmadds f4,f18,f0,f21
	ctx.f4.f64 = double(float(ctx.f18.f64 * ctx.f0.f64 + ctx.f21.f64));
	// lfs f23,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f0,f24,f0,f9
	ctx.f0.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f10,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f9,f27,f23,f5
	ctx.f9.f64 = double(float(-(ctx.f27.f64 * ctx.f23.f64 - ctx.f5.f64)));
	// lfs f7,2076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2076);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f11,f7,f10,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f10.f64 + ctx.f11.f64));
	// lfs f5,2068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2068);
	ctx.f5.f64 = double(temp.f32);
	// lfs f10,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f5,f10,f8
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f10.f64 - ctx.f8.f64)));
	// stfs f28,4624(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4624, temp.u32);
	// lfs f27,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,2592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2592);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f28,f28,f27,f19
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f27.f64 + ctx.f19.f64));
	// lfs f7,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f7.f64 = double(temp.f32);
	// lfs f27,1536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1536);
	ctx.f27.f64 = double(temp.f32);
	// fmr f5,f7
	ctx.f5.f64 = ctx.f7.f64;
	// lfs f8,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f8.f64 = double(temp.f32);
	// stfs f2,2784(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2784, temp.u32);
	// fmr f2,f8
	ctx.f2.f64 = ctx.f8.f64;
	// lfs f25,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f27,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f27,1528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1528);
	ctx.f27.f64 = double(temp.f32);
	// fmr f7,f23
	ctx.f7.f64 = ctx.f23.f64;
	// stfs f26,2656(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2656, temp.u32);
	// fnmsubs f10,f27,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f27.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// lfs f18,1520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1520);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f6,f30,f25,f6
	ctx.f6.f64 = double(float(ctx.f30.f64 * ctx.f25.f64 - ctx.f6.f64));
	// lfs f8,2052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2052);
	ctx.f8.f64 = double(temp.f32);
	// fmr f26,f25
	ctx.f26.f64 = ctx.f25.f64;
	// stfs f29,2640(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2640, temp.u32);
	// lfs f29,2712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2712);
	ctx.f29.f64 = double(temp.f32);
	// fadds f12,f8,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 + ctx.f12.f64));
	// stfs f17,2728(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2728, temp.u32);
	// fmuls f29,f3,f29
	ctx.f29.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// lfs f27,4116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4116);
	ctx.f27.f64 = double(temp.f32);
	// lfs f8,2704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2704);
	ctx.f8.f64 = double(temp.f32);
	// lfs f23,2584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2584);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f11,f18,f5,f11
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f5.f64 + ctx.f11.f64));
	// lfs f18,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f9,f31,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// lfs f7,4860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4860);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f10,f18,f2,f10
	ctx.f10.f64 = double(float(ctx.f18.f64 * ctx.f2.f64 + ctx.f10.f64));
	// lfs f31,4604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4604);
	ctx.f31.f64 = double(temp.f32);
	// lfs f5,4164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4164);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f24,f1,f7
	ctx.f24.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// lfs f2,2848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2848);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f6,f15,f26,f6
	ctx.f6.f64 = double(float(-(ctx.f15.f64 * ctx.f26.f64 - ctx.f6.f64)));
	// lfs f18,4036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4036);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f26,f13,f27
	ctx.f26.f64 = double(float(ctx.f13.f64 * ctx.f27.f64));
	// lfs f17,2528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2528);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f3,f3,f23
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f23.f64));
	// stfs f22,3256(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3256, temp.u32);
	// fmuls f22,f13,f31
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f31.f64));
	// stfs f20,2648(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2648, temp.u32);
	// fmuls f20,f1,f8
	ctx.f20.f64 = double(float(ctx.f1.f64 * ctx.f8.f64));
	// stfs f16,4140(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4140, temp.u32);
	// fmuls f5,f1,f5
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfs f30,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f25,460(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 460);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f18,f13,f18
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f18.f64));
	// lfs f21,792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 792);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f17,f13,f17
	ctx.f17.f64 = double(float(ctx.f13.f64 * ctx.f17.f64));
	// lfs f19,2044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2044);
	ctx.f19.f64 = double(temp.f32);
	// lfs f16,2696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2696);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,2688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2688);
	ctx.f14.f64 = double(temp.f32);
	// stfs f3,820(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// fmuls f16,f25,f16
	ctx.f16.f64 = double(float(ctx.f25.f64 * ctx.f16.f64));
	// lfs f3,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f9,f28,f1,f9
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f1.f64 + ctx.f9.f64));
	// stfs f17,624(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// stfs f24,1248(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// lfs f17,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f17.f64 = double(temp.f32);
	// lfs f24,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f3,f3,f15,f17
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f15.f64 - ctx.f17.f64));
	// fnmsubs f11,f24,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f24.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f17,5000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5000);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f15,f1,f14
	ctx.f15.f64 = double(float(ctx.f1.f64 * ctx.f14.f64));
	// lfs f14,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f14.f64 = double(temp.f32);
	// lfs f24,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f4,f17,f14,f4
	ctx.f4.f64 = double(float(ctx.f17.f64 * ctx.f14.f64 + ctx.f4.f64));
	// fmuls f24,f24,f21
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f21.f64));
	// lfs f17,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f6,f29,f17,f6
	ctx.f6.f64 = double(float(ctx.f29.f64 * ctx.f17.f64 + ctx.f6.f64));
	// lfs f30,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f30.f64 = double(temp.f32);
	// lfs f14,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f14.f64 = double(temp.f32);
	// stfs f25,4360(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4360, temp.u32);
	// fmadds f0,f14,f30,f0
	ctx.f0.f64 = double(float(ctx.f14.f64 * ctx.f30.f64 + ctx.f0.f64));
	// lfs f25,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f25.f64 = double(temp.f32);
	// stfs f20,812(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// stfs f15,548(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// fadds f11,f11,f10
	ctx.f11.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// stfs f11,5544(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5544, temp.u32);
	// lfs f15,2036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2036);
	ctx.f15.f64 = double(temp.f32);
	// lfs f30,252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	ctx.f30.f64 = double(temp.f32);
	// lfs f20,668(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f24,f25
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// lfs f11,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f20,f15,f20
	ctx.f20.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// stfs f12,5540(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5540, temp.u32);
	// fmadds f11,f16,f11,f3
	ctx.f11.f64 = double(float(ctx.f16.f64 * ctx.f11.f64 + ctx.f3.f64));
	// lfs f24,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f12,f30,f19
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f19.f64));
	// stfs f23,2712(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2712, temp.u32);
	// fnmsubs f6,f26,f24,f6
	ctx.f6.f64 = double(float(-(ctx.f26.f64 * ctx.f24.f64 - ctx.f6.f64)));
	// lfs f23,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f23.f64 = double(temp.f32);
	// fmr f24,f23
	ctx.f24.f64 = ctx.f23.f64;
	// lfs f28,2020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2020);
	ctx.f28.f64 = double(temp.f32);
	// lfs f14,2028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2028);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f9,f5,f23,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f23.f64 - ctx.f9.f64)));
	// fadds f10,f28,f14
	ctx.f10.f64 = double(float(ctx.f28.f64 + ctx.f14.f64));
	// stfs f8,2696(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2696, temp.u32);
	// lfs f8,412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	ctx.f8.f64 = double(temp.f32);
	// lfs f5,2840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2840);
	ctx.f5.f64 = double(temp.f32);
	// lfs f26,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f11,f20,f26,f11
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f26.f64 - ctx.f11.f64)));
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f12,f8,f5,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f5.f64 - ctx.f12.f64));
	// lfs f5,2832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2832);
	ctx.f5.f64 = double(temp.f32);
	// stfs f21,2592(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2592, temp.u32);
	// fnmsubs f0,f22,f26,f0
	ctx.f0.f64 = double(float(-(ctx.f22.f64 * ctx.f26.f64 - ctx.f0.f64)));
	// lfs f21,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f21.f64 = double(temp.f32);
	// stfs f27,2848(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2848, temp.u32);
	// fmuls f5,f5,f24
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f24.f64));
	// stfs f31,2776(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2776, temp.u32);
	// fmadds f9,f2,f21,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f21.f64 + ctx.f9.f64));
	// stfs f7,2704(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2704, temp.u32);
	// fmuls f10,f10,f30
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f30.f64));
	// lfs f7,3504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3504);
	ctx.f7.f64 = double(temp.f32);
	// lfs f3,3512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3512);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,2960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2960);
	ctx.f31.f64 = double(temp.f32);
	// fadds f29,f3,f7
	ctx.f29.f64 = double(float(ctx.f3.f64 + ctx.f7.f64));
	// lfs f28,928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 928);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f31,f8,f31
	ctx.f31.f64 = double(float(ctx.f8.f64 * ctx.f31.f64));
	// lfs f27,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,2576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2576);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f28,f27
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// lfs f24,2824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2824);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f23,f13,f26
	ctx.f23.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// lfs f22,2768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2768);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f8,f24
	ctx.f24.f64 = double(float(ctx.f8.f64 * ctx.f24.f64));
	// lfs f20,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f22,f1
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f1.f64));
	// lfs f2,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f6,f18,f20,f6
	ctx.f6.f64 = double(float(-(ctx.f18.f64 * ctx.f20.f64 - ctx.f6.f64)));
	// stfs f15,2688(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2688, temp.u32);
	// stfs f19,1528(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1528, temp.u32);
	// fnmsubs f12,f31,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f31,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f31.f64 = double(temp.f32);
	// fmr f2,f20
	ctx.f2.f64 = ctx.f20.f64;
	// lfs f18,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f5,f29,f31,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f31.f64 + ctx.f5.f64));
	// lfs f20,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f9,f18,f20,f9
	ctx.f9.f64 = double(float(-(ctx.f18.f64 * ctx.f20.f64 - ctx.f9.f64)));
	// lfs f20,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f0,f4,f13,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f13.f64 + ctx.f0.f64));
	// lfs f4,1664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1664);
	ctx.f4.f64 = double(temp.f32);
	// lfs f19,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f6,f18,f19,f6
	ctx.f6.f64 = double(float(-(ctx.f18.f64 * ctx.f19.f64 - ctx.f6.f64)));
	// lfs f21,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f21.f64 = double(temp.f32);
	// lfs f29,2568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2568);
	ctx.f29.f64 = double(temp.f32);
	// fmsubs f10,f27,f21,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f21.f64 - ctx.f10.f64));
	// lfs f19,2760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2760);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f29,f13,f29
	ctx.f29.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// fnmsubs f12,f24,f20,f12
	ctx.f12.f64 = double(float(-(ctx.f24.f64 * ctx.f20.f64 - ctx.f12.f64)));
	// lfs f24,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f11,f25,f2,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f2.f64 - ctx.f11.f64)));
	// stfs f13,2760(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2760, temp.u32);
	// lfs f13,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f13.f64 = double(temp.f32);
	// lfs f20,2944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2944);
	ctx.f20.f64 = double(temp.f32);
	// stfs f7,2944(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2944, temp.u32);
	// lfs f7,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f7.f64 = double(temp.f32);
	// lfs f2,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,2952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2952);
	ctx.f31.f64 = double(temp.f32);
	// stfs f4,2824(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2824, temp.u32);
	// fmuls f31,f2,f31
	ctx.f31.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fmadds f10,f8,f19,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f19.f64 + ctx.f10.f64));
	// stfs f2,2068(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2068, temp.u32);
	// lfs f27,3156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3156);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f11,f22,f24,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f24.f64 - ctx.f11.f64)));
	// lfs f24,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f24.f64 = double(temp.f32);
	// fmadds f5,f4,f24,f5
	ctx.f5.f64 = double(float(ctx.f4.f64 * ctx.f24.f64 + ctx.f5.f64));
	// lfs f4,2936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2936);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f22,f30,f20
	ctx.f22.f64 = double(float(ctx.f30.f64 * ctx.f20.f64));
	// lfs f20,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f0,f23,f20,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f23,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f23.f64 = double(temp.f32);
	// lfs f25,2680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2680);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f21,f1,f27
	ctx.f21.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f24,4252(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4252);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f25,f1
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f1.f64));
	// lfs f2,4560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4560);
	ctx.f2.f64 = double(temp.f32);
	// lfs f19,4792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4792);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f10,f31,f23,f10
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f23.f64 + ctx.f10.f64));
	// lfs f31,680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	ctx.f31.f64 = double(temp.f32);
	// lfs f23,5036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5036);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,3304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3304);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f23,f8,f23
	ctx.f23.f64 = double(float(ctx.f8.f64 * ctx.f23.f64));
	// fmadds f11,f5,f1,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f11.f64));
	// lfs f5,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f13,f5,f13,f6
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f13.f64 + ctx.f6.f64));
	// lfs f6,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f9,f6,f7,f9
	ctx.f9.f64 = double(float(-(ctx.f6.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// lfs f6,944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 944);
	ctx.f6.f64 = double(temp.f32);
	// fmr f7,f20
	ctx.f7.f64 = ctx.f20.f64;
	// lfs f5,948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f12,f6,f4,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f4,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f4.f64 = double(temp.f32);
	// lfs f20,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f20.f64 = double(temp.f32);
	// stfs f3,2840(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2840, temp.u32);
	// fmuls f3,f24,f1
	ctx.f3.f64 = double(float(ctx.f24.f64 * ctx.f1.f64));
	// stfs f26,2832(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2832, temp.u32);
	// fmuls f26,f2,f5
	ctx.f26.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// lfs f16,652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 652);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f20,f28,f20
	ctx.f20.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lfs f14,672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 672);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f13,f29,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f29,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f9,f29,f4,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f4.f64 + ctx.f9.f64));
	// lfs f4,1672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1672);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f17,f6,f4
	ctx.f17.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f29,940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 940);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f31,f7
	ctx.f7.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// stfs f17,1248(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// fmuls f15,f19,f29
	ctx.f15.f64 = double(float(ctx.f19.f64 * ctx.f29.f64));
	// fnmsubs f12,f30,f18,f12
	ctx.f12.f64 = double(float(-(ctx.f30.f64 * ctx.f18.f64 - ctx.f12.f64)));
	// stfs f28,3504(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3504, temp.u32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// lfs f28,932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 932);
	ctx.f28.f64 = double(temp.f32);
	// stfs f15,1664(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1664, temp.u32);
	// fmuls f15,f30,f28
	ctx.f15.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// stfs f15,908(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// lfs f15,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f15.f64 = double(temp.f32);
	// lfs f17,1680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1680);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f15,f21,f15
	ctx.f15.f64 = double(float(ctx.f21.f64 * ctx.f15.f64));
	// stfs f3,3512(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3512, temp.u32);
	// fmuls f3,f1,f17
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f17.f64));
	// stfs f15,1672(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1672, temp.u32);
	// stfs f11,1680(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1680, temp.u32);
	// stfs f3,624(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// lfs f3,4244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4244);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f3,f30,f3
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f3.f64));
	// lfs f11,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f22,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// lfs f10,4616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4616);
	ctx.f10.f64 = double(temp.f32);
	// lfs f13,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f30,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// stfs f0,5548(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5548, temp.u32);
	// fmuls f13,f27,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f13.f64));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// stfs f10,636(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// lfs f10,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f10.f64 = double(temp.f32);
	// lfs f27,2560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2560);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f10,f25,f10,f9
	ctx.f10.f64 = double(float(-(ctx.f25.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// lfs f15,3028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3028);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f27,f30,f27
	ctx.f27.f64 = double(float(ctx.f30.f64 * ctx.f27.f64));
	// stfs f7,916(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// fmuls f22,f15,f16
	ctx.f22.f64 = double(float(ctx.f15.f64 * ctx.f16.f64));
	// fmadds f12,f3,f0,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f7,3424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3424);
	ctx.f7.f64 = double(temp.f32);
	// lfs f9,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f11,f6,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f25,2100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2100);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f9,f21,f9
	ctx.f9.f64 = double(float(ctx.f21.f64 * ctx.f9.f64));
	// stfs f27,548(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// lfs f15,2604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2604);
	ctx.f15.f64 = double(temp.f32);
	// lfs f27,5096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5096);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f15,f15,f8
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f8.f64));
	// stfs f29,2952(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2952, temp.u32);
	// fmuls f27,f31,f27
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// stfs f23,2100(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2100, temp.u32);
	// lfs f0,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f0.f64 = double(temp.f32);
	// lfs f23,1680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1680);
	ctx.f23.f64 = double(temp.f32);
	// lfs f7,876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	ctx.f7.f64 = double(temp.f32);
	// lfs f29,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f29.f64 = double(temp.f32);
	// lfs f21,1672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1672);
	ctx.f21.f64 = double(temp.f32);
	// stfs f5,2968(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2968, temp.u32);
	// fnmsubs f29,f21,f29,f23
	ctx.f29.f64 = double(float(-(ctx.f21.f64 * ctx.f29.f64 - ctx.f23.f64)));
	// stfs f4,1560(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1560, temp.u32);
	// fmuls f4,f0,f7
	ctx.f4.f64 = double(float(ctx.f0.f64 * ctx.f7.f64));
	// lfs f5,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f5.f64 = double(temp.f32);
	// stfs f27,820(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// fmuls f13,f13,f5
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f5.f64));
	// stfs f15,660(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// stfs f31,4560(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4560, temp.u32);
	// lfs f15,1736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1736);
	ctx.f15.f64 = double(temp.f32);
	// lfs f27,3564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3564);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f26,f14,f15,f26
	ctx.f26.f64 = double(float(ctx.f14.f64 * ctx.f15.f64 + ctx.f26.f64));
	// lfs f0,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f27,f8,f27
	ctx.f27.f64 = double(float(ctx.f8.f64 * ctx.f27.f64));
	// lfs f3,3672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3672);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,3144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3144);
	ctx.f31.f64 = double(temp.f32);
	// lfs f23,1664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1664);
	ctx.f23.f64 = double(temp.f32);
	// stfs f24,2680(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2680, temp.u32);
	// fnmsubs f12,f23,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// stfs f2,2960(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2960, temp.u32);
	// fmuls f2,f1,f25
	ctx.f2.f64 = double(float(ctx.f1.f64 * ctx.f25.f64));
	// stfs f28,2052(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2052, temp.u32);
	// fmuls f28,f3,f31
	ctx.f28.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f5,900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 900);
	ctx.f5.f64 = double(temp.f32);
	// lfs f24,372(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	ctx.f24.f64 = double(temp.f32);
	// stfs f19,3304(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3304, temp.u32);
	// stfs f18,2020(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2020, temp.u32);
	// stfs f16,2076(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2076, temp.u32);
	// stfs f20,812(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// stfs f14,2044(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2044, temp.u32);
	// stfs f17,2768(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2768, temp.u32);
	// fnmsubs f11,f22,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// fmsubs f13,f4,f0,f13
	ctx.f13.f64 = double(float(ctx.f4.f64 * ctx.f0.f64 - ctx.f13.f64));
	// lfs f21,3856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3856);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f0,f21,f0
	ctx.f0.f64 = double(float(ctx.f21.f64 * ctx.f0.f64));
	// lfs f4,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f24,f24,f5
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f5.f64));
	// fadds f4,f20,f4
	ctx.f4.f64 = double(float(ctx.f20.f64 + ctx.f4.f64));
	// lfs f23,1044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	ctx.f23.f64 = double(temp.f32);
	// lfs f21,3416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3416);
	ctx.f21.f64 = double(temp.f32);
	// lfs f22,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f22.f64 = double(temp.f32);
	// lfs f20,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f22,f22,f23
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f19,2108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2108);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f20,f20,f21
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f21.f64));
	// stfs f27,2108(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2108, temp.u32);
	// fmuls f17,f1,f19
	ctx.f17.f64 = double(float(ctx.f1.f64 * ctx.f19.f64));
	// stfs f21,1032(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// stfs f23,1680(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1680, temp.u32);
	// lfs f27,3512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3512);
	ctx.f27.f64 = double(temp.f32);
	// lfs f21,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,2116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2116);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f10,f27,f21,f10
	ctx.f10.f64 = double(float(ctx.f27.f64 * ctx.f21.f64 + ctx.f10.f64));
	// fmuls f23,f30,f23
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// stfs f10,2116(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2116, temp.u32);
	// stfs f23,1664(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1664, temp.u32);
	// lfs f21,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,3504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3504);
	ctx.f23.f64 = double(temp.f32);
	// lfs f27,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f27.f64 = double(temp.f32);
	// lfs f10,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f27,f23,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f27.f64));
	// fmuls f10,f10,f21
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f21.f64));
	// stfs f27,3512(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3512, temp.u32);
	// stfs f10,1672(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1672, temp.u32);
	// lfs f27,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f27.f64 = double(temp.f32);
	// lfs f10,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f10,f27,f29
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f27.f64 - ctx.f29.f64)));
	// lfs f21,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f21.f64 = double(temp.f32);
	// lfs f29,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f29.f64 = double(temp.f32);
	// lfs f27,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f11,f26,f29,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f29.f64 + ctx.f11.f64));
	// fnmsubs f12,f27,f21,f12
	ctx.f12.f64 = double(float(-(ctx.f27.f64 * ctx.f21.f64 - ctx.f12.f64)));
	// lfs f15,2012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2012);
	ctx.f15.f64 = double(temp.f32);
	// lfs f26,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f4,f4,f15,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f15.f64 + ctx.f20.f64));
	// lfs f18,1088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1088);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f13,f24,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// lfs f21,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,4396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4396);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f24,f21,f18,f22
	ctx.f24.f64 = double(float(ctx.f21.f64 * ctx.f18.f64 + ctx.f22.f64));
	// lfs f26,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f26.f64 = double(temp.f32);
	// lfs f20,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f20.f64 = double(temp.f32);
	// fmsubs f0,f16,f26,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f26.f64 - ctx.f0.f64));
	// lfs f22,3408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3408);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f10,f9,f20,f10
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f20.f64 + ctx.f10.f64));
	// stfs f7,3408(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3408, temp.u32);
	// stfs f5,3424(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3424, temp.u32);
	// lfs f14,3112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3112);
	ctx.f14.f64 = double(temp.f32);
	// lfs f27,1184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1184);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f29,f27
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f27.f64));
	// lfs f21,3400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3400);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f26,f22
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f22.f64));
	// lfs f7,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f28,f14,f21,f28
	ctx.f28.f64 = double(float(ctx.f14.f64 * ctx.f21.f64 + ctx.f28.f64));
	// lfs f9,2552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2552);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f6,f9,f11
	ctx.f11.f64 = double(float(ctx.f6.f64 * ctx.f9.f64 + ctx.f11.f64));
	// stfs f25,3856(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3856, temp.u32);
	// fmadds f12,f5,f7,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f7.f64 + ctx.f12.f64));
	// stfs f1,2036(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2036, temp.u32);
	// stfs f19,3112(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3112, temp.u32);
	// stfs f23,2084(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2084, temp.u32);
	// stfs f3,1552(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1552, temp.u32);
	// stfs f31,1152(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// stfs f18,1160(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// stfs f16,2936(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2936, temp.u32);
	// stfs f15,1168(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// stfs f14,3144(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3144, temp.u32);
	// lfs f5,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f5.f64 = double(temp.f32);
	// lfs f9,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f24,f5,f13
	ctx.f13.f64 = double(float(ctx.f24.f64 * ctx.f5.f64 + ctx.f13.f64));
	// lfs f3,2544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2544);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f10,f2,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f5,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f5.f64 = double(temp.f32);
	// lfs f31,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f0,f3,f5,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f5.f64 + ctx.f0.f64));
	// fmuls f3,f28,f31
	ctx.f3.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f31,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f11,f2,f31,f11
	ctx.f11.f64 = double(float(-(ctx.f2.f64 * ctx.f31.f64 - ctx.f11.f64)));
	// lfs f7,3392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3392);
	ctx.f7.f64 = double(temp.f32);
	// lfs f28,3088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3088);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f1,f26,f7
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// lfs f5,4564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4564);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f4,f7
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f7.f64));
	// lfs f31,3096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3096);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f12,f8,f5,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f5.f64 - ctx.f12.f64)));
	// fadds f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 + ctx.f31.f64));
	// lfs f9,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f9.f64 = double(temp.f32);
	// lfs f26,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f10,f17,f9,f10
	ctx.f10.f64 = double(float(ctx.f17.f64 * ctx.f9.f64 + ctx.f10.f64));
	// fnmsubs f13,f29,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// lfs f5,3848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3848);
	ctx.f5.f64 = double(temp.f32);
	// lfs f28,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f0,f5,f28,f0
	ctx.f0.f64 = double(float(-(ctx.f5.f64 * ctx.f28.f64 - ctx.f0.f64)));
	// lfs f2,3384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3384);
	ctx.f2.f64 = double(temp.f32);
	// lfs f29,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f29.f64 = double(temp.f32);
	// lfs f17,3072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3072);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f4,f4,f29,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f29.f64 + ctx.f3.f64));
	// lfs f26,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f12,f30,f2,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f2.f64 + ctx.f12.f64));
	// lfs f16,4028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4028);
	ctx.f16.f64 = double(temp.f32);
	// fadds f31,f31,f17
	ctx.f31.f64 = double(float(ctx.f31.f64 + ctx.f17.f64));
	// lfs f17,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,2116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2116);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f11,f26,f28,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f28.f64 + ctx.f11.f64));
	// fadds f10,f15,f10
	ctx.f10.f64 = double(float(ctx.f15.f64 + ctx.f10.f64));
	// lfs f15,2520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2520);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f13,f16,f17,f13
	ctx.f13.f64 = double(float(ctx.f16.f64 * ctx.f17.f64 + ctx.f13.f64));
	// lfs f17,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f17.f64 = double(temp.f32);
	// lfs f26,2928(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2928);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f0,f15,f17,f0
	ctx.f0.f64 = double(float(-(ctx.f15.f64 * ctx.f17.f64 - ctx.f0.f64)));
	// lfs f17,2100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2100);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f15,f26,f7
	ctx.f15.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// stfs f15,2100(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2100, temp.u32);
	// lfs f15,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f15.f64 = double(temp.f32);
	// lfs f14,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f4,f1,f14,f4
	ctx.f4.f64 = double(float(-(ctx.f1.f64 * ctx.f14.f64 - ctx.f4.f64)));
	// lfs f1,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f17,f15,f12
	ctx.f12.f64 = double(float(ctx.f17.f64 * ctx.f15.f64 + ctx.f12.f64));
	// lfs f17,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f17.f64 = double(temp.f32);
	// lfs f29,3104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3104);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f1,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// lfs f3,3528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3528);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f20,f29,f7
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f7.f64));
	// lfs f9,3536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3536);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f25,f3,f7
	ctx.f25.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// lfs f28,3080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3080);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f23,f9,f7
	ctx.f23.f64 = double(float(ctx.f9.f64 * ctx.f7.f64));
	// lfs f24,3376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3376);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f16,f28,f7
	ctx.f16.f64 = double(float(ctx.f28.f64 * ctx.f7.f64));
	// lfs f21,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f21,f24
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f24.f64));
	// lfs f18,4628(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4628);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f19,f22
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f22.f64));
	// stfs f10,5552(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5552, temp.u32);
	// fmuls f18,f18,f7
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f7.f64));
	// lfs f15,4828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4828);
	ctx.f15.f64 = double(temp.f32);
	// lfs f1,3064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3064);
	ctx.f1.f64 = double(temp.f32);
	// lfs f10,664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 664);
	ctx.f10.f64 = double(temp.f32);
	// stfs f3,3104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3104, temp.u32);
	// stfs f5,3384(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3384, temp.u32);
	// stfs f9,3848(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3848, temp.u32);
	// stfs f2,1520(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1520, temp.u32);
	// stfs f29,3088(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3088, temp.u32);
	// stfs f28,3080(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3080, temp.u32);
	// stfs f26,3096(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3096, temp.u32);
	// lfs f5,3368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3368);
	ctx.f5.f64 = double(temp.f32);
	// fadds f9,f15,f1
	ctx.f9.f64 = double(float(ctx.f15.f64 + ctx.f1.f64));
	// fmadds f12,f5,f10,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f10.f64 + ctx.f12.f64));
	// lfs f5,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f21,f7
	ctx.f3.f64 = double(float(ctx.f21.f64 * ctx.f7.f64));
	// fmadds f11,f5,f2,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f2.f64 + ctx.f11.f64));
	// lfs f2,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f2.f64 = double(temp.f32);
	// lfs f14,2108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2108);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f1,f19,f7
	ctx.f1.f64 = double(float(ctx.f19.f64 * ctx.f7.f64));
	// lfs f29,4992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4992);
	ctx.f29.f64 = double(temp.f32);
	// fmr f15,f2
	ctx.f15.f64 = ctx.f2.f64;
	// stfs f1,660(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// stfs f3,636(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// lfs f1,3844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3844);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f13,f1,f3,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 + ctx.f13.f64));
	// stfs f22,3536(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3536, temp.u32);
	// fnmsubs f12,f6,f29,f12
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f29.f64 - ctx.f12.f64)));
	// lfs f28,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f28.f64 = double(temp.f32);
	// lfs f3,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f11,f14,f2,f11
	ctx.f11.f64 = double(float(-(ctx.f14.f64 * ctx.f2.f64 - ctx.f11.f64)));
	// lfs f22,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f22.f64 = double(temp.f32);
	// stfs f16,2108(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2108, temp.u32);
	// fnmsubs f4,f25,f3,f4
	ctx.f4.f64 = double(float(-(ctx.f25.f64 * ctx.f3.f64 - ctx.f4.f64)));
	// lfs f16,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,4984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4984);
	ctx.f14.f64 = double(temp.f32);
	// stfs f20,3528(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3528, temp.u32);
	// fnmsubs f0,f14,f16,f0
	ctx.f0.f64 = double(float(-(ctx.f14.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// lfs f3,2124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2124);
	ctx.f3.f64 = double(temp.f32);
	// lfs f20,2272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2272);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f3,f7
	ctx.f25.f64 = double(float(ctx.f3.f64 * ctx.f7.f64));
	// lfs f5,3360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3360);
	ctx.f5.f64 = double(temp.f32);
	// stfs f7,2116(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2116, temp.u32);
	// fmadds f12,f30,f5,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f5.f64 + ctx.f12.f64));
	// stfs f18,2124(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2124, temp.u32);
	// fnmsubs f11,f22,f28,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f11.f64)));
	// lfs f22,2132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2132);
	ctx.f22.f64 = double(temp.f32);
	// stfs f31,2132(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2132, temp.u32);
	// fmuls f31,f20,f7
	ctx.f31.f64 = double(float(ctx.f20.f64 * ctx.f7.f64));
	// fmuls f7,f22,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64));
	// stfs f7,812(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// lfs f7,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f7.f64 = double(temp.f32);
	// stfs f31,3504(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3504, temp.u32);
	// fmuls f7,f7,f24
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f24.f64));
	// lfs f16,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f16.f64 = double(temp.f32);
	// lfs f28,484(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 484);
	ctx.f28.f64 = double(temp.f32);
	// lfs f18,1680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1680);
	ctx.f18.f64 = double(temp.f32);
	// lfs f31,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// fmuls f31,f28,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// stfs f7,1680(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1680, temp.u32);
	// lfs f16,4904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4904);
	ctx.f16.f64 = double(temp.f32);
	// lfs f7,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f7.f64 = double(temp.f32);
	// stfs f31,820(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// lfs f31,2248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2248);
	ctx.f31.f64 = double(temp.f32);
	// lfs f24,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f24.f64 = double(temp.f32);
	// stfs f9,1248(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// fnmsubs f9,f16,f7,f0
	ctx.f9.f64 = double(float(-(ctx.f16.f64 * ctx.f7.f64 - ctx.f0.f64)));
	// lfs f0,1672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1672);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f31,f24,f13
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f24.f64 + ctx.f13.f64));
	// lfs f21,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f21.f64 = double(temp.f32);
	// lfs f1,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f0,f0,f21,f11
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f21.f64 - ctx.f11.f64)));
	// lfs f26,3352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3352);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f1,f1,f27
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f27.f64));
	// lfs f31,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f26,f10,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f10.f64 + ctx.f12.f64));
	// lfs f17,3344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3344);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f11,f23,f31,f4
	ctx.f11.f64 = double(float(-(ctx.f23.f64 * ctx.f31.f64 - ctx.f4.f64)));
	// lfs f2,3336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3336);
	ctx.f2.f64 = double(temp.f32);
	// lfs f19,4020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4020);
	ctx.f19.f64 = double(temp.f32);
	// lfs f29,952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 952);
	ctx.f29.f64 = double(temp.f32);
	// stfs f1,1672(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1672, temp.u32);
	// stfs f27,916(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// stfs f20,3336(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3336, temp.u32);
	// stfs f25,624(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// stfs f3,3344(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3344, temp.u32);
	// stfs f18,548(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// stfs f28,3400(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3400, temp.u32);
	// stfs f22,3352(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3352, temp.u32);
	// fnmsubs f12,f8,f19,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f19.f64 - ctx.f12.f64)));
	// lfs f7,1664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1664);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f0,f7,f29,f0
	ctx.f0.f64 = double(float(ctx.f7.f64 * ctx.f29.f64 + ctx.f0.f64));
	// stfs f10,1536(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1536, temp.u32);
	// stfs f5,2028(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2028, temp.u32);
	// lfs f7,4784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4784);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f13,f7,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// lfs f4,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,2132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2132);
	ctx.f5.f64 = double(temp.f32);
	// stfs f6,3416(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3416, temp.u32);
	// fmadds f9,f5,f4,f9
	ctx.f9.f64 = double(float(ctx.f5.f64 * ctx.f4.f64 + ctx.f9.f64));
	// lfs f7,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f7.f64 = double(temp.f32);
	// lfs f5,2124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2124);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f12,f17,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f10,3512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3512);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f0,f10,f15,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 * ctx.f15.f64 + ctx.f0.f64));
	// lfs f10,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f11,f5,f7,f11
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f7.f64 - ctx.f11.f64)));
	// lfs f4,1176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1176);
	ctx.f4.f64 = double(temp.f32);
	// lfs f5,3544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3544);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,4200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4200);
	ctx.f3.f64 = double(temp.f32);
	// stfs f30,908(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// stfs f8,2012(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2012, temp.u32);
	// lfs f7,3296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3296);
	ctx.f7.f64 = double(temp.f32);
	// lfs f30,1996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1996);
	ctx.f30.f64 = double(temp.f32);
	// lfs f29,3056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3056);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f30,f30,f7
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f7.f64));
	// fmadds f12,f6,f2,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f2.f64 + ctx.f12.f64));
	// lfs f6,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f10,f10,f6
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f26,3536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3536);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f2,f5,f4
	ctx.f2.f64 = double(float(ctx.f5.f64 * ctx.f4.f64));
	// lfs f27,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f29,f7
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f7.f64));
	// fmuls f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f19,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f19.f64 = double(temp.f32);
	// lfs f25,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f25.f64 = double(temp.f32);
	// lfs f18,972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	ctx.f18.f64 = double(temp.f32);
	// lfs f1,4080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4080);
	ctx.f1.f64 = double(temp.f32);
	// lfs f24,4072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4072);
	ctx.f24.f64 = double(temp.f32);
	// lfs f31,2004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2004);
	ctx.f31.f64 = double(temp.f32);
	// fadds f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f12.f64));
	// stfs f0,5556(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5556, temp.u32);
	// lfs f0,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f26,f27,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f27.f64 + ctx.f10.f64));
	// lfs f12,4104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4104);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f2,f2,f19
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f19.f64));
	// fmuls f8,f12,f0
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f27,4744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4744);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f0,f3,f0
	ctx.f0.f64 = double(float(ctx.f3.f64 * ctx.f0.f64));
	// lfs f26,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f13,f27,f26,f13
	ctx.f13.f64 = double(float(-(ctx.f27.f64 * ctx.f26.f64 - ctx.f13.f64)));
	// lfs f27,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,3528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3528);
	ctx.f26.f64 = double(temp.f32);
	// lfs f19,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f11,f26,f27,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f27.f64 - ctx.f11.f64)));
	// fmuls f30,f30,f19
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f19.f64));
	// lfs f20,2116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2116);
	ctx.f20.f64 = double(temp.f32);
	// lfs f27,3552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3552);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f28,f20
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lfs f26,3048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3048);
	ctx.f26.f64 = double(temp.f32);
	// lfs f22,3560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3560);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f23,f26,f7
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// lfs f19,796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f10,f10,f20
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// fmadds f8,f1,f25,f8
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f25.f64 + ctx.f8.f64));
	// lfs f25,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f0,f24,f18,f0
	ctx.f0.f64 = double(float(ctx.f24.f64 * ctx.f18.f64 + ctx.f0.f64));
	// lfs f18,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f18.f64 = double(temp.f32);
	// stfs f17,1664(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1664, temp.u32);
	// fmadds f9,f31,f25,f9
	ctx.f9.f64 = double(float(ctx.f31.f64 * ctx.f25.f64 + ctx.f9.f64));
	// fmuls f29,f29,f18
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f18.f64));
	// lfs f21,4736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4736);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,4064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4064);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f25,f27,f4
	ctx.f25.f64 = double(float(ctx.f27.f64 * ctx.f4.f64));
	// lfs f17,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f19,f22,f19
	ctx.f19.f64 = double(float(ctx.f22.f64 * ctx.f19.f64));
	// stfs f14,3368(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3368, temp.u32);
	// stfs f16,3376(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3376, temp.u32);
	// lfs f14,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f8,f18,f17,f8
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f17.f64 + ctx.f8.f64));
	// lfs f15,4896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4896);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f13,f15,f14,f13
	ctx.f13.f64 = double(float(-(ctx.f15.f64 * ctx.f14.f64 - ctx.f13.f64)));
	// stfs f12,1996(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1996, temp.u32);
	// lfs f15,804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 804);
	ctx.f15.f64 = double(temp.f32);
	// lfs f12,2108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2108);
	ctx.f12.f64 = double(temp.f32);
	// stfs f6,3560(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3560, temp.u32);
	// fnmsubs f12,f12,f15,f11
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f15.f64 - ctx.f11.f64)));
	// lfs f6,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f6.f64 = double(temp.f32);
	// stfs f5,4200(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4200, temp.u32);
	// fnmsubs f9,f21,f6,f9
	ctx.f9.f64 = double(float(-(ctx.f21.f64 * ctx.f6.f64 - ctx.f9.f64)));
	// lfs f11,2296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2296);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f5.f64 = double(temp.f32);
	// stfs f3,4104(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4104, temp.u32);
	// lfs f3,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f13,f11,f5,f13
	ctx.f13.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 + ctx.f13.f64));
	// lfs f11,4048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4048);
	ctx.f11.f64 = double(temp.f32);
	// lfs f5,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f0,f11,f5,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f5.f64 + ctx.f0.f64));
	// lfs f11,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f5,f25,f3,f30
	ctx.f5.f64 = double(float(ctx.f25.f64 * ctx.f3.f64 - ctx.f30.f64));
	// lfs f16,4056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4056);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f11,f10,f11,f2
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64 + ctx.f2.f64));
	// lfs f3,2956(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2956);
	ctx.f3.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f17,f16,f7
	ctx.f17.f64 = double(float(ctx.f16.f64 * ctx.f7.f64));
	// lfs f16,3192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3192);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f10,f3,f10,f9
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f10.f64 - ctx.f9.f64)));
	// lfs f18,1136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1136);
	ctx.f18.f64 = double(temp.f32);
	// lfs f2,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// lfs f3,2100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2100);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f12,f3,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// stfs f31,1032(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1032, temp.u32);
	// fmr f3,f14
	ctx.f3.f64 = ctx.f14.f64;
	// lfs f31,4192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4192);
	ctx.f31.f64 = double(temp.f32);
	// stfs f26,4080(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4080, temp.u32);
	// fmuls f26,f31,f20
	ctx.f26.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// lfs f9,3516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3516);
	ctx.f9.f64 = double(temp.f32);
	// lfs f14,2140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2140);
	ctx.f14.f64 = double(temp.f32);
	// stfs f24,3192(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3192, temp.u32);
	// lfs f24,3584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3584);
	ctx.f24.f64 = double(temp.f32);
	// stfs f26,3552(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3552, temp.u32);
	// fmuls f26,f14,f4
	ctx.f26.f64 = double(float(ctx.f14.f64 * ctx.f4.f64));
	// fmuls f6,f18,f4
	ctx.f6.f64 = double(float(ctx.f18.f64 * ctx.f4.f64));
	// stfs f6,2132(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2132, temp.u32);
	// lfs f6,2148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2148);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f18,f24,f4
	ctx.f18.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// lfs f25,3576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3576);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f13,f9,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f3,3184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3184);
	ctx.f3.f64 = double(temp.f32);
	// stfs f26,3584(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3584, temp.u32);
	// fmuls f26,f6,f4
	ctx.f26.f64 = double(float(ctx.f6.f64 * ctx.f4.f64));
	// lfs f2,3568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3568);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f30,f3,f20
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f20.f64));
	// stfs f18,3576(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3576, temp.u32);
	// lfs f18,2156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2156);
	ctx.f18.f64 = double(temp.f32);
	// stfs f26,3568(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3568, temp.u32);
	// lfs f26,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f26.f64 = double(temp.f32);
	// stfs f30,2148(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2148, temp.u32);
	// fmuls f30,f18,f4
	ctx.f30.f64 = double(float(ctx.f18.f64 * ctx.f4.f64));
	// stfs f22,2004(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2004, temp.u32);
	// fmuls f22,f25,f4
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f4.f64));
	// fmsubs f29,f23,f26,f29
	ctx.f29.f64 = double(float(ctx.f23.f64 * ctx.f26.f64 - ctx.f29.f64));
	// stfs f30,2124(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2124, temp.u32);
	// stfs f22,2156(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2156, temp.u32);
	// lfs f26,568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 568);
	ctx.f26.f64 = double(temp.f32);
	// lfs f30,2580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2580);
	ctx.f30.f64 = double(temp.f32);
	// lfs f22,1680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1680);
	ctx.f22.f64 = double(temp.f32);
	// lfs f9,4516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4516);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f30,f30,f26,f22
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f26.f64 + ctx.f22.f64));
	// stfs f1,3072(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3072, temp.u32);
	// fmuls f1,f9,f20
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f20.f64));
	// stfs f27,3064(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3064, temp.u32);
	// stfs f21,3392(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3392, temp.u32);
	// lfs f27,4184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4184);
	ctx.f27.f64 = double(temp.f32);
	// lfs f21,2948(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2948);
	ctx.f21.f64 = double(temp.f32);
	// lfs f15,4040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4040);
	ctx.f15.f64 = double(temp.f32);
	// lfs f23,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,4096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4096);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f23,f2,f23,f19
	ctx.f23.f64 = double(float(ctx.f2.f64 * ctx.f23.f64 + ctx.f19.f64));
	// lfs f22,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f22.f64 = double(temp.f32);
	// lfs f19,3176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3176);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f8,f26,f22,f8
	ctx.f8.f64 = double(float(ctx.f26.f64 * ctx.f22.f64 + ctx.f8.f64));
	// fmuls f22,f19,f20
	ctx.f22.f64 = double(float(ctx.f19.f64 * ctx.f20.f64));
	// lfs f26,4168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4168);
	ctx.f26.f64 = double(temp.f32);
	// lfs f19,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f19.f64 = double(temp.f32);
	// stfs f22,2140(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2140, temp.u32);
	// fmadds f26,f27,f19,f26
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f19.f64 + ctx.f26.f64));
	// lfs f22,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f10,f21,f22,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f22.f64 + ctx.f10.f64));
	// stfs f26,3544(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3544, temp.u32);
	// lfs f19,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 856);
	ctx.f22.f64 = double(temp.f32);
	// lfs f26,3504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3504);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f0,f15,f22,f0
	ctx.f0.f64 = double(float(ctx.f15.f64 * ctx.f22.f64 + ctx.f0.f64));
	// fmadds f12,f26,f19,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f19.f64 + ctx.f12.f64));
	// lfs f26,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f26.f64 = double(temp.f32);
	// lfs f22,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f5,f17,f26,f5
	ctx.f5.f64 = double(float(-(ctx.f17.f64 * ctx.f26.f64 - ctx.f5.f64)));
	// fnmsubs f11,f28,f22,f11
	ctx.f11.f64 = double(float(-(ctx.f28.f64 * ctx.f22.f64 - ctx.f11.f64)));
	// lfs f26,2240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2240);
	ctx.f26.f64 = double(temp.f32);
	// lfs f28,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f8,f8,f7,f29
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f29.f64));
	// fnmsubs f13,f26,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f26.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f28,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f28.f64 = double(temp.f32);
	// lfs f22,3592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3592);
	ctx.f22.f64 = double(temp.f32);
	// stfs f9,4064(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4064, temp.u32);
	// fnmsubs f10,f22,f28,f10
	ctx.f10.f64 = double(float(-(ctx.f22.f64 * ctx.f28.f64 - ctx.f10.f64)));
	// stfs f6,4168(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4168, temp.u32);
	// lfs f9,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f6.f64 = double(temp.f32);
	// stfs f3,4072(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4072, temp.u32);
	// lfs f3,1672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1672);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f0,f0,f7,f5
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f7.f64 + ctx.f5.f64));
	// stfs f31,4040(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4040, temp.u32);
	// stfs f27,4096(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4096, temp.u32);
	// fmadds f13,f9,f6,f13
	ctx.f13.f64 = double(float(ctx.f9.f64 * ctx.f6.f64 + ctx.f13.f64));
	// lfs f9,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f9.f64 = double(temp.f32);
	// lfs f5,3324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3324);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f6,f23,f4
	ctx.f6.f64 = double(float(ctx.f23.f64 * ctx.f4.f64));
	// fnmsubs f10,f3,f9,f10
	ctx.f10.f64 = double(float(-(ctx.f3.f64 * ctx.f9.f64 - ctx.f10.f64)));
	// lfs f31,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f28.f64 = double(temp.f32);
	// lfs f27,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f12,f28,f27,f12
	ctx.f12.f64 = double(float(-(ctx.f28.f64 * ctx.f27.f64 - ctx.f12.f64)));
	// stfs f2,4056(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4056, temp.u32);
	// lfs f29,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f29.f64 = double(temp.f32);
	// lfs f2,2164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2164);
	ctx.f2.f64 = double(temp.f32);
	// lfs f3,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f3.f64 = double(temp.f32);
	// lfs f28,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f28.f64 = double(temp.f32);
	// fmr f26,f3
	ctx.f26.f64 = ctx.f3.f64;
	// fmadds f13,f5,f31,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f9,3704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3704);
	ctx.f9.f64 = double(temp.f32);
	// lfs f19,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f19.f64 = double(temp.f32);
	// fmr f23,f28
	ctx.f23.f64 = ctx.f28.f64;
	// fmadds f10,f2,f29,f10
	ctx.f10.f64 = double(float(ctx.f2.f64 * ctx.f29.f64 + ctx.f10.f64));
	// lfs f29,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f11,f1,f29,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f29.f64 + ctx.f11.f64));
	// stfs f24,3184(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3184, temp.u32);
	// lfs f24,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f19,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f19.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f31,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f31.f64 = double(temp.f32);
	// lfs f28,3288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3288);
	ctx.f28.f64 = double(temp.f32);
	// lfs f1,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f1.f64 = double(temp.f32);
	// lfs f29,496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 496);
	ctx.f29.f64 = double(temp.f32);
	// stfs f25,3176(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3176, temp.u32);
	// fmr f25,f31
	ctx.f25.f64 = ctx.f31.f64;
	// fnmsubs f13,f9,f3,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f3.f64 - ctx.f13.f64)));
	// lfs f9,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f9.f64 = double(temp.f32);
	// lfs f3,1128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1128);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f9,f28,f9
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f9.f64));
	// stfs f21,3360(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3360, temp.u32);
	// fmuls f21,f24,f20
	ctx.f21.f64 = double(float(ctx.f24.f64 * ctx.f20.f64));
	// lfs f5,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f1,f1,f3
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f27,2940(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2940);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f29,f24
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f24.f64));
	// stfs f15,3296(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3296, temp.u32);
	// stfs f14,4048(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4048, temp.u32);
	// stfs f18,4192(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4192, temp.u32);
	// fnmsubs f10,f5,f31,f10
	ctx.f10.f64 = double(float(-(ctx.f5.f64 * ctx.f31.f64 - ctx.f10.f64)));
	// lfs f31,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,4152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4152);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f12,f31,f23,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f23.f64 - ctx.f12.f64)));
	// fmuls f23,f17,f7
	ctx.f23.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// lfs f31,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f31.f64 = double(temp.f32);
	// lfs f17,2156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2156);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f13,f27,f26,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f26.f64 + ctx.f13.f64));
	// fnmsubs f8,f17,f31,f8
	ctx.f8.f64 = double(float(-(ctx.f17.f64 * ctx.f31.f64 - ctx.f8.f64)));
	// lfs f17,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f17.f64 = double(temp.f32);
	// lfs f15,2148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2148);
	ctx.f15.f64 = double(temp.f32);
	// lfs f24,1120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1120);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f11,f15,f17,f11
	ctx.f11.f64 = double(float(-(ctx.f15.f64 * ctx.f17.f64 - ctx.f11.f64)));
	// lfs f15,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f24,f16,f24
	ctx.f24.f64 = double(float(ctx.f16.f64 * ctx.f24.f64));
	// lfs f18,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f18.f64 = double(temp.f32);
	// lfs f16,3560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3560);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f10,f15,f25,f10
	ctx.f10.f64 = double(float(-(ctx.f15.f64 * ctx.f25.f64 - ctx.f10.f64)));
	// lfs f31,5116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5116);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f30,f18,f16,f30
	ctx.f30.f64 = double(float(ctx.f18.f64 * ctx.f16.f64 + ctx.f30.f64));
	// lfs f18,2908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2908);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f31,f31,f18
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f18.f64));
	// lfs f18,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f21,f21,f18
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// lfs f5,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f5.f64 = double(temp.f32);
	// lfs f14,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f12,f14,f5,f12
	ctx.f12.f64 = double(float(-(ctx.f14.f64 * ctx.f5.f64 - ctx.f12.f64)));
	// lfs f28,2172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2172);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f5,f1,f15
	ctx.f5.f64 = double(float(ctx.f1.f64 * ctx.f15.f64));
	// fmuls f19,f28,f20
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f20.f64));
	// lfs f1,2140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2140);
	ctx.f1.f64 = double(temp.f32);
	// stfs f28,4184(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 4184, temp.u32);
	// lfs f27,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f27.f64 = double(temp.f32);
	// fadds f13,f13,f10
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f10.f64));
	// stfs f13,5560(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5560, temp.u32);
	// lfs f13,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f30,f20
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f20.f64));
	// lfs f30,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f1,f13,f11
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// lfs f28,3584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3584);
	ctx.f28.f64 = double(temp.f32);
	// stfs f2,1160(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1160, temp.u32);
	// fmsubs f6,f28,f30,f6
	ctx.f6.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 - ctx.f6.f64));
	// lfs f11,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f2,f29,f20
	ctx.f2.f64 = double(float(ctx.f29.f64 * ctx.f20.f64));
	// lfs f28,3576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3576);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f11,f21,f11
	ctx.f11.f64 = double(float(ctx.f21.f64 * ctx.f11.f64));
	// lfs f29,980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	ctx.f29.f64 = double(temp.f32);
	// lfs f17,2180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2180);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f29,f28,f29,f0
	ctx.f29.f64 = double(float(-(ctx.f28.f64 * ctx.f29.f64 - ctx.f0.f64)));
	// lfs f30,848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 848);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f17,f17,f4
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f4.f64));
	// lfs f28,3568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3568);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f8,f28,f30,f8
	ctx.f8.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 + ctx.f8.f64));
	// lfs f1,4088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4088);
	ctx.f1.f64 = double(temp.f32);
	// lfs f30,4144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4144);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f24,f4
	ctx.f28.f64 = double(float(ctx.f24.f64 * ctx.f4.f64));
	// lfs f24,4136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4136);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f1,f1,f7
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f7.f64));
	// fmuls f30,f30,f7
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f7.f64));
	// lfs f0,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f7,f24,f7
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f7.f64));
	// lfs f25,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f25.f64 = double(temp.f32);
	// lfs f24,3552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3552);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f11,f19,f0,f11
	ctx.f11.f64 = double(float(ctx.f19.f64 * ctx.f0.f64 - ctx.f11.f64));
	// lfs f18,3280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3280);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f13,f24,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f22,1168(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1168, temp.u32);
	// fmsubs f9,f18,f25,f9
	ctx.f9.f64 = double(float(ctx.f18.f64 * ctx.f25.f64 - ctx.f9.f64));
	// lfs f22,768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 768);
	ctx.f22.f64 = double(temp.f32);
	// lfs f26,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f6,f31,f22,f6
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f22.f64 + ctx.f6.f64));
	// lfs f25,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f27,f26
	ctx.f26.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// lfs f0,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f25,f17,f25
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f25.f64));
	// lfs f24,4032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4032);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f29,f23,f0,f29
	ctx.f29.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f29.f64)));
	// lfs f21,3248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3248);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,3240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3240);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f31.f64 = double(temp.f32);
	// fmr f23,f15
	ctx.f23.f64 = ctx.f15.f64;
	// lfs f18,4024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4024);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f0,f1,f0,f8
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f1,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f12,f10,f1,f12
	ctx.f12.f64 = double(float(ctx.f10.f64 * ctx.f1.f64 + ctx.f12.f64));
	// lfs f10,3544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3544);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f13,f10,f20,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f20.f64 + ctx.f13.f64));
	// lfs f10,2188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2188);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f22,f10,f20
	ctx.f22.f64 = double(float(ctx.f10.f64 * ctx.f20.f64));
	// stfs f10,3240(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3240, temp.u32);
	// lfs f10,2132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2132);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f9,f24,f31,f9
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f31.f64 - ctx.f9.f64)));
	// fadds f8,f19,f21
	ctx.f8.f64 = double(float(ctx.f19.f64 + ctx.f21.f64));
	// lfs f31,644(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 644);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f6,f25,f31,f6
	ctx.f6.f64 = double(float(-(ctx.f25.f64 * ctx.f31.f64 - ctx.f6.f64)));
	// lfs f1,2196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2196);
	ctx.f1.f64 = double(temp.f32);
	// lfs f21,2204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2204);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f19,f1,f4
	ctx.f19.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// lfs f31,3272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3272);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f5,f26,f23,f5
	ctx.f5.f64 = double(float(ctx.f26.f64 * ctx.f23.f64 - ctx.f5.f64));
	// lfs f26,648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f30,f30,f26,f29
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f26.f64 + ctx.f29.f64));
	// lfs f29,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f3,f29,f3
	ctx.f3.f64 = double(float(ctx.f29.f64 * ctx.f3.f64));
	// lfs f29,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f11,f2,f29,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f29.f64 + ctx.f11.f64));
	// lfs f29,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f29.f64 = double(temp.f32);
	// lfs f23,872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f26,f21,f4
	ctx.f26.f64 = double(float(ctx.f21.f64 * ctx.f4.f64));
	// fnmsubs f0,f7,f23,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f23.f64 - ctx.f0.f64)));
	// stfs f0,5576(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5576, temp.u32);
	// lfs f0,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f0.f64 = double(temp.f32);
	// lfs f7,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f7,f16
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f16.f64));
	// lfs f2,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f2.f64 = double(temp.f32);
	// lfs f23,4320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4320);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f5,f18,f29,f5
	ctx.f5.f64 = double(float(ctx.f18.f64 * ctx.f29.f64 + ctx.f5.f64));
	// lfs f29,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f13,f10,f29,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f29.f64 + ctx.f13.f64));
	// lfs f10,2124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2124);
	ctx.f10.f64 = double(temp.f32);
	// stfs f1,3272(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3272, temp.u32);
	// fmadds f9,f23,f2,f9
	ctx.f9.f64 = double(float(ctx.f23.f64 * ctx.f2.f64 + ctx.f9.f64));
	// stfs f30,5568(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5568, temp.u32);
	// stfs f4,2100(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2100, temp.u32);
	// lfs f25,4328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4328);
	ctx.f25.f64 = double(temp.f32);
	// lfs f1,356(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	ctx.f1.f64 = double(temp.f32);
	// fadds f8,f8,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f25.f64));
	// lfs f30,4016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4016);
	ctx.f30.f64 = double(temp.f32);
	// lfs f4,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,1112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1112);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f4,f1,f4
	ctx.f4.f64 = double(float(ctx.f1.f64 * ctx.f4.f64));
	// stfs f12,5564(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5564, temp.u32);
	// fmuls f25,f2,f31
	ctx.f25.f64 = double(float(ctx.f2.f64 * ctx.f31.f64));
	// fnmsubs f0,f10,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f10,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f10.f64 = double(temp.f32);
	// fmr f13,f29
	ctx.f13.f64 = ctx.f29.f64;
	// lfs f1,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f1.f64 = double(temp.f32);
	// stfs f31,2132(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2132, temp.u32);
	// fmuls f12,f7,f20
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f20.f64));
	// stfs f6,5572(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5572, temp.u32);
	// fnmsubs f11,f22,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// lfs f10,1220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1220);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f1,f27,f1
	ctx.f1.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f7,1560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1560);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 376);
	ctx.f6.f64 = double(temp.f32);
	// lfs f31,4128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4128);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f6,f10,f6
	ctx.f6.f64 = double(float(ctx.f10.f64 * ctx.f6.f64));
	// lfs f29,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f31,f20
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// lfs f27,836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f7,f29
	ctx.f29.f64 = double(float(ctx.f7.f64 * ctx.f29.f64));
	// stfs f24,3280(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3280, temp.u32);
	// fnmsubs f0,f28,f13,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// lfs f13,1552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1552);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// lfs f28,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f28.f64 = double(temp.f32);
	// lfs f30,3140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3140);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f30,f30,f28
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// lfs f28,1180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1180);
	ctx.f28.f64 = double(temp.f32);
	// stfs f21,3248(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3248, temp.u32);
	// stfs f18,3288(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3288, temp.u32);
	// fnmsubs f0,f19,f27,f0
	ctx.f0.f64 = double(float(-(ctx.f19.f64 * ctx.f27.f64 - ctx.f0.f64)));
	// lfs f24,492(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 492);
	ctx.f24.f64 = double(temp.f32);
	// lfs f19,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f3,f19,f24,f3
	ctx.f3.f64 = double(float(ctx.f19.f64 * ctx.f24.f64 + ctx.f3.f64));
	// lfs f23,4008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4008);
	ctx.f23.f64 = double(temp.f32);
	// lfs f16,3168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3168);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f19.f64 = double(temp.f32);
	// lfs f14,3152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3152);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f19,f23,f19,f16
	ctx.f19.f64 = double(float(ctx.f23.f64 * ctx.f19.f64 + ctx.f16.f64));
	// lfs f16,3960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3960);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f14,f20
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f20.f64));
	// fadds f8,f8,f16
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f16.f64));
	// lfs f15,3160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3160);
	ctx.f15.f64 = double(temp.f32);
	// stfs f14,2164(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2164, temp.u32);
	// fmuls f15,f15,f20
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// lfs f16,3264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3264);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,1664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1664);
	ctx.f14.f64 = double(temp.f32);
	// fadds f16,f14,f16
	ctx.f16.f64 = double(float(ctx.f14.f64 + ctx.f16.f64));
	// stfs f15,2172(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2172, temp.u32);
	// lfs f14,2076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2076);
	ctx.f14.f64 = double(temp.f32);
	// lfs f22,4120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4120);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,1080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1080);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f18,f22,f20
	ctx.f18.f64 = double(float(ctx.f22.f64 * ctx.f20.f64));
	// stfs f15,2148(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2148, temp.u32);
	// fmuls f15,f15,f14
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f14.f64));
	// stfs f14,2180(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2180, temp.u32);
	// lfs f17,4000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4000);
	ctx.f17.f64 = double(temp.f32);
	// lfs f14,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f14.f64 = double(temp.f32);
	// stfs f18,2196(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2196, temp.u32);
	// fnmsubs f9,f17,f14,f9
	ctx.f9.f64 = double(float(-(ctx.f17.f64 * ctx.f14.f64 - ctx.f9.f64)));
	// lfs f21,2084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2084);
	ctx.f21.f64 = double(temp.f32);
	// lfs f18,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f18,f21,f18
	ctx.f18.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// stfs f18,2188(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2188, temp.u32);
	// lfs f18,4312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4312);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f5,f18,f14,f5
	ctx.f5.f64 = double(float(-(ctx.f18.f64 * ctx.f14.f64 - ctx.f5.f64)));
	// lfs f18,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f18.f64 = double(temp.f32);
	// stfs f13,2140(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2140, temp.u32);
	// fmadds f11,f26,f18,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f18.f64 + ctx.f11.f64));
	// stfs f0,3544(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3544, temp.u32);
	// lfs f0,3952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3952);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f13.f64 = double(temp.f32);
	// stfs f8,2204(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2204, temp.u32);
	// fmadds f13,f0,f13,f9
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f9.f64));
	// lfs f8,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f8,f25,f8
	ctx.f8.f64 = double(float(ctx.f25.f64 * ctx.f8.f64));
	// lfs f0,108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	ctx.f0.f64 = double(temp.f32);
	// stfs f10,3512(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3512, temp.u32);
	// lfs f10,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f10.f64 = double(temp.f32);
	// stfs f6,3584(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3584, temp.u32);
	// fmadds f10,f3,f10,f5
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f10.f64 + ctx.f5.f64));
	// fnmsubs f12,f12,f0,f11
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f11,4304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4304);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// stfs f4,3576(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3576, temp.u32);
	// stfs f7,2124(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2124, temp.u32);
	// lfs f7,2068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2068);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f13,f11,f0,f13
	ctx.f13.f64 = double(float(-(ctx.f11.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// lfs f4,3328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3328);
	ctx.f4.f64 = double(temp.f32);
	// lfs f27,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f27.f64 = double(temp.f32);
	// lfs f9,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f27,f28,f27
	ctx.f27.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// lfs f6,4368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4368);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f8,f16,f9,f8
	ctx.f8.f64 = double(float(ctx.f16.f64 * ctx.f9.f64 + ctx.f8.f64));
	// stfs f31,3592(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3592, temp.u32);
	// fmuls f5,f6,f7
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f7.f64));
	// stfs f1,3568(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3568, temp.u32);
	// fmuls f1,f7,f4
	ctx.f1.f64 = double(float(ctx.f7.f64 * ctx.f4.f64));
	// stfs f30,3552(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3552, temp.u32);
	// fmuls f30,f2,f4
	ctx.f30.f64 = double(float(ctx.f2.f64 * ctx.f4.f64));
	// lfs f3,1536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1536);
	ctx.f3.f64 = double(temp.f32);
	// lfs f31,656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 656);
	ctx.f31.f64 = double(temp.f32);
	// lfs f11,3624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3624);
	ctx.f11.f64 = double(temp.f32);
	// stfs f28,916(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 916, temp.u32);
	// stfs f24,548(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 548, temp.u32);
	// stfs f23,2084(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2084, temp.u32);
	// stfs f22,3264(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3264, temp.u32);
	// stfs f29,3560(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3560, temp.u32);
	// stfs f21,2156(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2156, temp.u32);
	// stfs f17,2076(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2076, temp.u32);
	// lfs f29,4296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4296);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f10,f27,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f28,1528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1528);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f27,f31,f4
	ctx.f27.f64 = double(float(ctx.f31.f64 * ctx.f4.f64));
	// fmuls f28,f28,f3
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// lfs f24,4288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4288);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f9,f29
	ctx.f29.f64 = double(float(ctx.f9.f64 * ctx.f29.f64));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f12,f19,f20,f12
	ctx.f12.f64 = double(float(ctx.f19.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f26,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f26.f64 = double(temp.f32);
	// lfs f22,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f8,f15,f26,f8
	ctx.f8.f64 = double(float(-(ctx.f15.f64 * ctx.f26.f64 - ctx.f8.f64)));
	// fnmsubs f13,f24,f22,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f22.f64 - ctx.f13.f64)));
	// lfs f23,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f5,f5,f0
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f0.f64));
	// lfs f20,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f11,f9,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f11.f64));
	// lfs f19,2204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2204);
	ctx.f19.f64 = double(temp.f32);
	// lfs f22,612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f10,f19,f20,f10
	ctx.f10.f64 = double(float(ctx.f19.f64 * ctx.f20.f64 + ctx.f10.f64));
	// lfs f20,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f0,f27,f0
	ctx.f0.f64 = double(float(ctx.f27.f64 * ctx.f0.f64));
	// lfs f27,3944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3944);
	ctx.f27.f64 = double(temp.f32);
	// lfs f18,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f29,f29,f23,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f23.f64 - ctx.f28.f64));
	// lfs f23,2196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2196);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f12,f23,f22,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f22.f64 - ctx.f12.f64)));
	// lfs f22,2188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2188);
	ctx.f22.f64 = double(temp.f32);
	// lfs f16,696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f8,f22,f20,f8
	ctx.f8.f64 = double(float(-(ctx.f22.f64 * ctx.f20.f64 - ctx.f8.f64)));
	// fmadds f13,f27,f16,f13
	ctx.f13.f64 = double(float(ctx.f27.f64 * ctx.f16.f64 + ctx.f13.f64));
	// lfs f20,1520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1520);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,4280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4280);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f11,f11,f18,f5
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f18.f64 + ctx.f5.f64));
	// lfs f16,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f16.f64 = double(temp.f32);
	// lfs f25,3420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3420);
	ctx.f25.f64 = double(temp.f32);
	// lfs f26,1072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1072);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f10,f15,f16,f10
	ctx.f10.f64 = double(float(-(ctx.f15.f64 * ctx.f16.f64 - ctx.f10.f64)));
	// fmsubs f0,f20,f3,f0
	ctx.f0.f64 = double(float(ctx.f20.f64 * ctx.f3.f64 - ctx.f0.f64));
	// lfs f20,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f20.f64 = double(temp.f32);
	// lfs f15,2172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2172);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f21,f25,f26
	ctx.f21.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f18,3320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3320);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f25,f7
	ctx.f19.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// fnmsubs f12,f15,f20,f12
	ctx.f12.f64 = double(float(-(ctx.f15.f64 * ctx.f20.f64 - ctx.f12.f64)));
	// lfs f17,2180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2180);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f29,f9,f18,f29
	ctx.f29.f64 = double(float(-(ctx.f9.f64 * ctx.f18.f64 - ctx.f29.f64)));
	// stfs f31,2180(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2180, temp.u32);
	// stfs f6,2188(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2188, temp.u32);
	// lfs f18,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f18.f64 = double(temp.f32);
	// lfs f6,3136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3136);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f11,f1,f18,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f18.f64 - ctx.f11.f64));
	// lfs f31,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f6,f31,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f31.f64 - ctx.f13.f64)));
	// lfs f31,4264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4264);
	ctx.f31.f64 = double(temp.f32);
	// lfs f18,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f18.f64 = double(temp.f32);
	// lfs f23,1720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1720);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f10,f31,f18,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f18.f64 - ctx.f10.f64)));
	// lfs f16,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f23,f23,f17
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f17.f64));
	// stfs f17,636(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 636, temp.u32);
	// fnmsubs f8,f21,f16,f8
	ctx.f8.f64 = double(float(-(ctx.f21.f64 * ctx.f16.f64 - ctx.f8.f64)));
	// lfs f17,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,2164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2164);
	ctx.f18.f64 = double(temp.f32);
	// lfs f28,2052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2052);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f12,f18,f17,f12
	ctx.f12.f64 = double(float(ctx.f18.f64 * ctx.f17.f64 + ctx.f12.f64));
	// lfs f22,1064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1064);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f28,f28,f3
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f3.f64));
	// lfs f5,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f5.f64 = double(temp.f32);
	// lfs f20,1056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1056);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f5,f22,f5
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f5.f64));
	// lfs f21,3508(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3508);
	ctx.f21.f64 = double(temp.f32);
	// stfs f24,1152(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1152, temp.u32);
	// fmuls f1,f21,f20
	ctx.f1.f64 = double(float(ctx.f21.f64 * ctx.f20.f64));
	// stfs f27,1552(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1552, temp.u32);
	// fmuls f27,f25,f2
	ctx.f27.f64 = double(float(ctx.f25.f64 * ctx.f2.f64));
	// lfs f6,4272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4272);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f24,f4,f26
	ctx.f24.f64 = double(float(ctx.f4.f64 * ctx.f26.f64));
	// lfs f31,780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 780);
	ctx.f31.f64 = double(temp.f32);
	// lfs f18,3128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3128);
	ctx.f18.f64 = double(temp.f32);
	// lfs f17,3232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3232);
	ctx.f17.f64 = double(temp.f32);
	// fmadds f13,f6,f31,f13
	ctx.f13.f64 = double(float(ctx.f6.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f16,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,3592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3592);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f10,f17,f16,f10
	ctx.f10.f64 = double(float(-(ctx.f17.f64 * ctx.f16.f64 - ctx.f10.f64)));
	// lfs f14,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f14.f64 = double(temp.f32);
	// lfs f17,4432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4432);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f12,f15,f14,f12
	ctx.f12.f64 = double(float(-(ctx.f15.f64 * ctx.f14.f64 - ctx.f12.f64)));
	// lfs f16,3224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3224);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f17,f20
	ctx.f14.f64 = double(float(ctx.f17.f64 * ctx.f20.f64));
	// stfs f14,3592(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3592, temp.u32);
	// fmuls f14,f7,f16
	ctx.f14.f64 = double(float(ctx.f7.f64 * ctx.f16.f64));
	// stfs f24,3528(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3528, temp.u32);
	// lfs f24,3216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3216);
	ctx.f24.f64 = double(temp.f32);
	// lfs f31,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f31.f64 = double(temp.f32);
	// stfs f14,3624(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3624, temp.u32);
	// stfs f7,2164(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2164, temp.u32);
	// fmuls f7,f7,f24
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f24.f64));
	// lfs f15,1972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1972);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f13,f18,f31,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f31.f64 + ctx.f13.f64));
	// lfs f14,2148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2148);
	ctx.f14.f64 = double(temp.f32);
	// stfs f22,2196(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2196, temp.u32);
	// fmuls f22,f15,f20
	ctx.f22.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// stfs f7,2148(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2148, temp.u32);
	// stfs f2,1672(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1672, temp.u32);
	// lfs f2,520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f31.f64 = double(temp.f32);
	// lfs f7,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f31,f31,f2
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f22,3536(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3536, temp.u32);
	// fmuls f7,f5,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f7.f64));
	// lfs f22,2156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2156);
	ctx.f22.f64 = double(temp.f32);
	// stfs f31,2156(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2156, temp.u32);
	// lfs f5,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f5.f64 = double(temp.f32);
	// lfs f31,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f5,f19,f5,f0
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f5.f64 + ctx.f0.f64));
	// stfs f9,2172(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2172, temp.u32);
	// fmadds f8,f30,f31,f8
	ctx.f8.f64 = double(float(ctx.f30.f64 * ctx.f31.f64 + ctx.f8.f64));
	// lfs f9,2900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2900);
	ctx.f9.f64 = double(temp.f32);
	// lfs f0,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f9,f20
	ctx.f30.f64 = double(float(ctx.f9.f64 * ctx.f20.f64));
	// lfs f19,988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f29,f23,f0,f29
	ctx.f29.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f29.f64)));
	// lfs f31,1012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f23,f19,f2
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f2.f64));
	// stfs f30,1680(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1680, temp.u32);
	// fmuls f31,f22,f31
	ctx.f31.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// stfs f3,2116(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2116, temp.u32);
	// lfs f3,2044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2044);
	ctx.f3.f64 = double(temp.f32);
	// lfs f30,5072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5072);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f14,f3,f14
	ctx.f14.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// lfs f0,3120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3120);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f30,f3,f30
	ctx.f30.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// lfs f19,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f19.f64 = double(temp.f32);
	// stfs f31,2108(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2108, temp.u32);
	// fnmsubs f0,f0,f19,f10
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f19.f64 - ctx.f10.f64)));
	// lfs f31,2140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2140);
	ctx.f31.f64 = double(temp.f32);
	// stfs f30,2140(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2140, temp.u32);
	// lfs f10,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f10.f64 = double(temp.f32);
	// lfs f30,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f11,f28,f10,f11
	ctx.f11.f64 = double(float(-(ctx.f28.f64 * ctx.f10.f64 - ctx.f11.f64)));
	// stfs f14,2204(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2204, temp.u32);
	// fnmsubs f12,f31,f30,f12
	ctx.f12.f64 = double(float(-(ctx.f31.f64 * ctx.f30.f64 - ctx.f12.f64)));
	// lfs f14,1000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f25,f25,f14
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f14.f64));
	// lfs f31,4112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4112);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f20,f24,f20
	ctx.f20.f64 = double(float(ctx.f24.f64 * ctx.f20.f64));
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// stfs f21,2068(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2068, temp.u32);
	// fnmsubs f13,f31,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// stfs f17,1520(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1520, temp.u32);
	// stfs f16,624(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 624, temp.u32);
	// stfs f15,2052(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2052, temp.u32);
	// stfs f4,3504(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3504, temp.u32);
	// stfs f26,660(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 660, temp.u32);
	// stfs f9,2044(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2044, temp.u32);
	// stfs f6,908(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 908, temp.u32);
	// stfs f22,1664(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1664, temp.u32);
	// stfs f2,812(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 812, temp.u32);
	// stfs f14,820(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 820, temp.u32);
	// stfs f3,1248(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1248, temp.u32);
	// lfs f10,3584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3584);
	ctx.f10.f64 = double(temp.f32);
	// fmr f3,f19
	ctx.f3.f64 = ctx.f19.f64;
	// lfs f9,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f10,f9,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f9.f64 + ctx.f13.f64));
	// lfs f10,3896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3896);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f0,f10,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f10.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f6,3576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3576);
	ctx.f6.f64 = double(temp.f32);
	// lfs f10,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f10.f64 = double(temp.f32);
	// lfs f4,3888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3888);
	ctx.f4.f64 = double(temp.f32);
	// lfs f21,3552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3552);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	ctx.f19.f64 = double(temp.f32);
	// lfs f9,3632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3632);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f21,f19,f12
	ctx.f12.f64 = double(float(ctx.f21.f64 * ctx.f19.f64 + ctx.f12.f64));
	// lfs f2,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f2.f64 = double(temp.f32);
	// lfs f30,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f13,f6,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f6.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f31,3880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3880);
	ctx.f31.f64 = double(temp.f32);
	// lfs f21,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f0,f4,f3,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f3.f64 + ctx.f0.f64));
	// lfs f10,3568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3568);
	ctx.f10.f64 = double(temp.f32);
	// fmr f3,f30
	ctx.f3.f64 = ctx.f30.f64;
	// lfs f6,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,3040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3040);
	ctx.f4.f64 = double(temp.f32);
	// lfs f16,2188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2188);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,3640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3640);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,2164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2164);
	ctx.f15.f64 = double(temp.f32);
	// lfs f28,3560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3560);
	ctx.f28.f64 = double(temp.f32);
	// lfs f26,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f13,f9,f2,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f2.f64 - ctx.f13.f64)));
	// stfs f18,1560(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1560, temp.u32);
	// fmr f2,f30
	ctx.f2.f64 = ctx.f30.f64;
	// lfs f18,3872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3872);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f31,f31,f30,f0
	ctx.f31.f64 = double(float(-(ctx.f31.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// lfs f0,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f0.f64 = double(temp.f32);
	// fmr f30,f21
	ctx.f30.f64 = ctx.f21.f64;
	// lfs f19,2196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2196);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f8,f27,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f27.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// lfs f27,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f0,f16,f0,f20
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f0.f64 + ctx.f20.f64));
	// lfs f20,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,3024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3024);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f10,f6,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 * ctx.f6.f64 + ctx.f13.f64));
	// lfs f10,3032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3032);
	ctx.f10.f64 = double(temp.f32);
	// lfs f6,2180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2180);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f4,f4,f3,f31
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f3.f64 - ctx.f31.f64)));
	// lfs f3,2124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2124);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f11,f1,f30,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f30.f64 - ctx.f11.f64)));
	// lfs f30,3624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3624);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f5,f30,f27,f5
	ctx.f5.f64 = double(float(-(ctx.f30.f64 * ctx.f27.f64 - ctx.f5.f64)));
	// lfs f30,2204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2204);
	ctx.f30.f64 = double(temp.f32);
	// lfs f1,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f6,f6,f24,f25
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f24.f64 + ctx.f25.f64));
	// fnmsubs f1,f30,f1,f29
	ctx.f1.f64 = double(float(-(ctx.f30.f64 * ctx.f1.f64 - ctx.f29.f64)));
	// lfs f30,3544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3544);
	ctx.f30.f64 = double(temp.f32);
	// fadds f12,f30,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 + ctx.f12.f64));
	// lfs f30,2172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2172);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f10,f30,f10
	ctx.f10.f64 = double(float(ctx.f30.f64 * ctx.f10.f64));
	// lfs f27,2132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2132);
	ctx.f27.f64 = double(temp.f32);
	// stfs f10,3640(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3640, temp.u32);
	// fmuls f14,f15,f27
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f27.f64));
	// lfs f10,3592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3592);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f13,f28,f26,f13
	ctx.f13.f64 = double(float(ctx.f28.f64 * ctx.f26.f64 + ctx.f13.f64));
	// stfs f12,5580(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5580, temp.u32);
	// stfs f14,3632(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3632, temp.u32);
	// fmadds f11,f10,f20,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f20.f64 + ctx.f11.f64));
	// lfs f25,840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f14.f64 = double(temp.f32);
	// lfs f20,2156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2156);
	ctx.f20.f64 = double(temp.f32);
	// lfs f12,2148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2148);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f5,f20,f25,f5
	ctx.f5.f64 = double(float(-(ctx.f20.f64 * ctx.f25.f64 - ctx.f5.f64)));
	// lfs f29,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f12,f12,f14,f8
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f14.f64 - ctx.f8.f64)));
	// lfs f31,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f17,f19,f29
	ctx.f17.f64 = double(float(ctx.f19.f64 * ctx.f29.f64));
	// lfs f10,1228(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1228);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f31,f3,f31
	ctx.f31.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// lfs f25,5016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5016);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f1,f30,f16,f1
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f16.f64 + ctx.f1.f64));
	// lfs f28,1688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1688);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f8,f18,f2,f4
	ctx.f8.f64 = double(float(ctx.f18.f64 * ctx.f2.f64 + ctx.f4.f64));
	// lfs f26,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f13,f22,f21,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f21.f64 + ctx.f13.f64));
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// stfs f22,1528(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1528, temp.u32);
	// fmr f26,f21
	ctx.f26.f64 = ctx.f21.f64;
	// lfs f22,3536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3536);
	ctx.f22.f64 = double(temp.f32);
	// stfs f24,3568(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3568, temp.u32);
	// fmuls f10,f10,f29
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f29.f64));
	// lfs f24,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f24.f64 = double(temp.f32);
	// stfs f27,3584(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3584, temp.u32);
	// lfs f27,3208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3208);
	ctx.f27.f64 = double(temp.f32);
	// lfs f29,3428(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3428);
	ctx.f29.f64 = double(temp.f32);
	// stfs f9,1536(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1536, temp.u32);
	// fadds f29,f27,f29
	ctx.f29.f64 = double(float(ctx.f27.f64 + ctx.f29.f64));
	// fadds f9,f28,f25
	ctx.f9.f64 = double(float(ctx.f28.f64 + ctx.f25.f64));
	// lfs f27,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f27.f64 = double(temp.f32);
	// fadds f13,f8,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 + ctx.f13.f64));
	// stfs f13,5584(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5584, temp.u32);
	// lfs f13,2036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2036);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f11,f22,f26,f11
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f26.f64 - ctx.f11.f64)));
	// lfs f22,3528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3528);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f5,f22,f24,f5
	ctx.f5.f64 = double(float(-(ctx.f22.f64 * ctx.f24.f64 - ctx.f5.f64)));
	// lfs f26,2028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2028);
	ctx.f26.f64 = double(temp.f32);
	// lfs f24,2116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2116);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f4,f13,f3
	ctx.f4.f64 = double(float(ctx.f13.f64 * ctx.f3.f64));
	// lfs f8,2212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2212);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f1,f26,f24,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f24.f64 + ctx.f1.f64));
	// lfs f26,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f2,f8,f13
	ctx.f2.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fnmsubs f12,f23,f26,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f26.f64 - ctx.f12.f64)));
	// lfs f23,3200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3200);
	ctx.f23.f64 = double(temp.f32);
	// lfs f26,3008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3008);
	ctx.f26.f64 = double(temp.f32);
	// lfs f3,3312(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3312);
	ctx.f3.f64 = double(temp.f32);
	// fadds f26,f23,f26
	ctx.f26.f64 = double(float(ctx.f23.f64 + ctx.f26.f64));
	// lfs f23,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f3,f27
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f27.f64));
	// lfs f20,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f29,f29,f23
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f23.f64));
	// fmadds f9,f9,f20,f31
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f20.f64 + ctx.f31.f64));
	// lfs f23,832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	ctx.f23.f64 = double(temp.f32);
	// lfs f31,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f11,f7,f31,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f31.f64 + ctx.f11.f64));
	// lfs f31,2108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2108);
	ctx.f31.f64 = double(temp.f32);
	// lfs f7,2920(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2920);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f2,f2,f23
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// lfs f23,728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 728);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f5,f31,f23,f5
	ctx.f5.f64 = double(float(-(ctx.f31.f64 * ctx.f23.f64 - ctx.f5.f64)));
	// lfs f31,608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 608);
	ctx.f31.f64 = double(temp.f32);
	// lfs f18,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f7,f30,f7,f1
	ctx.f7.f64 = double(float(-(ctx.f30.f64 * ctx.f7.f64 - ctx.f1.f64)));
	// lfs f22,3016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3016);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f4,f4,f18
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f18.f64));
	// fmadds f1,f22,f31,f27
	ctx.f1.f64 = double(float(ctx.f22.f64 * ctx.f31.f64 + ctx.f27.f64));
	// stfs f15,1688(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1688, temp.u32);
	// lfs f15,480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 480);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f18.f64 = double(temp.f32);
	// lfs f27,2152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2152);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f29,f26,f18,f29
	ctx.f29.f64 = double(float(ctx.f26.f64 * ctx.f18.f64 + ctx.f29.f64));
	// fmadds f9,f27,f15,f9
	ctx.f9.f64 = double(float(ctx.f27.f64 * ctx.f15.f64 + ctx.f9.f64));
	// lfs f15,540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	ctx.f15.f64 = double(temp.f32);
	// lfs f18,1680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1680);
	ctx.f18.f64 = double(temp.f32);
	// lfs f31,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f11,f18,f15,f11
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f15.f64 + ctx.f11.f64));
	// lfs f26,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f12,f6,f31,f12
	ctx.f12.f64 = double(float(ctx.f6.f64 * ctx.f31.f64 + ctx.f12.f64));
	// lfs f15,1672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1672);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f7,f17,f26,f7
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f26.f64 + ctx.f7.f64));
	// stfs f19,2164(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2164, temp.u32);
	// fmadds f5,f0,f15,f5
	ctx.f5.f64 = double(float(ctx.f0.f64 * ctx.f15.f64 + ctx.f5.f64));
	// lfs f21,3000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3000);
	ctx.f21.f64 = double(temp.f32);
	// lfs f31,2992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2992);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f21,f13,f21
	ctx.f21.f64 = double(float(ctx.f13.f64 * ctx.f21.f64));
	// lfs f23,2100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2100);
	ctx.f23.f64 = double(temp.f32);
	// lfs f20,220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f31,f31,f23
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f23.f64));
	// lfs f19,2136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2136);
	ctx.f19.f64 = double(temp.f32);
	// lfs f6,3712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3712);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f16,f20,f19
	ctx.f16.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// lfs f18,2984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2984);
	ctx.f18.f64 = double(temp.f32);
	// lfs f0,2912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2912);
	ctx.f0.f64 = double(temp.f32);
	// lfs f26,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f26.f64 = double(temp.f32);
	// fmsubs f2,f21,f26,f2
	ctx.f2.f64 = double(float(ctx.f21.f64 * ctx.f26.f64 - ctx.f2.f64));
	// lfs f26,860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	ctx.f26.f64 = double(temp.f32);
	// fadds f18,f18,f0
	ctx.f18.f64 = double(float(ctx.f18.f64 + ctx.f0.f64));
	// lfs f0,2904(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2904);
	ctx.f0.f64 = double(temp.f32);
	// lfs f21,216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f26,f0,f26
	ctx.f26.f64 = double(float(ctx.f0.f64 * ctx.f26.f64));
	// stfs f30,3624(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3624, temp.u32);
	// fmadds f0,f6,f21,f1
	ctx.f0.f64 = double(float(ctx.f6.f64 * ctx.f21.f64 + ctx.f1.f64));
	// stfs f12,3712(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3712, temp.u32);
	// fmr f30,f21
	ctx.f30.f64 = ctx.f21.f64;
	// stfs f6,2100(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2100, temp.u32);
	// lfs f6,2140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2140);
	ctx.f6.f64 = double(temp.f32);
	// lfs f12,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f12,f6,f12,f11
	ctx.f12.f64 = double(float(-(ctx.f6.f64 * ctx.f12.f64 - ctx.f11.f64)));
	// stfs f3,3528(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3528, temp.u32);
	// lfs f3,3720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3720);
	ctx.f3.f64 = double(temp.f32);
	// stfs f8,2036(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2036, temp.u32);
	// lfs f11,2512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2512);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f11,f11,f24,f5
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f24.f64 + ctx.f5.f64));
	// lfs f6,424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f8,f0,f13
	ctx.f8.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f0,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f9,f3,f30,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f30.f64 + ctx.f9.f64));
	// lfs f30,3640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3640);
	ctx.f30.f64 = double(temp.f32);
	// fnmsubs f10,f10,f6,f7
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f6.f64 - ctx.f7.f64)));
	// stfs f27,3320(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3320, temp.u32);
	// lfs f17,2596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2596);
	ctx.f17.f64 = double(temp.f32);
	// lfs f27,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f12,f30,f0,f12
	ctx.f12.f64 = double(float(ctx.f30.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f7,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f7,f17,f7
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f7.f64));
	// lfs f14,2976(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2976);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f31,f31,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// lfs f27,3632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3632);
	ctx.f27.f64 = double(temp.f32);
	// lfs f6,2020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2020);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f14,f14,f23
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f23.f64));
	// fmadds f0,f27,f0,f11
	ctx.f0.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f11,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f11,f4,f11,f2
	ctx.f11.f64 = double(float(-(ctx.f4.f64 * ctx.f11.f64 - ctx.f2.f64)));
	// stfs f22,3328(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3328, temp.u32);
	// lfs f5,3712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3712);
	ctx.f5.f64 = double(temp.f32);
	// stfs f15,2212(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2212, temp.u32);
	// fnmsubs f10,f6,f24,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f24.f64 - ctx.f10.f64)));
	// stfs f25,1672(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1672, temp.u32);
	// lfs f25,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f25.f64 = double(temp.f32);
	// fadds f12,f5,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// lfs f4,1988(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1988);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f2.f64 = double(temp.f32);
	// lfs f22,2000(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2000);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f7,f4,f2,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f2.f64 + ctx.f7.f64));
	// lfs f15,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f15.f64 = double(temp.f32);
	// lfs f30,2112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2112);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f16,f16,f15
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f15.f64));
	// lfs f5,616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f30,f30,f25,f29
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f25.f64 + ctx.f29.f64));
	// stfs f14,5016(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5016, temp.u32);
	// fmuls f5,f5,f22
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f22.f64));
	// lfs f6,1108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	ctx.f6.f64 = double(temp.f32);
	// lfs f14,2864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2864);
	ctx.f14.f64 = double(temp.f32);
	// lfs f1,2896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2896);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f14,f14,f6
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f6.f64));
	// lfs f29,1808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1808);
	ctx.f29.f64 = double(temp.f32);
	// fadds f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// lfs f27,4360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4360);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f29,f29,f6
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f6.f64));
	// lfs f4,2888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2888);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,2128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2128);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f25,f13,f4
	ctx.f25.f64 = double(float(ctx.f13.f64 * ctx.f4.f64));
	// lfs f21,3148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3148);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f2,f2,f6
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f6.f64));
	// lfs f22,2880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2880);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f21,f6
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f6.f64));
	// lfs f17,308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f22,f22,f23
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f15,1608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1608);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f17,f27
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f27.f64));
	// stfs f28,2028(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2028, temp.u32);
	// fmuls f28,f13,f1
	ctx.f28.f64 = double(float(ctx.f13.f64 * ctx.f1.f64));
	// stfs f14,3640(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3640, temp.u32);
	// fmuls f15,f15,f20
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f20.f64));
	// lfs f14,4716(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4716);
	ctx.f14.f64 = double(temp.f32);
	// stfs f22,3712(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3712, temp.u32);
	// fmadds f11,f9,f13,f11
	ctx.f11.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f11.f64));
	// lfs f22,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f22.f64 = double(temp.f32);
	// fadds f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f10.f64));
	// fmadds f26,f18,f22,f26
	ctx.f26.f64 = double(float(ctx.f18.f64 * ctx.f22.f64 + ctx.f26.f64));
	// lfs f22,172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	ctx.f22.f64 = double(temp.f32);
	// stfs f3,2020(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2020, temp.u32);
	// fmsubs f9,f28,f22,f8
	ctx.f9.f64 = double(float(ctx.f28.f64 * ctx.f22.f64 - ctx.f8.f64));
	// fmsubs f3,f30,f6,f31
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f6.f64 - ctx.f31.f64));
	// stfs f0,5588(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5588, temp.u32);
	// lfs f10,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f14,f13,f14
	ctx.f14.f64 = double(float(ctx.f13.f64 * ctx.f14.f64));
	// lfs f0,604(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f10.f64));
	// lfs f10,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f10.f64 = double(temp.f32);
	// stfs f7,3632(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3632, temp.u32);
	// stfs f15,3720(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3720, temp.u32);
	// lfs f15,4332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4332);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f11,f17,f10,f11
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f10.f64 + ctx.f11.f64));
	// lfs f7,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f18,f13,f15
	ctx.f18.f64 = double(float(ctx.f13.f64 * ctx.f15.f64));
	// lfs f8,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f10,f25,f7,f9
	ctx.f10.f64 = double(float(-(ctx.f25.f64 * ctx.f7.f64 - ctx.f9.f64)));
	// lfs f15,4900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4900);
	ctx.f15.f64 = double(temp.f32);
	// fnmsubs f8,f2,f8,f3
	ctx.f8.f64 = double(float(-(ctx.f2.f64 * ctx.f8.f64 - ctx.f3.f64)));
	// lfs f9,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f9,f15,f9,f26
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f9.f64 + ctx.f26.f64));
	// stfs f4,1680(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1680, temp.u32);
	// lfs f4,2808(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2808);
	ctx.f4.f64 = double(temp.f32);
	// lfs f7,2816(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2816);
	ctx.f7.f64 = double(temp.f32);
	// lfs f31,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f31.f64 = double(temp.f32);
	// fadds f7,f4,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 + ctx.f7.f64));
	// lfs f3,4892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4892);
	ctx.f3.f64 = double(temp.f32);
	// lfs f4,2744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2744);
	ctx.f4.f64 = double(temp.f32);
	// stfs f29,2204(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2204, temp.u32);
	// fadds f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f3.f64));
	// lfs f29,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f29.f64 = double(temp.f32);
	// lfs f17,2464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2464);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f11,f16,f29,f11
	ctx.f11.f64 = double(float(-(ctx.f16.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// fnmsubs f8,f5,f31,f8
	ctx.f8.f64 = double(float(-(ctx.f5.f64 * ctx.f31.f64 - ctx.f8.f64)));
	// lfs f5,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f5.f64 = double(temp.f32);
	// lfs f12,1600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1600);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f10,f14,f5,f10
	ctx.f10.f64 = double(float(-(ctx.f14.f64 * ctx.f5.f64 - ctx.f10.f64)));
	// stfs f1,2116(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2116, temp.u32);
	// fmuls f12,f12,f27
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f27.f64));
	// lfs f2,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f2.f64 = double(temp.f32);
	// fadds f9,f9,f17
	ctx.f9.f64 = double(float(ctx.f9.f64 + ctx.f17.f64));
	// lfs f1,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f0,f2,f19,f0
	ctx.f0.f64 = double(float(ctx.f2.f64 * ctx.f19.f64 + ctx.f0.f64));
	// lfs f3,1664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1664);
	ctx.f3.f64 = double(temp.f32);
	// lfs f16,1984(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1984);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f3,f1,f3
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f3.f64));
	// lfs f14,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f16,f16,f6
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f6.f64));
	// lfs f17,1916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1916);
	ctx.f17.f64 = double(temp.f32);
	// stfs f12,2152(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2152, temp.u32);
	// fmuls f17,f17,f14
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f14.f64));
	// stfs f27,2124(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2124, temp.u32);
	// lfs f12,1592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1592);
	ctx.f12.f64 = double(temp.f32);
	// lfs f2,1860(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1860);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f12,f12,f20
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f20.f64));
	// lfs f1,1688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1688);
	ctx.f1.f64 = double(temp.f32);
	// lfs f5,3256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3256);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f30,f1,f2
	ctx.f30.f64 = double(float(ctx.f1.f64 * ctx.f2.f64));
	// lfs f31,2672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2672);
	ctx.f31.f64 = double(temp.f32);
	// lfs f29,4160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4160);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f31,f31,f6
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f6.f64));
	// lfs f28,4220(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4220);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f20,f29
	ctx.f29.f64 = double(float(ctx.f20.f64 * ctx.f29.f64));
	// lfs f27,2504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2504);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f28,f28,f23
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f26,2480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2480);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f27,f5,f27
	ctx.f27.f64 = double(float(ctx.f5.f64 * ctx.f27.f64));
	// lfs f22,2472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2472);
	ctx.f22.f64 = double(temp.f32);
	// stfs f24,3592(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3592, temp.u32);
	// fmuls f24,f13,f26
	ctx.f24.f64 = double(float(ctx.f13.f64 * ctx.f26.f64));
	// stfs f16,2172(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2172, temp.u32);
	// fmuls f22,f13,f22
	ctx.f22.f64 = double(float(ctx.f13.f64 * ctx.f22.f64));
	// lfs f25,3500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3500);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f0,f16,f25,f0
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f25.f64 + ctx.f0.f64));
	// lfs f16,2456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2456);
	ctx.f16.f64 = double(temp.f32);
	// fadds f4,f4,f16
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f16.f64));
	// lfs f16,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f7,f7,f14
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f14.f64));
	// lfs f14,4560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4560);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f3,f14,f16,f3
	ctx.f3.f64 = double(float(ctx.f14.f64 * ctx.f16.f64 + ctx.f3.f64));
	// lfs f16,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f8,f21,f16,f8
	ctx.f8.f64 = double(float(ctx.f21.f64 * ctx.f16.f64 + ctx.f8.f64));
	// stfs f24,1688(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1688, temp.u32);
	// lfs f24,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f24.f64 = double(temp.f32);
	// lfs f21,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f30,f30,f24
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// fnmsubs f10,f18,f21,f10
	ctx.f10.f64 = double(float(-(ctx.f18.f64 * ctx.f21.f64 - ctx.f10.f64)));
	// lfs f16,3720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3720);
	ctx.f16.f64 = double(temp.f32);
	// lfs f24,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f11,f16,f24,f11
	ctx.f11.f64 = double(float(-(ctx.f16.f64 * ctx.f24.f64 - ctx.f11.f64)));
	// stfs f31,2196(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2196, temp.u32);
	// stfs f5,3536(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3536, temp.u32);
	// lfs f5,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f5.f64 = double(temp.f32);
	// lfs f31,3712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3712);
	ctx.f31.f64 = double(temp.f32);
	// stfs f23,2140(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2140, temp.u32);
	// fnmsubs f8,f31,f5,f8
	ctx.f8.f64 = double(float(-(ctx.f31.f64 * ctx.f5.f64 - ctx.f8.f64)));
	// lfs f5,2152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2152);
	ctx.f5.f64 = double(temp.f32);
	// lfs f23,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f23.f64 = double(temp.f32);
	// stfs f20,3544(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3544, temp.u32);
	// fmuls f0,f0,f23
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f23.f64));
	// fmadds f10,f9,f13,f10
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f13.f64 + ctx.f10.f64));
	// lfs f9,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f9.f64 = double(temp.f32);
	// lfs f21,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f11,f5,f9,f11
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f9.f64 - ctx.f11.f64)));
	// lfs f20,5016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5016);
	ctx.f20.f64 = double(temp.f32);
	// lfs f23,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f23.f64 = double(temp.f32);
	// lfs f18,232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	ctx.f18.f64 = double(temp.f32);
	// stfs f28,2188(r1)
	temp.f32 = float(ctx.f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2188, temp.u32);
	// stfs f27,2180(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2180, temp.u32);
	// fnmsubs f8,f20,f21,f8
	ctx.f8.f64 = double(float(-(ctx.f20.f64 * ctx.f21.f64 - ctx.f8.f64)));
	// lfs f21,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f21.f64 = double(temp.f32);
	// lfs f20,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f20.f64 = double(temp.f32);
	// lfs f27,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f27.f64 = double(temp.f32);
	// lfs f28,3304(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3304);
	ctx.f28.f64 = double(temp.f32);
	// lfs f31,2212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2212);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f28,f27
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f27.f64));
	// fmadds f12,f12,f23,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f16,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f11,f22,f21,f10
	ctx.f11.f64 = double(float(-(ctx.f22.f64 * ctx.f21.f64 - ctx.f10.f64)));
	// lfs f21,5104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5104);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f21,f20,f18,f21
	ctx.f21.f64 = double(float(ctx.f20.f64 * ctx.f18.f64 + ctx.f21.f64));
	// lfs f20,5020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5020);
	ctx.f20.f64 = double(temp.f32);
	// lfs f18,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f18.f64 = double(temp.f32);
	// fadds f4,f4,f20
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f20.f64));
	// lfs f10,1764(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1764);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f3,f3,f18,f30
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f18.f64 + ctx.f30.f64));
	// stfs f29,3720(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3720, temp.u32);
	// fmuls f2,f31,f2
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f2.f64));
	// stfs f26,2108(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2108, temp.u32);
	// fmsubs f7,f17,f16,f7
	ctx.f7.f64 = double(float(ctx.f17.f64 * ctx.f16.f64 - ctx.f7.f64));
	// stfs f25,2156(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2156, temp.u32);
	// fmuls f31,f31,f10
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f10.f64));
	// lfs f24,3020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3020);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f10,f1,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f10.f64));
	// lfs f9,2548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2548);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f24,f24,f6
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f6.f64));
	// lfs f5,3512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3512);
	ctx.f5.f64 = double(temp.f32);
	// lfs f29,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f29.f64 = double(temp.f32);
	// lfs f26,1584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1584);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f29,f29,f9
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// lfs f27,1852(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1852);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f26,f5,f26
	ctx.f26.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// lfs f25,2224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2224);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,2332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2332);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f25,f27
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f27.f64));
	// lfs f20,784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 784);
	ctx.f20.f64 = double(temp.f32);
	// lfs f30,3624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3624);
	ctx.f30.f64 = double(temp.f32);
	// lfs f18,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f20,f30
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// lfs f22,3504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3504);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f18,f18,f23
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f23.f64));
	// lfs f17,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,2564(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2564);
	ctx.f16.f64 = double(temp.f32);
	// stfs f15,1664(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1664, temp.u32);
	// lfs f15,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f17,f17,f22
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f22.f64));
	// fmadds f28,f16,f15,f28
	ctx.f28.f64 = double(float(ctx.f16.f64 * ctx.f15.f64 + ctx.f28.f64));
	// lfs f14,752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 752);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f16,f14,f16
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f16.f64));
	// stfs f17,5104(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5104, temp.u32);
	// lfs f17,1148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1148);
	ctx.f17.f64 = double(temp.f32);
	// stfs f16,3712(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3712, temp.u32);
	// fmuls f19,f17,f19
	ctx.f19.f64 = double(float(ctx.f17.f64 * ctx.f19.f64));
	// lfs f16,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f16.f64 = double(temp.f32);
	// stfs f19,2212(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2212, temp.u32);
	// fmuls f26,f26,f16
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f16.f64));
	// lfs f19,3640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3640);
	ctx.f19.f64 = double(temp.f32);
	// lfs f16,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f8,f19,f16,f8
	ctx.f8.f64 = double(float(-(ctx.f19.f64 * ctx.f16.f64 - ctx.f8.f64)));
	// lfs f17,3720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3720);
	ctx.f17.f64 = double(temp.f32);
	// lfs f19,264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	ctx.f19.f64 = double(temp.f32);
	// stfs f13,3560(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3560, temp.u32);
	// fnmsubs f12,f17,f19,f12
	ctx.f12.f64 = double(float(-(ctx.f17.f64 * ctx.f19.f64 - ctx.f12.f64)));
	// lfs f13,3632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3632);
	ctx.f13.f64 = double(temp.f32);
	// lfs f17,1688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1688);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f16.f64 = double(temp.f32);
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f11,f17,f16,f11
	ctx.f11.f64 = double(float(-(ctx.f17.f64 * ctx.f16.f64 - ctx.f11.f64)));
	// fmsubs f0,f29,f19,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f19.f64 - ctx.f0.f64));
	// lfs f17,3476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3476);
	ctx.f17.f64 = double(temp.f32);
	// lfs f29,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f29,f29,f17,f25
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f17.f64 + ctx.f25.f64));
	// lfs f25,300(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f13,f13,f6,f8
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f6.f64 + ctx.f8.f64));
	// lfs f19,624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 624);
	ctx.f19.f64 = double(temp.f32);
	// stfs f9,2132(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2132, temp.u32);
	// fmuls f9,f25,f19
	ctx.f9.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// stfs f9,1688(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1688, temp.u32);
	// stfs f5,3640(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3640, temp.u32);
	// lfs f5,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f5.f64 = double(temp.f32);
	// lfs f8,2204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2204);
	ctx.f8.f64 = double(temp.f32);
	// lfs f16,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f16.f64 = double(temp.f32);
	// lfs f9,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f7,f4,f16,f7
	ctx.f7.f64 = double(float(ctx.f4.f64 * ctx.f16.f64 + ctx.f7.f64));
	// fnmsubs f9,f2,f9,f3
	ctx.f9.f64 = double(float(-(ctx.f2.f64 * ctx.f9.f64 - ctx.f3.f64)));
	// lfs f4,2180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2180);
	ctx.f4.f64 = double(temp.f32);
	// lfs f3,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f13,f8,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f8.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// lfs f14,1248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1248);
	ctx.f14.f64 = double(temp.f32);
	// fmr f8,f5
	ctx.f8.f64 = ctx.f5.f64;
	// lfs f5,3416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3416);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f12,f4,f3,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f3.f64 + ctx.f12.f64));
	// lfs f15,3424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3424);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f3,f21,f5,f31
	ctx.f3.f64 = double(float(ctx.f21.f64 * ctx.f5.f64 + ctx.f31.f64));
	// stfs f14,2152(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2152, temp.u32);
	// fmr f21,f16
	ctx.f21.f64 = ctx.f16.f64;
	// lfs f4,2012(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2012);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f14,f14,f15
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f15.f64));
	// lfs f2,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f2.f64 = double(temp.f32);
	// lfs f31,2872(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2872);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f2,f2,f4,f20
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f4.f64 + ctx.f20.f64));
	// lfs f20,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f20.f64 = double(temp.f32);
	// stfs f23,3576(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3576, temp.u32);
	// lfs f25,2196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2196);
	ctx.f25.f64 = double(temp.f32);
	// lfs f23,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f13,f25,f23,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f23.f64 - ctx.f13.f64)));
	// lfs f25,548(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f25.f64 = double(temp.f32);
	// stfs f22,2148(r1)
	temp.f32 = float(ctx.f22.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2148, temp.u32);
	// fadds f12,f12,f11
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// lfs f22,2188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2188);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f1,f1,f25
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f25.f64));
	// fnmsubs f0,f31,f21,f0
	ctx.f0.f64 = double(float(-(ctx.f31.f64 * ctx.f21.f64 - ctx.f0.f64)));
	// lfs f31,3972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3972);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f26,f14,f20,f26
	ctx.f26.f64 = double(float(ctx.f14.f64 * ctx.f20.f64 - ctx.f26.f64));
	// lfs f20,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f7,f31,f21,f7
	ctx.f7.f64 = double(float(-(ctx.f31.f64 * ctx.f21.f64 - ctx.f7.f64)));
	// lfs f31,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f10,f10,f20,f9
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f20.f64 - ctx.f9.f64)));
	// lfs f17,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f17.f64 = double(temp.f32);
	// lfs f9,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f31,f31,f27
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f27.f64));
	// lfs f23,828(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	ctx.f23.f64 = double(temp.f32);
	// lfs f11,636(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	ctx.f11.f64 = double(temp.f32);
	// lfs f21,2588(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2588);
	ctx.f21.f64 = double(temp.f32);
	// lfs f16,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f13,f22,f17,f13
	ctx.f13.f64 = double(float(-(ctx.f22.f64 * ctx.f17.f64 - ctx.f13.f64)));
	// fmuls f21,f21,f16
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// lfs f16,3680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3680);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f14,f15,f11
	ctx.f14.f64 = double(float(ctx.f15.f64 * ctx.f11.f64));
	// stfs f14,3680(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3680, temp.u32);
	// lfs f14,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f20,f30,f23
	ctx.f20.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmadds f0,f28,f14,f0
	ctx.f0.f64 = double(float(ctx.f28.f64 * ctx.f14.f64 + ctx.f0.f64));
	// lfs f17,2788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2788);
	ctx.f17.f64 = double(temp.f32);
	// lfs f28,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f17,f28
	ctx.f28.f64 = double(float(ctx.f17.f64 * ctx.f28.f64));
	// lfs f17,2172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2172);
	ctx.f17.f64 = double(temp.f32);
	// lfs f22,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f22.f64 = double(temp.f32);
	// stfs f20,3720(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3720, temp.u32);
	// fmadds f2,f2,f22,f26
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f22.f64 + ctx.f26.f64));
	// lfs f20,5064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5064);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f13,f17,f8,f13
	ctx.f13.f64 = double(float(-(ctx.f17.f64 * ctx.f8.f64 - ctx.f13.f64)));
	// lfs f22,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f20,f5,f20
	ctx.f20.f64 = double(float(ctx.f5.f64 * ctx.f20.f64));
	// lfs f26,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f10,f3,f22,f10
	ctx.f10.f64 = double(float(ctx.f3.f64 * ctx.f22.f64 + ctx.f10.f64));
	// stfs f11,3632(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3632, temp.u32);
	// fmuls f5,f5,f16
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// lfs f11,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f7,f29,f26,f7
	ctx.f7.f64 = double(float(ctx.f29.f64 * ctx.f26.f64 + ctx.f7.f64));
	// lfs f29,756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 756);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f29,f4,f29
	ctx.f29.f64 = double(float(ctx.f4.f64 * ctx.f29.f64));
	// stfs f12,5592(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5592, temp.u32);
	// lfs f3,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f3.f64 = double(temp.f32);
	// lfs f12,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f3,f3,f19,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f19.f64 + ctx.f31.f64));
	// stfs f25,3624(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3624, temp.u32);
	// lfs f25,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f13,f24,f9,f13
	ctx.f13.f64 = double(float(-(ctx.f24.f64 * ctx.f9.f64 - ctx.f13.f64)));
	// stfs f13,5596(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5596, temp.u32);
	// lfs f13,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f0,f18,f13,f0
	ctx.f0.f64 = double(float(-(ctx.f18.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// lfs f13,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f1,f13,f2
	ctx.f13.f64 = double(float(-(ctx.f1.f64 * ctx.f13.f64 - ctx.f2.f64)));
	// lfs f9,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f11,f5,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f5.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// lfs f1,5104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5104);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f12,f21,f12,f7
	ctx.f12.f64 = double(float(-(ctx.f21.f64 * ctx.f12.f64 - ctx.f7.f64)));
	// lfs f24,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f24.f64 = double(temp.f32);
	// stfs f27,2172(r1)
	temp.f32 = float(ctx.f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2172, temp.u32);
	// stfs f6,2180(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2180, temp.u32);
	// lfs f26,1856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1856);
	ctx.f26.f64 = double(temp.f32);
	// lfs f31,3864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3864);
	ctx.f31.f64 = double(temp.f32);
	// lfs f22,2080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2080);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f26,f26,f31
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f31.f64));
	// lfs f10,2392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2392);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f0,f1,f9,f0
	ctx.f0.f64 = double(float(-(ctx.f1.f64 * ctx.f9.f64 - ctx.f0.f64)));
	// lfs f1,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f13,f20,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f20.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f25,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f11,f29,f25,f11
	ctx.f11.f64 = double(float(-(ctx.f29.f64 * ctx.f25.f64 - ctx.f11.f64)));
	// lfs f29,2448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2448);
	ctx.f29.f64 = double(temp.f32);
	// fmr f25,f24
	ctx.f25.f64 = ctx.f24.f64;
	// lfs f9,2032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2032);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f3,f24,f12
	ctx.f12.f64 = double(float(ctx.f3.f64 * ctx.f24.f64 + ctx.f12.f64));
	// lfs f3,2384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2384);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f9,f9,f1
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f1.f64));
	// lfs f8,3408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3408);
	ctx.f8.f64 = double(temp.f32);
	// fadds f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 + ctx.f29.f64));
	// lfs f7,2800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2800);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,1440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1440);
	ctx.f6.f64 = double(temp.f32);
	// fadds f7,f22,f7
	ctx.f7.f64 = double(float(ctx.f22.f64 + ctx.f7.f64));
	// lfs f5,4980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4980);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,2496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2496);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f5,f6,f5
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// lfs f27,776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f2,f10,f2
	ctx.f2.f64 = double(float(ctx.f10.f64 * ctx.f2.f64));
	// lfs f1,2968(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2968);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f27,f27,f30
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f30.f64));
	// lfs f29,3712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3712);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f1,f8,f1
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f1.f64));
	// stfs f23,2196(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2196, temp.u32);
	// fmadds f0,f29,f25,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f25.f64 + ctx.f0.f64));
	// stfs f15,5016(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5016, temp.u32);
	// stfs f19,3552(r1)
	temp.f32 = float(ctx.f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3552, temp.u32);
	// lfs f14,3720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3720);
	ctx.f14.f64 = double(temp.f32);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfs f15,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f15.f64 = double(temp.f32);
	// lis r10,-32254
	ctx.r10.s64 = -2113798144;
	// fnmsubs f13,f14,f15,f13
	ctx.f13.f64 = double(float(-(ctx.f14.f64 * ctx.f15.f64 - ctx.f13.f64)));
	// lfs f15,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f15.f64 = double(temp.f32);
	// lfs f24,2164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2164);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f12,f9,f15,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f15.f64 - ctx.f12.f64)));
	// lfs f25,3400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3400);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// lfs f22,3680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3680);
	ctx.f22.f64 = double(temp.f32);
	// lfs f15,2152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2152);
	ctx.f15.f64 = double(temp.f32);
	// fmadds f22,f25,f24,f22
	ctx.f22.f64 = double(float(ctx.f25.f64 * ctx.f24.f64 + ctx.f22.f64));
	// stfs f25,2152(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2152, temp.u32);
	// lfs f25,2212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2212);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f14.f64 = double(temp.f32);
	// stfs f8,2164(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2164, temp.u32);
	// fnmsubs f0,f25,f14,f0
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// fmr f8,f14
	ctx.f8.f64 = ctx.f14.f64;
	// lfs f16,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f16.f64 = double(temp.f32);
	// lfs f18,880(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f5,f5,f16
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f16.f64));
	// lfs f21,3144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3144);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f18,f18,f4
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f4.f64));
	// lfs f19,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f19.f64 = double(temp.f32);
	// lfs f17,4212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4212);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f26,f21,f17,f26
	ctx.f26.f64 = double(float(ctx.f21.f64 * ctx.f17.f64 + ctx.f26.f64));
	// lfs f20,2960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2960);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f2,f2,f16
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f16.f64));
	// stfs f4,3680(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3680, temp.u32);
	// fmuls f20,f20,f19
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f19.f64));
	// lfs f4,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f4.f64 = double(temp.f32);
	// lfs f29,2856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2856);
	ctx.f29.f64 = double(temp.f32);
	// fmadds f13,f22,f4,f13
	ctx.f13.f64 = double(float(ctx.f22.f64 * ctx.f4.f64 + ctx.f13.f64));
	// lfs f23,1144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1144);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f7,f7,f29
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f29.f64));
	// lfs f16,912(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 912);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// lfs f19,2932(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2932);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f1,f15,f16,f1
	ctx.f1.f64 = double(float(ctx.f15.f64 * ctx.f16.f64 + ctx.f1.f64));
	// lfs f4,1688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1688);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f19,f19,f23
	ctx.f19.f64 = double(float(ctx.f19.f64 * ctx.f23.f64));
	// lfs f9,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f0,f4,f8,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f8.f64 + ctx.f0.f64));
	// lfs f16,1848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1848);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f11,f27,f9,f11
	ctx.f11.f64 = double(float(-(ctx.f27.f64 * ctx.f9.f64 - ctx.f11.f64)));
	// fmuls f27,f16,f31
	ctx.f27.f64 = double(float(ctx.f16.f64 * ctx.f31.f64));
	// lfs f25,2432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2432);
	ctx.f25.f64 = double(temp.f32);
	// lfs f4,2488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2488);
	ctx.f4.f64 = double(temp.f32);
	// stfs f24,2204(r1)
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2204, temp.u32);
	// fadds f4,f4,f25
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f25.f64));
	// lfs f24,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f24.f64 = double(temp.f32);
	// lfs f25,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f26,f24
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f24.f64));
	// lfs f24,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f24.f64 = double(temp.f32);
	// fmsubs f7,f7,f25,f5
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f25.f64 - ctx.f5.f64));
	// lfs f8,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f8.f64 = double(temp.f32);
	// fmsubs f5,f19,f24,f2
	ctx.f5.f64 = double(float(ctx.f19.f64 * ctx.f24.f64 - ctx.f2.f64));
	// stfs f23,3720(r1)
	temp.f32 = float(ctx.f23.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3720, temp.u32);
	// lfs f24,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f0,f28,f8,f0
	ctx.f0.f64 = double(float(-(ctx.f28.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f23,2352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2352);
	ctx.f23.f64 = double(temp.f32);
	// fmr f2,f8
	ctx.f2.f64 = ctx.f8.f64;
	// lfs f9,1424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1424);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f18,f24,f13
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f24.f64 + ctx.f13.f64));
	// lfs f16,2440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2440);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f27,f23,f17,f27
	ctx.f27.f64 = double(float(ctx.f23.f64 * ctx.f17.f64 + ctx.f27.f64));
	// lfs f25,1076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f9,f29,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f9.f64));
	// lfs f8,2120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2120);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f16,f10,f16
	ctx.f16.f64 = double(float(ctx.f10.f64 * ctx.f16.f64));
	// lfs f28,2376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2376);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f25,f25,f30
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f30.f64));
	// lfs f24,2424(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2424);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f8,f31,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f8.f64));
	// lfs f23,4532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4532);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f28,f10,f28
	ctx.f28.f64 = double(float(ctx.f10.f64 * ctx.f28.f64));
	// lfs f22,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f22.f64 = double(temp.f32);
	// stfs f21,2188(r1)
	temp.f32 = float(ctx.f21.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2188, temp.u32);
	// stfs f15,2212(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2212, temp.u32);
	// fadds f24,f24,f23
	ctx.f24.f64 = double(float(ctx.f24.f64 + ctx.f23.f64));
	// lfs f23,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f4,f4,f22
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f22.f64));
	// lfs f22,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f22.f64 = double(temp.f32);
	// fmsubs f3,f3,f23,f26
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f23.f64 - ctx.f26.f64));
	// lfs f26,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f9,f9,f22,f7
	ctx.f9.f64 = double(float(-(ctx.f9.f64 * ctx.f22.f64 - ctx.f7.f64)));
	// lfs f22,1840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1840);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f7,f16,f26,f5
	ctx.f7.f64 = double(float(-(ctx.f16.f64 * ctx.f26.f64 - ctx.f5.f64)));
	// lfs f5,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f11,f1,f5,f11
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f5.f64 + ctx.f11.f64));
	// lfs f1,2792(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2792);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f5,f20,f2,f0
	ctx.f5.f64 = double(float(-(ctx.f20.f64 * ctx.f2.f64 - ctx.f0.f64)));
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f22,f31
	ctx.f31.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// lfs f2,2320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2320);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f23,f2,f1
	ctx.f23.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f20,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f20.f64 = double(temp.f32);
	// lfs f22,1028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f13,f25,f20,f13
	ctx.f13.f64 = double(float(-(ctx.f25.f64 * ctx.f20.f64 - ctx.f13.f64)));
	// stfs f11,3712(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3712, temp.u32);
	// fmuls f30,f22,f30
	ctx.f30.f64 = double(float(ctx.f22.f64 * ctx.f30.f64));
	// lfs f21,2008(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2008);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f2,f2,f17
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f17.f64));
	// fmadds f3,f27,f0,f3
	ctx.f3.f64 = double(float(ctx.f27.f64 * ctx.f0.f64 + ctx.f3.f64));
	// lfs f27,1376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1376);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f9,f8,f0,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f0,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f8,f17,f27
	ctx.f8.f64 = double(float(ctx.f17.f64 * ctx.f27.f64));
	// lfs f26,3316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3316);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f4,f24,f0,f4
	ctx.f4.f64 = double(float(ctx.f24.f64 * ctx.f0.f64 + ctx.f4.f64));
	// lfs f19,2264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2264);
	ctx.f19.f64 = double(temp.f32);
	// fmr f27,f0
	ctx.f27.f64 = ctx.f0.f64;
	// lfs f0,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// fadds f12,f5,f12
	ctx.f12.f64 = double(float(ctx.f5.f64 + ctx.f12.f64));
	// stfs f12,5600(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5600, temp.u32);
	// fmuls f21,f29,f21
	ctx.f21.f64 = double(float(ctx.f29.f64 * ctx.f21.f64));
	// lfs f25,2344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2344);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f17,f26
	ctx.f26.f64 = double(float(ctx.f17.f64 * ctx.f26.f64));
	// stfs f26,5064(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5064, temp.u32);
	// lfs f26,3680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3680);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f20,f25,f19
	ctx.f20.f64 = double(float(ctx.f25.f64 * ctx.f19.f64));
	// stfs f1,1688(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1688, temp.u32);
	// lfs f18,-19384(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -19384);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f9,f23,f0,f9
	ctx.f9.f64 = double(float(-(ctx.f23.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// lfs f1,2104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2104);
	ctx.f1.f64 = double(temp.f32);
	// lfs f19,1784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1784);
	ctx.f19.f64 = double(temp.f32);
	// fmr f24,f18
	ctx.f24.f64 = ctx.f18.f64;
	// fmuls f12,f4,f10
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// lfs f5,3124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3124);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f11,f28,f27,f7
	ctx.f11.f64 = double(float(-(ctx.f28.f64 * ctx.f27.f64 - ctx.f7.f64)));
	// lfs f27,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f7,f31,f0,f3
	ctx.f7.f64 = double(float(-(ctx.f31.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// lfs f28,1060(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	ctx.f28.f64 = double(temp.f32);
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// lfs f3,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f3.f64 = double(temp.f32);
	// fmadds f13,f30,f27,f13
	ctx.f13.f64 = double(float(ctx.f30.f64 * ctx.f27.f64 + ctx.f13.f64));
	// lfs f30,2288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2288);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f28,f28,f26
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f26.f64));
	// lfs f26,2336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2336);
	ctx.f26.f64 = double(temp.f32);
	// fadds f30,f26,f30
	ctx.f30.f64 = double(float(ctx.f26.f64 + ctx.f30.f64));
	// lfs f31,2884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2884);
	ctx.f31.f64 = double(temp.f32);
	// lfs f27,2256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2256);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f19,f29,f19
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// fmadds f9,f21,f3,f9
	ctx.f9.f64 = double(float(ctx.f21.f64 * ctx.f3.f64 + ctx.f9.f64));
	// lfs f3,1776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1776);
	ctx.f3.f64 = double(temp.f32);
	// lfs f26,1992(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1992);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f5,f10,f5
	ctx.f5.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// lfs f16,-19516(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19516);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f27,f27,f1
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f1.f64));
	// lfs f23,1752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1752);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f3,f3,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// stfs f18,448(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 448, temp.u32);
	// fmuls f26,f10,f26
	ctx.f26.f64 = double(float(ctx.f10.f64 * ctx.f26.f64));
	// fnmsubs f7,f6,f4,f7
	ctx.f7.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f7.f64)));
	// lfs f4,2416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2416);
	ctx.f4.f64 = double(temp.f32);
	// lfs f6,3964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3964);
	ctx.f6.f64 = double(temp.f32);
	// fadds f6,f6,f4
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f4.f64));
	// lfs f4,2236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2236);
	ctx.f4.f64 = double(temp.f32);
	// stfs f16,5104(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5104, temp.u32);
	// fmuls f4,f4,f1
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f1.f64));
	// fmadds f9,f8,f0,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f9.f64));
	// lfs f8,5064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5064);
	ctx.f8.f64 = double(temp.f32);
	// lfs f1,660(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 660);
	ctx.f1.f64 = double(temp.f32);
	// fnmsubs f0,f8,f0,f7
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f7.f64)));
	// fmadds f11,f5,f24,f11
	ctx.f11.f64 = double(float(ctx.f5.f64 * ctx.f24.f64 + ctx.f11.f64));
	// lfs f7,2972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2972);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f24,f23,f1
	ctx.f24.f64 = double(float(ctx.f23.f64 * ctx.f1.f64));
	// lfs f15,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f7,f25,f7
	ctx.f7.f64 = double(float(ctx.f25.f64 * ctx.f7.f64));
	// lfs f25,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f13,f28,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f25,448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	ctx.f25.f64 = double(temp.f32);
	// fmsubs f12,f26,f25,f12
	ctx.f12.f64 = double(float(ctx.f26.f64 * ctx.f25.f64 - ctx.f12.f64));
	// lfs f18,812(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 812);
	ctx.f18.f64 = double(temp.f32);
	// lfs f25,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f18,f15,f18
	ctx.f18.f64 = double(float(ctx.f15.f64 * ctx.f18.f64));
	// lfs f8,4652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4652);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f6,f6,f31
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f31.f64));
	// fmuls f8,f29,f8
	ctx.f8.f64 = double(float(ctx.f29.f64 * ctx.f8.f64));
	// lfs f14,2400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2400);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f10,f10,f14
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f14.f64));
	// lfs f14,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f14.f64 = double(temp.f32);
	// fnmsubs f0,f2,f14,f0
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// lfs f28,5104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5104);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f3,f3,f28
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f28.f64));
	// lfs f21,820(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 820);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f25,f24,f25
	ctx.f25.f64 = double(float(ctx.f24.f64 * ctx.f25.f64));
	// lfs f24,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f9,f19,f24,f9
	ctx.f9.f64 = double(float(-(ctx.f19.f64 * ctx.f24.f64 - ctx.f9.f64)));
	// lfs f19,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f19.f64 = double(temp.f32);
	// lfs f31,2232(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2232);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f23,f23,f21
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f21.f64));
	// lfs f28,3720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3720);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f29
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f29.f64));
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f18,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f18.f64 = double(temp.f32);
	// lfs f5,1768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1768);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f4,f31,f28,f4
	ctx.f4.f64 = double(float(ctx.f31.f64 * ctx.f28.f64 + ctx.f4.f64));
	// fmuls f16,f5,f21
	ctx.f16.f64 = double(float(ctx.f5.f64 * ctx.f21.f64));
	// lfs f26,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f26.f64 = double(temp.f32);
	// lfs f31,3592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3592);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f11,f27,f26,f11
	ctx.f11.f64 = double(float(ctx.f27.f64 * ctx.f26.f64 + ctx.f11.f64));
	// lfs f28,5016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5016);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f22,f22,f31
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f31.f64));
	// fmuls f28,f28,f31
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f27,1020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	ctx.f27.f64 = double(temp.f32);
	// lfs f26,1576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1576);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f9,f8,f18,f9
	ctx.f9.f64 = double(float(-(ctx.f8.f64 * ctx.f18.f64 - ctx.f9.f64)));
	// lfs f15,2408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2408);
	ctx.f15.f64 = double(temp.f32);
	// fmr f8,f14
	ctx.f8.f64 = ctx.f14.f64;
	// lfs f24,1568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1568);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f27,f27,f26
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f26.f64));
	// lfs f2,3696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3696);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f15,f29,f15
	ctx.f15.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// lfs f18,2216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2216);
	ctx.f18.f64 = double(temp.f32);
	// stfs f17,2216(r1)
	temp.f32 = float(ctx.f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2216, temp.u32);
	// fmuls f18,f18,f24
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f24.f64));
	// fmadds f2,f2,f24,f16
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f24.f64 + ctx.f16.f64));
	// lfs f16,236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	ctx.f16.f64 = double(temp.f32);
	// lfs f21,4156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4156);
	ctx.f21.f64 = double(temp.f32);
	// lfs f17,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f21,f29,f21
	ctx.f21.f64 = double(float(ctx.f29.f64 * ctx.f21.f64));
	// fmuls f28,f28,f16
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f16,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f0,f20,f8,f0
	ctx.f0.f64 = double(float(-(ctx.f20.f64 * ctx.f8.f64 - ctx.f0.f64)));
	// lfs f8,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f8.f64 = double(temp.f32);
	// fmr f20,f14
	ctx.f20.f64 = ctx.f14.f64;
	// lfs f14,2368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2368);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f27,f27,f16,f25
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f16.f64 + ctx.f25.f64));
	// fmr f25,f8
	ctx.f25.f64 = ctx.f8.f64;
	// fmadds f9,f15,f8,f9
	ctx.f9.f64 = double(float(ctx.f15.f64 * ctx.f8.f64 + ctx.f9.f64));
	// lfs f8,3712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3712);
	ctx.f8.f64 = double(temp.f32);
	// fadds f13,f8,f13
	ctx.f13.f64 = double(float(ctx.f8.f64 + ctx.f13.f64));
	// lfs f8,552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f14,f29,f14
	ctx.f14.f64 = double(float(ctx.f29.f64 * ctx.f14.f64));
	// fmadds f2,f2,f17,f19
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f17.f64 + ctx.f19.f64));
	// fnmsubs f0,f7,f20,f0
	ctx.f0.f64 = double(float(-(ctx.f7.f64 * ctx.f20.f64 - ctx.f0.f64)));
	// lfs f7,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f7.f64 = double(temp.f32);
	// fmsubs f7,f6,f7,f3
	ctx.f7.f64 = double(float(ctx.f6.f64 * ctx.f7.f64 - ctx.f3.f64));
	// lfs f20,800(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 800);
	ctx.f20.f64 = double(temp.f32);
	// lfs f6,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f0,f30,f6,f0
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f6.f64 + ctx.f0.f64));
	// lfs f6,212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f9,f21,f25,f9
	ctx.f9.f64 = double(float(-(ctx.f21.f64 * ctx.f25.f64 - ctx.f9.f64)));
	// lfs f21,1432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1432);
	ctx.f21.f64 = double(temp.f32);
	// fnmsubs f11,f10,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f10.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// lfs f10,3632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3632);
	ctx.f10.f64 = double(temp.f32);
	// fmadds f12,f4,f20,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f20.f64 + ctx.f12.f64));
	// lfs f4,3640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3640);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f6,f6,f21
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f21.f64));
	// lfs f21,1368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1368);
	ctx.f21.f64 = double(temp.f32);
	// lfs f15,2196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2196);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f3,f4,f10
	ctx.f3.f64 = double(float(ctx.f4.f64 * ctx.f10.f64));
	// fmuls f10,f21,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f10.f64));
	// lfs f17,1696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1696);
	ctx.f17.f64 = double(temp.f32);
	// lfs f16,3624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3624);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f15,f15,f31
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f31.f64));
	// lfs f21,2204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2204);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f17,f17,f24
	ctx.f17.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// stfs f15,1696(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1696, temp.u32);
	// fmuls f21,f21,f16
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f16.f64));
	// lfs f15,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f15.f64 = double(temp.f32);
	// lfs f19,2152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2152);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f2,f18,f15,f2
	ctx.f2.f64 = double(float(ctx.f18.f64 * ctx.f15.f64 - ctx.f2.f64));
	// lfs f16,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f30,f19,f1
	ctx.f30.f64 = double(float(ctx.f19.f64 * ctx.f1.f64));
	// fmsubs f28,f28,f16,f27
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f16.f64 - ctx.f27.f64));
	// lfs f19,2212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2212);
	ctx.f19.f64 = double(temp.f32);
	// lfs f18,536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 536);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f4,f19,f4
	ctx.f4.f64 = double(float(ctx.f19.f64 * ctx.f4.f64));
	// lfs f27,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f27,f27,f18
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f18.f64));
	// lfs f18,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f18.f64 = double(temp.f32);
	// lfs f19,2752(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2752);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f14,f18,f9
	ctx.f9.f64 = double(float(ctx.f14.f64 * ctx.f18.f64 + ctx.f9.f64));
	// stfs f11,5612(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5612, temp.u32);
	// fmuls f19,f29,f19
	ctx.f19.f64 = double(float(ctx.f29.f64 * ctx.f19.f64));
	// lfs f11,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f11.f64 = double(temp.f32);
	// stfs f0,5608(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5608, temp.u32);
	// lfs f0,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f11,f23,f11,f2
	ctx.f11.f64 = double(float(ctx.f23.f64 * ctx.f11.f64 + ctx.f2.f64));
	// lfs f20,1544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1544);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f0,f30,f0,f28
	ctx.f0.f64 = double(float(ctx.f30.f64 * ctx.f0.f64 + ctx.f28.f64));
	// lfs f16,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f16.f64 = double(temp.f32);
	// stfs f12,5620(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5620, temp.u32);
	// fmuls f16,f16,f20
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f20.f64));
	// fmuls f12,f5,f1
	ctx.f12.f64 = double(float(ctx.f5.f64 * ctx.f1.f64));
	// lfs f8,916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 916);
	ctx.f8.f64 = double(temp.f32);
	// lfs f1,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f1.f64 = double(temp.f32);
	// lfs f25,2952(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2952);
	ctx.f25.f64 = double(temp.f32);
	// stfs f7,5616(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5616, temp.u32);
	// fmuls f25,f8,f25
	ctx.f25.f64 = double(float(ctx.f8.f64 * ctx.f25.f64));
	// stfs f8,3680(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3680, temp.u32);
	// fnmsubs f9,f19,f1,f9
	ctx.f9.f64 = double(float(-(ctx.f19.f64 * ctx.f1.f64 - ctx.f9.f64)));
	// lfs f7,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f7.f64 = double(temp.f32);
	// lfs f1,476(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	ctx.f1.f64 = double(temp.f32);
	// lfs f8,324(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f11,f6,f1,f11
	ctx.f11.f64 = double(float(-(ctx.f6.f64 * ctx.f1.f64 - ctx.f11.f64)));
	// lfs f30,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f30.f64 = double(temp.f32);
	// fmuls f8,f7,f8
	ctx.f8.f64 = double(float(ctx.f7.f64 * ctx.f8.f64));
	// lfs f28,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f3,f3,f30,f0
	ctx.f3.f64 = double(float(-(ctx.f3.f64 * ctx.f30.f64 - ctx.f0.f64)));
	// lfs f5,2280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2280);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f1,f28,f31
	ctx.f1.f64 = double(float(ctx.f28.f64 * ctx.f31.f64));
	// lfs f7,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f5,f29,f5
	ctx.f5.f64 = double(float(ctx.f29.f64 * ctx.f5.f64));
	// lfs f15,3532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3532);
	ctx.f15.f64 = double(temp.f32);
	// fmuls f7,f27,f7
	ctx.f7.f64 = double(float(ctx.f27.f64 * ctx.f7.f64));
	// lfs f18,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f18.f64 = double(temp.f32);
	// lfs f14,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f14.f64 = double(temp.f32);
	// lfs f2,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f2.f64 = double(temp.f32);
	// lfs f6,2156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2156);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f2,f16,f2
	ctx.f2.f64 = double(float(ctx.f16.f64 * ctx.f2.f64));
	// lfs f30,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f30.f64 = double(temp.f32);
	// lfs f28,2328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2328);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f30,f30,f6
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f6.f64));
	// stfs f13,5604(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5604, temp.u32);
	// fmadds f13,f18,f14,f15
	ctx.f13.f64 = double(float(ctx.f18.f64 * ctx.f14.f64 + ctx.f15.f64));
	// lfs f0,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f29,f29,f28
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f28.f64));
	// lfs f27,1084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	ctx.f27.f64 = double(temp.f32);
	// stfs f20,3696(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3696, temp.u32);
	// lfs f28,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f1,f1,f0
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// fmadds f28,f28,f0,f27
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f0.f64 + ctx.f27.f64));
	// lfs f27,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f27.f64 = double(temp.f32);
	// lfs f23,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f8,f8,f27,f2
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f27.f64 - ctx.f2.f64));
	// fmadds f13,f13,f23,f7
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f23.f64 + ctx.f7.f64));
	// lfs f2,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f2.f64 = double(temp.f32);
	// lfs f7,1024(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1024);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f9,f5,f2,f9
	ctx.f9.f64 = double(float(-(ctx.f5.f64 * ctx.f2.f64 - ctx.f9.f64)));
	// fmr f5,f0
	ctx.f5.f64 = ctx.f0.f64;
	// lfs f2,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f7,f7,f26
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f26.f64));
	// lfs f23,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f3,f25,f2,f3
	ctx.f3.f64 = double(float(-(ctx.f25.f64 * ctx.f2.f64 - ctx.f3.f64)));
	// lfs f25,1016(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1016);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f11,f17,f23,f11
	ctx.f11.f64 = double(float(ctx.f17.f64 * ctx.f23.f64 + ctx.f11.f64));
	// lfs f23,1004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f25,f25,f26
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// lfs f27,2148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2148);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f26,f23,f26,f22
	ctx.f26.f64 = double(float(ctx.f23.f64 * ctx.f26.f64 + ctx.f22.f64));
	// lfs f23,2188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2188);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,1688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1688);
	ctx.f22.f64 = double(temp.f32);
	// lfs f0,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f0.f64 = double(temp.f32);
	// lfs f2,1196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1196);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,3712(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3712, temp.u32);
	// fmuls f5,f27,f5
	ctx.f5.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// lfs f20,1096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1096);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f7,f28,f31,f7
	ctx.f7.f64 = double(float(ctx.f28.f64 * ctx.f31.f64 + ctx.f7.f64));
	// lfs f31,3584(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3584);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f28,f23,f22
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f23,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f23.f64 = double(temp.f32);
	// lfs f22,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f4,f4,f0,f3
	ctx.f4.f64 = double(float(-(ctx.f4.f64 * ctx.f0.f64 - ctx.f3.f64)));
	// fmadds f13,f31,f23,f13
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f23.f64 + ctx.f13.f64));
	// lfs f3,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f8,f30,f22,f8
	ctx.f8.f64 = double(float(-(ctx.f30.f64 * ctx.f22.f64 - ctx.f8.f64)));
	// lfs f30,2180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2180);
	ctx.f30.f64 = double(temp.f32);
	// lfs f31,3840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3840);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f9,f29,f3,f9
	ctx.f9.f64 = double(float(ctx.f29.f64 * ctx.f3.f64 + ctx.f9.f64));
	// fnmsubs f11,f21,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f21.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f29,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f29.f64 = double(temp.f32);
	// lfs f3,2736(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2736);
	ctx.f3.f64 = double(temp.f32);
	// fmuls f31,f30,f31
	ctx.f31.f64 = double(float(ctx.f30.f64 * ctx.f31.f64));
	// lfs f23,316(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f3,f3,f29
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f29.f64));
	// lfs f0,4204(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4204);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f5,f23,f2,f5
	ctx.f5.f64 = double(float(ctx.f23.f64 * ctx.f2.f64 + ctx.f5.f64));
	// lfs f29,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f24,f20,f24
	ctx.f24.f64 = double(float(ctx.f20.f64 * ctx.f24.f64));
	// lfs f23,2540(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2540);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f29,f0,f29
	ctx.f29.f64 = double(float(ctx.f0.f64 * ctx.f29.f64));
	// lfs f2,2216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2216);
	ctx.f2.f64 = double(temp.f32);
	// lfs f0,560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 560);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f2,f2,f23
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f23.f64));
	// lfs f23,296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f10,f10,f0,f4
	ctx.f10.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f4.f64)));
	// lfs f0,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f0.f64 = double(temp.f32);
	// lfs f4,3832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3832);
	ctx.f4.f64 = double(temp.f32);
	// fnmsubs f11,f25,f23,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f23.f64 - ctx.f11.f64)));
	// lfs f25,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f31,f31,f0
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmuls f4,f4,f0
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f0.f64));
	// lfs f22,2664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2664);
	ctx.f22.f64 = double(temp.f32);
	// fmr f0,f23
	ctx.f0.f64 = ctx.f23.f64;
	// lfs f23,2784(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2784);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f13,f13,f25,f8
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f25.f64 + ctx.f8.f64));
	// lfs f8,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f8.f64 = double(temp.f32);
	// lfs f25,2848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2848);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f9,f28,f8,f9
	ctx.f9.f64 = double(float(-(ctx.f28.f64 * ctx.f8.f64 - ctx.f9.f64)));
	// fmuls f28,f30,f25
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f25,4868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4868);
	ctx.f25.f64 = double(temp.f32);
	// lfs f8,4972(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4972);
	ctx.f8.f64 = double(temp.f32);
	// fadds f8,f8,f25
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f25.f64));
	// lfs f25,1696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1696);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f10,f25,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f25.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f25,2776(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2776);
	ctx.f25.f64 = double(temp.f32);
	// fadds f25,f23,f25
	ctx.f25.f64 = double(float(ctx.f23.f64 + ctx.f25.f64));
	// lfs f23,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f11,f1,f0,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f0.f64 - ctx.f11.f64)));
	// lfs f1,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f10,f26,f0,f10
	ctx.f10.f64 = double(float(ctx.f26.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f21,2840(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2840);
	ctx.f21.f64 = double(temp.f32);
	// fmadds f13,f5,f1,f13
	ctx.f13.f64 = double(float(ctx.f5.f64 * ctx.f1.f64 + ctx.f13.f64));
	// lfs f5,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f23,f23,f22
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f22,3292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3292);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f5,f22,f5,f3
	ctx.f5.f64 = double(float(ctx.f22.f64 * ctx.f5.f64 + ctx.f3.f64));
	// lfs f22,2944(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2944);
	ctx.f22.f64 = double(temp.f32);
	// fmadds f11,f7,f0,f11
	ctx.f11.f64 = double(float(ctx.f7.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f0,348(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	ctx.f0.f64 = double(temp.f32);
	// fadds f22,f21,f22
	ctx.f22.f64 = double(float(ctx.f21.f64 + ctx.f22.f64));
	// lfs f21,2536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2536);
	ctx.f21.f64 = double(temp.f32);
	// lfs f19,2656(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2656);
	ctx.f19.f64 = double(temp.f32);
	// fadds f8,f8,f21
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f21.f64));
	// lfs f21,3596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3596);
	ctx.f21.f64 = double(temp.f32);
	// lfs f1,1900(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1900);
	ctx.f1.f64 = double(temp.f32);
	// lfs f3,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f3.f64 = double(temp.f32);
	// fnmsubs f10,f24,f0,f10
	ctx.f10.f64 = double(float(-(ctx.f24.f64 * ctx.f0.f64 - ctx.f10.f64)));
	// lfs f24,1512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1512);
	ctx.f24.f64 = double(temp.f32);
	// fadds f24,f24,f19
	ctx.f24.f64 = double(float(ctx.f24.f64 + ctx.f19.f64));
	// lfs f19,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f21,f21,f19
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f19.f64));
	// lfs f18,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f18.f64 = double(temp.f32);
	// fmsubs f3,f1,f3,f29
	ctx.f3.f64 = double(float(ctx.f1.f64 * ctx.f3.f64 - ctx.f29.f64));
	// lfs f19,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f19.f64 = double(temp.f32);
	// lfs f1,368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	ctx.f1.f64 = double(temp.f32);
	// fmsubs f31,f28,f18,f31
	ctx.f31.f64 = double(float(ctx.f28.f64 * ctx.f18.f64 - ctx.f31.f64));
	// fmadds f4,f25,f19,f4
	ctx.f4.f64 = double(float(ctx.f25.f64 * ctx.f19.f64 + ctx.f4.f64));
	// lfs f19,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f2,f1,f9
	ctx.f9.f64 = double(float(ctx.f2.f64 * ctx.f1.f64 + ctx.f9.f64));
	// lfs f26,1844(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1844);
	ctx.f26.f64 = double(temp.f32);
	// lfs f25,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f1,f30,f26
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f26.f64));
	// lfs f28,3576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3576);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f13,f23,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f23.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// stfs f9,5624(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5624, temp.u32);
	// fmuls f28,f25,f28
	ctx.f28.f64 = double(float(ctx.f25.f64 * ctx.f28.f64));
	// fmadds f0,f12,f0,f10
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f25,2648(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2648);
	ctx.f25.f64 = double(temp.f32);
	// fmr f9,f18
	ctx.f9.f64 = ctx.f18.f64;
	// lfs f23,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f23.f64 = double(temp.f32);
	// fmadds f10,f25,f23,f5
	ctx.f10.f64 = double(float(ctx.f25.f64 * ctx.f23.f64 + ctx.f5.f64));
	// lfs f5,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f5.f64 = double(temp.f32);
	// lfs f12,1788(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1788);
	ctx.f12.f64 = double(temp.f32);
	// fnmsubs f12,f12,f5,f3
	ctx.f12.f64 = double(float(-(ctx.f12.f64 * ctx.f5.f64 - ctx.f3.f64)));
	// lfs f5,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f5.f64 = double(temp.f32);
	// lfs f2,2756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2756);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f3,f6,f30
	ctx.f3.f64 = double(float(ctx.f6.f64 * ctx.f30.f64));
	// fmuls f2,f30,f2
	ctx.f2.f64 = double(float(ctx.f30.f64 * ctx.f2.f64));
	// lfs f29,2140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2140);
	ctx.f29.f64 = double(temp.f32);
	// lfs f7,3824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3824);
	ctx.f7.f64 = double(temp.f32);
	// lfs f26,2728(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2728);
	ctx.f26.f64 = double(temp.f32);
	// fmuls f7,f30,f7
	ctx.f7.f64 = double(float(ctx.f30.f64 * ctx.f7.f64));
	// lfs f20,2720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2720);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f26,f29,f26
	ctx.f26.f64 = double(float(ctx.f29.f64 * ctx.f26.f64));
	// fadds f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 + ctx.f0.f64));
	// fnmsubs f11,f1,f9,f31
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f9.f64 - ctx.f31.f64)));
	// lfs f9,3112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3112);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f10,f9,f5,f10
	ctx.f10.f64 = double(float(-(ctx.f9.f64 * ctx.f5.f64 - ctx.f10.f64)));
	// lfs f1,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f1.f64 = double(temp.f32);
	// fmr f5,f19
	ctx.f5.f64 = ctx.f19.f64;
	// lfs f9,2096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2096);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f12,f8,f1,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f1.f64 + ctx.f12.f64));
	// lfs f8,632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 632);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f9,f8,f9
	ctx.f9.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// lfs f8,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f8.f64 = double(temp.f32);
	// lfs f31,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f20,f29,f20
	ctx.f20.f64 = double(float(ctx.f29.f64 * ctx.f20.f64));
	// lfs f1,400(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	ctx.f1.f64 = double(temp.f32);
	// fmadds f11,f2,f8,f11
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f8.f64 + ctx.f11.f64));
	// lfs f8,1780(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1780);
	ctx.f8.f64 = double(temp.f32);
	// lfs f2,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f10,f22,f31,f10
	ctx.f10.f64 = double(float(ctx.f22.f64 * ctx.f31.f64 + ctx.f10.f64));
	// fnmsubs f13,f28,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f28.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// lfs f5,2836(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2836);
	ctx.f5.f64 = double(temp.f32);
	// fnmsubs f12,f8,f2,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f2.f64 - ctx.f12.f64)));
	// lfs f31,4196(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4196);
	ctx.f31.f64 = double(temp.f32);
	// lfs f8,2632(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2632);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f1,f1,f5
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f5.f64));
	// lfs f2,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f2.f64 = double(temp.f32);
	// fmr f28,f19
	ctx.f28.f64 = ctx.f19.f64;
	// fnmsubs f13,f9,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f19,1744(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1744);
	ctx.f19.f64 = double(temp.f32);
	// fmr f28,f18
	ctx.f28.f64 = ctx.f18.f64;
	// lfs f18,456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f19,f18,f19
	ctx.f19.f64 = double(float(ctx.f18.f64 * ctx.f19.f64));
	// lfs f17,2824(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2824);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f18.f64 = double(temp.f32);
	// fadds f8,f31,f8
	ctx.f8.f64 = double(float(ctx.f31.f64 + ctx.f8.f64));
	// fnmsubs f10,f17,f18,f10
	ctx.f10.f64 = double(float(-(ctx.f17.f64 * ctx.f18.f64 - ctx.f10.f64)));
	// lfs f16,2532(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2532);
	ctx.f16.f64 = double(temp.f32);
	// lfs f18,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f11,f7,f2,f11
	ctx.f11.f64 = double(float(-(ctx.f7.f64 * ctx.f2.f64 - ctx.f11.f64)));
	// fmadds f12,f16,f18,f12
	ctx.f12.f64 = double(float(ctx.f16.f64 * ctx.f18.f64 + ctx.f12.f64));
	// lfs f16,2876(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2876);
	ctx.f16.f64 = double(temp.f32);
	// lfs f18,1892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1892);
	ctx.f18.f64 = double(temp.f32);
	// fadds f18,f16,f18
	ctx.f18.f64 = double(float(ctx.f16.f64 + ctx.f18.f64));
	// lfs f16,2624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2624);
	ctx.f16.f64 = double(temp.f32);
	// lfs f15,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f15.f64 = double(temp.f32);
	// lfs f9,1504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1504);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f13,f1,f15,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f15.f64 + ctx.f13.f64));
	// stfs f0,5628(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5628, temp.u32);
	// fmuls f9,f30,f9
	ctx.f9.f64 = double(float(ctx.f30.f64 * ctx.f9.f64));
	// lfs f0,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f28,f24,f28,f21
	ctx.f28.f64 = double(float(ctx.f24.f64 * ctx.f28.f64 + ctx.f21.f64));
	// fadds f8,f8,f16
	ctx.f8.f64 = double(float(ctx.f8.f64 + ctx.f16.f64));
	// lfs f16,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f27,f27,f16
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f16.f64));
	// lfs f16,2704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2704);
	ctx.f16.f64 = double(temp.f32);
	// lfs f31,704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 704);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f0,f16,f0,f10
	ctx.f0.f64 = double(float(ctx.f16.f64 * ctx.f0.f64 + ctx.f10.f64));
	// lfs f21,2712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2712);
	ctx.f21.f64 = double(temp.f32);
	// lfs f1,2616(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2616);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f21,f21,f29
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f29.f64));
	// stfs f9,704(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 704, temp.u32);
	// fmuls f1,f29,f1
	ctx.f1.f64 = double(float(ctx.f29.f64 * ctx.f1.f64));
	// lfs f9,4148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4148);
	ctx.f9.f64 = double(temp.f32);
	// lfs f10,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f10.f64 = double(temp.f32);
	// lfs f2,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f2.f64 = double(temp.f32);
	// fnmsubs f12,f9,f10,f12
	ctx.f12.f64 = double(float(-(ctx.f9.f64 * ctx.f10.f64 - ctx.f12.f64)));
	// lfs f7,3412(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3412);
	ctx.f7.f64 = double(temp.f32);
	// fmr f10,f15
	ctx.f10.f64 = ctx.f15.f64;
	// lfs f29,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f7,f2,f7
	ctx.f7.f64 = double(float(ctx.f2.f64 * ctx.f7.f64));
	// fnmsubs f11,f26,f29,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// lfs f17,3116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3116);
	ctx.f17.f64 = double(temp.f32);
	// stfs f6,2212(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2212, temp.u32);
	// fmuls f17,f31,f17
	ctx.f17.f64 = double(float(ctx.f31.f64 * ctx.f17.f64));
	// lfs f9,3856(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3856);
	ctx.f9.f64 = double(temp.f32);
	// lfs f6,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f6.f64 = double(temp.f32);
	// stfs f31,1688(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1688, temp.u32);
	// lfs f23,516(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 516);
	ctx.f23.f64 = double(temp.f32);
	// lfs f31,3392(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3392);
	ctx.f31.f64 = double(temp.f32);
	// lfs f2,2172(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2172);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f2,f23,f2
	ctx.f2.f64 = double(float(ctx.f23.f64 * ctx.f2.f64));
	// stfs f5,2152(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2152, temp.u32);
	// fnmsubs f13,f7,f10,f13
	ctx.f13.f64 = double(float(-(ctx.f7.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f10,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f10.f64 = double(temp.f32);
	// lfs f7,2608(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2608);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f11,f4,f30,f11
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f30.f64 + ctx.f11.f64));
	// lfs f4,1032(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	ctx.f4.f64 = double(temp.f32);
	// fmadds f0,f9,f10,f0
	ctx.f0.f64 = double(float(ctx.f9.f64 * ctx.f10.f64 + ctx.f0.f64));
	// fmadds f12,f7,f6,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f6.f64 + ctx.f12.f64));
	// lfs f7,268(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	ctx.f7.f64 = double(temp.f32);
	// lfs f9,1132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	ctx.f9.f64 = double(temp.f32);
	// fadds f4,f31,f4
	ctx.f4.f64 = double(float(ctx.f31.f64 + ctx.f4.f64));
	// lfs f25,2832(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2832);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f9,f7,f9
	ctx.f9.f64 = double(float(ctx.f7.f64 * ctx.f9.f64));
	// lfs f24,2640(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2640);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f25,f30,f25
	ctx.f25.f64 = double(float(ctx.f30.f64 * ctx.f25.f64));
	// lfs f22,3568(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3568);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f24,f30,f24
	ctx.f24.f64 = double(float(ctx.f30.f64 * ctx.f24.f64));
	// lfs f23,352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 352);
	ctx.f23.f64 = double(temp.f32);
	// fmr f7,f15
	ctx.f7.f64 = ctx.f15.f64;
	// lfs f5,3560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3560);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f23,f22,f23
	ctx.f23.f64 = double(float(ctx.f22.f64 * ctx.f23.f64));
	// lfs f10,2936(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2936);
	ctx.f10.f64 = double(temp.f32);
	// lfs f6,1496(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1496);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f10,f10,f5
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f5.f64));
	// lfs f31,1104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1104);
	ctx.f31.f64 = double(temp.f32);
	// fadds f6,f18,f6
	ctx.f6.f64 = double(float(ctx.f18.f64 + ctx.f6.f64));
	// fmuls f3,f3,f31
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f31.f64));
	// fmadds f13,f2,f7,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f7.f64 + ctx.f13.f64));
	// lfs f31,2768(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2768);
	ctx.f31.f64 = double(temp.f32);
	// lfs f7,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f11,f20,f29,f11
	ctx.f11.f64 = double(float(-(ctx.f20.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// fmadds f0,f31,f7,f0
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f7.f64 + ctx.f0.f64));
	// lfs f7,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f7.f64 = double(temp.f32);
	// lfs f2,2600(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2600);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f7,f22,f7,f27
	ctx.f7.f64 = double(float(ctx.f22.f64 * ctx.f7.f64 + ctx.f27.f64));
	// lfs f29,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f29.f64 = double(temp.f32);
	// fmr f18,f15
	ctx.f18.f64 = ctx.f15.f64;
	// fmadds f12,f2,f29,f12
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f29.f64 + ctx.f12.f64));
	// lfs f22,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f10,f10,f22
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f22.f64));
	// lfs f16,3352(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3352);
	ctx.f16.f64 = double(temp.f32);
	// lfs f22,3104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3104);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f4,f4,f5
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f5.f64));
	// fadds f22,f16,f22
	ctx.f22.f64 = double(float(ctx.f16.f64 + ctx.f22.f64));
	// lfs f16,2696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2696);
	ctx.f16.f64 = double(temp.f32);
	// lfs f14,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f9,f9,f30
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f30.f64));
	// lfs f2,3384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3384);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f11,f28,f30,f11
	ctx.f11.f64 = double(float(ctx.f28.f64 * ctx.f30.f64 + ctx.f11.f64));
	// lfs f28,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f28.f64 = double(temp.f32);
	// fnmsubs f0,f16,f14,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// lfs f14,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f14.f64 = double(temp.f32);
	// lfs f16,1068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f3,f3,f14
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// fnmsubs f13,f19,f18,f13
	ctx.f13.f64 = double(float(-(ctx.f19.f64 * ctx.f18.f64 - ctx.f13.f64)));
	// lfs f31,3376(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3376);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f12,f23,f28,f12
	ctx.f12.f64 = double(float(-(ctx.f23.f64 * ctx.f28.f64 - ctx.f12.f64)));
	// lfs f23,1480(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1480);
	ctx.f23.f64 = double(temp.f32);
	// lfs f28,2688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2688);
	ctx.f28.f64 = double(temp.f32);
	// fadds f6,f6,f23
	ctx.f6.f64 = double(float(ctx.f6.f64 + ctx.f23.f64));
	// lfs f27,3368(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3368);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f16,f28,f16
	ctx.f16.f64 = double(float(ctx.f28.f64 * ctx.f16.f64));
	// lfs f20,3360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3360);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f2,f2,f5
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f5.f64));
	// lfs f18,1488(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1488);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f20,f20,f5
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f5.f64));
	// lfs f14,1040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1040);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f31,f31,f5
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f5.f64));
	// lfs f23,1048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1048);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f27,f27,f5
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f5.f64));
	// stfs f16,2216(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2216, temp.u32);
	// fmuls f18,f18,f30
	ctx.f18.f64 = double(float(ctx.f18.f64 * ctx.f30.f64));
	// stfs f5,1696(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1696, temp.u32);
	// fadds f23,f23,f14
	ctx.f23.f64 = double(float(ctx.f23.f64 + ctx.f14.f64));
	// lfs f16,4624(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4624);
	ctx.f16.f64 = double(temp.f32);
	// lfs f5,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f5.f64 = double(temp.f32);
	// stfs f18,5064(r1)
	temp.f32 = float(ctx.f18.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5064, temp.u32);
	// fnmsubs f0,f16,f5,f0
	ctx.f0.f64 = double(float(-(ctx.f16.f64 * ctx.f5.f64 - ctx.f0.f64)));
	// lfs f14,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f11,f25,f14,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f14.f64 - ctx.f11.f64)));
	// fmr f16,f18
	ctx.f16.f64 = ctx.f18.f64;
	// lfs f5,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f5.f64 = double(temp.f32);
	// fmsubs f10,f4,f5,f10
	ctx.f10.f64 = double(float(ctx.f4.f64 * ctx.f5.f64 - ctx.f10.f64));
	// lfs f4,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f4.f64 = double(temp.f32);
	// lfs f26,2924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2924);
	ctx.f26.f64 = double(temp.f32);
	// fmadds f12,f8,f4,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f4.f64 + ctx.f12.f64));
	// lfs f25,704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 704);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f26,f26,f30
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f30.f64));
	// lfs f14,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f14.f64 = double(temp.f32);
	// lfs f15,2760(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2760);
	ctx.f15.f64 = double(temp.f32);
	// lfs f29,1772(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1772);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f22,f22,f15
	ctx.f22.f64 = double(float(ctx.f22.f64 * ctx.f15.f64));
	// lfs f19,3344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3344);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f29,f29,f30
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f30.f64));
	// lfs f4,3552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3552);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f19,f15,f19
	ctx.f19.f64 = double(float(ctx.f15.f64 * ctx.f19.f64));
	// fnmsubs f11,f25,f14,f11
	ctx.f11.f64 = double(float(-(ctx.f25.f64 * ctx.f14.f64 - ctx.f11.f64)));
	// stfs f26,1096(r1)
	temp.f32 = float(ctx.f26.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1096, temp.u32);
	// fmuls f6,f6,f16
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f16.f64));
	// lfs f26,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f26.f64 = double(temp.f32);
	// lfs f5,3696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3696);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// lfs f25,4140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4140);
	ctx.f25.f64 = double(temp.f32);
	// lfs f16,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f16.f64 = double(temp.f32);
	// lfs f8,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f8.f64 = double(temp.f32);
	// lfs f14,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f14.f64 = double(temp.f32);
	// stfs f20,704(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 704, temp.u32);
	// lfs f20,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f11,f24,f18,f11
	ctx.f11.f64 = double(float(-(ctx.f24.f64 * ctx.f18.f64 - ctx.f11.f64)));
	// fmsubs f9,f9,f20,f3
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f20.f64 - ctx.f3.f64));
	// lfs f24,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f24.f64 = double(temp.f32);
	// fmr f3,f20
	ctx.f3.f64 = ctx.f20.f64;
	// lfs f20,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f20.f64 = double(temp.f32);
	// fnmsubs f0,f25,f16,f0
	ctx.f0.f64 = double(float(-(ctx.f25.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// lfs f25,2592(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2592);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f16,f5,f14
	ctx.f16.f64 = double(float(ctx.f5.f64 * ctx.f14.f64));
	// lfs f14,3680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3680);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f7,f7,f24,f6
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f24.f64 + ctx.f6.f64));
	// lfs f6,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f6.f64 = double(temp.f32);
	// fnmsubs f10,f2,f20,f10
	ctx.f10.f64 = double(float(-(ctx.f2.f64 * ctx.f20.f64 - ctx.f10.f64)));
	// lfs f2,2680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2680);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f25,f14,f25
	ctx.f25.f64 = double(float(ctx.f14.f64 * ctx.f25.f64));
	// lfs f20,3612(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3612);
	ctx.f20.f64 = double(temp.f32);
	// fadds f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f18,2964(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2964);
	ctx.f18.f64 = double(temp.f32);
	// lfs f12,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f12.f64 = double(temp.f32);
	// fmr f24,f26
	ctx.f24.f64 = ctx.f26.f64;
	// fnmsubs f11,f21,f26,f11
	ctx.f11.f64 = double(float(-(ctx.f21.f64 * ctx.f26.f64 - ctx.f11.f64)));
	// lfs f21,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f21.f64 = double(temp.f32);
	// stfs f13,5632(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5632, temp.u32);
	// fmuls f12,f4,f12
	ctx.f12.f64 = double(float(ctx.f4.f64 * ctx.f12.f64));
	// fmuls f3,f19,f3
	ctx.f3.f64 = double(float(ctx.f19.f64 * ctx.f3.f64));
	// lfs f19,1140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1140);
	ctx.f19.f64 = double(temp.f32);
	// fnmsubs f0,f2,f6,f0
	ctx.f0.f64 = double(float(-(ctx.f2.f64 * ctx.f6.f64 - ctx.f0.f64)));
	// lfs f6,3544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3544);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f6,f20,f6
	ctx.f6.f64 = double(float(ctx.f20.f64 * ctx.f6.f64));
	// stfs f12,3696(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3696, temp.u32);
	// fmuls f20,f16,f30
	ctx.f20.f64 = double(float(ctx.f16.f64 * ctx.f30.f64));
	// lfs f16,284(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f13,f7,f30,f9
	ctx.f13.f64 = double(float(ctx.f7.f64 * ctx.f30.f64 + ctx.f9.f64));
	// lfs f2,3848(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3848);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f19,f28,f19
	ctx.f19.f64 = double(float(ctx.f28.f64 * ctx.f19.f64));
	// lfs f7,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f7.f64 = double(temp.f32);
	// fmadds f28,f23,f28,f25
	ctx.f28.f64 = double(float(ctx.f23.f64 * ctx.f28.f64 + ctx.f25.f64));
	// lfs f25,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f10,f31,f25,f10
	ctx.f10.f64 = double(float(ctx.f31.f64 * ctx.f25.f64 + ctx.f10.f64));
	// lfs f26,3336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3336);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f11,f17,f8,f11
	ctx.f11.f64 = double(float(-(ctx.f17.f64 * ctx.f8.f64 - ctx.f11.f64)));
	// stfs f6,3680(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3680, temp.u32);
	// fmr f8,f25
	ctx.f8.f64 = ctx.f25.f64;
	// lfs f6,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f6.f64 = double(temp.f32);
	// fmsubs f3,f22,f21,f3
	ctx.f3.f64 = double(float(ctx.f22.f64 * ctx.f21.f64 - ctx.f3.f64));
	// lfs f22,3332(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3332);
	ctx.f22.f64 = double(temp.f32);
	// fmuls f21,f16,f18
	ctx.f21.f64 = double(float(ctx.f16.f64 * ctx.f18.f64));
	// lfs f18,3296(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3296);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f12,f18,f22
	ctx.f12.f64 = double(float(ctx.f18.f64 * ctx.f22.f64));
	// stfs f5,3720(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 3720, temp.u32);
	// fmuls f2,f15,f2
	ctx.f2.f64 = double(float(ctx.f15.f64 * ctx.f2.f64));
	// lfs f31,704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 704);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f13,f29,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f29.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f7,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f26,f26,f15
	ctx.f26.f64 = double(float(ctx.f26.f64 * ctx.f15.f64));
	// lfs f4,1980(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1980);
	ctx.f4.f64 = double(temp.f32);
	// fmr f5,f6
	ctx.f5.f64 = ctx.f6.f64;
	// lfs f23,3096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3096);
	ctx.f23.f64 = double(temp.f32);
	// stfs f0,5640(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5640, temp.u32);
	// fmuls f4,f4,f30
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// fnmsubs f11,f1,f24,f11
	ctx.f11.f64 = double(float(-(ctx.f1.f64 * ctx.f24.f64 - ctx.f11.f64)));
	// stfs f11,5636(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5636, temp.u32);
	// lfs f11,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f10,f27,f8,f10
	ctx.f10.f64 = double(float(-(ctx.f27.f64 * ctx.f8.f64 - ctx.f10.f64)));
	// lfs f8,3192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3192);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f0,f23,f15
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f15.f64));
	// lfs f1,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f9,f21,f30
	ctx.f9.f64 = double(float(ctx.f21.f64 * ctx.f30.f64));
	// fmuls f12,f12,f7
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f7.f64));
	// stfs f20,5104(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5104, temp.u32);
	// fmr f7,f25
	ctx.f7.f64 = ctx.f25.f64;
	// fmadds f11,f2,f11,f3
	ctx.f11.f64 = double(float(ctx.f2.f64 * ctx.f11.f64 + ctx.f3.f64));
	// lfs f3,1236(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1236);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,2132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2132);
	ctx.f2.f64 = double(temp.f32);
	// fmuls f8,f8,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f22.f64));
	// fnmsubs f10,f31,f7,f10
	ctx.f10.f64 = double(float(-(ctx.f31.f64 * ctx.f7.f64 - ctx.f10.f64)));
	// lfs f31,1096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1096);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f11,f26,f6,f11
	ctx.f11.f64 = double(float(-(ctx.f26.f64 * ctx.f6.f64 - ctx.f11.f64)));
	// lfs f6,3088(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3088);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,3184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3184);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f13,f31,f5,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f5.f64 - ctx.f13.f64)));
	// lfs f29,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f2,f2,f1
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f1.f64));
	// lfs f1,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f1.f64 = double(temp.f32);
	// fmr f25,f29
	ctx.f25.f64 = ctx.f29.f64;
	// fmuls f7,f7,f3
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f3.f64));
	// lfs f27,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f27.f64 = double(temp.f32);
	// fmsubs f12,f8,f1,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f1.f64 - ctx.f12.f64));
	// lfs f31,4200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4200);
	ctx.f31.f64 = double(temp.f32);
	// fnmsubs f0,f0,f29,f11
	ctx.f0.f64 = double(float(-(ctx.f0.f64 * ctx.f29.f64 - ctx.f11.f64)));
	// lfs f29,2216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2216);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f6,f6,f15
	ctx.f6.f64 = double(float(ctx.f6.f64 * ctx.f15.f64));
	// lfs f26,1696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1696);
	ctx.f26.f64 = double(temp.f32);
	// fnmsubs f10,f29,f27,f10
	ctx.f10.f64 = double(float(-(ctx.f29.f64 * ctx.f27.f64 - ctx.f10.f64)));
	// lfs f5,908(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 908);
	ctx.f5.f64 = double(temp.f32);
	// lfs f27,5064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5064);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f31,f31,f3
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f3.f64));
	// lfs f18,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f5,f5,f26
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f26.f64));
	// lfs f17,3248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3248);
	ctx.f17.f64 = double(temp.f32);
	// addi r6,r31,100
	ctx.r6.s64 = ctx.r31.s64 + 100;
	// lfs f16,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f16.f64 = double(temp.f32);
	// fmuls f2,f2,f30
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// fnmsubs f13,f27,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f27.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f25,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f25.f64 = double(temp.f32);
	// lfs f27,1560(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1560);
	ctx.f27.f64 = double(temp.f32);
	// addi r5,r30,100
	ctx.r5.s64 = ctx.r30.s64 + 100;
	// fmadds f12,f7,f18,f12
	ctx.f12.f64 = double(float(ctx.f7.f64 * ctx.f18.f64 + ctx.f12.f64));
	// lfs f7,1164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1164);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f18,f17,f3
	ctx.f18.f64 = double(float(ctx.f17.f64 * ctx.f3.f64));
	// lfs f17,2124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2124);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f27,f27,f25
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f25.f64));
	// lfs f25,1152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1152);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,4192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4192);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f7,f7,f17
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f17.f64));
	// lfs f8,4104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4104);
	ctx.f8.f64 = double(temp.f32);
	// fmuls f25,f25,f26
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f26.f64));
	// stfs f7,2216(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2216, temp.u32);
	// fnmsubs f0,f6,f16,f0
	ctx.f0.f64 = double(float(-(ctx.f6.f64 * ctx.f16.f64 - ctx.f0.f64)));
	// lfs f7,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f8,f8,f22
	ctx.f8.f64 = double(float(ctx.f8.f64 * ctx.f22.f64));
	// stfs f25,704(r1)
	temp.f32 = float(ctx.f25.f64);
	PPC_STORE_U32(ctx.r1.u32 + 704, temp.u32);
	// fmuls f16,f14,f3
	ctx.f16.f64 = double(float(ctx.f14.f64 * ctx.f3.f64));
	// lfs f14,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f14.f64 = double(temp.f32);
	// addi r4,r1,5200
	ctx.r4.s64 = ctx.r1.s64 + 5200;
	// lfs f25,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f25.f64 = double(temp.f32);
	// fmadds f10,f28,f14,f10
	ctx.f10.f64 = double(float(ctx.f28.f64 * ctx.f14.f64 + ctx.f10.f64));
	// stfs f8,1096(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1096, temp.u32);
	// fmuls f8,f31,f7
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f7.f64));
	// fmr f7,f14
	ctx.f7.f64 = ctx.f14.f64;
	// lfs f1,1168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1168);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f5,f5,f25
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f25.f64));
	// lfs f25,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f25.f64 = double(temp.f32);
	// fnmsubs f13,f4,f25,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f25.f64 - ctx.f13.f64)));
	// lfs f4,3696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3696);
	ctx.f4.f64 = double(temp.f32);
	// lfs f11,1160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1160);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f1,f26,f1
	ctx.f1.f64 = double(float(ctx.f26.f64 * ctx.f1.f64));
	// lfs f29,3080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3080);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f11,f26,f11
	ctx.f11.f64 = double(float(ctx.f26.f64 * ctx.f11.f64));
	// lfs f24,3288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3288);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f29,f29,f15
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// lfs f23,1552(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1552);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f24,f24,f26
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f26.f64));
	// lfs f21,3280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3280);
	ctx.f21.f64 = double(temp.f32);
	// fmuls f23,f26,f23
	ctx.f23.f64 = double(float(ctx.f26.f64 * ctx.f23.f64));
	// lfs f20,3272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3272);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f21,f21,f26
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f26.f64));
	// lfs f6,4184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4184);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f20,f20,f3
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f3.f64));
	// fnmsubs f10,f19,f7,f10
	ctx.f10.f64 = double(float(-(ctx.f19.f64 * ctx.f7.f64 - ctx.f10.f64)));
	// lfs f7,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f7.f64 = double(temp.f32);
	// lfs f28,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f6,f15,f6
	ctx.f6.f64 = double(float(ctx.f15.f64 * ctx.f6.f64));
	// fnmsubs f13,f4,f7,f13
	ctx.f13.f64 = double(float(-(ctx.f4.f64 * ctx.f7.f64 - ctx.f13.f64)));
	// lfs f7,3680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3680);
	ctx.f7.f64 = double(temp.f32);
	// lfs f4,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f4.f64 = double(temp.f32);
	// li r3,11
	ctx.r3.s64 = 11;
	// lfs f31,5104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 5104);
	ctx.f31.f64 = double(temp.f32);
	// fmr f19,f28
	ctx.f19.f64 = ctx.f28.f64;
	// lfs f25,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f25.f64 = double(temp.f32);
	// lfs f14,3240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3240);
	ctx.f14.f64 = double(temp.f32);
	// stfs f12,1696(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1696, temp.u32);
	// fnmsubs f10,f7,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f7.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f4,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f4.f64 = double(temp.f32);
	// fmr f12,f28
	ctx.f12.f64 = ctx.f28.f64;
	// lfs f7,2164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2164);
	ctx.f7.f64 = double(temp.f32);
	// fnmsubs f13,f31,f28,f13
	ctx.f13.f64 = double(float(-(ctx.f31.f64 * ctx.f28.f64 - ctx.f13.f64)));
	// lfs f31,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f8,f20,f31,f8
	ctx.f8.f64 = double(float(ctx.f20.f64 * ctx.f31.f64 - ctx.f8.f64));
	// lfs f28,2916(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2916);
	ctx.f28.f64 = double(temp.f32);
	// fmr f20,f25
	ctx.f20.f64 = ctx.f25.f64;
	// lfs f31,4080(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4080);
	ctx.f31.f64 = double(temp.f32);
	// fmsubs f5,f24,f4,f5
	ctx.f5.f64 = double(float(ctx.f24.f64 * ctx.f4.f64 - ctx.f5.f64));
	// lfs f4,4168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4168);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f7,f26,f7
	ctx.f7.f64 = double(float(ctx.f26.f64 * ctx.f7.f64));
	// lfs f24,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f28,f30,f28
	ctx.f28.f64 = double(float(ctx.f30.f64 * ctx.f28.f64));
	// fmuls f31,f31,f22
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f22.f64));
	// fmadds f0,f29,f24,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f24.f64 + ctx.f0.f64));
	// stfs f0,5652(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5652, temp.u32);
	// fmadds f10,f1,f25,f10
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f25.f64 + ctx.f10.f64));
	// lfs f25,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f25.f64 = double(temp.f32);
	// fmuls f12,f14,f12
	ctx.f12.f64 = double(float(ctx.f14.f64 * ctx.f12.f64));
	// lfs f14,3176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3176);
	ctx.f14.f64 = double(temp.f32);
	// fadds f4,f4,f14
	ctx.f4.f64 = double(float(ctx.f4.f64 + ctx.f14.f64));
	// lfs f24,1156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1156);
	ctx.f24.f64 = double(temp.f32);
	// fnmsubs f13,f9,f19,f13
	ctx.f13.f64 = double(float(-(ctx.f9.f64 * ctx.f19.f64 - ctx.f13.f64)));
	// lfs f9,2076(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2076);
	ctx.f9.f64 = double(temp.f32);
	// fmr f14,f19
	ctx.f14.f64 = ctx.f19.f64;
	// lfs f19,288(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	ctx.f19.f64 = double(temp.f32);
	// fmadds f9,f9,f25,f27
	ctx.f9.f64 = double(float(ctx.f9.f64 * ctx.f25.f64 + ctx.f27.f64));
	// lfs f27,4096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4096);
	ctx.f27.f64 = double(temp.f32);
	// fmuls f7,f7,f19
	ctx.f7.f64 = double(float(ctx.f7.f64 * ctx.f19.f64));
	// lfs f19,3308(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3308);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f19,f30,f19
	ctx.f19.f64 = double(float(ctx.f30.f64 * ctx.f19.f64));
	// lfs f1,1036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	ctx.f1.f64 = double(temp.f32);
	// fmuls f25,f17,f24
	ctx.f25.f64 = double(float(ctx.f17.f64 * ctx.f24.f64));
	// lfs f24,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f1,f1,f17
	ctx.f1.f64 = double(float(ctx.f1.f64 * ctx.f17.f64));
	// lfs f29,2084(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2084);
	ctx.f29.f64 = double(temp.f32);
	// fnmsubs f11,f11,f20,f10
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f20.f64 - ctx.f10.f64)));
	// lfs f10,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f10,f23,f10,f5
	ctx.f10.f64 = double(float(-(ctx.f23.f64 * ctx.f10.f64 - ctx.f5.f64)));
	// lfs f20,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f20.f64 = double(temp.f32);
	// lfs f23,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f23.f64 = double(temp.f32);
	// fnmsubs f8,f18,f20,f8
	ctx.f8.f64 = double(float(-(ctx.f18.f64 * ctx.f20.f64 - ctx.f8.f64)));
	// fmuls f28,f28,f23
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f20,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f13,f2,f14,f13
	ctx.f13.f64 = double(float(ctx.f2.f64 * ctx.f14.f64 + ctx.f13.f64));
	// stfs f13,5644(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5644, temp.u32);
	// fmuls f0,f31,f20
	ctx.f0.f64 = double(float(ctx.f31.f64 * ctx.f20.f64));
	// lfs f23,1796(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1796);
	ctx.f23.f64 = double(temp.f32);
	// lfs f31,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f12,f27,f24,f12
	ctx.f12.f64 = double(float(ctx.f27.f64 * ctx.f24.f64 + ctx.f12.f64));
	// lfs f13,1756(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1756);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f29,f29,f15
	ctx.f29.f64 = double(float(ctx.f29.f64 * ctx.f15.f64));
	// fadds f13,f23,f13
	ctx.f13.f64 = double(float(ctx.f23.f64 + ctx.f13.f64));
	// lfs f23,244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	ctx.f23.f64 = double(temp.f32);
	// lfs f24,1244(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1244);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f4,f4,f3
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f3.f64));
	// fmuls f24,f24,f17
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f17.f64));
	// lfs f27,4072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4072);
	ctx.f27.f64 = double(temp.f32);
	// fmadds f10,f21,f31,f10
	ctx.f10.f64 = double(float(ctx.f21.f64 * ctx.f31.f64 + ctx.f10.f64));
	// lfs f31,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f31.f64 = double(temp.f32);
	// fmadds f8,f16,f23,f8
	ctx.f8.f64 = double(float(ctx.f16.f64 * ctx.f23.f64 + ctx.f8.f64));
	// lfs f23,500(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 500);
	ctx.f23.f64 = double(temp.f32);
	// fmsubs f31,f19,f31,f28
	ctx.f31.f64 = double(float(ctx.f19.f64 * ctx.f31.f64 - ctx.f28.f64));
	// lfs f28,4056(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4056);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f28,f28,f23
	ctx.f28.f64 = double(float(ctx.f28.f64 * ctx.f23.f64));
	// lfs f2,4064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4064);
	ctx.f2.f64 = double(temp.f32);
	// lfs f17,3652(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3652);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f27,f27,f15
	ctx.f27.f64 = double(float(ctx.f27.f64 * ctx.f15.f64));
	// lfs f5,3536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3536);
	ctx.f5.f64 = double(temp.f32);
	// fmuls f2,f2,f15
	ctx.f2.f64 = double(float(ctx.f2.f64 * ctx.f15.f64));
	// lfs f18,1804(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1804);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f5,f5,f17
	ctx.f5.f64 = double(float(ctx.f5.f64 * ctx.f17.f64));
	// lfs f21,4048(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4048);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,3072(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3072);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f21,f3
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f3.f64));
	// stfs f11,5648(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5648, temp.u32);
	// fmuls f11,f30,f18
	ctx.f11.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// lfs f20,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f20.f64 = double(temp.f32);
	// fmuls f23,f23,f22
	ctx.f23.f64 = double(float(ctx.f23.f64 * ctx.f22.f64));
	// lfs f19,2068(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2068);
	ctx.f19.f64 = double(temp.f32);
	// fmsubs f0,f4,f20,f0
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f20.f64 - ctx.f0.f64));
	// fmuls f20,f3,f19
	ctx.f20.f64 = double(float(ctx.f3.f64 * ctx.f19.f64));
	// lfs f4,420(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	ctx.f4.f64 = double(temp.f32);
	// fmr f19,f14
	ctx.f19.f64 = ctx.f14.f64;
	// lfs f17,704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 704);
	ctx.f17.f64 = double(temp.f32);
	// fnmsubs f10,f17,f4,f10
	ctx.f10.f64 = double(float(-(ctx.f17.f64 * ctx.f4.f64 - ctx.f10.f64)));
	// lfs f4,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f4.f64 = double(temp.f32);
	// lfs f18,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f13,f13,f30
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f30.f64));
	// fnmsubs f11,f11,f18,f31
	ctx.f11.f64 = double(float(-(ctx.f11.f64 * ctx.f18.f64 - ctx.f31.f64)));
	// lfs f31,3064(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3064);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f31,f3
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f3.f64));
	// lfs f17,2216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2216);
	ctx.f17.f64 = double(temp.f32);
	// lfs f18,580(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f21,f21,f18
	ctx.f21.f64 = double(float(ctx.f21.f64 * ctx.f18.f64));
	// lfs f18,2116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2116);
	ctx.f18.f64 = double(temp.f32);
	// fnmsubs f8,f6,f19,f8
	ctx.f8.f64 = double(float(-(ctx.f6.f64 * ctx.f19.f64 - ctx.f8.f64)));
	// lfs f19,1696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1696);
	ctx.f19.f64 = double(temp.f32);
	// lfs f6,1096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1096);
	ctx.f6.f64 = double(temp.f32);
	// fmadds f10,f9,f26,f10
	ctx.f10.f64 = double(float(ctx.f9.f64 * ctx.f26.f64 + ctx.f10.f64));
	// fnmsubs f6,f6,f4,f19
	ctx.f6.f64 = double(float(-(ctx.f6.f64 * ctx.f4.f64 - ctx.f19.f64)));
	// lfs f19,404(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	ctx.f19.f64 = double(temp.f32);
	// lfs f4,2004(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2004);
	ctx.f4.f64 = double(temp.f32);
	// fmsubs f4,f4,f19,f28
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f19.f64 - ctx.f28.f64));
	// lfs f28,732(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	ctx.f28.f64 = double(temp.f32);
	// fmadds f0,f23,f28,f0
	ctx.f0.f64 = double(float(ctx.f23.f64 * ctx.f28.f64 + ctx.f0.f64));
	// lfs f28,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f28.f64 = double(temp.f32);
	// fmr f9,f28
	ctx.f9.f64 = ctx.f28.f64;
	// lfs f19,1996(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1996);
	ctx.f19.f64 = double(temp.f32);
	// fmuls f23,f19,f22
	ctx.f23.f64 = double(float(ctx.f19.f64 * ctx.f22.f64));
	// lfs f22,720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 720);
	ctx.f22.f64 = double(temp.f32);
	// fnmsubs f8,f29,f28,f8
	ctx.f8.f64 = double(float(-(ctx.f29.f64 * ctx.f28.f64 - ctx.f8.f64)));
	// lfs f28,924(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	ctx.f28.f64 = double(temp.f32);
	// fmuls f29,f20,f22
	ctx.f29.f64 = double(float(ctx.f20.f64 * ctx.f22.f64));
	// lfs f20,1528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1528);
	ctx.f20.f64 = double(temp.f32);
	// fmadds f6,f31,f28,f6
	ctx.f6.f64 = double(float(ctx.f31.f64 * ctx.f28.f64 + ctx.f6.f64));
	// lfs f31,1536(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1536);
	ctx.f31.f64 = double(temp.f32);
	// fmuls f31,f31,f26
	ctx.f31.f64 = double(float(ctx.f31.f64 * ctx.f26.f64));
	// stfs f6,5656(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5656, temp.u32);
	// fmuls f26,f20,f26
	ctx.f26.f64 = double(float(ctx.f20.f64 * ctx.f26.f64));
	// lfs f20,3528(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3528);
	ctx.f20.f64 = double(temp.f32);
	// fadds f20,f20,f18
	ctx.f20.f64 = double(float(ctx.f20.f64 + ctx.f18.f64));
	// lfs f18,1456(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1456);
	ctx.f18.f64 = double(temp.f32);
	// fmadds f13,f13,f9,f11
	ctx.f13.f64 = double(float(ctx.f13.f64 * ctx.f9.f64 + ctx.f11.f64));
	// lfs f11,960(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 960);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,4040(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 4040);
	ctx.f9.f64 = double(temp.f32);
	// fnmsubs f0,f23,f11,f0
	ctx.f0.f64 = double(float(-(ctx.f23.f64 * ctx.f11.f64 - ctx.f0.f64)));
	// fmuls f28,f9,f15
	ctx.f28.f64 = double(float(ctx.f9.f64 * ctx.f15.f64));
	// lfs f9,3264(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3264);
	ctx.f9.f64 = double(temp.f32);
	// lfs f11,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f19,f9,f15
	ctx.f19.f64 = double(float(ctx.f9.f64 * ctx.f15.f64));
	// fnmsubs f11,f17,f11,f10
	ctx.f11.f64 = double(float(-(ctx.f17.f64 * ctx.f11.f64 - ctx.f10.f64)));
	// lfs f10,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,2052(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2052);
	ctx.f9.f64 = double(temp.f32);
	// fmadds f8,f12,f15,f8
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f15.f64 + ctx.f8.f64));
	// lfs f12,2108(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2108);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f9,f3,f9
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f9.f64));
	// lfs f17,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f17.f64 = double(temp.f32);
	// fmsubs f4,f4,f3,f21
	ctx.f4.f64 = double(float(ctx.f4.f64 * ctx.f3.f64 - ctx.f21.f64));
	// stfs f0,5664(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5664, temp.u32);
	// lfs f0,512(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 512);
	ctx.f0.f64 = double(temp.f32);
	// fnmsubs f29,f29,f10,f13
	ctx.f29.f64 = double(float(-(ctx.f29.f64 * ctx.f10.f64 - ctx.f13.f64)));
	// lfs f10,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f13,f12,f17
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f17.f64));
	// lfs f21,1472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1472);
	ctx.f21.f64 = double(temp.f32);
	// lfs f23,1464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1464);
	ctx.f23.f64 = double(temp.f32);
	// fmuls f21,f30,f21
	ctx.f21.f64 = double(float(ctx.f30.f64 * ctx.f21.f64));
	// stfs f4,5660(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5660, temp.u32);
	// fmr f4,f0
	ctx.f4.f64 = ctx.f0.f64;
	// fmadds f12,f1,f0,f11
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f0.f64 + ctx.f11.f64));
	// lfs f17,544(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 544);
	ctx.f17.f64 = double(temp.f32);
	// fmuls f1,f30,f18
	ctx.f1.f64 = double(float(ctx.f30.f64 * ctx.f18.f64));
	// fmuls f6,f9,f22
	ctx.f6.f64 = double(float(ctx.f9.f64 * ctx.f22.f64));
	// fmr f11,f0
	ctx.f11.f64 = ctx.f0.f64;
	// fmr f18,f0
	ctx.f18.f64 = ctx.f0.f64;
	// lfs f0,396(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f23,f30,f23
	ctx.f23.f64 = double(float(ctx.f30.f64 * ctx.f23.f64));
	// fmadds f9,f20,f10,f13
	ctx.f9.f64 = double(float(ctx.f20.f64 * ctx.f10.f64 + ctx.f13.f64));
	// lfs f13,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	ctx.f13.f64 = double(temp.f32);
	// lfs f20,3720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3720);
	ctx.f20.f64 = double(temp.f32);
	// lfs f16,3712(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3712);
	ctx.f16.f64 = double(temp.f32);
	// fnmsubs f25,f25,f11,f12
	ctx.f25.f64 = double(float(-(ctx.f25.f64 * ctx.f11.f64 - ctx.f12.f64)));
	// fmuls f20,f13,f20
	ctx.f20.f64 = double(float(ctx.f13.f64 * ctx.f20.f64));
	// lfs f11,2212(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2212);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f16,f13,f16
	ctx.f16.f64 = double(float(ctx.f13.f64 * ctx.f16.f64));
	// lfs f13,864(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f14,f11,f13
	ctx.f14.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f12,2152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2152);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f15,f12,f13
	ctx.f15.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// lfs f13,700(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,2100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2100);
	ctx.f11.f64 = double(temp.f32);
	// fmsubs f11,f11,f13,f9
	ctx.f11.f64 = double(float(ctx.f11.f64 * ctx.f13.f64 - ctx.f9.f64));
	// stfs f11,720(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 720, temp.u32);
	// stfd f10,3696(r1)
	PPC_STORE_U64(ctx.r1.u32 + 3696, ctx.f10.u64);
	// lfs f10,280(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	ctx.f10.f64 = double(temp.f32);
	// fnmsubs f8,f27,f10,f8
	ctx.f8.f64 = double(float(-(ctx.f27.f64 * ctx.f10.f64 - ctx.f8.f64)));
	// lfs f27,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f27.f64 = double(temp.f32);
	// fnmsubs f7,f7,f4,f25
	ctx.f7.f64 = double(float(-(ctx.f7.f64 * ctx.f4.f64 - ctx.f25.f64)));
	// lfs f4,1448(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1448);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f20,f20,f30
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f30.f64));
	// lfs f9,1680(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1680);
	ctx.f9.f64 = double(temp.f32);
	// fmuls f16,f16,f30
	ctx.f16.f64 = double(float(ctx.f16.f64 * ctx.f30.f64));
	// stfs f9,704(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 704, temp.u32);
	// fmuls f14,f14,f30
	ctx.f14.f64 = double(float(ctx.f14.f64 * ctx.f30.f64));
	// stfs f14,2216(r1)
	temp.f32 = float(ctx.f14.f64);
	PPC_STORE_U32(ctx.r1.u32 + 2216, temp.u32);
	// lfs f14,1416(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1416);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f15,f15,f30
	ctx.f15.f64 = double(float(ctx.f15.f64 * ctx.f30.f64));
	// fmuls f4,f30,f4
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f4.f64));
	// stfs f15,1696(r1)
	temp.f32 = float(ctx.f15.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1696, temp.u32);
	// fmuls f30,f30,f14
	ctx.f30.f64 = double(float(ctx.f30.f64 * ctx.f14.f64));
	// lfs f14,720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 720);
	ctx.f14.f64 = double(temp.f32);
	// fmr f9,f10
	ctx.f9.f64 = ctx.f10.f64;
	// stfs f16,1096(r1)
	temp.f32 = float(ctx.f16.f64);
	PPC_STORE_U32(ctx.r1.u32 + 1096, temp.u32);
	// fmr f15,f27
	ctx.f15.f64 = ctx.f27.f64;
	// lfs f16,1520(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1520);
	ctx.f16.f64 = double(temp.f32);
	// fmadds f8,f2,f27,f8
	ctx.f8.f64 = double(float(ctx.f2.f64 * ctx.f27.f64 + ctx.f8.f64));
	// lfs f11,596(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	ctx.f11.f64 = double(temp.f32);
	// fmadds f7,f24,f18,f7
	ctx.f7.f64 = double(float(ctx.f24.f64 * ctx.f18.f64 + ctx.f7.f64));
	// stfs f20,720(r1)
	temp.f32 = float(ctx.f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 720, temp.u32);
	// fmuls f27,f3,f16
	ctx.f27.f64 = double(float(ctx.f3.f64 * ctx.f16.f64));
	// lfs f20,704(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 704);
	ctx.f20.f64 = double(temp.f32);
	// lfs f10,676(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 676);
	ctx.f10.f64 = double(temp.f32);
	// lfs f25,2868(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2868);
	ctx.f25.f64 = double(temp.f32);
	// lfs f2,1884(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1884);
	ctx.f2.f64 = double(temp.f32);
	// stfd f13,704(r1)
	PPC_STORE_U64(ctx.r1.u32 + 704, ctx.f13.u64);
	// fnmsubs f8,f28,f15,f8
	ctx.f8.f64 = double(float(-(ctx.f28.f64 * ctx.f15.f64 - ctx.f8.f64)));
	// lfs f13,2028(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2028);
	ctx.f13.f64 = double(temp.f32);
	// fmr f16,f9
	ctx.f16.f64 = ctx.f9.f64;
	// lfs f12,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f20,f20,f11,f14
	ctx.f20.f64 = double(float(ctx.f20.f64 * ctx.f11.f64 + ctx.f14.f64));
	// lfs f14,2044(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2044);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f7,f5,f17,f7
	ctx.f7.f64 = double(float(ctx.f5.f64 * ctx.f17.f64 + ctx.f7.f64));
	// lfs f24,1688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1688);
	ctx.f24.f64 = double(temp.f32);
	// fmuls f3,f3,f14
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f14.f64));
	// lfs f14,1408(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1408);
	ctx.f14.f64 = double(temp.f32);
	// fmuls f24,f24,f14
	ctx.f24.f64 = double(float(ctx.f24.f64 * ctx.f14.f64));
	// lfs f14,2036(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2036);
	ctx.f14.f64 = double(temp.f32);
	// lfs f18,1672(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1672);
	ctx.f18.f64 = double(temp.f32);
	// fmuls f28,f27,f22
	ctx.f28.f64 = double(float(ctx.f27.f64 * ctx.f22.f64));
	// fadds f18,f18,f14
	ctx.f18.f64 = double(float(ctx.f18.f64 + ctx.f14.f64));
	// lfs f14,260(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	ctx.f14.f64 = double(temp.f32);
	// fmadds f8,f19,f16,f8
	ctx.f8.f64 = double(float(ctx.f19.f64 * ctx.f16.f64 + ctx.f8.f64));
	// stfs f8,5668(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5668, temp.u32);
	// fmadds f25,f25,f10,f20
	ctx.f25.f64 = double(float(ctx.f25.f64 * ctx.f10.f64 + ctx.f20.f64));
	// lfd f10,3696(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 3696);
	// fmadds f8,f31,f0,f7
	ctx.f8.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 + ctx.f7.f64));
	// fmuls f3,f3,f22
	ctx.f3.f64 = double(float(ctx.f3.f64 * ctx.f22.f64));
	// fnmsubs f5,f2,f11,f25
	ctx.f5.f64 = double(float(-(ctx.f2.f64 * ctx.f11.f64 - ctx.f25.f64)));
	// lfs f11,576(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f8,f26,f0,f8
	ctx.f8.f64 = double(float(-(ctx.f26.f64 * ctx.f0.f64 - ctx.f8.f64)));
	// stfs f8,5672(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5672, temp.u32);
	// fmr f0,f15
	ctx.f0.f64 = ctx.f15.f64;
	// fmadds f7,f13,f11,f5
	ctx.f7.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f5.f64));
	// fmadds f5,f21,f0,f29
	ctx.f5.f64 = double(float(ctx.f21.f64 * ctx.f0.f64 + ctx.f29.f64));
	// fnmsubs f8,f6,f12,f5
	ctx.f8.f64 = double(float(-(ctx.f6.f64 * ctx.f12.f64 - ctx.f5.f64)));
	// fmadds f8,f23,f0,f8
	ctx.f8.f64 = double(float(ctx.f23.f64 * ctx.f0.f64 + ctx.f8.f64));
	// fnmsubs f8,f1,f9,f8
	ctx.f8.f64 = double(float(-(ctx.f1.f64 * ctx.f9.f64 - ctx.f8.f64)));
	// fmadds f9,f4,f9,f8
	ctx.f9.f64 = double(float(ctx.f4.f64 * ctx.f9.f64 + ctx.f8.f64));
	// fnmsubs f0,f30,f0,f9
	ctx.f0.f64 = double(float(-(ctx.f30.f64 * ctx.f0.f64 - ctx.f9.f64)));
	// fnmsubs f9,f24,f14,f0
	ctx.f9.f64 = double(float(-(ctx.f24.f64 * ctx.f14.f64 - ctx.f0.f64)));
	// lfs f0,1664(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1664);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f10,f0,f10,f7
	ctx.f10.f64 = double(float(ctx.f0.f64 * ctx.f10.f64 + ctx.f7.f64));
	// lfs f8,3328(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3328);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f9,f28,f12,f9
	ctx.f9.f64 = double(float(-(ctx.f28.f64 * ctx.f12.f64 - ctx.f9.f64)));
	// lfs f7,504(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,3320(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 3320);
	ctx.f6.f64 = double(temp.f32);
	// lfd f13,704(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 704);
	// lfs f0,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f0.f64 = double(temp.f32);
	// lfs f5,2020(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2020);
	ctx.f5.f64 = double(temp.f32);
	// fmadds f10,f8,f7,f10
	ctx.f10.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f10.f64));
	// lfs f8,720(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 720);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f12,f3,f12,f9
	ctx.f12.f64 = double(float(-(ctx.f3.f64 * ctx.f12.f64 - ctx.f9.f64)));
	// fnmsubs f10,f6,f13,f10
	ctx.f10.f64 = double(float(-(ctx.f6.f64 * ctx.f13.f64 - ctx.f10.f64)));
	// fmadds f12,f8,f0,f12
	ctx.f12.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f8,1096(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1096);
	ctx.f8.f64 = double(temp.f32);
	// fmadds f11,f18,f11,f10
	ctx.f11.f64 = double(float(ctx.f18.f64 * ctx.f11.f64 + ctx.f10.f64));
	// fnmsubs f12,f8,f0,f12
	ctx.f12.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f12.f64)));
	// lfs f8,1696(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 1696);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f13,f5,f13,f11
	ctx.f13.f64 = double(float(-(ctx.f5.f64 * ctx.f13.f64 - ctx.f11.f64)));
	// stfs f13,5680(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5680, temp.u32);
	// fmadds f13,f8,f0,f12
	ctx.f13.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lfs f8,2216(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 2216);
	ctx.f8.f64 = double(temp.f32);
	// fnmsubs f0,f8,f0,f13
	ctx.f0.f64 = double(float(-(ctx.f8.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// stfs f0,5676(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 5676, temp.u32);
	// bl 0x82242e58
	ctx.lr = 0x8225DC2C;
	sub_82242E58(ctx, base);
loc_8225DC2C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,5872
	ctx.r1.s64 = ctx.r1.s64 + 5872;
	// addi r12,r1,-32
	ctx.r12.s64 = ctx.r1.s64 + -32;
	// bl 0x82ca751c
	ctx.lr = 0x8225DC3C;
	sub_82CA751C(ctx, base);
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8225DC40"))) PPC_WEAK_FUNC(sub_8225DC40);
PPC_FUNC_IMPL(__imp__sub_8225DC40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bb0
	ctx.lr = 0x8225DC48;
	sub_82CA2BB0(ctx, base);
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f31.u64);
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,76(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 76);
	// mr r28,r7
	ctx.r28.u64 = ctx.r7.u64;
	// lwz r9,72(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 72);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r10,12
	ctx.r10.s64 = 12;
	// stw r28,436(r1)
	PPC_STORE_U32(ctx.r1.u32 + 436, ctx.r28.u32);
	// subf r7,r9,r11
	ctx.r7.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r29,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r29.u32);
	// divw. r3,r7,r10
	ctx.r3.s32 = ctx.r7.s32 / ctx.r10.s32;
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq 0x8225df1c
	if (ctx.cr0.eq) goto loc_8225DF1C;
	// li r16,0
	ctx.r16.s64 = 0;
	// lwz r30,208(r5)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r5.u32 + 208);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r23,0(r6)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// lwz r31,8(r6)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// addi r21,r6,20
	ctx.r21.s64 = ctx.r6.s64 + 20;
	// lwz r17,4(r6)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lwz r18,12(r6)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// mr r9,r16
	ctx.r9.u64 = ctx.r16.u64;
	// stw r16,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r16.u32);
	// li r10,6
	ctx.r10.s64 = 6;
	// stw r7,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r7.u32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_8225DCB0:
	// std r9,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x8225dcb0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225DCB0;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// clrlwi r10,r8,24
	ctx.r10.u64 = ctx.r8.u32 & 0xFF;
	// addi r11,r11,-27456
	ctx.r11.s64 = ctx.r11.s64 + -27456;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lfs f31,-12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12);
	ctx.f31.f64 = double(temp.f32);
	// beq cr6,0x8225dd64
	if (ctx.cr6.eq) goto loc_8225DD64;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,176
	ctx.r10.s64 = ctx.r1.s64 + 176;
	// stfs f31,172(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// addi r11,r4,8
	ctx.r11.s64 = ctx.r4.s64 + 8;
	// stfs f31,188(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// li r9,3
	ctx.r9.s64 = 3;
	// stfs f31,204(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// stfs f0,220(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
loc_8225DCF4:
	// lfs f0,-8(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	ctx.f0.f64 = double(temp.f32);
	// addic. r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lfs f13,-4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,4(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// stfs f0,-16(r10)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + -16, temp.u32);
	// stfs f13,0(r10)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stfs f12,16(r10)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r10.u32 + 16, temp.u32);
	// stfs f11,32(r10)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r10.u32 + 32, temp.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne 0x8225dcf4
	if (!ctx.cr0.eq) goto loc_8225DCF4;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82242ed0
	ctx.lr = 0x8225DD38;
	sub_82242ED0(ctx, base);
	// addi r6,r28,16
	ctx.r6.s64 = ctx.r28.s64 + 16;
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82242ed0
	ctx.lr = 0x8225DD4C;
	sub_82242ED0(ctx, base);
	// addi r6,r28,32
	ctx.r6.s64 = ctx.r28.s64 + 32;
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82242ed0
	ctx.lr = 0x8225DD60;
	sub_82242ED0(ctx, base);
	// b 0x8225dd88
	goto loc_8225DD88;
loc_8225DD64:
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8225DD74:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x8225dd74
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225DD74;
loc_8225DD88:
	// mullw r11,r18,r17
	ctx.r11.s64 = int64_t(ctx.r18.s32) * int64_t(ctx.r17.s32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// mullw r19,r23,r23
	ctx.r19.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r23.s32);
	// mullw r10,r11,r31
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r31.s32);
	// addi r9,r18,1
	ctx.r9.s64 = ctx.r18.s64 + 1;
	// srawi r8,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// mullw r7,r9,r19
	ctx.r7.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r19.s32);
	// addze r6,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r6.s64 = temp.s64;
	// mullw r30,r7,r17
	ctx.r30.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r17.s32);
	// stw r6,192(r29)
	PPC_STORE_U32(ctx.r29.u32 + 192, ctx.r6.u32);
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r30.u32);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x8225df14
	if (ctx.cr6.eq) goto loc_8225DF14;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r31.u32);
	// mr r20,r16
	ctx.r20.u64 = ctx.r16.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// rlwinm r14,r19,3,0,28
	ctx.r14.u64 = __builtin_rotateleft64(ctx.r19.u32 | (ctx.r19.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r15,r19,2,0,29
	ctx.r15.u64 = __builtin_rotateleft64(ctx.r19.u32 | (ctx.r19.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r22,r29,16
	ctx.r22.s64 = ctx.r29.s64 + 16;
loc_8225DDD8:
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// add r4,r16,r11
	ctx.r4.u64 = ctx.r16.u64 + ctx.r11.u64;
	// bl 0x8225df28
	ctx.lr = 0x8225DDEC;
	sub_8225DF28(ctx, base);
	// stfs f1,-16(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r22.u32 + -16, temp.u32);
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// add r4,r15,r11
	ctx.r4.u64 = ctx.r15.u64 + ctx.r11.u64;
	// bl 0x8225df28
	ctx.lr = 0x8225DE04;
	sub_8225DF28(ctx, base);
	// stfs f1,-12(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r22.u32 + -12, temp.u32);
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// add r4,r14,r11
	ctx.r4.u64 = ctx.r14.u64 + ctx.r11.u64;
	// bl 0x8225df28
	ctx.lr = 0x8225DE1C;
	sub_8225DF28(ctx, base);
	// stfs f1,-8(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r22.u32 + -8, temp.u32);
	// stfs f31,-4(r22)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r22.u32 + -4, temp.u32);
	// cmplwi cr6,r18,0
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, 0, ctx.xer);
	// beq cr6,0x8225dee8
	if (ctx.cr6.eq) goto loc_8225DEE8;
	// addi r10,r17,2
	ctx.r10.s64 = ctx.r17.s64 + 2;
	// addi r9,r17,1
	ctx.r9.s64 = ctx.r17.s64 + 1;
	// mullw r11,r19,r17
	ctx.r11.s64 = int64_t(ctx.r19.s32) * int64_t(ctx.r17.s32);
	// mullw r7,r10,r19
	ctx.r7.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r19.s32);
	// mullw r8,r9,r19
	ctx.r8.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r19.s32);
	// rlwinm r9,r18,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r18.u32 | (ctx.r18.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r18,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r20,r11
	ctx.r6.u64 = ctx.r20.u64 + ctx.r11.u64;
	// add r5,r7,r20
	ctx.r5.u64 = ctx.r7.u64 + ctx.r20.u64;
	// add r4,r8,r20
	ctx.r4.u64 = ctx.r8.u64 + ctx.r20.u64;
	// mr r27,r22
	ctx.r27.u64 = ctx.r22.u64;
	// add r25,r9,r22
	ctx.r25.u64 = ctx.r9.u64 + ctx.r22.u64;
	// add r26,r10,r22
	ctx.r26.u64 = ctx.r10.u64 + ctx.r22.u64;
	// rlwinm r24,r11,2,0,29
	ctx.r24.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r6,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r5,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r4,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r31,r18
	ctx.r31.u64 = ctx.r18.u64;
loc_8225DE74:
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// add r4,r30,r11
	ctx.r4.u64 = ctx.r30.u64 + ctx.r11.u64;
	// bl 0x8225df28
	ctx.lr = 0x8225DE88;
	sub_8225DF28(ctx, base);
	// stfs f1,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r27.u32 + 0, temp.u32);
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// add r4,r29,r11
	ctx.r4.u64 = ctx.r29.u64 + ctx.r11.u64;
	// bl 0x8225df28
	ctx.lr = 0x8225DEA0;
	sub_8225DF28(ctx, base);
	// stfs f1,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r26.u32 + 0, temp.u32);
	// lwz r11,4(r21)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r21.u32 + 4);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// add r4,r28,r11
	ctx.r4.u64 = ctx.r28.u64 + ctx.r11.u64;
	// bl 0x8225df28
	ctx.lr = 0x8225DEB8;
	sub_8225DF28(ctx, base);
	// stfs f1,0(r25)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r25.u32 + 0, temp.u32);
	// addic. r31,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r31.s64 = ctx.r31.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// add r30,r24,r30
	ctx.r30.u64 = ctx.r24.u64 + ctx.r30.u64;
	// add r29,r24,r29
	ctx.r29.u64 = ctx.r24.u64 + ctx.r29.u64;
	// add r28,r24,r28
	ctx.r28.u64 = ctx.r24.u64 + ctx.r28.u64;
	// addi r27,r27,4
	ctx.r27.s64 = ctx.r27.s64 + 4;
	// addi r26,r26,4
	ctx.r26.s64 = ctx.r26.s64 + 4;
	// addi r25,r25,4
	ctx.r25.s64 = ctx.r25.s64 + 4;
	// bne 0x8225de74
	if (!ctx.cr0.eq) goto loc_8225DE74;
	// lwz r29,404(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// lwz r28,436(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	// lwz r30,88(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_8225DEE8:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// rlwinm r11,r30,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// add r20,r20,r30
	ctx.r20.u64 = ctx.r20.u64 + ctx.r30.u64;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// add r22,r9,r22
	ctx.r22.u64 = ctx.r9.u64 + ctx.r22.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// add r16,r11,r16
	ctx.r16.u64 = ctx.r11.u64 + ctx.r16.u64;
	// add r15,r11,r15
	ctx.r15.u64 = ctx.r11.u64 + ctx.r15.u64;
	// add r14,r11,r14
	ctx.r14.u64 = ctx.r11.u64 + ctx.r14.u64;
	// bne 0x8225ddd8
	if (!ctx.cr0.eq) goto loc_8225DDD8;
loc_8225DF14:
	// lwz r11,52(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 52);
	// stw r11,196(r29)
	PPC_STORE_U32(ctx.r29.u32 + 196, ctx.r11.u32);
loc_8225DF1C:
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	sub_82CA2C00(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_8225DF28"))) PPC_WEAK_FUNC(sub_8225DF28);
PPC_FUNC_IMPL(__imp__sub_8225DF28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mullw r10,r3,r3
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r3.s32);
	// lfs f0,0(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// blelr cr6
	if (!ctx.cr6.gt) return;
	// addi r11,r5,4
	ctx.r11.s64 = ctx.r5.s64 + 4;
	// subf r9,r5,r4
	ctx.r9.s64 = ctx.r4.s64 - ctx.r5.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
loc_8225DF4C:
	// lfsx f0,r9,r11
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	ctx.f0.f64 = double(temp.f32);
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// fmadds f1,f0,f13,f1
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64 + ctx.f1.f64));
	// bne 0x8225df4c
	if (!ctx.cr0.eq) goto loc_8225DF4C;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225DF68"))) PPC_WEAK_FUNC(sub_8225DF68);
PPC_FUNC_IMPL(__imp__sub_8225DF68) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x8225DF70;
	sub_82CA2BE4(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// std r4,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r4.u64);
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// li r9,6
	ctx.r9.s64 = 6;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8225DF8C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225df8c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225DF8C;
	// lwz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// li r27,24
	ctx.r27.s64 = 24;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8225dfb8
	if (!ctx.cr6.eq) goto loc_8225DFB8;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x8225dfc4
	goto loc_8225DFC4;
loc_8225DFB8:
	// lwz r10,12(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 12);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r11.s64;
	// divw r9,r9,r27
	ctx.r9.s32 = ctx.r9.s32 / ctx.r27.s32;
loc_8225DFC4:
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// lis r10,2730
	ctx.r10.s64 = 178913280;
	// subf r8,r11,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r11.s64;
	// ori r10,r10,43690
	ctx.r10.u64 = ctx.r10.u64 | 43690;
	// divw r11,r8,r27
	ctx.r11.s32 = ctx.r8.s32 / ctx.r27.s32;
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplwi cr6,r7,1
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 1, ctx.xer);
	// bge cr6,0x8225dff0
	if (!ctx.cr6.lt) goto loc_8225DFF0;
	// bl 0x82a97648
	ctx.lr = 0x8225DFE8;
	sub_82A97648(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
loc_8225DFF0:
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// bge cr6,0x8225e158
	if (!ctx.cr6.lt) goto loc_8225E158;
	// rlwinm r11,r9,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// li r29,0
	ctx.r29.s64 = 0;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// blt cr6,0x8225e014
	if (ctx.cr6.lt) goto loc_8225E014;
	// add r29,r11,r9
	ctx.r29.u64 = ctx.r11.u64 + ctx.r9.u64;
loc_8225E014:
	// cmplw cr6,r29,r8
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r8.u32, ctx.xer);
	// bge cr6,0x8225e020
	if (!ctx.cr6.lt) goto loc_8225E020;
	// mr r29,r8
	ctx.r29.u64 = ctx.r8.u64;
loc_8225E020:
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825bbad0
	ctx.lr = 0x8225E02C;
	sub_825BBAD0(ctx, base);
	// lwz r8,4(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// lwz r6,188(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplw cr6,r8,r6
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, ctx.xer);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// beq cr6,0x8225e080
	if (ctx.cr6.eq) goto loc_8225E080;
loc_8225E044:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225e070
	if (ctx.cr6.eq) goto loc_8225E070;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// li r7,6
	ctx.r7.s64 = 6;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_8225E05C:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x8225e05c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E05C;
loc_8225E070:
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// cmplw cr6,r8,r6
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, ctx.xer);
	// bne cr6,0x8225e044
	if (!ctx.cr6.eq) goto loc_8225E044;
loc_8225E080:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225e0ac
	if (ctx.cr6.eq) goto loc_8225E0AC;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// li r8,6
	ctx.r8.s64 = 6;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_8225E098:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bdnz 0x8225e098
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E098;
loc_8225E0AC:
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// addi r9,r11,24
	ctx.r9.s64 = ctx.r11.s64 + 24;
	// cmplw cr6,r6,r5
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x8225e104
	if (ctx.cr6.eq) goto loc_8225E104;
	// subf r11,r11,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r11.s64;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// addi r8,r11,-24
	ctx.r8.s64 = ctx.r11.s64 + -24;
loc_8225E0C8:
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8225e0f4
	if (ctx.cr6.eq) goto loc_8225E0F4;
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// li r7,6
	ctx.r7.s64 = 6;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_8225E0E0:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225e0e0
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E0E0;
loc_8225E0F4:
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// cmplw cr6,r8,r5
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r5.u32, ctx.xer);
	// bne cr6,0x8225e0c8
	if (!ctx.cr6.eq) goto loc_8225E0C8;
loc_8225E104:
	// lwz r3,4(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// lwz r11,8(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// subf r10,r3,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r3.s64;
	// divw r11,r10,r27
	ctx.r11.s32 = ctx.r10.s32 / ctx.r27.s32;
	// addi r31,r11,1
	ctx.r31.s64 = ctx.r11.s64 + 1;
	// beq cr6,0x8225e124
	if (ctx.cr6.eq) goto loc_8225E124;
	// bl 0x8221be68
	ctx.lr = 0x8225E124;
	sub_8221BE68(ctx, base);
loc_8225E124:
	// rlwinm r11,r31,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r30,4(r28)
	PPC_STORE_U32(ctx.r28.u32 + 4, ctx.r30.u32);
	// rlwinm r10,r29,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r31,r11
	ctx.r9.u64 = ctx.r31.u64 + ctx.r11.u64;
	// add r10,r29,r10
	ctx.r10.u64 = ctx.r29.u64 + ctx.r10.u64;
	// rlwinm r11,r9,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r11,r30
	ctx.r7.u64 = ctx.r11.u64 + ctx.r30.u64;
	// add r8,r10,r30
	ctx.r8.u64 = ctx.r10.u64 + ctx.r30.u64;
	// stw r7,8(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8, ctx.r7.u32);
	// stw r8,12(r28)
	PPC_STORE_U32(ctx.r28.u32 + 12, ctx.r8.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
loc_8225E158:
	// lwz r31,188(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// subf r11,r31,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r31.s64;
	// divw r10,r11,r27
	ctx.r10.s32 = ctx.r11.s32 / ctx.r27.s32;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bge cr6,0x8225e25c
	if (!ctx.cr6.lt) goto loc_8225E25C;
	// addi r8,r31,24
	ctx.r8.s64 = ctx.r31.s64 + 24;
	// cmplw cr6,r31,r5
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x8225e1b8
	if (ctx.cr6.eq) goto loc_8225E1B8;
	// addi r7,r8,-24
	ctx.r7.s64 = ctx.r8.s64 + -24;
loc_8225E17C:
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8225e1a8
	if (ctx.cr6.eq) goto loc_8225E1A8;
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// li r9,6
	ctx.r9.s64 = 6;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8225E194:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225e194
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E194;
loc_8225E1A8:
	// addi r7,r7,24
	ctx.r7.s64 = ctx.r7.s64 + 24;
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// cmplw cr6,r7,r5
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r5.u32, ctx.xer);
	// bne cr6,0x8225e17c
	if (!ctx.cr6.eq) goto loc_8225E17C;
loc_8225E1B8:
	// lwz r8,8(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// subf r11,r31,r8
	ctx.r11.s64 = ctx.r8.s64 - ctx.r31.s64;
	// divw r10,r11,r27
	ctx.r10.s32 = ctx.r11.s32 / ctx.r27.s32;
	// subfic r7,r10,1
	ctx.xer.ca = ctx.r10.u32 <= 1;
	ctx.r7.s64 = 1 - ctx.r10.s64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8225e208
	if (ctx.cr6.eq) goto loc_8225E208;
loc_8225E1D0:
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8225e1fc
	if (ctx.cr6.eq) goto loc_8225E1FC;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// li r9,6
	ctx.r9.s64 = 6;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_8225E1E8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225e1e8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E1E8;
loc_8225E1FC:
	// addic. r7,r7,-1
	ctx.xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// bne 0x8225e1d0
	if (!ctx.cr0.eq) goto loc_8225E1D0;
loc_8225E208:
	// lwz r11,8(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// mr r9,r31
	ctx.r9.u64 = ctx.r31.u64;
	// addi r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 + 24;
	// addi r7,r11,-24
	ctx.r7.s64 = ctx.r11.s64 + -24;
	// stw r11,8(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8, ctx.r11.u32);
	// cmplw cr6,r31,r7
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x8225e2f8
	if (ctx.cr6.eq) goto loc_8225E2F8;
loc_8225E224:
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// li r8,6
	ctx.r8.s64 = 6;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_8225E234:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225e234
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E234;
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// bne cr6,0x8225e224
	if (!ctx.cr6.eq) goto loc_8225E224;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
loc_8225E25C:
	// addi r4,r5,-24
	ctx.r4.s64 = ctx.r5.s64 + -24;
	// mr r9,r5
	ctx.r9.u64 = ctx.r5.u64;
	// mr r8,r4
	ctx.r8.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x8225e2ac
	if (ctx.cr6.eq) goto loc_8225E2AC;
loc_8225E270:
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8225e29c
	if (ctx.cr6.eq) goto loc_8225E29C;
	// mr r11,r8
	ctx.r11.u64 = ctx.r8.u64;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// li r7,6
	ctx.r7.s64 = 6;
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
loc_8225E288:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225e288
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E288;
loc_8225E29C:
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// cmplw cr6,r8,r5
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r5.u32, ctx.xer);
	// bne cr6,0x8225e270
	if (!ctx.cr6.eq) goto loc_8225E270;
loc_8225E2AC:
	// stw r9,8(r28)
	PPC_STORE_U32(ctx.r28.u32 + 8, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82aa4418
	ctx.lr = 0x8225E2B8;
	sub_82AA4418(ctx, base);
	// addi r7,r31,24
	ctx.r7.s64 = ctx.r31.s64 + 24;
	// mr r9,r31
	ctx.r9.u64 = ctx.r31.u64;
	// cmplw cr6,r31,r7
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x8225e2f8
	if (ctx.cr6.eq) goto loc_8225E2F8;
loc_8225E2C8:
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// li r8,6
	ctx.r8.s64 = 6;
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
loc_8225E2D8:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x8225e2d8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8225E2D8;
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// bne cr6,0x8225e2c8
	if (!ctx.cr6.eq) goto loc_8225E2C8;
loc_8225E2F8:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_8225E300"))) PPC_WEAK_FUNC(sub_8225E300);
PPC_FUNC_IMPL(__imp__sub_8225E300) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x8225E308;
	sub_82CA2BE4(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// addi r31,r8,32
	ctx.r31.s64 = ctx.r8.s64 + 32;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82a6c1c0
	ctx.lr = 0x8225E324;
	sub_82A6C1C0(ctx, base);
	// lwz r10,24(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 24);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// beq cr6,0x8225e340
	if (ctx.cr6.eq) goto loc_8225E340;
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// add r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
loc_8225E340:
	// addi r11,r11,4095
	ctx.r11.s64 = ctx.r11.s64 + 4095;
	// lwz r9,28(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// rlwinm r29,r11,0,0,19
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFF000;
	// subf r11,r29,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r29.s64;
	// add r6,r11,r10
	ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmplw cr6,r6,r28
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r28.u32, ctx.xer);
	// bge cr6,0x8225e3b4
	if (!ctx.cr6.lt) goto loc_8225E3B4;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r5,r11,28344
	ctx.r5.s64 = ctx.r11.s64 + 28344;
	// stw r7,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r7.u32);
loc_8225E36C:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r10,0,r5
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r5.u32);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r5
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r5.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8225e36c
	if (!ctx.cr0.eq) goto loc_8225E36C;
	// lis r4,-32241
	ctx.r4.s64 = -2112946176;
	// lwz r7,28(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// addi r4,r4,25408
	ctx.r4.s64 = ctx.r4.s64 + 25408;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x821e3a10
	ctx.lr = 0x8225E3A0;
	sub_821E3A10(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82214f08
	ctx.lr = 0x8225E3A8;
	sub_82214F08(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
loc_8225E3B4:
	// stw r6,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r6.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// bgt cr6,0x8225e3d8
	if (ctx.cr6.gt) goto loc_8225E3D8;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82a6c620
	ctx.lr = 0x8225E3D8;
	sub_82A6C620(ctx, base);
loc_8225E3D8:
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x8225e3f4
	if (ctx.cr6.gt) goto loc_8225E3F4;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
loc_8225E3F4:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// rlwinm r30,r11,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r30,r10
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x8225e418
	if (!ctx.cr6.eq) goto loc_8225E418;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	ctx.lr = 0x8225E410;
	sub_8221F388(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stwx r3,r30,r11
	PPC_STORE_U32(ctx.r30.u32 + ctx.r11.u32, ctx.r3.u32);
loc_8225E418:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwzx r11,r30,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225e434
	if (ctx.cr6.eq) goto loc_8225E434;
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r27.u32);
	// stw r28,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r28.u32);
	// stw r29,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r29.u32);
loc_8225E434:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_8225E44C"))) PPC_WEAK_FUNC(sub_8225E44C);
PPC_FUNC_IMPL(__imp__sub_8225E44C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8225E450"))) PPC_WEAK_FUNC(sub_8225E450);
PPC_FUNC_IMPL(__imp__sub_8225E450) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,-8(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225E464"))) PPC_WEAK_FUNC(sub_8225E464);
PPC_FUNC_IMPL(__imp__sub_8225E464) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8225E468"))) PPC_WEAK_FUNC(sub_8225E468);
PPC_FUNC_IMPL(__imp__sub_8225E468) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd0
	ctx.lr = 0x8225E470;
	sub_82CA2BD0(ctx, base);
	// addi r12,r1,-88
	ctx.r12.s64 = ctx.r1.s64 + -88;
	// bl 0x82ca74f8
	ctx.lr = 0x8225E478;
	sub_82CA74F8(ctx, base);
	// li r12,-208
	ctx.r12.s64 = -208;
	// stvx128 v125,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-192
	ctx.r12.s64 = -192;
	// stvx128 v126,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-176
	ctx.r12.s64 = -176;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-1024(r1)
	ea = -1024 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// li r23,0
	ctx.r23.s64 = 0;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// lbz r10,-27892(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -27892);
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225e4c0
	if (ctx.cr6.eq) goto loc_8225E4C0;
	// lbz r11,16(r26)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r26.u32 + 16);
	// b 0x8225e4c4
	goto loc_8225E4C4;
loc_8225E4C0:
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225E4C4:
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// addi r24,r10,-8700
	ctx.r24.s64 = ctx.r10.s64 + -8700;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// lfs f30,-18756(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -18756);
	ctx.f30.f64 = double(temp.f32);
	// lfs f24,-10164(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -10164);
	ctx.f24.f64 = double(temp.f32);
	// lfs f31,-18768(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -18768);
	ctx.f31.f64 = double(temp.f32);
	// beq cr6,0x8225e61c
	if (ctx.cr6.eq) goto loc_8225E61C;
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// rlwinm r9,r10,15,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 15) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8225e5e8
	if (ctx.cr6.eq) goto loc_8225E5E8;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225e524
	if (ctx.cr6.eq) goto loc_8225E524;
	// lbz r10,49(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 49);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8225e5ec
	goto loc_8225E5EC;
loc_8225E524:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r23,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r23.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8225e590
	if (!ctx.cr0.gt) goto loc_8225E590;
loc_8225E540:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,49
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 49, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8225e560
	if (ctx.cr6.lt) goto loc_8225E560;
	// mr r7,r23
	ctx.r7.u64 = ctx.r23.u64;
loc_8225E560:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8225e57c
	if (ctx.cr6.eq) goto loc_8225E57C;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8225e584
	goto loc_8225E584;
loc_8225E57C:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8225E584:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8225e540
	if (ctx.cr6.gt) goto loc_8225E540;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_8225E590:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8225e5d4
	if (ctx.cr6.eq) goto loc_8225E5D4;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,49
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 49, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8225e5ac
	if (ctx.cr6.gt) goto loc_8225E5AC;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225E5AC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8225e5d4
	if (!ctx.cr6.eq) goto loc_8225E5D4;
	// ld r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8225e5ec
	goto loc_8225E5EC;
loc_8225E5D4:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8225e5ec
	goto loc_8225E5EC;
loc_8225E5E8:
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225E5EC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225e61c
	if (ctx.cr6.eq) goto loc_8225E61C;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8225E608;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x8225e61c
	if (!ctx.cr6.eq) goto loc_8225E61C;
	// stfs f30,128(r26)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 128, temp.u32);
	// b 0x8225e634
	goto loc_8225E634;
loc_8225E61C:
	// lfs f0,128(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f0,f24
	ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f24.f64));
	// stfs f13,128(r26)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r26.u32 + 128, temp.u32);
	// fcmpu cr6,f13,f31
	ctx.cr6.compare(ctx.f13.f64, ctx.f31.f64);
	// bge cr6,0x8225e634
	if (!ctx.cr6.lt) goto loc_8225E634;
	// stfs f31,128(r26)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r26.u32 + 128, temp.u32);
loc_8225E634:
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lbz r10,-6039(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -6039);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225e760
	if (ctx.cr6.eq) goto loc_8225E760;
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// lis r29,-31924
	ctx.r29.s64 = -2092171264;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lfs f0,-6036(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -6036);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,2948(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2948);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// stfs f0,-6036(r29)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + -6036, temp.u32);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,64(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8225E678;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r7,r1,256
	ctx.r7.s64 = ctx.r1.s64 + 256;
	// addi r31,r26,1136
	ctx.r31.s64 = ctx.r26.s64 + 1136;
	// lwz r4,4(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e70c8
	ctx.lr = 0x8225E694;
	sub_821E70C8(ctx, base);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lfs f0,-19152(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -19152);
	ctx.f0.f64 = double(temp.f32);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r31
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v11,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vmulfp128 v9,v13,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v10.f32)));
	// lfs f0,-6036(r29)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -6036);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-9212(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -9212);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// vsubfp v8,v12,v9
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v9.f32)));
	// stvx128 v8,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v7,v8,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)ctx.v8.u8));
	// stvx128 v7,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82239f68
	ctx.lr = 0x8225E6D8;
	sub_82239F68(ctx, base);
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// lfs f10,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f10.f64 = double(temp.f32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lfs f12,-9404(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -9404);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r4,-28208
	ctx.r11.s64 = ctx.r4.s64 + -28208;
	// lvx128 v6,r0,r31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lfs f0,-6036(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -6036);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,-9212(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -9212);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmadds f9,f11,f12,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f12.f64 + ctx.f10.f64));
	// stfs f9,80(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v5,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v4,v6,v5,v0
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v4,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v3,v4,v4
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_load_si128((__m128i*)ctx.v4.u8));
	// stvx128 v3,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82239e88
	ctx.lr = 0x8225E728;
	sub_82239E88(ctx, base);
	// frsp f8,f1
	ctx.fpscr.disableFlushMode();
	ctx.f8.f64 = double(float(ctx.f1.f64));
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lfs f0,-9404(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -9404);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f7,132(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f7.f64 = double(temp.f32);
	// addi r7,r9,-28192
	ctx.r7.s64 = ctx.r9.s64 + -28192;
	// lvx128 v2,r0,r31
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmadds f6,f8,f0,f7
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64 + ctx.f7.f64));
	// stfs f6,80(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v1,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v31,v2,v1,v0
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v31,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f30,128(r26)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r26.u32 + 128, temp.u32);
loc_8225E760:
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// li r10,1136
	ctx.r10.s64 = 1136;
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lvx128 v127,r26,r10
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r26.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,64(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8225E784;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r7,r1,240
	ctx.r7.s64 = ctx.r1.s64 + 240;
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r6,-28384
	ctx.r4.s64 = ctx.r6.s64 + -28384;
	// lis r25,-32246
	ctx.r25.s64 = -2113273856;
	// lvx128 v13,r0,r7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// vsubfp128 v12,v13,v127
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v127.f32)));
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,-25888(r25)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// vand v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v11,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82260bc8
	ctx.lr = 0x8225E7B8;
	sub_82260BC8(ctx, base);
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,68(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 68);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x8225E7D4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// ld r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// ld r4,8(r8)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r8.u32 + 8);
	// bl 0x821f5a28
	ctx.lr = 0x8225E7E4;
	sub_821F5A28(ctx, base);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// vor128 v126,v1,v1
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vor128 v2,v126,v126
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v126.u8));
	// lvx128 v127,r0,r7
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// bl 0x82260808
	ctx.lr = 0x8225E7FC;
	sub_82260808(ctx, base);
	// vpermwi128 v10,v126,99
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v126.u32), 0x9C));
	// vpermwi128 v9,v127,135
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x78));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// vpermwi128 v8,v126,135
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v126.u32), 0x78));
	// lfs f28,-25888(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + -25888);
	ctx.f28.f64 = double(temp.f32);
	// vpermwi128 v7,v127,99
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x9C));
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// fmr f29,f1
	ctx.f29.f64 = ctx.f1.f64;
	// vmulfp128 v6,v9,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v10.f32)));
	// fmr f1,f28
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f1.f64 = ctx.f28.f64;
	// vmulfp128 v5,v7,v8
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v8.f32)));
	// vsubfp v4,v5,v6
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v6.f32)));
	// stvx128 v4,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82260bc8
	ctx.lr = 0x8225E834;
	sub_82260BC8(ctx, base);
	// fcmpu cr6,f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f1.f64, ctx.f28.f64);
	// ble cr6,0x8225eb9c
	if (!ctx.cr6.gt) goto loc_8225EB9C;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r4,r11,-19228
	ctx.r4.s64 = ctx.r11.s64 + -19228;
	// bl 0x821b5720
	ctx.lr = 0x8225E84C;
	sub_821B5720(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x821f5980
	ctx.lr = 0x8225E868;
	sub_821F5980(ctx, base);
	// lfs f0,-9060(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -9060);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// fmuls f28,f29,f0
	ctx.f28.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
	// lis r29,-32246
	ctx.r29.s64 = -2113273856;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lvx128 v127,r0,r10
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// lfs f2,-28492(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// fmr f1,f28
	ctx.f1.f64 = ctx.f28.f64;
	// bl 0x8223a048
	ctx.lr = 0x8225E890;
	sub_8223A048(ctx, base);
	// lis r9,-31926
	ctx.r9.s64 = -2092302336;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r9,-19576
	ctx.r3.s64 = ctx.r9.s64 + -19576;
	// bl 0x8226d800
	ctx.lr = 0x8225E8A0;
	sub_8226D800(ctx, base);
	// rlwinm r11,r3,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// add r6,r3,r11
	ctx.r6.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lfs f2,-28492(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// fmr f1,f28
	ctx.f1.f64 = ctx.f28.f64;
	// rlwinm r11,r6,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r5,-28480
	ctx.r28.s64 = ctx.r5.s64 + -28480;
	// vpermwi128 v12,v0,24
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xE7));
	// addi r22,r4,-28224
	ctx.r22.s64 = ctx.r4.s64 + -28224;
	// vpermwi128 v11,v0,97
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9E));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vspltw v10,v0,3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// vpermwi128 v9,v0,134
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x79));
	// ld r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// ld r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// lvx128 v0,r0,r28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r22
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// vand v8,v0,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// std r7,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r7.u64);
	// std r5,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r5.u64);
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v7,v0,252
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// vmulfp128 v6,v12,v7
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v7.f32)));
	// vpermwi128 v5,v0,133
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// vpermwi128 v4,v0,98
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vmulfp128 v3,v11,v5
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v5.f32)));
	// vmulfp128 v2,v9,v4
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v4.f32)));
	// vxor v31,v6,v8
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmaddfp v30,v10,v0,v31
	_mm_store_ps(ctx.v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v31.f32)));
	// vxor v29,v3,v8
	_mm_store_si128((__m128i*)ctx.v29.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vaddfp v28,v30,v29
	_mm_store_ps(ctx.v28.f32, _mm_add_ps(_mm_load_ps(ctx.v30.f32), _mm_load_ps(ctx.v29.f32)));
	// vsubfp v27,v28,v2
	_mm_store_ps(ctx.v27.f32, _mm_sub_ps(_mm_load_ps(ctx.v28.f32), _mm_load_ps(ctx.v2.f32)));
	// stvx128 v27,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r10,8(r6)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r6.u32 + 8);
	// ld r4,0(r6)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r6.u32 + 0);
	// std r4,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r4.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// bl 0x8223a048
	ctx.lr = 0x8225E95C;
	sub_8223A048(ctx, base);
	// lis r9,-31926
	ctx.r9.s64 = -2092302336;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r9,-19568
	ctx.r3.s64 = ctx.r9.s64 + -19568;
	// bl 0x8226d800
	ctx.lr = 0x8225E96C;
	sub_8226D800(ctx, base);
	// lvx128 v13,r0,r28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r22
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// rlwinm r11,r3,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// vand v26,v13,v12
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// add r6,r3,r11
	ctx.r6.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lvx128 v0,r0,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vpermwi128 v25,v0,24
	_mm_store_si128((__m128i*)ctx.v25.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xE7));
	// rlwinm r11,r6,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 4) & 0xFFFFFFF0;
	// vpermwi128 v24,v0,97
	_mm_store_si128((__m128i*)ctx.v24.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9E));
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// vspltw v23,v0,3
	_mm_store_si128((__m128i*)ctx.v23.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// vpermwi128 v22,v0,134
	_mm_store_si128((__m128i*)ctx.v22.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x79));
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lfs f2,-28492(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// fmr f1,f28
	ctx.f1.f64 = ctx.f28.f64;
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// ld r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// std r9,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.r9.u64);
	// std r8,8(r7)
	PPC_STORE_U64(ctx.r7.u32 + 8, ctx.r8.u64);
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v21,v0,252
	_mm_store_si128((__m128i*)ctx.v21.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// vmulfp128 v20,v25,v21
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v20.f32, _mm_mul_ps(_mm_load_ps(ctx.v25.f32), _mm_load_ps(ctx.v21.f32)));
	// vpermwi128 v19,v0,133
	_mm_store_si128((__m128i*)ctx.v19.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// vpermwi128 v18,v0,98
	_mm_store_si128((__m128i*)ctx.v18.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vmulfp128 v17,v24,v19
	_mm_store_ps(ctx.v17.f32, _mm_mul_ps(_mm_load_ps(ctx.v24.f32), _mm_load_ps(ctx.v19.f32)));
	// vmulfp128 v16,v22,v18
	_mm_store_ps(ctx.v16.f32, _mm_mul_ps(_mm_load_ps(ctx.v22.f32), _mm_load_ps(ctx.v18.f32)));
	// vxor v15,v20,v26
	_mm_store_si128((__m128i*)ctx.v15.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v20.u8), _mm_load_si128((__m128i*)ctx.v26.u8)));
	// vmaddfp v14,v23,v0,v15
	_mm_store_ps(ctx.v14.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v23.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v15.f32)));
	// vxor128 v63,v17,v26
	_mm_store_si128((__m128i*)ctx.v63.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v17.u8), _mm_load_si128((__m128i*)ctx.v26.u8)));
	// vaddfp128 v62,v14,v63
	_mm_store_ps(ctx.v62.f32, _mm_add_ps(_mm_load_ps(ctx.v14.f32), _mm_load_ps(ctx.v63.f32)));
	// vsubfp128 v61,v62,v16
	_mm_store_ps(ctx.v61.f32, _mm_sub_ps(_mm_load_ps(ctx.v62.f32), _mm_load_ps(ctx.v16.f32)));
	// stvx128 v61,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v61.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
	// ld r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// std r7,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r7.u64);
	// std r6,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r6.u64);
	// bl 0x8223a048
	ctx.lr = 0x8225EA18;
	sub_8223A048(ctx, base);
	// lis r5,-31926
	ctx.r5.s64 = -2092302336;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r5,-19580
	ctx.r3.s64 = ctx.r5.s64 + -19580;
	// bl 0x8226d800
	ctx.lr = 0x8225EA28;
	sub_8226D800(ctx, base);
	// rlwinm r11,r3,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lvx128 v13,r0,r28
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r8,r3,r11
	ctx.r8.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lvx128 v12,r0,r22
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r27,48
	ctx.r11.s64 = ctx.r27.s64 + 48;
	// vand128 v60,v13,v12
	_mm_store_si128((__m128i*)ctx.v60.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// rlwinm r10,r8,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// lfs f2,-28492(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// add r11,r10,r27
	ctx.r11.u64 = ctx.r10.u64 + ctx.r27.u64;
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vpermwi128 v59,v0,24
	_mm_store_si128((__m128i*)ctx.v59.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xE7));
	// vpermwi128 v58,v0,97
	_mm_store_si128((__m128i*)ctx.v58.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9E));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// ldx r5,r10,r27
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r10.u32 + ctx.r27.u32);
	// vspltw v13,v0,3
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// vpermwi128 v12,v0,134
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x79));
	// fneg f1,f29
	ctx.f1.u64 = ctx.f29.u64 ^ 0x8000000000000000;
	// ld r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// std r5,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r5.u64);
	// std r8,8(r9)
	PPC_STORE_U64(ctx.r9.u32 + 8, ctx.r8.u64);
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v11,v0,252
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// vmulfp128 v9,v59,v11
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v59.f32), _mm_load_ps(ctx.v11.f32)));
	// vpermwi128 v10,v0,133
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// vpermwi128 v8,v0,98
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vmulfp128 v7,v58,v10
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v58.f32), _mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v6,v12,v8
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32)));
	// vxor128 v5,v9,v60
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v60.u8)));
	// vmaddfp v3,v13,v0,v5
	_mm_store_ps(ctx.v3.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v5.f32)));
	// vxor128 v4,v7,v60
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v60.u8)));
	// vaddfp v2,v3,v4
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(ctx.v4.f32)));
	// vsubfp v31,v2,v6
	_mm_store_ps(ctx.v31.f32, _mm_sub_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v6.f32)));
	// stvx128 v31,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r6.u32 + 0);
	// ld r9,8(r6)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r6.u32 + 8);
	// std r9,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r9.u64);
	// stdx r5,r10,r27
	PPC_STORE_U64(ctx.r10.u32 + ctx.r27.u32, ctx.r5.u64);
	// ld r7,48(r27)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r27.u32 + 48);
	// ld r8,56(r27)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r27.u32 + 56);
	// std r8,8(r4)
	PPC_STORE_U64(ctx.r4.u32 + 8, ctx.r8.u64);
	// std r7,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r7.u64);
	// bl 0x8223a048
	ctx.lr = 0x8225EAEC;
	sub_8223A048(ctx, base);
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// lvx128 v12,r0,r28
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lvx128 v11,r0,r22
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v30,v12,v11
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// li r7,1
	ctx.r7.s64 = 1;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// lvx128 v13,r0,r5
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v29,v0,252
	_mm_store_si128((__m128i*)ctx.v29.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// vpermwi128 v28,v13,24
	_mm_store_si128((__m128i*)ctx.v28.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xE7));
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// vpermwi128 v27,v0,133
	_mm_store_si128((__m128i*)ctx.v27.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// vpermwi128 v26,v13,97
	_mm_store_si128((__m128i*)ctx.v26.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0x9E));
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// vspltw v25,v13,3
	_mm_store_si128((__m128i*)ctx.v25.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0x0));
	// vmulfp128 v24,v28,v29
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v24.f32, _mm_mul_ps(_mm_load_ps(ctx.v28.f32), _mm_load_ps(ctx.v29.f32)));
	// vpermwi128 v23,v0,98
	_mm_store_si128((__m128i*)ctx.v23.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vpermwi128 v22,v13,134
	_mm_store_si128((__m128i*)ctx.v22.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0x79));
	// vmulfp128 v21,v26,v27
	_mm_store_ps(ctx.v21.f32, _mm_mul_ps(_mm_load_ps(ctx.v26.f32), _mm_load_ps(ctx.v27.f32)));
	// vmulfp128 v20,v22,v23
	_mm_store_ps(ctx.v20.f32, _mm_mul_ps(_mm_load_ps(ctx.v22.f32), _mm_load_ps(ctx.v23.f32)));
	// vxor v19,v24,v30
	_mm_store_si128((__m128i*)ctx.v19.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v24.u8), _mm_load_si128((__m128i*)ctx.v30.u8)));
	// vxor v18,v21,v30
	_mm_store_si128((__m128i*)ctx.v18.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v21.u8), _mm_load_si128((__m128i*)ctx.v30.u8)));
	// vmaddfp v17,v25,v0,v19
	_mm_store_ps(ctx.v17.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v25.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v19.f32)));
	// vaddfp v16,v17,v18
	_mm_store_ps(ctx.v16.f32, _mm_add_ps(_mm_load_ps(ctx.v17.f32), _mm_load_ps(ctx.v18.f32)));
	// vsubfp v15,v16,v20
	_mm_store_ps(ctx.v15.f32, _mm_sub_ps(_mm_load_ps(ctx.v16.f32), _mm_load_ps(ctx.v20.f32)));
	// stvx128 v15,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v15.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
	// ld r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// std r9,48(r27)
	PPC_STORE_U64(ctx.r27.u32 + 48, ctx.r9.u64);
	// std r8,56(r27)
	PPC_STORE_U64(ctx.r27.u32 + 56, ctx.r8.u64);
	// bl 0x821f5980
	ctx.lr = 0x8225EB78;
	sub_821F5980(ctx, base);
	// addi r7,r1,304
	ctx.r7.s64 = ctx.r1.s64 + 304;
	// addi r6,r1,352
	ctx.r6.s64 = ctx.r1.s64 + 352;
	// addi r11,r27,64
	ctx.r11.s64 = ctx.r27.s64 + 64;
	// lvx128 v14,r0,r7
	_mm_store_si128((__m128i*)ctx.v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v63,r0,r6
	_mm_store_si128((__m128i*)ctx.v63.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v62,v14,v63
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v62.f32, _mm_sub_ps(_mm_load_ps(ctx.v14.f32), _mm_load_ps(ctx.v63.f32)));
	// lvx128 v61,r0,r11
	_mm_store_si128((__m128i*)ctx.v61.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp128 v60,v61,v62
	_mm_store_ps(ctx.v60.f32, _mm_add_ps(_mm_load_ps(ctx.v61.f32), _mm_load_ps(ctx.v62.f32)));
	// stvx128 v60,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v60.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8225EB9C:
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// mr r31,r23
	ctx.r31.u64 = ctx.r23.u64;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// rlwinm r9,r10,26,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8225ec84
	if (ctx.cr6.eq) goto loc_8225EC84;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225ebd8
	if (ctx.cr6.eq) goto loc_8225EBD8;
	// lbz r10,6(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8225ec84
	goto loc_8225EC84;
loc_8225EBD8:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r23,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r23.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8225ec44
	if (!ctx.cr0.gt) goto loc_8225EC44;
loc_8225EBF4:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 6, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8225ec14
	if (ctx.cr6.lt) goto loc_8225EC14;
	// mr r7,r23
	ctx.r7.u64 = ctx.r23.u64;
loc_8225EC14:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8225ec30
	if (ctx.cr6.eq) goto loc_8225EC30;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8225ec38
	goto loc_8225EC38;
loc_8225EC30:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8225EC38:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8225ebf4
	if (ctx.cr6.gt) goto loc_8225EBF4;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_8225EC44:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8225ec7c
	if (ctx.cr6.eq) goto loc_8225EC7C;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8225ec60
	if (ctx.cr6.gt) goto loc_8225EC60;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225EC60:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8225ec7c
	if (!ctx.cr6.eq) goto loc_8225EC7C;
	// ld r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x8225ec80
	goto loc_8225EC80;
loc_8225EC7C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_8225EC80:
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_8225EC84:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lwz r30,148(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,6732
	ctx.r4.s64 = ctx.r11.s64 + 6732;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8225EC9C;
	sub_8222CF18(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8226d800
	ctx.lr = 0x8225ECA4;
	sub_8226D800(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8225ECB0;
	sub_82214F08(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,6800
	ctx.r4.s64 = ctx.r10.s64 + 6800;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r29,148(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// bl 0x8222cf18
	ctx.lr = 0x8225ECC8;
	sub_8222CF18(ctx, base);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8226d800
	ctx.lr = 0x8225ECD0;
	sub_8226D800(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8225ECDC;
	sub_82214F08(ctx, base);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r9,6776
	ctx.r4.s64 = ctx.r9.s64 + 6776;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r28,148(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// bl 0x8222cf18
	ctx.lr = 0x8225ECF4;
	sub_8222CF18(ctx, base);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8226d800
	ctx.lr = 0x8225ECFC;
	sub_8226D800(ctx, base);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8225ED08;
	sub_82214F08(ctx, base);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r8,6624
	ctx.r4.s64 = ctx.r8.s64 + 6624;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r22,148(r31)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// bl 0x8222cf18
	ctx.lr = 0x8225ED20;
	sub_8222CF18(ctx, base);
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// bl 0x8226d800
	ctx.lr = 0x8225ED28;
	sub_8226D800(ctx, base);
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8225ED34;
	sub_82214F08(ctx, base);
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lwz r6,148(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,480
	ctx.r3.s64 = ctx.r1.s64 + 480;
	// bl 0x821f5980
	ctx.lr = 0x8225ED4C;
	sub_821F5980(ctx, base);
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lwz r6,148(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x821f5980
	ctx.lr = 0x8225ED64;
	sub_821F5980(ctx, base);
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lwz r6,148(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x821f5980
	ctx.lr = 0x8225ED7C;
	sub_821F5980(ctx, base);
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lwz r6,148(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// bl 0x821f5980
	ctx.lr = 0x8225ED94;
	sub_821F5980(ctx, base);
	// lwz r5,4(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// stfs f31,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// stfs f31,112(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f30,84(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r11,r1,88
	ctx.r11.s64 = ctx.r1.s64 + 88;
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r10,r1,84
	ctx.r10.s64 = ctx.r1.s64 + 84;
	// lwz r4,124(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 124);
	// lvlx v0,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lvlx v12,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r8,68(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 68);
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v13,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// vrlimi128 v11,v0,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// vrlimi128 v12,v11,3,2
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vor128 v127,v12,v12
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_load_si128((__m128i*)ctx.v12.u8));
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8225EDF0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// ld r4,8(r7)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
	// bl 0x821f5a28
	ctx.lr = 0x8225EE00;
	sub_821F5A28(ctx, base);
	// vpermwi128 v10,v127,99
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x9C));
	// lwz r6,4(r26)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// vpermwi128 v9,v127,135
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x78));
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// vpermwi128 v8,v1,135
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), 0x78));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// vpermwi128 v7,v1,99
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), 0x9C));
	// addi r31,r5,-28400
	ctx.r31.s64 = ctx.r5.s64 + -28400;
	// lwz r4,124(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 124);
	// vmulfp128 v6,v8,v10
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v10.f32)));
	// vmulfp128 v5,v7,v9
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v9.f32)));
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	// vsubfp v4,v5,v6
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v6.f32)));
	// vand128 v127,v4,v0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8225EE48;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// ld r3,0(r9)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// ld r4,8(r9)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// bl 0x821f5a28
	ctx.lr = 0x8225EE58;
	sub_821F5A28(ctx, base);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stfs f31,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f30,84(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r5,4(r26)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v2,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,224
	ctx.r3.s64 = ctx.r1.s64 + 224;
	// lvlx v31,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vand128 v126,v1,v0
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lwz r11,124(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 124);
	// lvlx v30,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// lvlx v3,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v31,v2,4,3
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v2.f32), 57), 4));
	// vrlimi128 v3,v30,4,3
	_mm_store_ps(ctx.v3.f32, _mm_blend_ps(_mm_load_ps(ctx.v3.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// vrlimi128 v31,v3,3,2
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 78), 3));
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// vand128 v125,v31,v0
	_mm_store_si128((__m128i*)ctx.v125.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x8225EEC0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// vmrghw128 v29,v127,v125
	_mm_store_si128((__m128i*)ctx.v29.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v125.u32), _mm_load_si128((__m128i*)ctx.v127.u32)));
	// addi r7,r1,88
	ctx.r7.s64 = ctx.r1.s64 + 88;
	// stfs f30,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r6,-28160
	ctx.r5.s64 = ctx.r6.s64 + -28160;
	// stfs f30,88(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// stfs f30,112(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r3,r1,496
	ctx.r3.s64 = ctx.r1.s64 + 496;
	// vor v28,v29,v29
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_load_si128((__m128i*)ctx.v29.u8));
	// lvlx v27,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,192
	ctx.r11.s64 = ctx.r1.s64 + 192;
	// lvlx v26,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// stfs f30,84(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f30,88(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvx128 v24,r0,r4
	_mm_store_si128((__m128i*)ctx.v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglw128 v25,v127,v125
	_mm_store_si128((__m128i*)ctx.v25.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v125.u32), _mm_load_si128((__m128i*)ctx.v127.u32)));
	// vperm v23,v24,v27,v0
	_mm_store_si128((__m128i*)ctx.v23.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v24.u8), _mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r1,304
	ctx.r8.s64 = ctx.r1.s64 + 304;
	// addi r7,r1,352
	ctx.r7.s64 = ctx.r1.s64 + 352;
	// vor v20,v29,v29
	_mm_store_si128((__m128i*)ctx.v20.u8, _mm_load_si128((__m128i*)ctx.v29.u8));
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// vor v19,v29,v29
	_mm_store_si128((__m128i*)ctx.v19.u8, _mm_load_si128((__m128i*)ctx.v29.u8));
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// vor v21,v25,v25
	_mm_store_si128((__m128i*)ctx.v21.u8, _mm_load_si128((__m128i*)ctx.v25.u8));
	// vmrghw128 v18,v126,v23
	_mm_store_si128((__m128i*)ctx.v18.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v23.u32), _mm_load_si128((__m128i*)ctx.v126.u32)));
	// vor v17,v25,v25
	_mm_store_si128((__m128i*)ctx.v17.u8, _mm_load_si128((__m128i*)ctx.v25.u8));
	// vmrglw128 v16,v126,v23
	_mm_store_si128((__m128i*)ctx.v16.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v23.u32), _mm_load_si128((__m128i*)ctx.v126.u32)));
	// vor v15,v25,v25
	_mm_store_si128((__m128i*)ctx.v15.u8, _mm_load_si128((__m128i*)ctx.v25.u8));
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lvx128 v6,r0,r8
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v22,v13,v26,v0
	_mm_store_si128((__m128i*)ctx.v22.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v26.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v14,v18,v18
	_mm_store_si128((__m128i*)ctx.v14.u8, _mm_load_si128((__m128i*)ctx.v18.u8));
	// lvx128 v5,r0,r7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor128 v63,v16,v16
	_mm_store_si128((__m128i*)ctx.v63.u8, _mm_load_si128((__m128i*)ctx.v16.u8));
	// lvlx128 v62,r0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v62.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor128 v61,v18,v18
	_mm_store_si128((__m128i*)ctx.v61.u8, _mm_load_si128((__m128i*)ctx.v18.u8));
	// lvlx128 v60,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v60.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vor128 v57,v16,v16
	_mm_store_si128((__m128i*)ctx.v57.u8, _mm_load_si128((__m128i*)ctx.v16.u8));
	// vperm128 v59,v5,v62,v0
	_mm_store_si128((__m128i*)ctx.v59.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v62.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor128 v56,v18,v18
	_mm_store_si128((__m128i*)ctx.v56.u8, _mm_load_si128((__m128i*)ctx.v18.u8));
	// vperm128 v58,v6,v60,v0
	_mm_store_si128((__m128i*)ctx.v58.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v60.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor128 v54,v16,v16
	_mm_store_si128((__m128i*)ctx.v54.u8, _mm_load_si128((__m128i*)ctx.v16.u8));
	// vmrghw128 v55,v28,v14
	_mm_store_si128((__m128i*)ctx.v55.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v14.u32), _mm_load_si128((__m128i*)ctx.v28.u32)));
	// vmrglw128 v53,v28,v14
	_mm_store_si128((__m128i*)ctx.v53.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v14.u32), _mm_load_si128((__m128i*)ctx.v28.u32)));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghw128 v52,v21,v63
	_mm_store_si128((__m128i*)ctx.v52.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v63.u32), _mm_load_si128((__m128i*)ctx.v21.u32)));
	// stvx128 v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrglw128 v51,v21,v63
	_mm_store_si128((__m128i*)ctx.v51.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v63.u32), _mm_load_si128((__m128i*)ctx.v21.u32)));
	// stvx128 v12,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghw128 v50,v20,v61
	_mm_store_si128((__m128i*)ctx.v50.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v61.u32), _mm_load_si128((__m128i*)ctx.v20.u32)));
	// vmsum4fp128 v42,v22,v55
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v42.f32, _mm_dp_ps(_mm_load_ps(ctx.v22.f32), _mm_load_ps(ctx.v55.f32), 0xFF));
	// vmrglw128 v49,v20,v61
	_mm_store_si128((__m128i*)ctx.v49.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v61.u32), _mm_load_si128((__m128i*)ctx.v20.u32)));
	// vmsum4fp128 v41,v22,v53
	_mm_store_ps(ctx.v41.f32, _mm_dp_ps(_mm_load_ps(ctx.v22.f32), _mm_load_ps(ctx.v53.f32), 0xFF));
	// vmrghw128 v48,v17,v57
	_mm_store_si128((__m128i*)ctx.v48.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v57.u32), _mm_load_si128((__m128i*)ctx.v17.u32)));
	// vmsum4fp128 v39,v22,v52
	_mm_store_ps(ctx.v39.f32, _mm_dp_ps(_mm_load_ps(ctx.v22.f32), _mm_load_ps(ctx.v52.f32), 0xFF));
	// vmrglw128 v47,v17,v57
	_mm_store_si128((__m128i*)ctx.v47.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v57.u32), _mm_load_si128((__m128i*)ctx.v17.u32)));
	// vmsum4fp128 v38,v22,v51
	_mm_store_ps(ctx.v38.f32, _mm_dp_ps(_mm_load_ps(ctx.v22.f32), _mm_load_ps(ctx.v51.f32), 0xFF));
	// vmrghw128 v45,v19,v56
	_mm_store_si128((__m128i*)ctx.v45.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v56.u32), _mm_load_si128((__m128i*)ctx.v19.u32)));
	// vmsum4fp128 v37,v58,v50
	_mm_store_ps(ctx.v37.f32, _mm_dp_ps(_mm_load_ps(ctx.v58.f32), _mm_load_ps(ctx.v50.f32), 0xFF));
	// vmrglw128 v44,v19,v56
	_mm_store_si128((__m128i*)ctx.v44.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v56.u32), _mm_load_si128((__m128i*)ctx.v19.u32)));
	// vmsum4fp128 v36,v58,v49
	_mm_store_ps(ctx.v36.f32, _mm_dp_ps(_mm_load_ps(ctx.v58.f32), _mm_load_ps(ctx.v49.f32), 0xFF));
	// vmrghw128 v43,v15,v54
	_mm_store_si128((__m128i*)ctx.v43.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v54.u32), _mm_load_si128((__m128i*)ctx.v15.u32)));
	// vmsum4fp128 v35,v58,v48
	_mm_store_ps(ctx.v35.f32, _mm_dp_ps(_mm_load_ps(ctx.v58.f32), _mm_load_ps(ctx.v48.f32), 0xFF));
	// vmrglw128 v40,v15,v54
	_mm_store_si128((__m128i*)ctx.v40.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v54.u32), _mm_load_si128((__m128i*)ctx.v15.u32)));
	// vmsum4fp128 v34,v58,v47
	_mm_store_ps(ctx.v34.f32, _mm_dp_ps(_mm_load_ps(ctx.v58.f32), _mm_load_ps(ctx.v47.f32), 0xFF));
	// vmsum4fp128 v33,v59,v45
	_mm_store_ps(ctx.v33.f32, _mm_dp_ps(_mm_load_ps(ctx.v59.f32), _mm_load_ps(ctx.v45.f32), 0xFF));
	// vmrghw v11,v29,v18
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v18.u32), _mm_load_si128((__m128i*)ctx.v29.u32)));
	// vmsum4fp128 v32,v59,v44
	_mm_store_ps(ctx.v32.f32, _mm_dp_ps(_mm_load_ps(ctx.v59.f32), _mm_load_ps(ctx.v44.f32), 0xFF));
	// vmrglw v10,v29,v18
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v18.u32), _mm_load_si128((__m128i*)ctx.v29.u32)));
	// vmsum4fp128 v7,v59,v43
	_mm_store_ps(ctx.v7.f32, _mm_dp_ps(_mm_load_ps(ctx.v59.f32), _mm_load_ps(ctx.v43.f32), 0xFF));
	// lfs f13,136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f13.f64 = double(temp.f32);
	// vmsum4fp128 v3,v59,v40
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v3.f32, _mm_dp_ps(_mm_load_ps(ctx.v59.f32), _mm_load_ps(ctx.v40.f32), 0xFF));
	// lvlx128 v46,r0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v46.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f0,152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// vmrghw128 v29,v33,v7
	_mm_store_si128((__m128i*)ctx.v29.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), _mm_load_si128((__m128i*)ctx.v33.u32)));
	// lfs f13,-19148(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -19148);
	ctx.f13.f64 = double(temp.f32);
	// vmrghw128 v2,v42,v39
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v39.u32), _mm_load_si128((__m128i*)ctx.v42.u32)));
	// vmrghw128 v1,v41,v38
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v38.u32), _mm_load_si128((__m128i*)ctx.v41.u32)));
	// vmrghw128 v31,v37,v35
	_mm_store_si128((__m128i*)ctx.v31.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v35.u32), _mm_load_si128((__m128i*)ctx.v37.u32)));
	// vmrghw128 v30,v36,v34
	_mm_store_si128((__m128i*)ctx.v30.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v34.u32), _mm_load_si128((__m128i*)ctx.v36.u32)));
	// vmrghw128 v28,v32,v3
	_mm_store_si128((__m128i*)ctx.v28.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v3.u32), _mm_load_si128((__m128i*)ctx.v32.u32)));
	// vperm128 v0,v12,v46,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v46.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmrghw v9,v25,v16
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v16.u32), _mm_load_si128((__m128i*)ctx.v25.u32)));
	// vmrglw v8,v25,v16
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v16.u32), _mm_load_si128((__m128i*)ctx.v25.u32)));
	// vmrghw v7,v2,v1
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrghw v4,v31,v30
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v30.u32), _mm_load_si128((__m128i*)ctx.v31.u32)));
	// vmrghw v3,v29,v28
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v28.u32), _mm_load_si128((__m128i*)ctx.v29.u32)));
	// blt cr6,0x8225f090
	if (ctx.cr6.lt) goto loc_8225F090;
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f0,f12
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// fabs f10,f11
	ctx.f10.u64 = ctx.f11.u64 & ~0x8000000000000000;
	// fcmpu cr6,f10,f13
	ctx.cr6.compare(ctx.f10.f64, ctx.f13.f64);
	// blt cr6,0x8225f090
	if (ctx.cr6.lt) goto loc_8225F090;
	// vmsum4fp128 v12,v0,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32), 0xFF));
	// addi r30,r26,2000
	ctx.r30.s64 = ctx.r26.s64 + 2000;
	// vmsum4fp128 v10,v0,v9
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32), 0xFF));
	// lfs f0,200(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f0.f64 = double(temp.f32);
	// vmsum4fp128 v13,v0,v8
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v8.f32), 0xFF));
	// vmsum4fp128 v9,v0,v11
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v11.f32), 0xFF));
	// vmrghw v8,v12,v13
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), _mm_load_si128((__m128i*)ctx.v12.u32)));
	// vmrghw v7,v9,v10
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), _mm_load_si128((__m128i*)ctx.v9.u32)));
	// vmrghw v2,v7,v8
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), _mm_load_si128((__m128i*)ctx.v7.u32)));
	// stvx128 v2,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8225f09c
	goto loc_8225F09C;
loc_8225F090:
	// addi r30,r26,2000
	ctx.r30.s64 = ctx.r26.s64 + 2000;
	// lfs f0,504(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 504);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v7,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8225F09C:
	// fcmpu cr6,f0,f24
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f24.f64);
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x8225f0ac
	if (ctx.cr6.lt) goto loc_8225F0AC;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225F0AC:
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// stb r11,44(r26)
	PPC_STORE_U8(ctx.r26.u32 + 44, ctx.r11.u8);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx128 v6,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,136(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// lfs f0,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// blt cr6,0x8225f108
	if (ctx.cr6.lt) goto loc_8225F108;
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// stvx128 v6,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f11,f0,f12
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// fabs f10,f11
	ctx.f10.u64 = ctx.f11.u64 & ~0x8000000000000000;
	// fcmpu cr6,f10,f13
	ctx.cr6.compare(ctx.f10.f64, ctx.f13.f64);
	// blt cr6,0x8225f108
	if (ctx.cr6.lt) goto loc_8225F108;
	// addi r29,r26,2016
	ctx.r29.s64 = ctx.r26.s64 + 2016;
	// lfs f0,360(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v3,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8225f114
	goto loc_8225F114;
loc_8225F108:
	// addi r29,r26,2016
	ctx.r29.s64 = ctx.r26.s64 + 2016;
	// lfs f0,312(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v4,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8225F114:
	// fcmpu cr6,f0,f24
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f24.f64);
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x8225f124
	if (ctx.cr6.lt) goto loc_8225F124;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225F124:
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f29,-18744(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -18744);
	ctx.f29.f64 = double(temp.f32);
	// clrlwi r9,r11,24
	ctx.r9.u64 = ctx.r11.u32 & 0xFF;
	// lfs f25,-952(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + -952);
	ctx.f25.f64 = double(temp.f32);
	// li r28,25
	ctx.r28.s64 = 25;
	// lis r31,-31927
	ctx.r31.s64 = -2092367872;
	// stb r9,45(r26)
	PPC_STORE_U8(ctx.r26.u32 + 45, ctx.r9.u8);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lfs f26,3040(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3040);
	ctx.f26.f64 = double(temp.f32);
	// fmr f28,f26
	ctx.f28.f64 = ctx.f26.f64;
	// beq cr6,0x8225f250
	if (ctx.cr6.eq) goto loc_8225F250;
	// addi r11,r1,84
	ctx.r11.s64 = ctx.r1.s64 + 84;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// stfs f29,112(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r7,r1,164
	ctx.r7.s64 = ctx.r1.s64 + 164;
	// stfs f31,160(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// stfs f31,164(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// lvlx v10,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,400
	ctx.r11.s64 = ctx.r1.s64 + 400;
	// lvlx v8,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,528
	ctx.r3.s64 = ctx.r1.s64 + 528;
	// lvlx v7,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stw r28,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, ctx.r28.u32);
	// stfs f29,80(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f31,96(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvx128 v6,r0,r29
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,384
	ctx.r10.s64 = ctx.r1.s64 + 384;
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v7,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 57), 4));
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v8,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// vrlimi128 v9,v10,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v13,v12,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// vrlimi128 v9,v11,3,2
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vaddfp v4,v6,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v13.f32)));
	// vsubfp v5,v6,v9
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v9.f32)));
	// stvx128 v4,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8236b840
	ctx.lr = 0x8225F1F0;
	sub_8236B840(ctx, base);
	// lwz r11,26912(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 26912);
	// addi r5,r1,528
	ctx.r5.s64 = ctx.r1.s64 + 528;
	// addi r4,r1,384
	ctx.r4.s64 = ctx.r1.s64 + 384;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r3,24(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,68(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 68);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8225F21C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lfs f0,548(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 548);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x8225f230
	if (ctx.cr6.lt) goto loc_8225F230;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225F230:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f248
	if (ctx.cr6.eq) goto loc_8225F248;
	// lfs f0,576(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f25
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// fneg f28,f13
	ctx.f28.u64 = ctx.f13.u64 ^ 0x8000000000000000;
loc_8225F248:
	// addi r3,r1,528
	ctx.r3.s64 = ctx.r1.s64 + 528;
	// bl 0x821e07d8
	ctx.lr = 0x8225F250;
	sub_821E07D8(ctx, base);
loc_8225F250:
	// lbz r11,44(r26)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r26.u32 + 44);
	// fmr f27,f26
	ctx.fpscr.disableFlushMode();
	ctx.f27.f64 = ctx.f26.f64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f360
	if (ctx.cr6.eq) goto loc_8225F360;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// stfs f31,112(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stfs f31,96(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f29,88(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,164
	ctx.r10.s64 = ctx.r1.s64 + 164;
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,448
	ctx.r11.s64 = ctx.r1.s64 + 448;
	// stfs f31,160(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// addi r3,r1,672
	ctx.r3.s64 = ctx.r1.s64 + 672;
	// stfs f29,96(r1)
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stw r28,464(r1)
	PPC_STORE_U32(ctx.r1.u32 + 464, ctx.r28.u32);
	// stfs f31,164(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v10,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lvlx v8,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// lvlx v7,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v8,v9,4,3
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 57), 4));
	// vrlimi128 v7,v10,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// lvx128 v6,r0,r30
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v11,v8,3,2
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 78), 3));
	// vrlimi128 v13,v7,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 78), 3));
	// vsubfp v5,v6,v11
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v11.f32)));
	// vaddfp v4,v6,v13
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v13.f32)));
	// stvx128 v5,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v4,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8236b840
	ctx.lr = 0x8225F300;
	sub_8236B840(ctx, base);
	// lwz r11,26912(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 26912);
	// addi r5,r1,672
	ctx.r5.s64 = ctx.r1.s64 + 672;
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r3,24(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 24);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,68(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 68);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8225F32C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lfs f0,692(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x8225f340
	if (ctx.cr6.lt) goto loc_8225F340;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_8225F340:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f358
	if (ctx.cr6.eq) goto loc_8225F358;
	// lfs f0,720(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 720);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f25
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f25.f64));
	// fneg f27,f13
	ctx.f27.u64 = ctx.f13.u64 ^ 0x8000000000000000;
loc_8225F358:
	// addi r3,r1,672
	ctx.r3.s64 = ctx.r1.s64 + 672;
	// bl 0x821e07d8
	ctx.lr = 0x8225F360;
	sub_821E07D8(ctx, base);
loc_8225F360:
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// stfs f31,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f31,96(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r8,r1,160
	ctx.r8.s64 = ctx.r1.s64 + 160;
	// stfs f31,164(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r10,r1,164
	ctx.r10.s64 = ctx.r1.s64 + 164;
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// stfs f30,160(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// lvlx v10,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,208
	ctx.r6.s64 = ctx.r1.s64 + 208;
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fcmpu cr6,f28,f26
	ctx.cr6.compare(ctx.f28.f64, ctx.f26.f64);
	// lvlx128 v127,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v0,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v127,v13,4,3
	_mm_store_ps(ctx.v127.f32, _mm_blend_ps(_mm_load_ps(ctx.v127.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// stvx128 v11,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v127,v10,3,2
	_mm_store_ps(ctx.v127.f32, _mm_blend_ps(_mm_load_ps(ctx.v127.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 78), 3));
	// beq cr6,0x8225f5d4
	if (ctx.cr6.eq) goto loc_8225F5D4;
	// fcmpu cr6,f27,f26
	ctx.cr6.compare(ctx.f27.f64, ctx.f26.f64);
	// beq cr6,0x8225f5d4
	if (ctx.cr6.eq) goto loc_8225F5D4;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r29
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,144
	ctx.r8.s64 = ctx.r1.s64 + 144;
	// vor v13,v0,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f1,-25888(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r31,r11,-28176
	ctx.r31.s64 = ctx.r11.s64 + -28176;
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lfs f0,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f0.f64 = double(temp.f32);
	// fadds f13,f0,f28
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f28.f64));
	// fadds f12,f13,f29
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f29.f64));
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v11,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v11,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lvx128 v10,r0,r30
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor v9,v10,v10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)ctx.v10.u8));
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f11,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f11,f27
	ctx.f10.f64 = double(float(ctx.f11.f64 + ctx.f27.f64));
	// fadds f9,f10,f29
	ctx.f9.f64 = double(float(ctx.f10.f64 + ctx.f29.f64));
	// stfs f9,96(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v7,v10,v8,v0
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor v6,v7,v7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvx128 v7,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v5,r0,r29
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v4,v5,v6
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v6.f32)));
	// stvx128 v4,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82260bc8
	ctx.lr = 0x8225F454;
	sub_82260BC8(ctx, base);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lfs f0,0(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// addi r30,r11,368
	ctx.r30.s64 = ctx.r11.s64 + 368;
	// lvx128 v126,r0,r5
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f8,f13,f0
	ctx.f8.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// fcmpu cr6,f8,f31
	ctx.cr6.compare(ctx.f8.f64, ctx.f31.f64);
	// mfcr r3
	ctx.r3.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r3.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r3.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r3.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r3.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r3.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r3.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r3.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r3.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r3.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r3.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r3.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r3.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r3.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r3.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r3.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r3.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r3.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r3.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r3.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r3.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r3.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r3.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r3.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r3.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r3.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r3.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r3.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r3.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r3.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r3.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r3.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r11,r3,27,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 27) & 0x4;
	// rlwinm r10,r3,30,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x4;
	// or r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 | ctx.r10.u64;
	// lfsx f7,r30,r9
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r9.u32);
	ctx.f7.f64 = double(temp.f32);
	// fsel f6,f7,f13,f0
	ctx.f6.f64 = ctx.f7.f64 >= 0.0 ? ctx.f13.f64 : ctx.f0.f64;
	// fsubs f5,f6,f30
	ctx.f5.f64 = double(float(ctx.f6.f64 - ctx.f30.f64));
	// fcmpu cr6,f5,f31
	ctx.cr6.compare(ctx.f5.f64, ctx.f31.f64);
	// mfcr r8
	ctx.r8.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r8.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r8.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r8.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r8.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r8.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r8.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r8.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r8.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r8.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r8.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r8.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r8.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r8.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r8.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r8.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r8.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r8.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r8.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r8.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r8.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r8.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r8.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r8.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r8.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r8.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r8.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r8.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r8.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r8.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r8.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r8.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r7,r8,27,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x4;
	// rlwinm r6,r8,30,29,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 30) & 0x4;
	// or r5,r7,r6
	ctx.r5.u64 = ctx.r7.u64 | ctx.r6.u64;
	// lfsx f4,r30,r5
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r5.u32);
	ctx.f4.f64 = double(temp.f32);
	// fsel f1,f4,f30,f6
	ctx.f1.f64 = ctx.f4.f64 >= 0.0 ? ctx.f30.f64 : ctx.f6.f64;
	// fcmpu cr6,f1,f31
	ctx.cr6.compare(ctx.f1.f64, ctx.f31.f64);
	// ble cr6,0x8225f4c8
	if (!ctx.cr6.gt) goto loc_8225F4C8;
	// bl 0x8227efe8
	ctx.lr = 0x8225F4C0;
	sub_8227EFE8(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// b 0x8225f4d8
	goto loc_8225F4D8;
loc_8225F4C8:
	// fneg f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = ctx.f1.u64 ^ 0x8000000000000000;
	// bl 0x8227efe8
	ctx.lr = 0x8225F4D0;
	sub_8227EFE8(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// fneg f12,f0
	ctx.f12.u64 = ctx.f0.u64 ^ 0x8000000000000000;
loc_8225F4D8:
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lfs f0,360(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 360);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// lfs f13,1976(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1976);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f11,f31
	ctx.cr6.compare(ctx.f11.f64, ctx.f31.f64);
	// mfcr r9
	ctx.r9.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r9.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r9.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r9.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r9.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r9.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r9.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r9.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r9.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r9.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r9.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r9.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r9.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r9.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r9.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r9.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r9.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r9.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r9.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r9.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r9.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r9.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r9.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r9.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r9.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r9.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r9.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r9.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r9.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r9.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r9.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r9.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r8,r9,27,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x4;
	// rlwinm r7,r9,30,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x4;
	// or r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 | ctx.r7.u64;
	// lfsx f10,r30,r6
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r6.u32);
	ctx.f10.f64 = double(temp.f32);
	// fsel f9,f10,f0,f12
	ctx.f9.f64 = ctx.f10.f64 >= 0.0 ? ctx.f0.f64 : ctx.f12.f64;
	// fsubs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
	// fcmpu cr6,f8,f31
	ctx.cr6.compare(ctx.f8.f64, ctx.f31.f64);
	// mfcr r5
	ctx.r5.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r5.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r5.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r5.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r5.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r5.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r5.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r5.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r5.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r5.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r5.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r5.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r5.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r5.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r5.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r5.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r5.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r5.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r5.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r5.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r5.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r5.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r5.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r5.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r5.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r5.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r5.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r5.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r5.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r5.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r5.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r5.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r4,r5,27,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x4;
	// rlwinm r11,r5,30,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x4;
	// or r10,r4,r11
	ctx.r10.u64 = ctx.r4.u64 | ctx.r11.u64;
	// lfsx f7,r30,r10
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r10.u32);
	ctx.f7.f64 = double(temp.f32);
	// fsel f31,f7,f9,f13
	ctx.f31.f64 = ctx.f7.f64 >= 0.0 ? ctx.f9.f64 : ctx.f13.f64;
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// bl 0x821b46a0
	ctx.lr = 0x8225F534;
	sub_821B46A0(ctx, base);
	// lwz r8,4(r26)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// fadds f6,f28,f29
	ctx.fpscr.disableFlushMode();
	ctx.f6.f64 = double(float(ctx.f28.f64 + ctx.f29.f64));
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f6,96(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// lwz r4,124(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 124);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm128 v127,v127,v13,v0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lwz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r6,64(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 64);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x8225F568;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r4,4(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// lvx128 v125,r0,r29
	_mm_store_si128((__m128i*)ctx.v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e70c8
	ctx.lr = 0x8225F578;
	sub_821E70C8(ctx, base);
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// lvx128 v11,r0,r5
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// vsubfp128 v10,v125,v11
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(ctx.v125.f32), _mm_load_ps(ctx.v11.f32)));
	// stvx128 v127,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f5,152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f5.f64 = double(temp.f32);
	// vmsum3fp128 v9,v12,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32), 0xEF));
	// stvx128 v9,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v8,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v7,v8,0
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), 0xFF));
	// vmulfp128 v6,v126,v7
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v126.f32), _mm_load_ps(ctx.v7.f32)));
	// stvx128 v6,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f4,152(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f4.f64 = double(temp.f32);
	// fsubs f3,f5,f4
	ctx.f3.f64 = double(float(ctx.f5.f64 - ctx.f4.f64));
	// stfs f3,96(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v5,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm128 v127,v127,v5,v0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_8225F5D4:
	// stfs f24,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f24.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r31,r26,2032
	ctx.r31.s64 = ctx.r26.s64 + 2032;
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v13,v127,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_sub_ps(_mm_load_ps(ctx.v127.f32), _mm_load_ps(ctx.v0.f32)));
	// vor v12,v0,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// lvlx v11,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vmaddfp v9,v13,v10,v12
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v12.f32)));
	// stvx128 v9,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,2048(r26)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 2048);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f31,f0
	ctx.f13.f64 = double(float(ctx.f31.f64 - ctx.f0.f64));
	// fmadds f1,f13,f24,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f24.f64 + ctx.f0.f64));
	// stfs f1,2048(r26)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r26.u32 + 2048, temp.u32);
	// bl 0x821b46a0
	ctx.lr = 0x8225F614;
	sub_821B46A0(ctx, base);
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// addi r9,r1,192
	ctx.r9.s64 = ctx.r1.s64 + 192;
	// lvx128 v8,r0,r31
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,176
	ctx.r8.s64 = ctx.r1.s64 + 176;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// addi r3,r27,48
	ctx.r3.s64 = ctx.r27.s64 + 48;
	// ld r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
	// stvx128 v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// std r7,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r7.u64);
	// std r6,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r6.u64);
	// bl 0x821f57a8
	ctx.lr = 0x8225F644;
	sub_821F57A8(ctx, base);
	// addi r1,r1,1024
	ctx.r1.s64 = ctx.r1.s64 + 1024;
	// li r0,-208
	ctx.r0.s64 = -208;
	// lvx128 v125,r1,r0
	_mm_store_si128((__m128i*)ctx.v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r0,-192
	ctx.r0.s64 = -192;
	// lvx128 v126,r1,r0
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r0,-176
	ctx.r0.s64 = -176;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r12,r1,-88
	ctx.r12.s64 = ctx.r1.s64 + -88;
	// bl 0x82ca7544
	ctx.lr = 0x8225F668;
	sub_82CA7544(ctx, base);
	// b 0x82ca2c20
	// ERROR 82CA2C20
	return;
}

__attribute__((alias("__imp__sub_8225F66C"))) PPC_WEAK_FUNC(sub_8225F66C);
PPC_FUNC_IMPL(__imp__sub_8225F66C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8225F670"))) PPC_WEAK_FUNC(sub_8225F670);
PPC_FUNC_IMPL(__imp__sub_8225F670) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd8
	ctx.lr = 0x8225F678;
	sub_82CA2BD8(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32241
	ctx.r11.s64 = -2112946176;
	// li r29,0
	ctx.r29.s64 = 0;
	// addi r10,r11,5732
	ctx.r10.s64 = ctx.r11.s64 + 5732;
	// li r9,-1
	ctx.r9.s64 = -1;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r29.u32);
	// addi r3,r31,800
	ctx.r3.s64 = ctx.r31.s64 + 800;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// bl 0x8228ed70
	ctx.lr = 0x8225F6B4;
	sub_8228ED70(ctx, base);
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r25,r8,5732
	ctx.r25.s64 = ctx.r8.s64 + 5732;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// bl 0x821940c8
	ctx.lr = 0x8225F6C8;
	sub_821940C8(ctx, base);
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r30,808(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 808);
	// addi r28,r30,-16
	ctx.r28.s64 = ctx.r30.s64 + -16;
	// bl 0x82241480
	ctx.lr = 0x8225F6E0;
	sub_82241480(ctx, base);
	// mr r24,r3
	ctx.r24.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// bl 0x82978470
	ctx.lr = 0x8225F6F0;
	sub_82978470(ctx, base);
	// lwz r7,8(r24)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r7,-8(r30)
	PPC_STORE_U32(ctx.r30.u32 + -8, ctx.r7.u32);
	// lwz r6,12(r24)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r24.u32 + 12);
	// stw r6,-4(r30)
	PPC_STORE_U32(ctx.r30.u32 + -4, ctx.r6.u32);
	// stw r25,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r25.u32);
	// bl 0x821940c8
	ctx.lr = 0x8225F70C;
	sub_821940C8(ctx, base);
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r29.u32);
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82daa510
	ctx.lr = 0x8225F720;
	sub_82DAA510(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c28
	// ERROR 82CA2C28
	return;
}

__attribute__((alias("__imp__sub_8225F728"))) PPC_WEAK_FUNC(sub_8225F728);
PPC_FUNC_IMPL(__imp__sub_8225F728) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// addi r31,r30,128
	ctx.r31.s64 = ctx.r30.s64 + 128;
	// lwz r11,132(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 132);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f784
	if (ctx.cr6.eq) goto loc_8225F784;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225f810
	if (ctx.cr6.eq) goto loc_8225F810;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f784
	if (ctx.cr6.eq) goto loc_8225F784;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825575c8
	ctx.lr = 0x8225F770;
	sub_825575C8(ctx, base);
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8225f788
	if (!ctx.cr6.eq) goto loc_8225F788;
loc_8225F784:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8225F788:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f7f8
	if (ctx.cr6.eq) goto loc_8225F7F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825575c8
	ctx.lr = 0x8225F79C;
	sub_825575C8(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x8225F7B8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r31,r30,32
	ctx.r31.s64 = ctx.r30.s64 + 32;
	// lbz r7,617(r30)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r30.u32 + 617);
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// lvx128 v1,r0,r8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v1,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x8225f7f8
	if (ctx.cr6.eq) goto loc_8225F7F8;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,24
	ctx.r5.s64 = 24;
	// lfs f2,-17752(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -17752);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,2000(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2000);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82683428
	ctx.lr = 0x8225F7F4;
	sub_82683428(ctx, base);
	// stvx128 v1,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8225F7F8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8225F810:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821940c8
	ctx.lr = 0x8225F818;
	sub_821940C8(ctx, base);
	// b 0x8225f784
	goto loc_8225F784;
}

__attribute__((alias("__imp__sub_8225F81C"))) PPC_WEAK_FUNC(sub_8225F81C);
PPC_FUNC_IMPL(__imp__sub_8225F81C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8225F820"))) PPC_WEAK_FUNC(sub_8225F820);
PPC_FUNC_IMPL(__imp__sub_8225F820) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// lwz r9,8(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lwz r3,8(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r8,16
	ctx.r8.s64 = 16;
	// li r7,32
	ctx.r7.s64 = 32;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// ld r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r31,8(r11)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// ld r30,16(r11)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r11.u32 + 16);
	// vspltw v12,v0,1
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xAA));
	// ld r11,24(r11)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r11.u32 + 24);
	// vspltw v0,v0,2
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x55));
	// std r6,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r6.u64);
	// std r31,8(r10)
	PPC_STORE_U64(ctx.r10.u32 + 8, ctx.r31.u64);
	// std r30,16(r10)
	PPC_STORE_U64(ctx.r10.u32 + 16, ctx.r30.u64);
	// std r11,24(r10)
	PPC_STORE_U64(ctx.r10.u32 + 24, ctx.r11.u64);
	// lvx128 v11,r0,r9
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v13,v11,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v11,r9,r8
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r9,r7
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vmaddfp v13,v11,v12,v13
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v13.f32)));
	// vmaddfp v0,v10,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v13.f32)));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8225F8B0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225F8C8"))) PPC_WEAK_FUNC(sub_8225F8C8);
PPC_FUNC_IMPL(__imp__sub_8225F8C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r10,r11,3620
	ctx.r10.s64 = ctx.r11.s64 + 3620;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// rlwinm r9,r11,0,0,0
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x8225f920
	if (!ctx.cr6.eq) goto loc_8225F920;
	// clrlwi r10,r11,2
	ctx.r10.u64 = ctx.r11.u32 & 0x3FFFFFFF;
	// lwz r9,0(r13)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0);
	// rlwinm r11,r11,1,1,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0x7FFFFFFE;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// li r8,4
	ctx.r8.s64 = 4;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r6,22
	ctx.r6.s64 = 22;
	// rlwinm r5,r7,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// lwzx r3,r8,r9
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// bl 0x82d4eca8
	ctx.lr = 0x8225F920;
	sub_82D4ECA8(ctx, base);
loc_8225F920:
	// lis r11,-32253
	ctx.r11.s64 = -2113732608;
	// addi r10,r11,12976
	ctx.r10.s64 = ctx.r11.s64 + 12976;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225F940"))) PPC_WEAK_FUNC(sub_8225F940);
PPC_FUNC_IMPL(__imp__sub_8225F940) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// lis r9,768
	ctx.r9.s64 = 50331648;
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// li r8,0
	ctx.r8.s64 = 0;
	// rlwinm r7,r11,0,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// rlwinm r10,r10,0,6,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x3FFFFFC;
	// mr r9,r7
	ctx.r9.u64 = ctx.r7.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,10
	ctx.r5.s64 = 10;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x821f61d0
	ctx.lr = 0x8225F980;
	sub_821F61D0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225F990"))) PPC_WEAK_FUNC(sub_8225F990);
PPC_FUNC_IMPL(__imp__sub_8225F990) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x8225F998;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// li r11,-1
	ctx.r11.s64 = -1;
	// lwz r10,56(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 56);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225fa28
	if (ctx.cr6.eq) goto loc_8225FA28;
	// rotlwi r9,r10,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x8225f9c8
	if (!ctx.cr6.eq) goto loc_8225F9C8;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// b 0x8225f9cc
	goto loc_8225F9CC;
loc_8225F9C8:
	// lwz r9,72(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 72);
loc_8225F9CC:
	// cmpw cr6,r9,r4
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r4.s32, ctx.xer);
	// bne cr6,0x8225fa28
	if (!ctx.cr6.eq) goto loc_8225FA28;
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225f9f0
	if (ctx.cr6.eq) goto loc_8225F9F0;
	// lwz r11,40(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	// b 0x8225f9f4
	goto loc_8225F9F4;
loc_8225F9F0:
	// li r10,-1
	ctx.r10.s64 = -1;
loc_8225F9F4:
	// cmpwi cr6,r28,1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 1, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8225fa04
	if (ctx.cr6.gt) goto loc_8225FA04;
	// li r11,0
	ctx.r11.s64 = 0;
loc_8225FA04:
	// clrlwi r4,r11,24
	ctx.r4.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x8225fa18
	if (!ctx.cr6.eq) goto loc_8225FA18;
	// cmplwi cr6,r10,5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 5, ctx.xer);
	// blt cr6,0x8225fb14
	if (ctx.cr6.lt) goto loc_8225FB14;
loc_8225FA18:
	// addi r3,r29,-20
	ctx.r3.s64 = ctx.r29.s64 + -20;
	// bl 0x82a6e9c0
	ctx.lr = 0x8225FA20;
	sub_82A6E9C0(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_8225FA28:
	// lwz r9,48(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8225fb14
	if (ctx.cr6.eq) goto loc_8225FB14;
	// rotlwi r10,r9,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225fa44
	if (ctx.cr6.eq) goto loc_8225FA44;
	// lwz r11,72(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 72);
loc_8225FA44:
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// bne cr6,0x8225fb14
	if (!ctx.cr6.eq) goto loc_8225FB14;
	// lbz r11,109(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 109);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8225fb14
	if (!ctx.cr6.eq) goto loc_8225FB14;
	// lwz r11,8(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225fa74
	if (ctx.cr6.eq) goto loc_8225FA74;
	// lwz r11,40(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// lwz r10,24(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// subf r30,r10,r11
	ctx.r30.s64 = ctx.r11.s64 - ctx.r10.s64;
	// b 0x8225fa78
	goto loc_8225FA78;
loc_8225FA74:
	// li r30,-1
	ctx.r30.s64 = -1;
loc_8225FA78:
	// cmpwi cr6,r28,2
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 2, ctx.xer);
	// beq cr6,0x8225fa88
	if (ctx.cr6.eq) goto loc_8225FA88;
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// blt cr6,0x8225fb14
	if (ctx.cr6.lt) goto loc_8225FB14;
loc_8225FA88:
	// addi r11,r28,-2
	ctx.r11.s64 = ctx.r28.s64 + -2;
	// addi r31,r29,-20
	ctx.r31.s64 = ctx.r29.s64 + -20;
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rlwinm r4,r10,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// bl 0x82a6e0d8
	ctx.lr = 0x8225FAA0;
	sub_82A6E0D8(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x8225fb08
	if (!ctx.cr6.eq) goto loc_8225FB08;
	// lwz r11,48(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225fb14
	if (ctx.cr6.eq) goto loc_8225FB14;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x8225fb14
	if (ctx.cr6.eq) goto loc_8225FB14;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lbz r10,27760(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 27760);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225fae4
	if (ctx.cr6.eq) goto loc_8225FAE4;
	// lwz r11,-16(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + -16);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// ble cr6,0x8225fae4
	if (!ctx.cr6.gt) goto loc_8225FAE4;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x8225faf4
	goto loc_8225FAF4;
loc_8225FAE4:
	// cmpwi cr6,r30,30
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 30, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// blt cr6,0x8225faf4
	if (ctx.cr6.lt) goto loc_8225FAF4;
	// li r11,1
	ctx.r11.s64 = 1;
loc_8225FAF4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225fb14
	if (ctx.cr6.eq) goto loc_8225FB14;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82a6dde0
	ctx.lr = 0x8225FB08;
	sub_82A6DDE0(ctx, base);
loc_8225FB08:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_8225FB14:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_8225FB20"))) PPC_WEAK_FUNC(sub_8225FB20);
PPC_FUNC_IMPL(__imp__sub_8225FB20) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x8225fb6c
	if (ctx.cr6.lt) goto loc_8225FB6C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82200f30
	ctx.lr = 0x8225FB50;
	sub_82200F30(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x8225fb6c
	if (ctx.cr6.eq) goto loc_8225FB6C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x822010e0
	ctx.lr = 0x8225FB68;
	sub_822010E0(ctx, base);
	// b 0x8225fb74
	goto loc_8225FB74;
loc_8225FB6C:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r11.u32);
loc_8225FB74:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225FB90"))) PPC_WEAK_FUNC(sub_8225FB90);
PPC_FUNC_IMPL(__imp__sub_8225FB90) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd4
	ctx.lr = 0x8225FB98;
	sub_82CA2BD4(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lis r11,-32241
	ctx.r11.s64 = -2112946176;
	// li r30,0
	ctx.r30.s64 = 0;
	// addi r10,r11,7536
	ctx.r10.s64 = ctx.r11.s64 + 7536;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// stw r30,4(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4, ctx.r30.u32);
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// stw r10,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r10.u32);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r24,r8
	ctx.r24.u64 = ctx.r8.u64;
	// bl 0x821ee710
	ctx.lr = 0x8225FBD0;
	sub_821EE710(ctx, base);
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82a6c330
	ctx.lr = 0x8225FBE0;
	sub_82A6C330(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8225e300
	ctx.lr = 0x8225FBF0;
	sub_8225E300(ctx, base);
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// mulli r7,r26,112
	ctx.r7.s64 = ctx.r26.s64 * 112;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r23,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r23.u32);
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r30.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r30.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// addi r26,r11,-9848
	ctx.r26.s64 = ctx.r11.s64 + -9848;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwzx r7,r7,r26
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r26.u32);
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// bl 0x821fba48
	ctx.lr = 0x8225FC34;
	sub_821FBA48(ctx, base);
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// bl 0x8225fcd8
	ctx.lr = 0x8225FC40;
	sub_8225FCD8(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x821fbed8
	ctx.lr = 0x8225FC48;
	sub_821FBED8(ctx, base);
	// stw r23,4(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4, ctx.r23.u32);
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x821fc048
	ctx.lr = 0x8225FC54;
	sub_821FC048(ctx, base);
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x8225fccc
	if (ctx.cr6.eq) goto loc_8225FCCC;
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82a6c330
	ctx.lr = 0x8225FC68;
	sub_82A6C330(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x821ee710
	ctx.lr = 0x8225FC70;
	sub_821EE710(ctx, base);
	// mulli r11,r24,112
	ctx.r11.s64 = ctx.r24.s64 * 112;
	// stw r30,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r30.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r30.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwzx r7,r11,r26
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r26.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// bl 0x821fba48
	ctx.lr = 0x8225FCAC;
	sub_821FBA48(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// bl 0x8225fcd8
	ctx.lr = 0x8225FCB8;
	sub_8225FCD8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x821fbed8
	ctx.lr = 0x8225FCC0;
	sub_821FBED8(ctx, base);
	// stw r30,4(r27)
	PPC_STORE_U32(ctx.r27.u32 + 4, ctx.r30.u32);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x821fc048
	ctx.lr = 0x8225FCCC;
	sub_821FC048(ctx, base);
loc_8225FCCC:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
}

__attribute__((alias("__imp__sub_8225FCD8"))) PPC_WEAK_FUNC(sub_8225FCD8);
PPC_FUNC_IMPL(__imp__sub_8225FCD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x821faba0
	ctx.lr = 0x8225FCF8;
	sub_821FABA0(ctx, base);
	// cmpwi cr6,r3,6
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 6, ctx.xer);
	// bgt cr6,0x8225fd44
	if (ctx.cr6.gt) goto loc_8225FD44;
	// beq cr6,0x8225fd34
	if (ctx.cr6.eq) goto loc_8225FD34;
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x8225fd20
	if (ctx.cr6.eq) goto loc_8225FD20;
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// beq cr6,0x8225fd88
	if (ctx.cr6.eq) goto loc_8225FD88;
	// cmpwi cr6,r3,3
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 3, ctx.xer);
	// beq cr6,0x8225fd64
	if (ctx.cr6.eq) goto loc_8225FD64;
	// b 0x8225fd94
	goto loc_8225FD94;
loc_8225FD20:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// rlwinm r11,r10,0,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFC;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwimi r11,r10,0,30,31
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x3) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFFC);
	// b 0x8225fd90
	goto loc_8225FD90;
loc_8225FD34:
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r11,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r11.u32);
	// b 0x8225fd94
	goto loc_8225FD94;
loc_8225FD44:
	// cmpwi cr6,r3,7
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 7, ctx.xer);
	// beq cr6,0x8225fd88
	if (ctx.cr6.eq) goto loc_8225FD88;
	// cmpwi cr6,r3,8
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 8, ctx.xer);
	// beq cr6,0x8225fd88
	if (ctx.cr6.eq) goto loc_8225FD88;
	// cmpwi cr6,r3,16
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 16, ctx.xer);
	// ble cr6,0x8225fd94
	if (!ctx.cr6.gt) goto loc_8225FD94;
	// cmpwi cr6,r3,20
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 20, ctx.xer);
	// bgt cr6,0x8225fd94
	if (ctx.cr6.gt) goto loc_8225FD94;
loc_8225FD64:
	// lwz r11,48(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// li r5,0
	ctx.r5.s64 = 0;
	// rlwinm. r11,r11,0,0,19
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFF000;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8225fd78
	if (ctx.cr0.eq) goto loc_8225FD78;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
loc_8225FD78:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8225fdb0
	ctx.lr = 0x8225FD84;
	sub_8225FDB0(ctx, base);
	// b 0x8225fd94
	goto loc_8225FD94;
loc_8225FD88:
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
loc_8225FD90:
	// stw r11,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r11.u32);
loc_8225FD94:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225FDAC"))) PPC_WEAK_FUNC(sub_8225FDAC);
PPC_FUNC_IMPL(__imp__sub_8225FDAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8225FDB0"))) PPC_WEAK_FUNC(sub_8225FDB0);
PPC_FUNC_IMPL(__imp__sub_8225FDB0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// lwz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// rlwinm r11,r10,0,0,19
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFF000;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// rlwimi r11,r10,0,20,31
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFF) | (ctx.r11.u64 & 0xFFFFFFFFFFFFF000);
	// stw r11,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r11.u32);
	// bl 0x8225fe18
	ctx.lr = 0x8225FDE4;
	sub_8225FE18(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// ble cr6,0x8225fe00
	if (!ctx.cr6.gt) goto loc_8225FE00;
	// lwz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// rlwinm r11,r10,0,0,19
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFF000;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwimi r11,r10,0,20,31
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFF) | (ctx.r11.u64 & 0xFFFFFFFFFFFFF000);
	// stw r11,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r11.u32);
loc_8225FE00:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225FE18"))) PPC_WEAK_FUNC(sub_8225FE18);
PPC_FUNC_IMPL(__imp__sub_8225FE18) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,44(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// rlwinm r11,r11,26,28,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 26) & 0xF;
	// addi r3,r11,1
	ctx.r3.s64 = ctx.r11.s64 + 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8225FE28"))) PPC_WEAK_FUNC(sub_8225FE28);
PPC_FUNC_IMPL(__imp__sub_8225FE28) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd4
	ctx.lr = 0x8225FE30;
	sub_82CA2BD4(ctx, base);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// mr r24,r6
	ctx.r24.u64 = ctx.r6.u64;
	// li r27,0
	ctx.r27.s64 = 0;
	// mr r26,r27
	ctx.r26.u64 = ctx.r27.u64;
	// lwz r6,-8(r23)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r23.u32 + -8);
	// lwz r11,36(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	// rlwinm r10,r11,12,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8225ff48
	if (ctx.cr6.eq) goto loc_8225FF48;
	// lwz r11,140(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225fe84
	if (ctx.cr6.eq) goto loc_8225FE84;
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// lwz r11,72(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r26,r11
	ctx.r26.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8225ff4c
	goto loc_8225FF4C;
loc_8225FE84:
	// lwz r10,72(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 72);
	// lwz r5,76(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 76);
	// stw r27,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r27.u32);
	// subf r11,r10,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r10.s64;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8225fef0
	if (!ctx.cr0.gt) goto loc_8225FEF0;
loc_8225FEA0:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,20
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 20, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8225fec0
	if (ctx.cr6.lt) goto loc_8225FEC0;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
loc_8225FEC0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8225fedc
	if (ctx.cr6.eq) goto loc_8225FEDC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8225fee4
	goto loc_8225FEE4;
loc_8225FEDC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8225FEE4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8225fea0
	if (ctx.cr6.gt) goto loc_8225FEA0;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
loc_8225FEF0:
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x8225ff34
	if (ctx.cr6.eq) goto loc_8225FF34;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,20
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 20, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8225ff0c
	if (ctx.cr6.gt) goto loc_8225FF0C;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_8225FF0C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8225ff34
	if (!ctx.cr6.eq) goto loc_8225FF34;
	// ld r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// std r11,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r11.u64);
	// lwz r11,124(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r26,r11
	ctx.r26.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8225ff4c
	goto loc_8225FF4C;
loc_8225FF34:
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r26,r11
	ctx.r26.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8225ff4c
	goto loc_8225FF4C;
loc_8225FF48:
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_8225FF4C:
	// clrlwi r10,r11,24
	ctx.r10.u64 = ctx.r11.u32 & 0xFF;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// addi r25,r11,4328
	ctx.r25.s64 = ctx.r11.s64 + 4328;
	// beq cr6,0x822605e8
	if (ctx.cr6.eq) goto loc_822605E8;
	// lwz r10,36(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 36);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// rlwinm r9,r10,4,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82260058
	if (ctx.cr6.eq) goto loc_82260058;
	// lwz r11,140(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8225ff9c
	if (ctx.cr6.eq) goto loc_8225FF9C;
	// lbz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 28);
	// lwz r11,72(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8226005c
	goto loc_8226005C;
loc_8225FF9C:
	// lwz r10,72(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 72);
	// lwz r6,76(r6)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r6.u32 + 76);
	// stw r27,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r27.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82260008
	if (!ctx.cr0.gt) goto loc_82260008;
loc_8225FFB8:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,28
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 28, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8225ffd8
	if (ctx.cr6.lt) goto loc_8225FFD8;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
loc_8225FFD8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8225fff4
	if (ctx.cr6.eq) goto loc_8225FFF4;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8225fffc
	goto loc_8225FFFC;
loc_8225FFF4:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8225FFFC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8225ffb8
	if (ctx.cr6.gt) goto loc_8225FFB8;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
loc_82260008:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82260048
	if (ctx.cr6.eq) goto loc_82260048;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,28
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 28, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82260024
	if (ctx.cr6.gt) goto loc_82260024;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_82260024:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82260048
	if (!ctx.cr6.eq) goto loc_82260048;
	// ld r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r11.u64);
	// lwz r11,124(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x8226005c
	goto loc_8226005C;
loc_82260048:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x8226005c
	goto loc_8226005C;
loc_82260058:
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
loc_8226005C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82260214
	if (ctx.cr6.eq) goto loc_82260214;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82199690
	ctx.lr = 0x82260078;
	sub_82199690(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226020c
	if (ctx.cr6.eq) goto loc_8226020C;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x8226020c
	if (!ctx.cr6.gt) goto loc_8226020C;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// addi r31,r1,80
	ctx.r31.s64 = ctx.r1.s64 + 80;
	// bl 0x821d9ff8
	ctx.lr = 0x822600A4;
	sub_821D9FF8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// bl 0x821e2cc8
	ctx.lr = 0x822600B4;
	sub_821E2CC8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// bl 0x821e2cc8
	ctx.lr = 0x822600C4;
	sub_821E2CC8(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x822600CC;
	sub_82214F08(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82214f08
	ctx.lr = 0x822600D4;
	sub_82214F08(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r31,r11,4328
	ctx.r31.s64 = ctx.r11.s64 + 4328;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bl 0x8222cf18
	ctx.lr = 0x822600EC;
	sub_8222CF18(ctx, base);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82267168
	ctx.lr = 0x822600F8;
	sub_82267168(ctx, base);
	// addi r30,r23,-12
	ctx.r30.s64 = ctx.r23.s64 + -12;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x821d9258
	ctx.lr = 0x82260110;
	sub_821D9258(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82289530
	ctx.lr = 0x82260118;
	sub_82289530(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x82260120;
	sub_82214F08(ctx, base);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822601ec
	if (!ctx.cr6.eq) goto loc_822601EC;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x821e2cc8
	ctx.lr = 0x8226013C;
	sub_821E2CC8(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x8226014C;
	sub_8222CF18(ctx, base);
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82267168
	ctx.lr = 0x82260158;
	sub_82267168(ctx, base);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821d9258
	ctx.lr = 0x8226016C;
	sub_821D9258(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82c0b768
	ctx.lr = 0x82260178;
	sub_82C0B768(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x822ade08
	ctx.lr = 0x82260180;
	sub_822ADE08(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82289530
	ctx.lr = 0x82260188;
	sub_82289530(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x82260190;
	sub_82214F08(ctx, base);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822601e4
	if (!ctx.cr6.eq) goto loc_822601E4;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x822601AC;
	sub_8222CF18(ctx, base);
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82267168
	ctx.lr = 0x822601B8;
	sub_82267168(ctx, base);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821d9258
	ctx.lr = 0x822601CC;
	sub_821D9258(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x822ade08
	ctx.lr = 0x822601D4;
	sub_822ADE08(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82289530
	ctx.lr = 0x822601DC;
	sub_82289530(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x822601E4;
	sub_82214F08(ctx, base);
loc_822601E4:
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82214f08
	ctx.lr = 0x822601EC;
	sub_82214F08(ctx, base);
loc_822601EC:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822ade08
	ctx.lr = 0x822601F4;
	sub_822ADE08(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82214f08
	ctx.lr = 0x822601FC;
	sub_82214F08(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x82260204;
	sub_82214F08(ctx, base);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
loc_8226020C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x82260214;
	sub_82214F08(ctx, base);
loc_82260214:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821d9ff8
	ctx.lr = 0x82260224;
	sub_821D9FF8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// bl 0x821e2cc8
	ctx.lr = 0x82260234;
	sub_821E2CC8(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82214f08
	ctx.lr = 0x8226023C;
	sub_82214F08(ctx, base);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x8226024C;
	sub_8222CF18(ctx, base);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82267168
	ctx.lr = 0x82260258;
	sub_82267168(ctx, base);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r23,-12
	ctx.r4.s64 = ctx.r23.s64 + -12;
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x821d9258
	ctx.lr = 0x8226026C;
	sub_821D9258(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82289530
	ctx.lr = 0x82260274;
	sub_82289530(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x8226027C;
	sub_82214F08(ctx, base);
	// lwz r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822605b0
	if (!ctx.cr6.eq) goto loc_822605B0;
	// lwz r11,-8(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + -8);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// rlwinm r9,r10,28,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226038c
	if (ctx.cr6.eq) goto loc_8226038C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822602c8
	if (ctx.cr6.eq) goto loc_822602C8;
	// lbz r10,100(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 100);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82260390
	goto loc_82260390;
loc_822602C8:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r27,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r27.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82260334
	if (!ctx.cr0.gt) goto loc_82260334;
loc_822602E4:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,100
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 100, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82260304
	if (ctx.cr6.lt) goto loc_82260304;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
loc_82260304:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82260320
	if (ctx.cr6.eq) goto loc_82260320;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82260328
	goto loc_82260328;
loc_82260320:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82260328:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x822602e4
	if (ctx.cr6.gt) goto loc_822602E4;
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
loc_82260334:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82260378
	if (ctx.cr6.eq) goto loc_82260378;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,100
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 100, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82260350
	if (ctx.cr6.gt) goto loc_82260350;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_82260350:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82260378
	if (!ctx.cr6.eq) goto loc_82260378;
	// ld r11,104(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r11.u64);
	// lwz r11,108(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82260390
	goto loc_82260390;
loc_82260378:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82260390
	goto loc_82260390;
loc_8226038C:
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_82260390:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822605d8
	if (ctx.cr6.eq) goto loc_822605D8;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821d9f40
	ctx.lr = 0x822603A8;
	sub_821D9F40(ctx, base);
	// bl 0x821b2710
	ctx.lr = 0x822603AC;
	sub_821B2710(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r4,r11,5928
	ctx.r4.s64 = ctx.r11.s64 + 5928;
	// bl 0x82345978
	ctx.lr = 0x822603B8;
	sub_82345978(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r5,-1
	ctx.r5.s64 = -1;
	// bl 0x8222cf18
	ctx.lr = 0x822603C8;
	sub_8222CF18(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r30,r11,2928
	ctx.r30.s64 = ctx.r11.s64 + 2928;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260404
	if (ctx.cr6.eq) goto loc_82260404;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x82260404
	if (!ctx.cr6.gt) goto loc_82260404;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x821f0108
	ctx.lr = 0x822603F4;
	sub_821F0108(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x821da550
	ctx.lr = 0x82260400;
	sub_821DA550(ctx, base);
	// b 0x82260414
	goto loc_82260414;
loc_82260404:
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// addi r4,r11,23404
	ctx.r4.s64 = ctx.r11.s64 + 23404;
	// bl 0x821f0108
	ctx.lr = 0x82260414;
	sub_821F0108(ctx, base);
loc_82260414:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8226041C;
	sub_82214F08(ctx, base);
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e2cc8
	ctx.lr = 0x8226042C;
	sub_821E2CC8(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r31,r11,2924
	ctx.r31.s64 = ctx.r11.s64 + 2924;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bl 0x8222cf18
	ctx.lr = 0x82260444;
	sub_8222CF18(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x82260454;
	sub_8222CF18(ctx, base);
	// addi r5,r1,92
	ctx.r5.s64 = ctx.r1.s64 + 92;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e6408
	ctx.lr = 0x82260464;
	sub_821E6408(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x8226046C;
	sub_82214F08(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82214f08
	ctx.lr = 0x82260474;
	sub_82214F08(ctx, base);
	// lwz r29,84(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lis r28,-31927
	ctx.r28.s64 = -2092367872;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x822604c8
	if (ctx.cr6.eq) goto loc_822604C8;
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x822604bc
	if (!ctx.cr6.gt) goto loc_822604BC;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821c1fc0
	ctx.lr = 0x8226049C;
	sub_821C1FC0(ctx, base);
	// lwz r11,26912(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r3,84(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	// bl 0x8219a6f8
	ctx.lr = 0x822604B0;
	sub_8219A6F8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822604c8
	if (ctx.cr6.eq) goto loc_822604C8;
loc_822604BC:
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x82260544
	if (!ctx.cr6.eq) goto loc_82260544;
loc_822604C8:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x821d9ff8
	ctx.lr = 0x822604D8;
	sub_821D9FF8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// bl 0x821e2cc8
	ctx.lr = 0x822604E8;
	sub_821E2CC8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82265160
	ctx.lr = 0x822604F4;
	sub_82265160(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82214f08
	ctx.lr = 0x822604FC;
	sub_82214F08(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x82260504;
	sub_82214F08(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x8222cf18
	ctx.lr = 0x82260514;
	sub_8222CF18(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x82260524;
	sub_8222CF18(ctx, base);
	// addi r5,r1,92
	ctx.r5.s64 = ctx.r1.s64 + 92;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e6408
	ctx.lr = 0x82260534;
	sub_821E6408(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x8226053C;
	sub_82214F08(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82214f08
	ctx.lr = 0x82260544;
	sub_82214F08(ctx, base);
loc_82260544:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821c1fc0
	ctx.lr = 0x82260550;
	sub_821C1FC0(ctx, base);
	// lwz r11,26912(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r3,84(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	// bl 0x8219a6f8
	ctx.lr = 0x82260564;
	sub_8219A6F8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bne cr6,0x822605c8
	if (!ctx.cr6.eq) goto loc_822605C8;
	// li r10,-1
	ctx.r10.s64 = -1;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,2
	ctx.r5.s64 = 2;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x82448530
	ctx.lr = 0x82260594;
	sub_82448530(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x822605c8
	if (!ctx.cr6.eq) goto loc_822605C8;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x822605A8;
	sub_82214F08(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82214f08
	ctx.lr = 0x822605B0;
	sub_82214F08(ctx, base);
loc_822605B0:
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x822ade08
	ctx.lr = 0x822605B8;
	sub_822ADE08(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82214f08
	ctx.lr = 0x822605C0;
	sub_82214F08(ctx, base);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
loc_822605C8:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x822605D0;
	sub_82214F08(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82214f08
	ctx.lr = 0x822605D8;
	sub_82214F08(ctx, base);
loc_822605D8:
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x822ade08
	ctx.lr = 0x822605E0;
	sub_822ADE08(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82214f08
	ctx.lr = 0x822605E8;
	sub_82214F08(ctx, base);
loc_822605E8:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x822605F8;
	sub_8222CF18(ctx, base);
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82267168
	ctx.lr = 0x82260604;
	sub_82267168(ctx, base);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r23,-12
	ctx.r4.s64 = ctx.r23.s64 + -12;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821d9258
	ctx.lr = 0x82260618;
	sub_821D9258(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x822ade08
	ctx.lr = 0x82260620;
	sub_822ADE08(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82289530
	ctx.lr = 0x82260628;
	sub_82289530(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x82260630;
	sub_82214F08(ctx, base);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
}

__attribute__((alias("__imp__sub_82260638"))) PPC_WEAK_FUNC(sub_82260638);
PPC_FUNC_IMPL(__imp__sub_82260638) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31949
	ctx.r11.s64 = -2093809664;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lbz r10,18960(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 18960);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82260694
	if (ctx.cr6.eq) goto loc_82260694;
	// lis r11,-31949
	ctx.r11.s64 = -2093809664;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r31,r11,18968
	ctx.r31.s64 = ctx.r11.s64 + 18968;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8223e958
	ctx.lr = 0x82260674;
	sub_8223E958(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82260688
	if (ctx.cr6.eq) goto loc_82260688;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r10,r11,4088
	ctx.r10.s64 = ctx.r11.s64 + 4088;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
loc_82260688:
	// stw r30,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r30.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8223e820
	ctx.lr = 0x82260694;
	sub_8223E820(ctx, base);
loc_82260694:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822606AC"))) PPC_WEAK_FUNC(sub_822606AC);
PPC_FUNC_IMPL(__imp__sub_822606AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822606B0"))) PPC_WEAK_FUNC(sub_822606B0);
PPC_FUNC_IMPL(__imp__sub_822606B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x822606B8;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r5,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r5.u64);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// lwz r11,160(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// std r5,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r5.u64);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822606dc
	if (!ctx.cr6.eq) goto loc_822606DC;
	// twi 31,r0,22
loc_822606DC:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r11,164(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x822606f0
	if (!ctx.cr6.eq) goto loc_822606F0;
	// twi 31,r0,22
loc_822606F0:
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r31,84(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmplw cr6,r31,r10
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r10.u32, ctx.xer);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r11.u32);
	// beq cr6,0x8226077c
	if (ctx.cr6.eq) goto loc_8226077C;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r30,r31,8
	ctx.r30.s64 = ctx.r31.s64 + 8;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r3,r30,8
	ctx.r3.s64 = ctx.r30.s64 + 8;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// bl 0x82214f08
	ctx.lr = 0x8226072C;
	sub_82214F08(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82260748
	if (ctx.cr6.eq) goto loc_82260748;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82260748;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82260748:
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,4(r30)
	PPC_STORE_U32(ctx.r30.u32 + 4, ctx.r11.u32);
	// stw r11,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r11.u32);
	// bl 0x8221be68
	ctx.lr = 0x8226075C;
	sub_8221BE68(ctx, base);
	// lwz r11,8(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// ld r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r9,r11,-1
	ctx.r9.s64 = ctx.r11.s64 + -1;
	// stw r9,8(r29)
	PPC_STORE_U32(ctx.r29.u32 + 8, ctx.r9.u32);
	// std r10,0(r28)
	PPC_STORE_U64(ctx.r28.u32 + 0, ctx.r10.u64);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_8226077C:
	// ld r11,160(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// std r11,0(r28)
	PPC_STORE_U64(ctx.r28.u32 + 0, ctx.r11.u64);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82260790"))) PPC_WEAK_FUNC(sub_82260790);
PPC_FUNC_IMPL(__imp__sub_82260790) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r10,r11,9956
	ctx.r10.s64 = ctx.r11.s64 + 9956;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// bl 0x82260638
	ctx.lr = 0x822607C0;
	sub_82260638(ctx, base);
	// lis r9,-32255
	ctx.r9.s64 = -2113863680;
	// lis r8,-32255
	ctx.r8.s64 = -2113863680;
	// addi r7,r9,-23676
	ctx.r7.s64 = ctx.r9.s64 + -23676;
	// addi r6,r8,-23664
	ctx.r6.s64 = ctx.r8.s64 + -23664;
	// clrlwi r5,r30,31
	ctx.r5.u64 = ctx.r30.u32 & 0x1;
	// stw r7,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r7.u32);
	// stw r6,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r6.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822607f0
	if (ctx.cr6.eq) goto loc_822607F0;
	// bl 0x824fe010
	ctx.lr = 0x822607EC;
	sub_824FE010(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
loc_822607F0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260808"))) PPC_WEAK_FUNC(sub_82260808);
PPC_FUNC_IMPL(__imp__sub_82260808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// vmsum3fp128 v0,v2,v2
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_dp_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v2.f32), 0xEF));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v13,v1,v1
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v1.f32), 0xEF));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fsqrts f13,f0
	ctx.f13.f64 = double(float(sqrt(ctx.f0.f64)));
	// stvx128 v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r11,-8700
	ctx.r10.s64 = ctx.r11.s64 + -8700;
	// lfs f12,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f12.f64 = double(temp.f32);
	// fsqrts f11,f12
	ctx.f11.f64 = double(float(sqrt(ctx.f12.f64)));
	// fmuls f12,f11,f13
	ctx.f12.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// lfs f0,-25888(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25888);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f12,f0
	ctx.cr6.compare(ctx.f12.f64, ctx.f0.f64);
	// ble cr6,0x822608ec
	if (!ctx.cr6.gt) goto loc_822608EC;
	// vmsum3fp128 v0,v1,v2
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_dp_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v2.f32), 0xEF));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f13,-18756(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -18756);
	ctx.f13.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// stvx128 v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f11,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f11.f64 = double(temp.f32);
	// fdivs f1,f11,f12
	ctx.f1.f64 = double(float(ctx.f11.f64 / ctx.f12.f64));
	// fsubs f10,f1,f13
	ctx.f10.f64 = double(float(ctx.f1.f64 - ctx.f13.f64));
	// fabs f9,f10
	ctx.f9.u64 = ctx.f10.u64 & ~0x8000000000000000;
	// fcmpu cr6,f9,f0
	ctx.cr6.compare(ctx.f9.f64, ctx.f0.f64);
	// blt cr6,0x82260888
	if (ctx.cr6.lt) goto loc_82260888;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82260888:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822608ec
	if (!ctx.cr6.eq) goto loc_822608EC;
	// lfs f13,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// fsubs f13,f1,f13
	ctx.f13.f64 = double(float(ctx.f1.f64 - ctx.f13.f64));
	// fabs f12,f13
	ctx.f12.u64 = ctx.f13.u64 & ~0x8000000000000000;
	// fcmpu cr6,f12,f0
	ctx.cr6.compare(ctx.f12.f64, ctx.f0.f64);
	// blt cr6,0x822608b0
	if (ctx.cr6.lt) goto loc_822608B0;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822608B0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822608d4
	if (ctx.cr6.eq) goto loc_822608D4;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f1,-28512(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -28512);
	ctx.f1.f64 = double(temp.f32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822608D4:
	// bl 0x82260900
	ctx.lr = 0x822608D8;
	sub_82260900(ctx, base);
	// frsp f1,f1
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f1.f64));
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_822608EC:
	// lfs f1,-18768(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -18768);
	ctx.f1.f64 = double(temp.f32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260900"))) PPC_WEAK_FUNC(sub_82260900);
PPC_FUNC_IMPL(__imp__sub_82260900) {
	PPC_FUNC_PROLOGUE();
	// li r4,1
	ctx.r4.s64 = 1;
	// b 0x82ca3fc8
	sub_82CA3FC8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82260908"))) PPC_WEAK_FUNC(sub_82260908);
PPC_FUNC_IMPL(__imp__sub_82260908) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82260910;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stb r11,456(r31)
	PPC_STORE_U8(ctx.r31.u32 + 456, ctx.r11.u8);
	// lwz r30,384(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// lwz r29,20(r10)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// bl 0x82260978
	ctx.lr = 0x82260930;
	sub_82260978(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// bl 0x82260ef0
	ctx.lr = 0x8226093C;
	sub_82260EF0(ctx, base);
	// bl 0x82260978
	ctx.lr = 0x82260940;
	sub_82260978(ctx, base);
	// lfs f0,196(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 196);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,84(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 84);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// beq cr6,0x82260970
	if (ctx.cr6.eq) goto loc_82260970;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r11,84(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 84);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82260964;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// bl 0x82260978
	ctx.lr = 0x82260968;
	sub_82260978(ctx, base);
	// lfs f0,76(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,196(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 196, temp.u32);
loc_82260970:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82260978"))) PPC_WEAK_FUNC(sub_82260978);
PPC_FUNC_IMPL(__imp__sub_82260978) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// lwz r11,-720(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -720);
	// lwz r3,52(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260988"))) PPC_WEAK_FUNC(sub_82260988);
PPC_FUNC_IMPL(__imp__sub_82260988) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82260990;
	sub_82CA2BE8(ctx, base);
	// stfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// fmr f30,f1
	ctx.f30.f64 = ctx.f1.f64;
	// li r28,0
	ctx.r28.s64 = 0;
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x82260ac4
	if (!ctx.cr6.gt) goto loc_82260AC4;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// addi r31,r29,96
	ctx.r31.s64 = ctx.r29.s64 + 96;
	// addi r30,r11,368
	ctx.r30.s64 = ctx.r11.s64 + 368;
	// lfs f31,-27468(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -27468);
	ctx.f31.f64 = double(temp.f32);
loc_822609C8:
	// lwz r11,-80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + -80);
	// lfs f0,556(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 556);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,-76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + -76);
	// lfs f13,24(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f30
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f30.f64));
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fsubs f7,f12,f0
	ctx.f7.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// bl 0x822d3d80
	ctx.lr = 0x822609E8;
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f31
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f7.f64, ctx.f31.f64);
	// mfcr r10
	ctx.r10.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r10.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r10.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r10.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r10.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r10.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r10.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r10.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r10.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r10.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r10.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r10.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r10.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r10.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r10.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r10.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r10.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r10.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r10.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r10.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r10.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r10.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r10.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r10.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r10.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r10.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r10.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r10.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r10.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r10.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r10.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r10.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r9,r10,27,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x4;
	// lwz r3,-76(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + -76);
	// rlwinm r8,r10,30,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x4;
	// or r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 | ctx.r8.u64;
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfsx f6,r30,r7
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r7.u32);
	ctx.f6.f64 = double(temp.f32);
	// fsel f5,f6,f7,f31
	ctx.f5.f64 = ctx.f6.f64 >= 0.0 ? ctx.f7.f64 : ctx.f31.f64;
	// lwz r5,20(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	// fsubs f4,f5,f1
	ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f1.f64));
	// fcmpu cr6,f4,f31
	ctx.cr6.compare(ctx.f4.f64, ctx.f31.f64);
	// mfcr r4
	ctx.r4.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r4.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r4.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r4.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r4.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r4.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r4.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r4.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r4.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r4.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r4.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r4.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r4.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r4.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r4.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r4.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r4.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r4.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r4.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r4.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r4.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r4.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r4.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r4.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r4.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r4.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r4.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r4.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r4.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r4.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r4.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r4.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r11,r4,27,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x4;
	// rlwinm r10,r4,30,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x4;
	// or r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 | ctx.r10.u64;
	// lfsx f3,r30,r9
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r9.u32);
	ctx.f3.f64 = double(temp.f32);
	// fsel f1,f3,f1,f5
	ctx.f1.f64 = ctx.f3.f64 >= 0.0 ? ctx.f1.f64 : ctx.f5.f64;
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// bctrl 
	ctx.lr = 0x82260A38;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260ab0
	if (ctx.cr6.eq) goto loc_82260AB0;
	// lfs f0,24(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// fadds f13,f0,f30
	ctx.f13.f64 = double(float(ctx.f0.f64 + ctx.f30.f64));
	// lfs f12,556(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 556);
	ctx.f12.f64 = double(temp.f32);
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fsubs f7,f13,f12
	ctx.f7.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// bl 0x822d3d80
	ctx.lr = 0x82260A60;
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f31
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f7.f64, ctx.f31.f64);
	// mfcr r11
	ctx.r11.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r11.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r11.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r11.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r11.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r11.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r11.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r11.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r11.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r11.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r11.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r11.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r11.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r11.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r11.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r11.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r11.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r11.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r11.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r11.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r11.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r11.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r11.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r11.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r11.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r11.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r11.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r11.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r11.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r11.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r11.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r11.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r10,r11,27,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x4;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// rlwinm r9,r11,30,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x4;
	// or r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 | ctx.r9.u64;
	// lwz r7,0(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfsx f6,r30,r8
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r8.u32);
	ctx.f6.f64 = double(temp.f32);
	// fsel f5,f6,f7,f31
	ctx.f5.f64 = ctx.f6.f64 >= 0.0 ? ctx.f7.f64 : ctx.f31.f64;
	// lwz r6,20(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// fsubs f4,f5,f1
	ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f1.f64));
	// fcmpu cr6,f4,f31
	ctx.cr6.compare(ctx.f4.f64, ctx.f31.f64);
	// mfcr r5
	ctx.r5.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r5.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r5.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r5.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r5.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r5.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r5.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r5.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r5.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r5.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r5.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r5.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r5.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r5.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r5.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r5.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r5.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r5.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r5.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r5.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r5.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r5.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r5.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r5.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r5.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r5.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r5.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r5.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r5.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r5.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r5.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r5.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r4,r5,27,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x4;
	// rlwinm r11,r5,30,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x4;
	// or r10,r4,r11
	ctx.r10.u64 = ctx.r4.u64 | ctx.r11.u64;
	// lfsx f3,r30,r10
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r10.u32);
	ctx.f3.f64 = double(temp.f32);
	// fsel f1,f3,f1,f5
	ctx.f1.f64 = ctx.f3.f64 >= 0.0 ? ctx.f1.f64 : ctx.f5.f64;
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x82260AB0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82260AB0:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r31,r31,176
	ctx.r31.s64 = ctx.r31.s64 + 176;
	// cmpw cr6,r28,r11
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822609c8
	if (ctx.cr6.lt) goto loc_822609C8;
loc_82260AC4:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82260AD4"))) PPC_WEAK_FUNC(sub_82260AD4);
PPC_FUNC_IMPL(__imp__sub_82260AD4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82260AD8"))) PPC_WEAK_FUNC(sub_82260AD8);
PPC_FUNC_IMPL(__imp__sub_82260AD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f1,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// b 0x822d3d80
	sub_822D3D80(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82260AE0"))) PPC_WEAK_FUNC(sub_82260AE0);
PPC_FUNC_IMPL(__imp__sub_82260AE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r7,48
	ctx.r7.s64 = 48;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// subf r11,r9,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r9.s64;
	// divw r11,r11,r7
	ctx.r11.s32 = ctx.r11.s32 / ctx.r7.s32;
	// cmplw cr6,r4,r11
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x82260b34
	if (!ctx.cr6.gt) goto loc_82260B34;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// subf r5,r11,r4
	ctx.r5.s64 = ctx.r4.s64 - ctx.r11.s64;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x82a77500
	ctx.lr = 0x82260B24;
	sub_82A77500(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_82260B34:
	// bge cr6,0x82260bb4
	if (!ctx.cr6.lt) goto loc_82260BB4;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r8,r11,r9
	ctx.r8.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmplw cr6,r8,r10
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x82260bb4
	if (ctx.cr6.eq) goto loc_82260BB4;
	// subf r11,r10,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r10.s64;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// divw r11,r11,r7
	ctx.r11.s32 = ctx.r11.s32 / ctx.r7.s32;
	// cmplw cr6,r10,r10
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r10.u32, ctx.xer);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rlwinm r11,r7,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r11,r8
	ctx.r4.u64 = ctx.r11.u64 + ctx.r8.u64;
	// beq cr6,0x82260bb0
	if (ctx.cr6.eq) goto loc_82260BB0;
	// addi r11,r8,32
	ctx.r11.s64 = ctx.r8.s64 + 32;
	// subf r8,r8,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r8.s64;
	// li r5,-32
	ctx.r5.s64 = -32;
	// li r6,16
	ctx.r6.s64 = 16;
	// li r7,-16
	ctx.r7.s64 = -16;
loc_82260B88:
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r11,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r9,r6
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r9,48
	ctx.r9.s64 = ctx.r9.s64 + 48;
	// stvx128 v13,r11,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r8,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplw cr6,r9,r10
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, ctx.xer);
	// stvx128 v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,48
	ctx.r11.s64 = ctx.r11.s64 + 48;
	// bne cr6,0x82260b88
	if (!ctx.cr6.eq) goto loc_82260B88;
loc_82260BB0:
	// stw r4,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r4.u32);
loc_82260BB4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260BC4"))) PPC_WEAK_FUNC(sub_82260BC4);
PPC_FUNC_IMPL(__imp__sub_82260BC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82260BC8"))) PPC_WEAK_FUNC(sub_82260BC8);
PPC_FUNC_IMPL(__imp__sub_82260BC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fsqrts f0,f0
	ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
	// fcmpu cr6,f0,f1
	ctx.cr6.compare(ctx.f0.f64, ctx.f1.f64);
	// blt cr6,0x82260c18
	if (ctx.cr6.lt) goto loc_82260C18;
	// addi r9,r11,-27468
	ctx.r9.s64 = ctx.r11.s64 + -27468;
	// fmr f1,f0
	ctx.f1.f64 = ctx.f0.f64;
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// lfs f13,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v13,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v11,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32)));
	// stvx128 v11,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
loc_82260C18:
	// vspltisw v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x0)));
	// lfs f1,-27468(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27468);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260C28"))) PPC_WEAK_FUNC(sub_82260C28);
PPC_FUNC_IMPL(__imp__sub_82260C28) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82260C30;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x82260c88
	if (!ctx.cr6.gt) goto loc_82260C88;
	// li r31,0
	ctx.r31.s64 = 0;
loc_82260C4C:
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// lwzx r11,r11,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260c74
	if (ctx.cr6.eq) goto loc_82260C74;
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// lwzx r3,r11,r31
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82260C74;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82260C74:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x82260c4c
	if (ctx.cr6.lt) goto loc_82260C4C;
loc_82260C88:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82260C90"))) PPC_WEAK_FUNC(sub_82260C90);
PPC_FUNC_IMPL(__imp__sub_82260C90) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lbz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 32);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne 0x82260cd0
	if (!ctx.cr0.eq) goto loc_82260CD0;
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260cd0
	if (ctx.cr6.eq) goto loc_82260CD0;
	// lwz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260cd0
	if (ctx.cr6.eq) goto loc_82260CD0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82e699d8
	ctx.lr = 0x82260CD0;
	sub_82E699D8(ctx, base);
loc_82260CD0:
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r11,32(r31)
	PPC_STORE_U8(ctx.r31.u32 + 32, ctx.r11.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260CEC"))) PPC_WEAK_FUNC(sub_82260CEC);
PPC_FUNC_IMPL(__imp__sub_82260CEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82260CF0"))) PPC_WEAK_FUNC(sub_82260CF0);
PPC_FUNC_IMPL(__imp__sub_82260CF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// rlwinm r8,r9,31,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x1;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x82260e08
	if (ctx.cr6.eq) goto loc_82260E08;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82260d40
	if (ctx.cr6.eq) goto loc_82260D40;
	// lbz r10,97(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 97);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82260e0c
	goto loc_82260E0C;
loc_82260D40:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// subf r8,r10,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// srawi. r11,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82260db0
	if (!ctx.cr0.gt) goto loc_82260DB0;
loc_82260D60:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,97
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 97, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82260d80
	if (ctx.cr6.lt) goto loc_82260D80;
	// li r7,0
	ctx.r7.s64 = 0;
loc_82260D80:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82260d9c
	if (ctx.cr6.eq) goto loc_82260D9C;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82260da4
	goto loc_82260DA4;
loc_82260D9C:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82260DA4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82260d60
	if (ctx.cr6.gt) goto loc_82260D60;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_82260DB0:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82260df4
	if (ctx.cr6.eq) goto loc_82260DF4;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,97
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 97, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82260dcc
	if (ctx.cr6.gt) goto loc_82260DCC;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82260DCC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82260df4
	if (!ctx.cr6.eq) goto loc_82260DF4;
	// ld r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82260e0c
	goto loc_82260E0C;
loc_82260DF4:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82260e0c
	goto loc_82260E0C;
loc_82260E08:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82260E0C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260edc
	if (ctx.cr6.eq) goto loc_82260EDC;
	// lwz r9,180(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// vspltisw v3,0
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_set1_epi32(int(0x0)));
	// cmpwi cr6,r9,4
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 4, ctx.xer);
	// bne cr6,0x82260e30
	if (!ctx.cr6.eq) goto loc_82260E30;
	// li r4,5
	ctx.r4.s64 = 5;
	// b 0x82260ec0
	goto loc_82260EC0;
loc_82260E30:
	// cmpwi cr6,r9,5
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 5, ctx.xer);
	// beq cr6,0x82260edc
	if (ctx.cr6.eq) goto loc_82260EDC;
	// lbz r11,20(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260e54
	if (ctx.cr6.eq) goto loc_82260E54;
	// li r8,1184
	ctx.r8.s64 = 1184;
	// li r11,1
	ctx.r11.s64 = 1;
	// lvx128 v3,r10,r8
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82260e58
	goto loc_82260E58;
loc_82260E54:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82260E58:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260e8c
	if (ctx.cr6.eq) goto loc_82260E8C;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// vor v1,v3,v3
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v3.u8));
	// li r4,2
	ctx.r4.s64 = 2;
	// addi r10,r11,-21136
	ctx.r10.s64 = ctx.r11.s64 + -21136;
	// lvx128 v2,r0,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82675ea8
	ctx.lr = 0x82260E7C;
	sub_82675EA8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_82260E8C:
	// cmpwi cr6,r9,2
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 2, ctx.xer);
	// bne cr6,0x82260ea8
	if (!ctx.cr6.eq) goto loc_82260EA8;
	// bl 0x82192268
	ctx.lr = 0x82260E98;
	sub_82192268(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_82260EA8:
	// lbz r11,184(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 184);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82260edc
	if (ctx.cr6.eq) goto loc_82260EDC;
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x82260edc
	if (!ctx.cr6.eq) goto loc_82260EDC;
	// li r4,3
	ctx.r4.s64 = 3;
loc_82260EC0:
	// li r11,560
	ctx.r11.s64 = 560;
	// li r10,544
	ctx.r10.s64 = 544;
	// li r9,528
	ctx.r9.s64 = 528;
	// lvx128 v3,r3,r11
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v2,r3,r10
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v1,r3,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82675f20
	ctx.lr = 0x82260EDC;
	sub_82675F20(ctx, base);
loc_82260EDC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260EEC"))) PPC_WEAK_FUNC(sub_82260EEC);
PPC_FUNC_IMPL(__imp__sub_82260EEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82260EF0"))) PPC_WEAK_FUNC(sub_82260EF0);
PPC_FUNC_IMPL(__imp__sub_82260EF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lfs f13,40(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,3084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3084);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// ble cr6,0x82260f84
	if (!ctx.cr6.gt) goto loc_82260F84;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x821c4358
	ctx.lr = 0x82260F28;
	sub_821C4358(ctx, base);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x82260f84
	if (!ctx.cr6.lt) goto loc_82260F84;
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// lwz r11,-720(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -720);
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x82260f84
	if (!ctx.cr6.lt) goto loc_82260F84;
	// cmpwi cr6,r3,-1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, -1, ctx.xer);
	// beq cr6,0x82260f84
	if (ctx.cr6.eq) goto loc_82260F84;
	// lwz r10,64(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	// mulli r11,r3,28
	ctx.r11.s64 = ctx.r3.s64 * 28;
	// add. r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82260f84
	if (ctx.cr0.eq) goto loc_82260F84;
	// lwz r9,24(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lfs f0,76(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	ctx.f0.f64 = double(temp.f32);
	// stfsx f0,r9,r10
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, temp.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lfs f0,8(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfsx f13,r9,r10
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// stfs f0,8(r11)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r11.u32 + 8, temp.u32);
loc_82260F84:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82260F9C"))) PPC_WEAK_FUNC(sub_82260F9C);
PPC_FUNC_IMPL(__imp__sub_82260F9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82260FA0"))) PPC_WEAK_FUNC(sub_82260FA0);
PPC_FUNC_IMPL(__imp__sub_82260FA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82260FA8;
	sub_82CA2BE8(ctx, base);
	// li r10,48
	ctx.r10.s64 = 48;
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r8,r1,-208
	ctx.r8.s64 = ctx.r1.s64 + -208;
	// li r11,32
	ctx.r11.s64 = 32;
	// addi r6,r1,-208
	ctx.r6.s64 = ctx.r1.s64 + -208;
	// lvx128 v12,r4,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-192
	ctx.r5.s64 = ctx.r1.s64 + -192;
	// vmsum3fp128 v10,v12,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// lvx128 v9,r4,r7
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v8,v12,v9
	_mm_store_ps(ctx.v8.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v9.f32), 0xEF));
	// addi r31,r1,-192
	ctx.r31.s64 = ctx.r1.s64 + -192;
	// lvx128 v13,r4,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-160
	ctx.r11.s64 = ctx.r1.s64 + -160;
	// addi r7,r1,-160
	ctx.r7.s64 = ctx.r1.s64 + -160;
	// vmsum3fp128 v11,v12,v13
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r4,r1,-176
	ctx.r4.s64 = ctx.r1.s64 + -176;
	// stvx128 v9,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stvx128 v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-224
	ctx.r5.s64 = ctx.r1.s64 + -224;
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// addi r9,r1,-128
	ctx.r9.s64 = ctx.r1.s64 + -128;
	// addi r4,r4,-28224
	ctx.r4.s64 = ctx.r4.s64 + -28224;
	// lfs f0,-27456(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -27456);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r1,-176
	ctx.r30.s64 = ctx.r1.s64 + -176;
	// stfs f0,-224(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -224, temp.u32);
	// addi r29,r1,-112
	ctx.r29.s64 = ctx.r1.s64 + -112;
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v6,v7,0
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v7.u32), 0xFF));
	// stvx128 v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,-96
	ctx.r28.s64 = ctx.r1.s64 + -96;
	// lfs f13,-208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	ctx.f13.f64 = double(temp.f32);
	// fneg f10,f13
	ctx.f10.u64 = ctx.f13.u64 ^ 0x8000000000000000;
	// stvx128 v8,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,-80
	ctx.r5.s64 = ctx.r1.s64 + -80;
	// stvx128 v11,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,-64
	ctx.r8.s64 = ctx.r1.s64 + -64;
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,-112
	ctx.r11.s64 = ctx.r1.s64 + -112;
	// lfs f0,-128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -128);
	ctx.f0.f64 = double(temp.f32);
	// fneg f9,f0
	ctx.f9.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// lfs f12,-208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -208);
	ctx.f12.f64 = double(temp.f32);
	// fneg f11,f12
	ctx.f11.u64 = ctx.f12.u64 ^ 0x8000000000000000;
	// stfs f10,-180(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + -180, temp.u32);
	// vand v5,v6,v0
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stfs f11,-164(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + -164, temp.u32);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// stfs f9,-148(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + -148, temp.u32);
	// li r9,8
	ctx.r9.s64 = 8;
	// lvx128 v0,r0,r30
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmrghw v3,v13,v12
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), _mm_load_si128((__m128i*)ctx.v13.u32)));
	// vmrghw v4,v0,v5
	_mm_store_si128((__m128i*)ctx.v4.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v5.u32), _mm_load_si128((__m128i*)ctx.v0.u32)));
	// vmrglw v2,v13,v12
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), _mm_load_si128((__m128i*)ctx.v13.u32)));
	// vmrglw v1,v0,v5
	_mm_store_si128((__m128i*)ctx.v1.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v5.u32), _mm_load_si128((__m128i*)ctx.v0.u32)));
	// vmrghw v31,v3,v4
	_mm_store_si128((__m128i*)ctx.v31.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v4.u32), _mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrglw v30,v3,v4
	_mm_store_si128((__m128i*)ctx.v30.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v4.u32), _mm_load_si128((__m128i*)ctx.v3.u32)));
	// vmrghw v29,v2,v1
	_mm_store_si128((__m128i*)ctx.v29.u32, _mm_unpackhi_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// vmrglw v28,v2,v1
	_mm_store_si128((__m128i*)ctx.v28.u32, _mm_unpacklo_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), _mm_load_si128((__m128i*)ctx.v2.u32)));
	// stvx128 v31,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v30,r0,r28
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v30.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v29,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v28,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822610B4:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822610b4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822610B4;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_822610CC"))) PPC_WEAK_FUNC(sub_822610CC);
PPC_FUNC_IMPL(__imp__sub_822610CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822610D0"))) PPC_WEAK_FUNC(sub_822610D0);
PPC_FUNC_IMPL(__imp__sub_822610D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd4
	ctx.lr = 0x822610D8;
	sub_82CA2BD4(ctx, base);
	// stfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f30.u64);
	// stfd f31,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f31.u64);
	// li r12,-128
	ctx.r12.s64 = -128;
	// stvx128 v126,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-112
	ctx.r12.s64 = -112;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r30,r11,-27468
	ctx.r30.s64 = ctx.r11.s64 + -27468;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r25,r11,-28224
	ctx.r25.s64 = ctx.r11.s64 + -28224;
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// lfs f30,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// stfs f30,96(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lis r6,-31950
	ctx.r6.s64 = -2093875200;
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvx128 v12,r0,r25
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lbz r9,33(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 33);
	// vand128 v127,v13,v12
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lwz r26,-27380(r6)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r6.u32 + -27380);
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// li r27,4
	ctx.r27.s64 = 4;
	// li r28,8
	ctx.r28.s64 = 8;
	// stvx128 v127,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82261308
	if (ctx.cr6.eq) goto loc_82261308;
	// lwz r5,4(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4);
	// li r11,48
	ctx.r11.s64 = 48;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 8);
	// subf r8,r5,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r5.s64;
	// divw. r7,r8,r11
	ctx.r7.s32 = ctx.r8.s32 / ctx.r11.s32;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne 0x82261170
	if (!ctx.cr0.eq) goto loc_82261170;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
loc_82261170:
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r4,88(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// bl 0x821f5980
	ctx.lr = 0x82261184;
	sub_821F5980(ctx, base);
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// stfs f30,176(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// addi r11,r31,68
	ctx.r11.s64 = ctx.r31.s64 + 68;
	// lfs f31,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stfs f31,96(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// addi r7,r1,272
	ctx.r7.s64 = ctx.r1.s64 + 272;
	// stfs f31,112(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lvx128 v10,r0,r9
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,224
	ctx.r6.s64 = ctx.r1.s64 + 224;
	// lvlx v1,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// lvlx v9,r11,r27
	temp.u32 = ctx.r11.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,272
	ctx.r30.s64 = ctx.r1.s64 + 272;
	// lvlx v8,r11,r27
	temp.u32 = ctx.r11.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v1,v9,4,3
	_mm_store_ps(ctx.v1.f32, _mm_blend_ps(_mm_load_ps(ctx.v1.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 57), 4));
	// lvlx v7,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r23,r1,256
	ctx.r23.s64 = ctx.r1.s64 + 256;
	// lvlx v6,r11,r28
	temp.u32 = ctx.r11.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v7,v8,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// lvlx v5,r11,r28
	temp.u32 = ctx.r11.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// ld r3,208(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 208);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v5,v12,4,3
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v3,v10,0
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// vrlimi128 v6,v13,4,3
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// vxor v11,v0,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_setzero_si128());
	// addi r8,r1,160
	ctx.r8.s64 = ctx.r1.s64 + 160;
	// vrlimi128 v7,v5,3,2
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 78), 3));
	// addi r7,r1,176
	ctx.r7.s64 = ctx.r1.s64 + 176;
	// lvx128 v4,r0,r6
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v1,v6,3,2
	_mm_store_ps(ctx.v1.f32, _mm_blend_ps(_mm_load_ps(ctx.v1.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 78), 3));
	// lvx128 v126,r0,r5
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r4,216(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 216);
	// stvx128 v11,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v4,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v3,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v7,r0,r23
	_mm_store_si128((__m128i*)(base + ((ctx.r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822a1400
	ctx.lr = 0x82261234;
	sub_822A1400(ctx, base);
	// addi r11,r31,16
	ctx.r11.s64 = ctx.r31.s64 + 16;
	// stfs f31,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lfs f0,80(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// addi r5,r1,288
	ctx.r5.s64 = ctx.r1.s64 + 288;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lvlx v2,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v31,r11,r27
	temp.u32 = ctx.r11.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v2,v31,4,3
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(ctx.v31.f32), 57), 4));
	// lvlx v30,r11,r28
	temp.u32 = ctx.r11.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v29,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v30,v29,4,3
	_mm_store_ps(ctx.v30.f32, _mm_blend_ps(_mm_load_ps(ctx.v30.f32), _mm_permute_ps(_mm_load_ps(ctx.v29.f32), 57), 4));
	// vor v28,v2,v2
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// stvx128 v1,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f0,304(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// vrlimi128 v28,v30,3,2
	_mm_store_ps(ctx.v28.f32, _mm_blend_ps(_mm_load_ps(ctx.v28.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 78), 3));
	// vor v1,v28,v28
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v28.u8));
	// bl 0x821927c8
	ctx.lr = 0x8226127C;
	sub_821927C8(ctx, base);
	// addi r11,r1,192
	ctx.r11.s64 = ctx.r1.s64 + 192;
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// addi r4,r1,256
	ctx.r4.s64 = ctx.r1.s64 + 256;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// stvx128 v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82d60c58
	ctx.lr = 0x8226129C;
	sub_82D60C58(ctx, base);
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82261308
	if (ctx.cr6.eq) goto loc_82261308;
	// vspltisw v0,-1
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0xFFFFFFFF)));
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// vor128 v13,v126,v126
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v126.u8));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r9,r11,-28400
	ctx.r9.s64 = ctx.r11.s64 + -28400;
	// ld r4,160(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 160);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// ld r5,168(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 168);
	// vslw v12,v0,v0
	ctx.v12.u32[0] = ctx.v0.u32[0] << (ctx.v0.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v0.u32[1] << (ctx.v0.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v0.u32[2] << (ctx.v0.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v0.u32[3] << (ctx.v0.u8[12] & 0x1F);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vxor128 v11,v126,v12
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// vsel v10,v13,v11,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8))));
	// stvx128 v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r6,112(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// ld r7,120(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// bl 0x822331d0
	ctx.lr = 0x822612E8;
	sub_822331D0(ctx, base);
	// ld r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// ld r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// stw r26,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r26.u32);
	// std r6,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r6.u64);
	// std r5,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r5.u64);
	// lvx128 v127,r0,r7
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82261308:
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// subf r9,r11,r26
	ctx.r9.s64 = ctx.r26.s64 - ctx.r11.s64;
	// extsw r8,r9
	ctx.r8.s64 = ctx.r9.s32;
	// lfd f0,-27376(r10)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r10.u32 + -27376);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// fcmpu cr6,f12,f0
	ctx.cr6.compare(ctx.f12.f64, ctx.f0.f64);
	// bge cr6,0x82261528
	if (!ctx.cr6.lt) goto loc_82261528;
	// lbz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82261424
	if (ctx.cr6.eq) goto loc_82261424;
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lfs f31,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f31.f64 = double(temp.f32);
	// addi r30,r31,8
	ctx.r30.s64 = ctx.r31.s64 + 8;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82261384
	if (ctx.cr6.eq) goto loc_82261384;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82261548
	if (ctx.cr6.eq) goto loc_82261548;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82261384
	if (ctx.cr6.eq) goto loc_82261384;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825575c8
	ctx.lr = 0x82261370;
	sub_825575C8(ctx, base);
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82261388
	if (!ctx.cr6.eq) goto loc_82261388;
loc_82261384:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82261388:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822613a4
	if (ctx.cr6.eq) goto loc_822613A4;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825575c8
	ctx.lr = 0x8226139C;
	sub_825575C8(ctx, base);
	// bl 0x82207928
	ctx.lr = 0x822613A0;
	sub_82207928(ctx, base);
	// fmuls f31,f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f1.f64 * ctx.f31.f64));
loc_822613A4:
	// addi r11,r31,52
	ctx.r11.s64 = ctx.r31.s64 + 52;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// li r10,12
	ctx.r10.s64 = 12;
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// ld r7,88(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,r11,r27
	temp.u32 = ctx.r11.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,r11,r10
	temp.u32 = ctx.r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v13,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvlx v11,r11,r28
	temp.u32 = ctx.r11.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lfs f31,-25888(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25888);
	ctx.f31.f64 = double(temp.f32);
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// vrlimi128 v0,v11,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// stvx128 v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// ld r5,120(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// bl 0x8220d058
	ctx.lr = 0x822613F8;
	sub_8220D058(ctx, base);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// ld r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// ld r4,8(r7)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
	// std r5,0(r6)
	PPC_STORE_U64(ctx.r6.u32 + 0, ctx.r5.u64);
	// std r4,8(r6)
	PPC_STORE_U64(ctx.r6.u32 + 8, ctx.r4.u64);
	// bl 0x821f2e50
	ctx.lr = 0x8226141C;
	sub_821F2E50(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v127,r0,r3
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82261424:
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r9,4(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v13,r0,r25
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r8,-28480
	ctx.r6.s64 = ctx.r8.s64 + -28480;
	// vpermwi128 v12,v127,24
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0xE7));
	// add r5,r11,r10
	ctx.r5.u64 = ctx.r11.u64 + ctx.r10.u64;
	// vpermwi128 v11,v127,97
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x9E));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// vspltw128 v10,v127,3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x0));
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stvx128 v127,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// rlwinm r11,r5,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// vand v8,v0,v13
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx128 v127,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// lfs f13,92(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	ctx.f13.f64 = double(temp.f32);
	// ld r3,8(r11)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// vpermwi128 v9,v127,134
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x79));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// std r5,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.r5.u64);
	// std r3,8(r7)
	PPC_STORE_U64(ctx.r7.u32 + 8, ctx.r3.u64);
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v7,v0,252
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// stvx128 v127,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v4,v12,v7
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v7.f32)));
	// lfs f11,136(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// vpermwi128 v6,v0,133
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vpermwi128 v5,v0,98
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// stvx128 v127,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lfs f12,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,84(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// vmulfp128 v3,v11,v6
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v6.f32)));
	// vxor v1,v4,v8
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vmulfp128 v2,v9,v5
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v5.f32)));
	// stfs f11,88(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f13,92(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// vmaddfp v30,v10,v0,v1
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v30.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v1.f32)));
	// lwz r5,12(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// vxor v31,v3,v8
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v8.u8)));
	// vaddfp v29,v30,v31
	_mm_store_ps(ctx.v29.f32, _mm_add_ps(_mm_load_ps(ctx.v30.f32), _mm_load_ps(ctx.v31.f32)));
	// vsubfp v28,v29,v2
	_mm_store_ps(ctx.v28.f32, _mm_sub_ps(_mm_load_ps(ctx.v29.f32), _mm_load_ps(ctx.v2.f32)));
	// stvx128 v28,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
	// std r3,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r3.u64);
	// std r10,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r10.u64);
	// stw r4,52(r31)
	PPC_STORE_U32(ctx.r31.u32 + 52, ctx.r4.u32);
	// stw r8,56(r31)
	PPC_STORE_U32(ctx.r31.u32 + 56, ctx.r8.u32);
	// stw r7,60(r31)
	PPC_STORE_U32(ctx.r31.u32 + 60, ctx.r7.u32);
	// stw r5,64(r31)
	PPC_STORE_U32(ctx.r31.u32 + 64, ctx.r5.u32);
loc_82261528:
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// li r0,-128
	ctx.r0.s64 = -128;
	// lvx128 v126,r1,r0
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r0,-112
	ctx.r0.s64 = -112;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
loc_82261548:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x821940c8
	ctx.lr = 0x82261550;
	sub_821940C8(ctx, base);
	// b 0x82261384
	goto loc_82261384;
}

__attribute__((alias("__imp__sub_82261554"))) PPC_WEAK_FUNC(sub_82261554);
PPC_FUNC_IMPL(__imp__sub_82261554) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82261558"))) PPC_WEAK_FUNC(sub_82261558);
PPC_FUNC_IMPL(__imp__sub_82261558) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// addi r30,r11,-7692
	ctx.r30.s64 = ctx.r11.s64 + -7692;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x829fdf28
	ctx.lr = 0x82261588;
	sub_829FDF28(ctx, base);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226159c
	if (ctx.cr6.eq) goto loc_8226159C;
	// cmplw cr6,r9,r30
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x822615a0
	if (ctx.cr6.eq) goto loc_822615A0;
loc_8226159C:
	// twi 31,r0,22
loc_822615A0:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822615d4
	if (ctx.cr6.eq) goto loc_822615D4;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r8,16(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x822615c8
	if (ctx.cr6.lt) goto loc_822615C8;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822615C8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226162c
	if (ctx.cr6.eq) goto loc_8226162C;
loc_822615D4:
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// bl 0x8322b0d0
	ctx.lr = 0x822615DC;
	sub_8322B0D0(ctx, base);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// li r9,12
	ctx.r9.s64 = 12;
	// stw r8,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_822615F4:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822615f4
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_822615F4;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x829fdc88
	ctx.lr = 0x8226161C;
	sub_829FDC88(ctx, base);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_8226162C:
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x82261638
	if (!ctx.cr6.eq) goto loc_82261638;
	// twi 31,r0,22
loc_82261638:
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82261648
	if (!ctx.cr6.eq) goto loc_82261648;
	// twi 31,r0,22
loc_82261648:
	// addi r3,r10,32
	ctx.r3.s64 = ctx.r10.s64 + 32;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82261664"))) PPC_WEAK_FUNC(sub_82261664);
PPC_FUNC_IMPL(__imp__sub_82261664) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82261668"))) PPC_WEAK_FUNC(sub_82261668);
PPC_FUNC_IMPL(__imp__sub_82261668) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82261670;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addi r31,r11,-7692
	ctx.r31.s64 = ctx.r11.s64 + -7692;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// bl 0x829fdf28
	ctx.lr = 0x82261690;
	sub_829FDF28(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822616a4
	if (ctx.cr6.eq) goto loc_822616A4;
	// cmplw cr6,r11,r31
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r31.u32, ctx.xer);
	// beq cr6,0x822616a8
	if (ctx.cr6.eq) goto loc_822616A8;
loc_822616A4:
	// twi 31,r0,22
loc_822616A8:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x822616e4
	if (ctx.cr6.eq) goto loc_822616E4;
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmpw cr6,r9,r8
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822616d0
	if (ctx.cr6.lt) goto loc_822616D0;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822616D0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822616e4
	if (!ctx.cr6.eq) goto loc_822616E4;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// b 0x822616f0
	goto loc_822616F0;
loc_822616E4:
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// addi r11,r1,88
	ctx.r11.s64 = ctx.r1.s64 + 88;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r31.u32);
loc_822616F0:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r10,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r10.u32);
	// stw r9,4(r29)
	PPC_STORE_U32(ctx.r29.u32 + 4, ctx.r9.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8226170C"))) PPC_WEAK_FUNC(sub_8226170C);
PPC_FUNC_IMPL(__imp__sub_8226170C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82261710"))) PPC_WEAK_FUNC(sub_82261710);
PPC_FUNC_IMPL(__imp__sub_82261710) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82261718;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lbz r11,782(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 782);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822617f4
	if (ctx.cr6.eq) goto loc_822617F4;
	// lwz r11,868(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 868);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822617f4
	if (ctx.cr6.eq) goto loc_822617F4;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// stb r29,782(r31)
	PPC_STORE_U8(ctx.r31.u32 + 782, ctx.r29.u8);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r9,20(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// rldicr r8,r9,32,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// or r4,r8,r10
	ctx.r4.u64 = ctx.r8.u64 | ctx.r10.u64;
	// bl 0x82331378
	ctx.lr = 0x8226175C;
	sub_82331378(ctx, base);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821f0108
	ctx.lr = 0x82261768;
	sub_821F0108(ctx, base);
	// lis r7,-32243
	ctx.r7.s64 = -2113077248;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r7,6584
	ctx.r4.s64 = ctx.r7.s64 + 6584;
	// bl 0x821da550
	ctx.lr = 0x82261778;
	sub_821DA550(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82214f08
	ctx.lr = 0x82261780;
	sub_82214F08(ctx, base);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// addi r4,r6,2804
	ctx.r4.s64 = ctx.r6.s64 + 2804;
	// bl 0x822d6b40
	ctx.lr = 0x82261790;
	sub_822D6B40(ctx, base);
	// lis r3,-31927
	ctx.r3.s64 = -2092367872;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lwz r10,868(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 868);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addi r30,r11,-15976
	ctx.r30.s64 = ctx.r11.s64 + -15976;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwzx r7,r8,r30
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r30.u32);
	// lwz r11,26788(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 26788);
	// lwz r9,32(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// lwz r3,28(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// bl 0x823b6d98
	ctx.lr = 0x822617C4;
	sub_823B6D98(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x821c6868
	ctx.lr = 0x822617CC;
	sub_821C6868(ctx, base);
	// lwz r7,868(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 868);
	// li r6,32767
	ctx.r6.s64 = 32767;
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwzx r7,r11,r30
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r30.u32);
	// bl 0x82771db0
	ctx.lr = 0x822617EC;
	sub_82771DB0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x822617F4;
	sub_82214F08(ctx, base);
loc_822617F4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821ddfc0
	ctx.lr = 0x822617FC;
	sub_821DDFC0(ctx, base);
	// clrlwi r30,r3,24
	ctx.r30.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x82261810
	if (ctx.cr6.eq) goto loc_82261810;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821f7cb0
	ctx.lr = 0x82261810;
	sub_821F7CB0(ctx, base);
loc_82261810:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8226b858
	ctx.lr = 0x82261818;
	sub_8226B858(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8275dc90
	ctx.lr = 0x82261820;
	sub_8275DC90(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8275de40
	ctx.lr = 0x82261828;
	sub_8275DE40(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8275df58
	ctx.lr = 0x82261830;
	sub_8275DF58(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82281b48
	ctx.lr = 0x82261838;
	sub_82281B48(ctx, base);
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lbz r10,-29024(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -29024);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82261864
	if (ctx.cr6.eq) goto loc_82261864;
	// lbz r11,779(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 779);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82261864
	if (ctx.cr6.eq) goto loc_82261864;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x82261864
	if (!ctx.cr6.eq) goto loc_82261864;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82763c68
	ctx.lr = 0x82261864;
	sub_82763C68(ctx, base);
loc_82261864:
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lbz r10,-29023(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -29023);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226188c
	if (ctx.cr6.eq) goto loc_8226188C;
	// lbz r11,779(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 779);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226188c
	if (ctx.cr6.eq) goto loc_8226188C;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82266740
	ctx.lr = 0x8226188C;
	sub_82266740(ctx, base);
loc_8226188C:
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lbz r10,-29022(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -29022);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822618ac
	if (ctx.cr6.eq) goto loc_822618AC;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x822618ac
	if (!ctx.cr6.eq) goto loc_822618AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82764410
	ctx.lr = 0x822618AC;
	sub_82764410(ctx, base);
loc_822618AC:
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82261904
	if (!ctx.cr6.eq) goto loc_82261904;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lbz r10,27489(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 27489);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82261904
	if (!ctx.cr6.eq) goto loc_82261904;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lbz r10,27490(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 27490);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822618ec
	if (ctx.cr6.eq) goto loc_822618EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82268468
	ctx.lr = 0x822618E0;
	sub_82268468(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82261904
	if (!ctx.cr6.eq) goto loc_82261904;
loc_822618EC:
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lbz r10,27492(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 27492);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82261904
	if (ctx.cr6.eq) goto loc_82261904;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82268638
	ctx.lr = 0x82261904;
	sub_82268638(ctx, base);
loc_82261904:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82268808
	ctx.lr = 0x8226190C;
	sub_82268808(ctx, base);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x82261a14
	if (!ctx.cr6.eq) goto loc_82261A14;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r29.u32);
	// lis r30,-31927
	ctx.r30.s64 = -2092367872;
	// stw r29,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r29.u32);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stw r29,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r29.u32);
	// li r4,2
	ctx.r4.s64 = 2;
	// lwz r6,800(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 800);
	// lwz r11,26912(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26912);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r3,16(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// bl 0x821c9258
	ctx.lr = 0x82261948;
	sub_821C9258(ctx, base);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmplw cr6,r3,r8
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r8.u32, ctx.xer);
	// ble cr6,0x8226195c
	if (!ctx.cr6.gt) goto loc_8226195C;
	// twi 31,r0,22
loc_8226195C:
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
loc_82261960:
	// cmplw cr6,r3,r8
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r8.u32, ctx.xer);
	// ble cr6,0x8226196c
	if (!ctx.cr6.gt) goto loc_8226196C;
	// twi 31,r0,22
loc_8226196C:
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// beq cr6,0x82261a08
	if (ctx.cr6.eq) goto loc_82261A08;
	// blt cr6,0x8226197c
	if (ctx.cr6.lt) goto loc_8226197C;
	// twi 31,r0,22
loc_8226197C:
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lwz r7,800(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 800);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// cmpw cr6,r10,r7
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r7.s32, ctx.xer);
	// ble cr6,0x82261994
	if (!ctx.cr6.gt) goto loc_82261994;
	// stw r10,800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 800, ctx.r10.u32);
loc_82261994:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r7,44(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 44);
	// lwz r6,20(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmplw cr6,r7,r6
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r6.u32, ctx.xer);
	// bne cr6,0x822619bc
	if (!ctx.cr6.eq) goto loc_822619BC;
	// lwz r11,48(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// lwz r10,24(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x822619c0
	if (ctx.cr6.eq) goto loc_822619C0;
loc_822619BC:
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_822619C0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822619e0
	if (!ctx.cr6.eq) goto loc_822619E0;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x822619d8
	if (ctx.cr6.lt) goto loc_822619D8;
	// twi 31,r0,22
loc_822619D8:
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// b 0x82261960
	goto loc_82261960;
loc_822619E0:
	// lwz r11,26912(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26912);
	// lwz r10,968(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 968);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r6,20(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// lwz r5,8(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// stw r10,968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 968, ctx.r10.u32);
	// stw r5,964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 964, ctx.r5.u32);
loc_82261A08:
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82261a14
	if (ctx.cr6.eq) goto loc_82261A14;
	// bl 0x8221be68
	ctx.lr = 0x82261A14;
	sub_8221BE68(ctx, base);
loc_82261A14:
	// lwz r7,980(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 980);
	// addi r11,r31,980
	ctx.r11.s64 = ctx.r31.s64 + 980;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq cr6,0x82261a74
	if (ctx.cr6.eq) goto loc_82261A74;
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// lwz r8,972(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 972);
	// lis r6,-31950
	ctx.r6.s64 = -2093875200;
	// lwz r9,-19420(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + -19420);
	// lwz r10,-27380(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + -27380);
	// add r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 + ctx.r9.u64;
	// cmpw cr6,r10,r5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r5.s32, ctx.xer);
	// ble cr6,0x82261a74
	if (!ctx.cr6.gt) goto loc_82261A74;
	// addi r9,r7,-1
	ctx.r9.s64 = ctx.r7.s64 + -1;
	// rotlwi r8,r9,0
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// srawi r7,r8,31
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// subf r5,r6,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r6.s64;
	// stw r5,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
	// stw r10,972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 972, ctx.r10.u32);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// bne cr6,0x82261a74
	if (!ctx.cr6.eq) goto loc_82261A74;
	// stw r29,984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 984, ctx.r29.u32);
loc_82261A74:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82261A7C"))) PPC_WEAK_FUNC(sub_82261A7C);
PPC_FUNC_IMPL(__imp__sub_82261A7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82261A80"))) PPC_WEAK_FUNC(sub_82261A80);
PPC_FUNC_IMPL(__imp__sub_82261A80) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82261A88;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// ble cr6,0x82261b08
	if (!ctx.cr6.gt) goto loc_82261B08;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r11,-2384
	ctx.r9.s64 = ctx.r11.s64 + -2384;
	// li r4,5
	ctx.r4.s64 = 5;
	// lwzx r31,r10,r9
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8222abd0
	ctx.lr = 0x82261AB8;
	sub_8222ABD0(ctx, base);
	// li r5,4
	ctx.r5.s64 = 4;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8222ac38
	ctx.lr = 0x82261AC8;
	sub_8222AC38(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82261ad4
	if (ctx.cr6.eq) goto loc_82261AD4;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r30.u32);
loc_82261AD4:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82261AEC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,44(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// stw r7,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r7.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82261B08:
	// bl 0x82a492b0
	ctx.lr = 0x82261B0C;
	sub_82A492B0(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lfd f1,3240(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r11.u32 + 3240);
	// bl 0x821f7bd8
	ctx.lr = 0x82261B20;
	sub_821F7BD8(ctx, base);
	// bl 0x82a493a8
	ctx.lr = 0x82261B24;
	sub_82A493A8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82261B2C"))) PPC_WEAK_FUNC(sub_82261B2C);
PPC_FUNC_IMPL(__imp__sub_82261B2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82261B30"))) PPC_WEAK_FUNC(sub_82261B30);
PPC_FUNC_IMPL(__imp__sub_82261B30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd0
	ctx.lr = 0x82261B38;
	sub_82CA2BD0(ctx, base);
	// addi r12,r1,-88
	ctx.r12.s64 = ctx.r1.s64 + -88;
	// bl 0x82ca7508
	ctx.lr = 0x82261B40;
	sub_82CA7508(ctx, base);
	// li r12,-144
	ctx.r12.s64 = -144;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-720(r1)
	ea = -720 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bl 0x822780c8
	ctx.lr = 0x82261B5C;
	sub_822780C8(ctx, base);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addi r30,r11,-27456
	ctx.r30.s64 = ctx.r11.s64 + -27456;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// lfs f31,-12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + -12);
	ctx.f31.f64 = double(temp.f32);
	// stfs f31,96(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// bl 0x822780c8
	ctx.lr = 0x82261B7C;
	sub_822780C8(ctx, base);
	// addi r11,r1,100
	ctx.r11.s64 = ctx.r1.s64 + 100;
	// lfs f0,-396(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + -396);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// li r26,4
	ctx.r26.s64 = 4;
	// li r10,12
	ctx.r10.s64 = 12;
	// lvlx v13,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r9,8
	ctx.r9.s64 = 8;
	// stfs f31,128(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// li r27,16
	ctx.r27.s64 = 16;
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// li r7,20
	ctx.r7.s64 = 20;
	// lvlx v12,r3,r26
	temp.u32 = ctx.r3.u32 + ctx.r26.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// lvlx v11,r29,r10
	temp.u32 = ctx.r29.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,r29,r9
	temp.u32 = ctx.r29.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v12,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lvlx v9,r3,r27
	temp.u32 = ctx.r3.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// lvlx v8,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r28,-31927
	ctx.r28.s64 = -2092367872;
	// lvlx v7,r29,r7
	temp.u32 = ctx.r29.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v6,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v7,v8,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// vrlimi128 v9,v6,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 57), 4));
	// vrlimi128 v10,v7,3,2
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 78), 3));
	// lwz r3,26912(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// vrlimi128 v13,v9,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 78), 3));
	// vspltw v5,v0,0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// vaddfp v4,v13,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v10.f32)));
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r4,20(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 20);
	// vmulfp128 v127,v4,v5
	_mm_store_ps(ctx.v127.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v5.f32)));
	// mtctr r4
	ctx.ctr.u64 = ctx.r4.u64;
	// bctrl 
	ctx.lr = 0x82261C08;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lfs f13,9756(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 9756);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lfs f0,8876(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8876);
	ctx.f0.f64 = double(temp.f32);
	// addi r29,r31,160
	ctx.r29.s64 = ctx.r31.s64 + 160;
	// stfs f0,100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// lvlx v2,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v29,v2,0
	_mm_store_si128((__m128i*)ctx.v29.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v2.u32), 0xFF));
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvlx v1,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lvlx v31,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r5,48
	ctx.r5.s64 = 48;
	// lvlx v30,0,r29
	temp.u32 = ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lvlx v3,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v3,v30,4,3
	_mm_store_ps(ctx.v3.f32, _mm_blend_ps(_mm_load_ps(ctx.v3.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// vrlimi128 v31,v1,4,3
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v1.f32), 57), 4));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// lvx128 v28,r7,r5
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// stvx128 v28,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v31,v3,3,2
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 78), 3));
	// vmulfp128 v27,v31,v29
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v27.f32, _mm_mul_ps(_mm_load_ps(ctx.v31.f32), _mm_load_ps(ctx.v29.f32)));
	// stvx128 v27,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,116(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v27,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,124(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	ctx.f13.f64 = double(temp.f32);
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx128 v27,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f11,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f11.f64 = double(temp.f32);
	// fctidz f12,f0
	ctx.f12.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f0.f64));
	// stvx128 v27,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fctidz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f11.f64));
	// lfs f8,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f8.f64 = double(temp.f32);
	// fctidz f7,f8
	ctx.f7.s64 = (ctx.f8.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f8.f64));
	// stfd f12,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.f12.u64);
	// lbz r25,119(r1)
	ctx.r25.u64 = PPC_LOAD_U8(ctx.r1.u32 + 119);
	// stfd f7,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.f7.u64);
	// lbz r24,119(r1)
	ctx.r24.u64 = PPC_LOAD_U8(ctx.r1.u32 + 119);
	// stfd f10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.f10.u64);
	// lbz r23,119(r1)
	ctx.r23.u64 = PPC_LOAD_U8(ctx.r1.u32 + 119);
	// fctidz f9,f13
	ctx.f9.s64 = (ctx.f13.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f13.f64));
	// stfd f9,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.f9.u64);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lbz r22,119(r1)
	ctx.r22.u64 = PPC_LOAD_U8(ctx.r1.u32 + 119);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// stvx128 v127,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821d4b08
	ctx.lr = 0x82261CE4;
	sub_821D4B08(ctx, base);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// lfs f6,20(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,16(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	ctx.f5.f64 = double(temp.f32);
	// fsubs f30,f6,f5
	ctx.f30.f64 = double(float(ctx.f6.f64 - ctx.f5.f64));
	// bl 0x821d4b08
	ctx.lr = 0x82261D00;
	sub_821D4B08(ctx, base);
	// mr r10,r23
	ctx.r10.u64 = ctx.r23.u64;
	// lfs f28,132(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	ctx.f28.f64 = double(temp.f32);
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// lfs f4,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// std r10,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r10.u64);
	// lfd f1,128(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r5,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r5.u64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
	// lwz r6,128(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// lfs f7,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f7.f64 = double(temp.f32);
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r11.u64);
	// lfd f2,112(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// lfd f0,112(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f9,f2
	ctx.f9.f64 = double(ctx.f2.s64);
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// addi r10,r1,256
	ctx.r10.s64 = ctx.r1.s64 + 256;
	// lfs f6,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f6.f64 = double(temp.f32);
	// addi r8,r4,6756
	ctx.r8.s64 = ctx.r4.s64 + 6756;
	// li r11,0
	ctx.r11.s64 = 0;
	// fsubs f5,f7,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 - ctx.f6.f64));
	// frsp f2,f9
	ctx.f2.f64 = double(float(ctx.f9.f64));
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// stw r6,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r6.u32);
	// fsubs f13,f28,f4
	ctx.f13.f64 = double(float(ctx.f28.f64 - ctx.f4.f64));
	// stw r8,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r8.u32);
	// fcfid f10,f0
	ctx.f10.f64 = double(ctx.f0.s64);
	// stw r11,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r11.u32);
	// stvx128 v127,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fcfid f8,f1
	ctx.f8.f64 = double(ctx.f1.s64);
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// fabs f3,f30
	ctx.f3.u64 = ctx.f30.u64 & ~0x8000000000000000;
	// stfs f3,116(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// frsp f4,f11
	ctx.f4.f64 = double(float(ctx.f11.f64));
	// stfs f31,280(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 280, temp.u32);
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// lfs f30,5396(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 5396);
	ctx.f30.f64 = double(temp.f32);
	// fabs f0,f5
	ctx.f0.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// fmuls f12,f2,f30
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// stfs f12,128(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f0,112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// ld r8,112(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// lvlx v26,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// frsp f3,f10
	ctx.f3.f64 = double(float(ctx.f10.f64));
	// frsp f1,f8
	ctx.f1.f64 = double(float(ctx.f8.f64));
	// fmuls f11,f4,f30
	ctx.f11.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// fmuls f10,f1,f30
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v23,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmuls f9,f3,f30
	ctx.f9.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfs f11,128(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fabs f8,f13
	ctx.f8.u64 = ctx.f13.u64 & ~0x8000000000000000;
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stb r11,304(r1)
	PPC_STORE_U8(ctx.r1.u32 + 304, ctx.r11.u8);
	// lvlx v25,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// std r8,272(r1)
	PPC_STORE_U64(ctx.r1.u32 + 272, ctx.r8.u64);
	// lvlx v24,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v24,v25,4,3
	_mm_store_ps(ctx.v24.f32, _mm_blend_ps(_mm_load_ps(ctx.v24.f32), _mm_permute_ps(_mm_load_ps(ctx.v25.f32), 57), 4));
	// vrlimi128 v23,v26,4,3
	_mm_store_ps(ctx.v23.f32, _mm_blend_ps(_mm_load_ps(ctx.v23.f32), _mm_permute_ps(_mm_load_ps(ctx.v26.f32), 57), 4));
	// stfs f8,284(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 284, temp.u32);
	// cmpwi cr6,r9,60
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 60, ctx.xer);
	// vrlimi128 v24,v23,3,2
	_mm_store_ps(ctx.v24.f32, _mm_blend_ps(_mm_load_ps(ctx.v24.f32), _mm_permute_ps(_mm_load_ps(ctx.v23.f32), 78), 3));
	// stvx128 v24,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v24.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x82261e68
	if (ctx.cr6.eq) goto loc_82261E68;
	// lwz r3,136(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpwi cr6,r3,-1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, -1, ctx.xer);
	// bne cr6,0x82261e5c
	if (!ctx.cr6.eq) goto loc_82261E5C;
	// lwz r3,26912(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82261E3C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bl 0x821dde30
	ctx.lr = 0x82261E54;
	sub_821DDE30(ctx, base);
	// stw r3,136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 136, ctx.r3.u32);
	// b 0x82261e68
	goto loc_82261E68;
loc_82261E5C:
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,240
	ctx.r4.s64 = ctx.r1.s64 + 240;
	// bl 0x82261a80
	ctx.lr = 0x82261E68;
	sub_82261A80(ctx, base);
loc_82261E68:
	// lbz r7,60(r31)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r31.u32 + 60);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// lbz r5,63(r31)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r31.u32 + 63);
	// lfs f12,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// lbz r3,62(r31)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r31.u32 + 62);
	// lfs f29,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f29.f64 = double(temp.f32);
	// fmuls f10,f12,f29
	ctx.f10.f64 = double(float(ctx.f12.f64 * ctx.f29.f64));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lbz r11,61(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 61);
	// std r7,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r7.u64);
	// lfd f0,112(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r5,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r5.u64);
	// fcfid f9,f0
	ctx.f9.f64 = double(ctx.f0.s64);
	// frsp f4,f9
	ctx.f4.f64 = double(float(ctx.f9.f64));
	// lfd f13,112(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r3,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r3.u64);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// fcfid f7,f13
	ctx.f7.f64 = double(ctx.f13.s64);
	// stfs f31,128(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// stfs f31,128(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// stfs f31,100(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lvlx v9,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// lvlx v11,0,r29
	temp.u32 = ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v12,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// fmuls f0,f4,f30
	ctx.f0.f64 = double(float(ctx.f4.f64 * ctx.f30.f64));
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lvlx v6,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// frsp f3,f7
	ctx.f3.f64 = double(float(ctx.f7.f64));
	// lfd f11,112(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r11,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r11.u64);
	// lfd f8,112(r1)
	ctx.f8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f6,f8
	ctx.f6.f64 = double(ctx.f8.s64);
	// fcfid f5,f11
	ctx.f5.f64 = double(ctx.f11.s64);
	// addi r7,r9,-28160
	ctx.r7.s64 = ctx.r9.s64 + -28160;
	// fmuls f13,f3,f30
	ctx.f13.f64 = double(float(ctx.f3.f64 * ctx.f30.f64));
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// frsp f2,f6
	ctx.f2.f64 = double(float(ctx.f6.f64));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// vrlimi128 v9,v10,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// lwz r11,26912(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lfs f0,8224(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8224);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// lvlx v8,0,r29
	temp.u32 = ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r25,r1,128
	ctx.r25.s64 = ctx.r1.s64 + 128;
	// vrlimi128 v9,v13,3,2
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// addi r24,r1,128
	ctx.r24.s64 = ctx.r1.s64 + 128;
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// frsp f1,f5
	ctx.f1.f64 = double(float(ctx.f5.f64));
	// stvx128 v9,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmuls f12,f2,f30
	ctx.f12.f64 = double(float(ctx.f2.f64 * ctx.f30.f64));
	// fmuls f11,f1,f30
	ctx.f11.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// stfs f11,100(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lvlx v7,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// vrlimi128 v6,v7,4,3
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 57), 4));
	// lvlx v5,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v4,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v5,v4,4,3
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(ctx.v4.f32), 57), 4));
	// vrlimi128 v5,v6,3,2
	_mm_store_ps(ctx.v5.f32, _mm_blend_ps(_mm_load_ps(ctx.v5.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 78), 3));
	// vperm128 v127,v5,v11,v0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v127,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// stvx128 v127,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f8,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f8.f64 = double(temp.f32);
	// lfs f10,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f10.f64 = double(temp.f32);
	// fmuls f9,f10,f0
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f9,128(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// fmuls f7,f8,f0
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stfs f7,96(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stvx128 v127,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lfs f6,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f6.f64 = double(temp.f32);
	// fmuls f5,f6,f0
	ctx.f5.f64 = double(float(ctx.f6.f64 * ctx.f0.f64));
	// lvlx v2,0,r25
	temp.u32 = ctx.r25.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lvlx v3,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r23,96(r7)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r7.u32 + 96);
	// stfs f5,128(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// vrlimi128 v2,v3,4,3
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 57), 4));
	// lvlx v1,0,r24
	temp.u32 = ctx.r24.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v1,v8,4,3
	_mm_store_ps(ctx.v1.f32, _mm_blend_ps(_mm_load_ps(ctx.v1.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// vrlimi128 v2,v1,3,2
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(ctx.v1.f32), 78), 3));
	// stvx128 v2,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82271b00
	ctx.lr = 0x82261FF0;
	sub_82271B00(ctx, base);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// stfs f31,128(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,432
	ctx.r3.s64 = ctx.r1.s64 + 432;
	// lvlx v31,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v30,r8,r26
	temp.u32 = ctx.r8.u32 + ctx.r26.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v29,r8,r27
	temp.u32 = ctx.r8.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v31,v30,4,3
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// lvlx v28,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v29,v28,4,3
	_mm_store_ps(ctx.v29.f32, _mm_blend_ps(_mm_load_ps(ctx.v29.f32), _mm_permute_ps(_mm_load_ps(ctx.v28.f32), 57), 4));
	// vrlimi128 v31,v29,3,2
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v29.f32), 78), 3));
	// stvx128 v31,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v31.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82271b00
	ctx.lr = 0x8226202C;
	sub_82271B00(ctx, base);
	// stfs f31,128(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lvlx v27,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lvlx v26,r3,r26
	temp.u32 = ctx.r3.u32 + ctx.r26.u32;
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvlx v25,r3,r27
	temp.u32 = ctx.r3.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v27,v26,4,3
	_mm_store_ps(ctx.v27.f32, _mm_blend_ps(_mm_load_ps(ctx.v27.f32), _mm_permute_ps(_mm_load_ps(ctx.v26.f32), 57), 4));
	// addi r25,r11,30580
	ctx.r25.s64 = ctx.r11.s64 + 30580;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r24,r1,192
	ctx.r24.s64 = ctx.r1.s64 + 192;
	// addi r23,r1,144
	ctx.r23.s64 = ctx.r1.s64 + 144;
	// addi r22,r31,140
	ctx.r22.s64 = ctx.r31.s64 + 140;
	// lvlx v24,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v25,v24,4,3
	_mm_store_ps(ctx.v25.f32, _mm_blend_ps(_mm_load_ps(ctx.v25.f32), _mm_permute_ps(_mm_load_ps(ctx.v24.f32), 57), 4));
	// vrlimi128 v27,v25,3,2
	_mm_store_ps(ctx.v27.f32, _mm_blend_ps(_mm_load_ps(ctx.v27.f32), _mm_permute_ps(_mm_load_ps(ctx.v25.f32), 78), 3));
	// stvx128 v27,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821f5b90
	ctx.lr = 0x8226207C;
	sub_821F5B90(ctx, base);
	// lfs f4,116(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f4.f64 = double(temp.f32);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// fsubs f3,f28,f4
	ctx.f3.f64 = double(float(ctx.f28.f64 - ctx.f4.f64));
	// lfs f30,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f30.f64 = double(temp.f32);
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// fmr f2,f30
	ctx.f2.f64 = ctx.f30.f64;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r23,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r23.u32);
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r24.u32);
	// mr r8,r22
	ctx.r8.u64 = ctx.r22.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// fabs f1,f3
	ctx.f1.u64 = ctx.f3.u64 & ~0x8000000000000000;
	// bl 0x82287628
	ctx.lr = 0x822620B4;
	sub_82287628(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x821de318
	ctx.lr = 0x822620BC;
	sub_821DE318(ctx, base);
	// lwz r3,144(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// cmpwi cr6,r3,-1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, -1, ctx.xer);
	// bne cr6,0x822620fc
	if (!ctx.cr6.eq) goto loc_822620FC;
	// lwz r3,26912(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822620DC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bl 0x821dde30
	ctx.lr = 0x822620F4;
	sub_821DDE30(ctx, base);
	// stw r3,144(r31)
	PPC_STORE_U32(ctx.r31.u32 + 144, ctx.r3.u32);
	// b 0x82262108
	goto loc_82262108;
loc_822620FC:
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,464
	ctx.r4.s64 = ctx.r1.s64 + 464;
	// bl 0x82261a80
	ctx.lr = 0x82262108;
	sub_82261A80(ctx, base);
loc_82262108:
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// stfs f31,128(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// stfs f31,112(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r11,r1,192
	ctx.r11.s64 = ctx.r1.s64 + 192;
	// lfs f0,8224(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8224);
	ctx.f0.f64 = double(temp.f32);
	// addi r9,r1,128
	ctx.r9.s64 = ctx.r1.s64 + 128;
	// lfs f13,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// fmuls f12,f13,f29
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f29.f64));
	// stvx128 v127,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lfs f11,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r1,100
	ctx.r6.s64 = ctx.r1.s64 + 100;
	// stvx128 v127,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// stvx128 v127,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lfs f7,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lfs f10,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f10.f64 = double(temp.f32);
	// lwz r11,26912(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmuls f8,f10,f0
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// stfs f8,112(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r30,r1,192
	ctx.r30.s64 = ctx.r1.s64 + 192;
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lvlx v7,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,432
	ctx.r3.s64 = ctx.r1.s64 + 432;
	// lvlx v8,0,r29
	temp.u32 = ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f9,112(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// stfs f31,100(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lvlx v12,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v12,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// fmuls f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stfs f6,128(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lvlx v10,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v8,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// vrlimi128 v0,v7,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 57), 4));
	// lwz r6,12(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// vrlimi128 v13,v0,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 78), 3));
	// lwz r29,96(r6)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + 96);
	// stvx128 v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v9,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v9,v10,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v9,v11,3,2
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// stvx128 v9,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821d5c50
	ctx.lr = 0x822621DC;
	sub_821D5C50(ctx, base);
	// stfs f31,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,208
	ctx.r10.s64 = ctx.r1.s64 + 208;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,544
	ctx.r3.s64 = ctx.r1.s64 + 544;
	// lvlx v6,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v5,r5,r26
	temp.u32 = ctx.r5.u32 + ctx.r26.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v4,r5,r27
	temp.u32 = ctx.r5.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v6,v5,4,3
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 57), 4));
	// lvlx v3,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v4,v3,4,3
	_mm_store_ps(ctx.v4.f32, _mm_blend_ps(_mm_load_ps(ctx.v4.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 57), 4));
	// vrlimi128 v6,v4,3,2
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v4.f32), 78), 3));
	// stvx128 v6,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821d5c50
	ctx.lr = 0x82262218;
	sub_821D5C50(ctx, base);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lvlx v2,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v1,r3,r26
	temp.u32 = ctx.r3.u32 + ctx.r26.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// lvlx v31,r3,r27
	temp.u32 = ctx.r3.u32 + ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// stfs f31,112(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// vrlimi128 v2,v1,4,3
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(ctx.v1.f32), 57), 4));
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lvlx v30,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v31,v30,4,3
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// addi r30,r1,160
	ctx.r30.s64 = ctx.r1.s64 + 160;
	// vrlimi128 v2,v31,3,2
	_mm_store_ps(ctx.v2.f32, _mm_blend_ps(_mm_load_ps(ctx.v2.f32), _mm_permute_ps(_mm_load_ps(ctx.v31.f32), 78), 3));
	// addi r29,r1,192
	ctx.r29.s64 = ctx.r1.s64 + 192;
	// addi r27,r31,148
	ctx.r27.s64 = ctx.r31.s64 + 148;
	// stvx128 v2,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821f5b90
	ctx.lr = 0x82262260;
	sub_821F5B90(ctx, base);
	// lfs f5,148(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f5.f64 = double(temp.f32);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// fsubs f4,f28,f5
	ctx.f4.f64 = double(float(ctx.f28.f64 - ctx.f5.f64));
	// addi r6,r1,208
	ctx.r6.s64 = ctx.r1.s64 + 208;
	// li r4,0
	ctx.r4.s64 = 0;
	// fmr f2,f30
	ctx.f2.f64 = ctx.f30.f64;
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// fabs f1,f4
	ctx.f1.u64 = ctx.f4.u64 & ~0x8000000000000000;
	// bl 0x82287628
	ctx.lr = 0x82262294;
	sub_82287628(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x821de318
	ctx.lr = 0x8226229C;
	sub_821DE318(ctx, base);
	// lwz r3,152(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// cmpwi cr6,r3,-1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, -1, ctx.xer);
	// bne cr6,0x822622dc
	if (!ctx.cr6.eq) goto loc_822622DC;
	// lwz r3,26912(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822622BC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r4,0(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bl 0x821dde30
	ctx.lr = 0x822622D4;
	sub_821DDE30(ctx, base);
	// stw r3,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r3.u32);
	// b 0x822622e8
	goto loc_822622E8;
loc_822622DC:
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,352
	ctx.r4.s64 = ctx.r1.s64 + 352;
	// bl 0x82261a80
	ctx.lr = 0x822622E8;
	sub_82261A80(ctx, base);
loc_822622E8:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r3,r1,388
	ctx.r3.s64 = ctx.r1.s64 + 388;
	// addi r31,r11,5536
	ctx.r31.s64 = ctx.r11.s64 + 5536;
	// stw r31,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, ctx.r31.u32);
	// bl 0x821c6868
	ctx.lr = 0x822622FC;
	sub_821C6868(ctx, base);
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x821de318
	ctx.lr = 0x82262304;
	sub_821DE318(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stw r31,464(r1)
	PPC_STORE_U32(ctx.r1.u32 + 464, ctx.r31.u32);
	// addi r3,r1,500
	ctx.r3.s64 = ctx.r1.s64 + 500;
	// addi r9,r10,26452
	ctx.r9.s64 = ctx.r10.s64 + 26452;
	// stw r9,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, ctx.r9.u32);
	// bl 0x821c6868
	ctx.lr = 0x8226231C;
	sub_821C6868(ctx, base);
	// addi r3,r1,496
	ctx.r3.s64 = ctx.r1.s64 + 496;
	// bl 0x821de318
	ctx.lr = 0x82262324;
	sub_821DE318(ctx, base);
	// addi r1,r1,720
	ctx.r1.s64 = ctx.r1.s64 + 720;
	// li r0,-144
	ctx.r0.s64 = -144;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r12,r1,-88
	ctx.r12.s64 = ctx.r1.s64 + -88;
	// bl 0x82ca7554
	ctx.lr = 0x82262338;
	sub_82CA7554(ctx, base);
	// b 0x82ca2c20
	// ERROR 82CA2C20
	return;
}

__attribute__((alias("__imp__sub_8226233C"))) PPC_WEAK_FUNC(sub_8226233C);
PPC_FUNC_IMPL(__imp__sub_8226233C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82262340"))) PPC_WEAK_FUNC(sub_82262340);
PPC_FUNC_IMPL(__imp__sub_82262340) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x82207928
	ctx.lr = 0x82262360;
	sub_82207928(ctx, base);
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lfd f0,-27376(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -27376);
	// fdiv f0,f1,f0
	ctx.f0.f64 = ctx.f1.f64 / ctx.f0.f64;
	// frsp f31,f0
	ctx.f31.f64 = double(float(ctx.f0.f64));
	// bl 0x82275680
	ctx.lr = 0x82262378;
	sub_82275680(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f13,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f1,f13,f31
	ctx.f1.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// lwz r9,20(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82262394;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f12,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f12.f64 = double(temp.f32);
	// fadds f1,f12,f31
	ctx.f1.f64 = double(float(ctx.f12.f64 + ctx.f31.f64));
	// lwz r7,20(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x822623B0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r3,56(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	// lwz r6,0(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f11,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f11.f64 = double(temp.f32);
	// fadds f1,f11,f31
	ctx.f1.f64 = double(float(ctx.f11.f64 + ctx.f31.f64));
	// lwz r5,20(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 20);
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// bctrl 
	ctx.lr = 0x822623CC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r3,64(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f1,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// lwz r11,20(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 20);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822623E4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822623FC"))) PPC_WEAK_FUNC(sub_822623FC);
PPC_FUNC_IMPL(__imp__sub_822623FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82262400"))) PPC_WEAK_FUNC(sub_82262400);
PPC_FUNC_IMPL(__imp__sub_82262400) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be0
	ctx.lr = 0x82262408;
	sub_82CA2BE0(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r5,4(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r10,r31,32
	ctx.r10.s64 = ctx.r31.s64 + 32;
	// lis r11,-31946
	ctx.r11.s64 = -2093613056;
	// clrldi r8,r10,32
	ctx.r8.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// rldicr r7,r9,63,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// srd r6,r7,r8
	ctx.r6.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r7.u64 >> (ctx.r8.u8 & 0x7F));
	// lwz r3,412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 412);
	// bl 0x821b7020
	ctx.lr = 0x82262438;
	sub_821B7020(ctx, base);
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// add r6,r31,r10
	ctx.r6.u64 = ctx.r31.u64 + ctx.r10.u64;
	// addi r4,r11,-16224
	ctx.r4.s64 = ctx.r11.s64 + -16224;
	// rlwinm r3,r6,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r4,2736
	ctx.r11.s64 = ctx.r4.s64 + 2736;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// addi r31,r10,-32624
	ctx.r31.s64 = ctx.r10.s64 + -32624;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lis r8,-31951
	ctx.r8.s64 = -2093940736;
	// addi r28,r10,3056
	ctx.r28.s64 = ctx.r10.s64 + 3056;
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r27,1
	ctx.r27.s64 = 1;
	// lis r29,-31943
	ctx.r29.s64 = -2093416448;
	// lwz r9,28104(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28104);
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// cmplw cr6,r7,r9
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, ctx.xer);
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226250c
	if (ctx.cr6.eq) goto loc_8226250C;
	// lwz r7,8192(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r30,r6,r7
	ctx.r30.u64 = ctx.r6.u64 & ctx.r7.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x822624e0
	if (!ctx.cr6.eq) goto loc_822624E0;
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 | ctx.r7.u64;
	// rlwinm r6,r9,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r7,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r7.u32);
	// stwx r11,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r6,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r7,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r7.u32);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// lwz r9,28104(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28104);
	// stw r7,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r7.u32);
loc_822624E0:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r5,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226250c
	if (!ctx.cr6.eq) goto loc_8226250C;
	// lwz r10,1000(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r5,r10,1
	ctx.r5.s64 = ctx.r10.s64 + 1;
	// stw r5,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r5.u32);
	// stwx r11,r8,r28
	PPC_STORE_U32(ctx.r8.u32 + ctx.r28.u32, ctx.r11.u32);
	// b 0x82262510
	goto loc_82262510;
loc_8226250C:
	// lwz r5,1000(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
loc_82262510:
	// addi r11,r4,3216
	ctx.r11.s64 = ctx.r4.s64 + 3216;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r8,r9
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x822625a0
	if (ctx.cr6.eq) goto loc_822625A0;
	// lwz r8,8192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x8226257c
	if (!ctx.cr6.eq) goto loc_8226257C;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 | ctx.r8.u64;
	// rlwinm r7,r6,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r8.u32);
	// stwx r11,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r8,r6,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r7,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r7.u32);
	// lwz r8,8196(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stw r8,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r8.u32);
loc_8226257C:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r9,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r9.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822625a0
	if (!ctx.cr6.eq) goto loc_822625A0;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r5,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r5.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_822625A0:
	// lis r11,-31946
	ctx.r11.s64 = -2093613056;
	// lwz r30,-2492(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + -2492);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x82262770
	if (ctx.cr6.eq) goto loc_82262770;
	// addi r11,r4,4656
	ctx.r11.s64 = ctx.r4.s64 + 4656;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// addi r9,r10,28240
	ctx.r9.s64 = ctx.r10.s64 + 28240;
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r6,4(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r8,r6
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r6.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226264c
	if (ctx.cr6.eq) goto loc_8226264C;
	// lwz r8,8192(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r26,r7,r8
	ctx.r26.u64 = ctx.r7.u64 & ctx.r8.u64;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// bne cr6,0x82262628
	if (!ctx.cr6.eq) goto loc_82262628;
	// lwz r26,8196(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 | ctx.r8.u64;
	// rlwinm r7,r26,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r8.u32);
	// stwx r11,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r8,8196(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r8,r8,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r7,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r7.u32);
	// lwz r8,8196(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r7,r8,1
	ctx.r7.s64 = ctx.r8.s64 + 1;
	// stw r7,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r7.u32);
loc_82262628:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r6,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r6.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226264c
	if (!ctx.cr6.eq) goto loc_8226264C;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r5,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r5.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226264C:
	// addi r11,r4,4176
	ctx.r11.s64 = ctx.r4.s64 + 4176;
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x822626e0
	if (ctx.cr6.eq) goto loc_822626E0;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x822626bc
	if (!ctx.cr6.eq) goto loc_822626BC;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r8,r6,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// stwx r11,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r6,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r8,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r8.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r9.u32);
loc_822626BC:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822626e0
	if (!ctx.cr6.eq) goto loc_822626E0;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r5,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r5.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_822626E0:
	// addi r11,r4,1296
	ctx.r11.s64 = ctx.r4.s64 + 1296;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r9,r30
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r30.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82262770
	if (ctx.cr6.eq) goto loc_82262770;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r7,r8,r9
	ctx.r7.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x8226274c
	if (!ctx.cr6.eq) goto loc_8226274C;
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r9,r7,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r6,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r6.u32);
	// stwx r11,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r8,8196(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r8,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r7,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r7.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r9.u32);
loc_8226274C:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r30,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r30.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82262770
	if (!ctx.cr6.eq) goto loc_82262770;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r5,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r5.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_82262770:
	// addi r11,r4,2256
	ctx.r11.s64 = ctx.r4.s64 + 2256;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// addi r9,r10,28252
	ctx.r9.s64 = ctx.r10.s64 + 28252;
	// lbz r8,21(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mulli r10,r8,8200
	ctx.r10.s64 = ctx.r8.s64 * 8200;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r6,r7
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226280c
	if (ctx.cr6.eq) goto loc_8226280C;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x822627e8
	if (!ctx.cr6.eq) goto loc_822627E8;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r8,r6,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// stwx r11,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r6,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r6,r9,1
	ctx.r6.s64 = ctx.r9.s64 + 1;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
loc_822627E8:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226280c
	if (!ctx.cr6.eq) goto loc_8226280C;
	// rlwinm r10,r5,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// stw r5,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r5.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226280C:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// addi r11,r4,1776
	ctx.r11.s64 = ctx.r4.s64 + 1776;
	// add r11,r3,r11
	ctx.r11.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lfs f0,-5768(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -5768);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lbz r9,21(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mulli r10,r9,8200
	ctx.r10.s64 = ctx.r9.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// lwz r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplw cr6,r8,r7
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x822628ac
	if (ctx.cr6.eq) goto loc_822628AC;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x82262888
	if (!ctx.cr6.eq) goto loc_82262888;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r4,r8,r9
	ctx.r4.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r3,r6,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r4,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r4.u32);
	// stwx r11,r3,r10
	PPC_STORE_U32(ctx.r3.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r8,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r8.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r4,r9,1
	ctx.r4.s64 = ctx.r9.s64 + 1;
	// stw r4,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r4.u32);
loc_82262888:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822628ac
	if (!ctx.cr6.eq) goto loc_822628AC;
	// rlwinm r9,r5,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r10,r5,1
	ctx.r10.s64 = ctx.r5.s64 + 1;
	// stw r10,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r10.u32);
	// stwx r11,r9,r28
	PPC_STORE_U32(ctx.r9.u32 + ctx.r28.u32, ctx.r11.u32);
loc_822628AC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	// ERROR 82CA2C30
	return;
}

__attribute__((alias("__imp__sub_822628B4"))) PPC_WEAK_FUNC(sub_822628B4);
PPC_FUNC_IMPL(__imp__sub_822628B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822628B8"))) PPC_WEAK_FUNC(sub_822628B8);
PPC_FUNC_IMPL(__imp__sub_822628B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x822628C0;
	sub_82CA2BE4(ctx, base);
	// stfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// li r12,-96
	ctx.r12.s64 = -96;
	// stvx128 v126,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-80
	ctx.r12.s64 = -80;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// vor128 v127,v1,v1
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822628F4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82262ac0
	if (ctx.cr6.eq) goto loc_82262AC0;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lbz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r30.u32 + 32);
	// addi r31,r30,16
	ctx.r31.s64 = ctx.r30.s64 + 16;
	// addi r11,r11,-27852
	ctx.r11.s64 = ctx.r11.s64 + -27852;
	// li r28,4
	ctx.r28.s64 = 4;
	// li r29,8
	ctx.r29.s64 = 8;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvlx128 v126,r0,r31
	temp.u32 = ctx.r31.u32;
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,1
	ctx.r8.s64 = 1;
	// lfs f31,384(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 384);
	ctx.f31.f64 = double(temp.f32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lvlx v0,r31,r28
	temp.u32 = ctx.r31.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,r31,r29
	temp.u32 = ctx.r31.u32 + ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v126,v0,4,3
	_mm_store_ps(ctx.v126.f32, _mm_blend_ps(_mm_load_ps(ctx.v126.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stb r8,33(r30)
	PPC_STORE_U8(ctx.r30.u32 + 33, ctx.r8.u8);
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v12,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v126,v13,3,2
	_mm_store_ps(ctx.v126.f32, _mm_blend_ps(_mm_load_ps(ctx.v126.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// beq cr6,0x82262a04
	if (ctx.cr6.eq) goto loc_82262A04;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lvlx v0,0,r31
	temp.u32 = ctx.r31.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,r31,r28
	temp.u32 = ctx.r31.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lvlx v12,r31,r29
	temp.u32 = ctx.r31.u32 + ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v13,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f0,396(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v11,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// vsubfp128 v10,v127,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(ctx.v127.f32), _mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v9,v10,v10
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v10.f32), 0xEF));
	// stvx128 v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bge cr6,0x82262a04
	if (!ctx.cr6.lt) goto loc_82262A04;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// lvlx v0,0,r31
	temp.u32 = ctx.r31.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lvlx v13,r31,r28
	temp.u32 = ctx.r31.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvlx v12,r31,r29
	temp.u32 = ctx.r31.u32 + ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v13,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v10,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v10,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// vspltw v9,v11,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vaddfp128 v8,v0,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v127.f32)));
	// vmulfp128 v7,v8,v9
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32)));
	// stvx128 v7,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,112(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// stvx128 v7,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f12.f64 = double(temp.f32);
	// stvx128 v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,104(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f0.f64 = double(temp.f32);
	// stfs f12,4(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// stfs f13,0(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// b 0x82262a30
	goto loc_82262A30;
loc_82262A04:
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx128 v127,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f12.f64 = double(temp.f32);
	// stvx128 v127,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,120(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v127,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// stfs f12,0(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
loc_82262A30:
	// stfs f0,8(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 8, temp.u32);
	// stw r27,28(r30)
	PPC_STORE_U32(ctx.r30.u32 + 28, ctx.r27.u32);
	// addi r3,r30,8
	ctx.r3.s64 = ctx.r30.s64 + 8;
	// bl 0x825575c8
	ctx.lr = 0x82262A40;
	sub_825575C8(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x82262A5C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// lvlx v13,r31,r28
	temp.u32 = ctx.r31.u32 + ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lvlx v12,0,r31
	temp.u32 = ctx.r31.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,r31,r29
	temp.u32 = ctx.r31.u32 + ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v13,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// stfs f31,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v10,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v12,v11,3,2
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v2,v126,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v2.f32, _mm_sub_ps(_mm_load_ps(ctx.v126.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v1,v12,v0
	_mm_store_ps(ctx.v1.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32)));
	// bl 0x82260808
	ctx.lr = 0x82262A94;
	sub_82260808(ctx, base);
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// lis r5,-32241
	ctx.r5.s64 = -2112946176;
	// lfs f0,512(r6)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 512);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f0,27524(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 27524);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// ble cr6,0x82262ab8
	if (!ctx.cr6.gt) goto loc_82262AB8;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r11,44(r30)
	PPC_STORE_U8(ctx.r30.u32 + 44, ctx.r11.u8);
loc_82262AB8:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82262ad4
	goto loc_82262AD4;
loc_82262AC0:
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,33(r30)
	PPC_STORE_U8(ctx.r30.u32 + 33, ctx.r11.u8);
	// stw r11,28(r30)
	PPC_STORE_U32(ctx.r30.u32 + 28, ctx.r11.u32);
	// stb r11,44(r30)
	PPC_STORE_U8(ctx.r30.u32 + 44, ctx.r11.u8);
loc_82262AD4:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// li r0,-96
	ctx.r0.s64 = -96;
	// lvx128 v126,r1,r0
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r0,-80
	ctx.r0.s64 = -80;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_82262AF0"))) PPC_WEAK_FUNC(sub_82262AF0);
PPC_FUNC_IMPL(__imp__sub_82262AF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x82262AF8;
	sub_82CA2BE4(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// li r27,0
	ctx.r27.s64 = 0;
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// lwz r11,4(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// rlwinm r9,r10,31,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82262c0c
	if (ctx.cr6.eq) goto loc_82262C0C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82262b48
	if (ctx.cr6.eq) goto loc_82262B48;
	// lbz r10,97(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 97);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82262c10
	goto loc_82262C10;
loc_82262B48:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r27.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82262bb4
	if (!ctx.cr0.gt) goto loc_82262BB4;
loc_82262B64:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,97
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 97, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82262b84
	if (ctx.cr6.lt) goto loc_82262B84;
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
loc_82262B84:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82262ba0
	if (ctx.cr6.eq) goto loc_82262BA0;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82262ba8
	goto loc_82262BA8;
loc_82262BA0:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82262BA8:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82262b64
	if (ctx.cr6.gt) goto loc_82262B64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82262BB4:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82262bf8
	if (ctx.cr6.eq) goto loc_82262BF8;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,97
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 97, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82262bd0
	if (ctx.cr6.gt) goto loc_82262BD0;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_82262BD0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82262bf8
	if (!ctx.cr6.eq) goto loc_82262BF8;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82262c10
	goto loc_82262C10;
loc_82262BF8:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82262c10
	goto loc_82262C10;
loc_82262C0C:
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_82262C10:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// li r29,1248
	ctx.r29.s64 = 1248;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82262c28
	if (ctx.cr6.eq) goto loc_82262C28;
	// vspltisw v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x0)));
	// stvx128 v0,r30,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32 + ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82262C28:
	// lwz r3,88(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 88);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x82262c3c
	if (!ctx.cr6.eq) goto loc_82262C3C;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// b 0x82262c4c
	goto loc_82262C4C;
loc_82262C3C:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82262C4C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82262C4C:
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82262d14
	if (ctx.cr6.eq) goto loc_82262D14;
	// lwz r4,88(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 88);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82262C70;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r9,88(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 88);
	// lwz r8,100(r28)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + 100);
	// lwz r7,12(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// andc r31,r7,r8
	ctx.r31.u64 = ctx.r7.u64 & ~ctx.r8.u64;
	// rlwinm r6,r31,0,28,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x82262cd0
	if (ctx.cr6.eq) goto loc_82262CD0;
	// lwz r11,112(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 112);
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 33);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82262cac
	if (ctx.cr6.eq) goto loc_82262CAC;
	// lbz r11,34(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 34);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x82262cb0
	if (!ctx.cr6.eq) goto loc_82262CB0;
loc_82262CAC:
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_82262CB0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82262ccc
	if (!ctx.cr6.eq) goto loc_82262CCC;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82704160
	ctx.lr = 0x82262CCC;
	sub_82704160(ctx, base);
loc_82262CCC:
	// addi r31,r31,-8
	ctx.r31.s64 = ctx.r31.s64 + -8;
loc_82262CD0:
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lwz r3,112(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 112);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822628b8
	ctx.lr = 0x82262CE4;
	sub_822628B8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82262d20
	if (ctx.cr6.eq) goto loc_82262D20;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x82262d20
	if (ctx.cr6.eq) goto loc_82262D20;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r30,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32 + ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r27,96(r28)
	PPC_STORE_U8(ctx.r28.u32 + 96, ctx.r27.u8);
	// stw r27,100(r28)
	PPC_STORE_U32(ctx.r28.u32 + 100, ctx.r27.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
loc_82262D14:
	// lwz r11,112(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 112);
	// stb r27,33(r11)
	PPC_STORE_U8(ctx.r11.u32 + 33, ctx.r27.u8);
	// stb r27,44(r11)
	PPC_STORE_U8(ctx.r11.u32 + 44, ctx.r27.u8);
loc_82262D20:
	// stb r27,96(r28)
	PPC_STORE_U8(ctx.r28.u32 + 96, ctx.r27.u8);
	// stw r27,100(r28)
	PPC_STORE_U32(ctx.r28.u32 + 100, ctx.r27.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_82262D30"))) PPC_WEAK_FUNC(sub_82262D30);
PPC_FUNC_IMPL(__imp__sub_82262D30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82262D38;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82262D58;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82262f28
	if (ctx.cr6.eq) goto loc_82262F28;
	// addi r30,r31,16
	ctx.r30.s64 = ctx.r31.s64 + 16;
	// li r31,0
	ctx.r31.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82227680
	ctx.lr = 0x82262D74;
	sub_82227680(ctx, base);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r10,r11,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 9) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82262e74
	if (ctx.cr6.eq) goto loc_82262E74;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82262db0
	if (ctx.cr6.eq) goto loc_82262DB0;
	// lbz r10,23(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 23);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82262e78
	goto loc_82262E78;
loc_82262DB0:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r31.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82262e1c
	if (!ctx.cr0.gt) goto loc_82262E1C;
loc_82262DCC:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,23
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 23, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82262dec
	if (ctx.cr6.lt) goto loc_82262DEC;
	// mr r7,r31
	ctx.r7.u64 = ctx.r31.u64;
loc_82262DEC:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82262e08
	if (ctx.cr6.eq) goto loc_82262E08;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82262e10
	goto loc_82262E10;
loc_82262E08:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82262E10:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82262dcc
	if (ctx.cr6.gt) goto loc_82262DCC;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_82262E1C:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82262e60
	if (ctx.cr6.eq) goto loc_82262E60;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,23
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 23, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82262e38
	if (ctx.cr6.gt) goto loc_82262E38;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
loc_82262E38:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82262e60
	if (!ctx.cr6.eq) goto loc_82262E60;
	// ld r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82262e78
	goto loc_82262E78;
loc_82262E60:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82262e78
	goto loc_82262E78;
loc_82262E74:
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
loc_82262E78:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82262e9c
	if (ctx.cr6.eq) goto loc_82262E9C;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x822376d8
	ctx.lr = 0x82262E90;
	sub_822376D8(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82262E9C:
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r10,r11,-27852
	ctx.r10.s64 = ctx.r11.s64 + -27852;
	// lfs f0,-27852(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27852);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lfs f0,384(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 384);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// bl 0x82227680
	ctx.lr = 0x82262EC4;
	sub_82227680(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r4,124(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 124);
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r7,64(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 64);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x82262EE0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lvlx v13,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v11,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lvx128 v10,r0,r10
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v13,v11,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vaddfp v9,v10,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v13.f32)));
	// stvx128 v9,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82262F28:
	// vspltisw v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x0)));
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stvx128 v0,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82262F3C"))) PPC_WEAK_FUNC(sub_82262F3C);
PPC_FUNC_IMPL(__imp__sub_82262F3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82262F40"))) PPC_WEAK_FUNC(sub_82262F40);
PPC_FUNC_IMPL(__imp__sub_82262F40) {
	PPC_FUNC_PROLOGUE();
	// addi r3,r3,16
	ctx.r3.s64 = ctx.r3.s64 + 16;
	// b 0x822276e8
	sub_822276E8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82262F48"))) PPC_WEAK_FUNC(sub_82262F48);
PPC_FUNC_IMPL(__imp__sub_82262F48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82262F50;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// mr r30,r29
	ctx.r30.u64 = ctx.r29.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// rlwinm r9,r10,11,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 11) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226306c
	if (ctx.cr6.eq) goto loc_8226306C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82262fa8
	if (ctx.cr6.eq) goto loc_82262FA8;
	// lbz r10,21(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82263070
	goto loc_82263070;
loc_82262FA8:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r29.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82263014
	if (!ctx.cr0.gt) goto loc_82263014;
loc_82262FC4:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,21
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 21, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82262fe4
	if (ctx.cr6.lt) goto loc_82262FE4;
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
loc_82262FE4:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82263000
	if (ctx.cr6.eq) goto loc_82263000;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82263008
	goto loc_82263008;
loc_82263000:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82263008:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82262fc4
	if (ctx.cr6.gt) goto loc_82262FC4;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82263014:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82263058
	if (ctx.cr6.eq) goto loc_82263058;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,21
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 21, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82263030
	if (ctx.cr6.gt) goto loc_82263030;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_82263030:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82263058
	if (!ctx.cr6.eq) goto loc_82263058;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82263070
	goto loc_82263070;
loc_82263058:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82263070
	goto loc_82263070;
loc_8226306C:
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_82263070:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82263090
	if (ctx.cr6.eq) goto loc_82263090;
	// li r4,53
	ctx.r4.s64 = 53;
	// bl 0x821ff4e0
	ctx.lr = 0x82263084;
	sub_821FF4E0(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822633f8
	if (!ctx.cr6.eq) goto loc_822633F8;
loc_82263090:
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r3,0(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// bl 0x82232748
	ctx.lr = 0x822630B0;
	sub_82232748(ctx, base);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226322c
	if (ctx.cr6.eq) goto loc_8226322C;
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
	// lwz r10,48(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 48);
	// rlwinm r9,r10,29,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822631b8
	if (ctx.cr6.eq) goto loc_822631B8;
	// lwz r11,140(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822630fc
	if (ctx.cr6.eq) goto loc_822630FC;
	// lbz r10,99(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 99);
	// lwz r11,72(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x822631bc
	goto loc_822631BC;
loc_822630FC:
	// lwz r10,72(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 72);
	// lwz r6,76(r5)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r5.u32 + 76);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r29.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82263168
	if (!ctx.cr0.gt) goto loc_82263168;
loc_82263118:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,99
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 99, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82263138
	if (ctx.cr6.lt) goto loc_82263138;
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
loc_82263138:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82263154
	if (ctx.cr6.eq) goto loc_82263154;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226315c
	goto loc_8226315C;
loc_82263154:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226315C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82263118
	if (ctx.cr6.gt) goto loc_82263118;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82263168:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x822631a8
	if (ctx.cr6.eq) goto loc_822631A8;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,99
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 99, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82263184
	if (ctx.cr6.gt) goto loc_82263184;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_82263184:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822631a8
	if (!ctx.cr6.eq) goto loc_822631A8;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x822631bc
	goto loc_822631BC;
loc_822631A8:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x822631bc
	goto loc_822631BC;
loc_822631B8:
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
loc_822631BC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82263220
	if (ctx.cr6.eq) goto loc_82263220;
	// lwz r11,40(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 ^ 1;
	// clrlwi r7,r8,24
	ctx.r7.u64 = ctx.r8.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82263220
	if (ctx.cr6.eq) goto loc_82263220;
	// lwz r4,124(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 124);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822631FC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lfs f0,-2932(r8)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -2932);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// ble cr6,0x822633f8
	if (!ctx.cr6.gt) goto loc_822633F8;
loc_82263220:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_8226322C:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r31,r29
	ctx.r31.u64 = ctx.r29.u64;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// rlwinm r9,r10,6,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82263328
	if (ctx.cr6.eq) goto loc_82263328;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226326c
	if (ctx.cr6.eq) goto loc_8226326C;
	// lbz r10,58(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 58);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8226332c
	goto loc_8226332C;
loc_8226326C:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r29.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x822632d8
	if (!ctx.cr0.gt) goto loc_822632D8;
loc_82263288:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,58
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 58, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822632a8
	if (ctx.cr6.lt) goto loc_822632A8;
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
loc_822632A8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822632c4
	if (ctx.cr6.eq) goto loc_822632C4;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x822632cc
	goto loc_822632CC;
loc_822632C4:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_822632CC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82263288
	if (ctx.cr6.gt) goto loc_82263288;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822632D8:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82263318
	if (ctx.cr6.eq) goto loc_82263318;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,58
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 58, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x822632f4
	if (ctx.cr6.gt) goto loc_822632F4;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_822632F4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82263318
	if (!ctx.cr6.eq) goto loc_82263318;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226332c
	goto loc_8226332C;
loc_82263318:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226332c
	goto loc_8226332C;
loc_82263328:
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_8226332C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822633f8
	if (ctx.cr6.eq) goto loc_822633F8;
	// lwz r11,48(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x822633b4
	if (!ctx.cr6.eq) goto loc_822633B4;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// li r30,1
	ctx.r30.s64 = 1;
	// bl 0x821e7120
	ctx.lr = 0x82263354;
	sub_821E7120(ctx, base);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822633b4
	if (ctx.cr6.eq) goto loc_822633B4;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r30,3
	ctx.r30.s64 = 3;
	// bl 0x821e7120
	ctx.lr = 0x82263370;
	sub_821E7120(ctx, base);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82263384;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// bne cr6,0x822633b4
	if (!ctx.cr6.eq) goto loc_822633B4;
	// lbz r11,115(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 115);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822633ac
	if (!ctx.cr6.eq) goto loc_822633AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821af308
	ctx.lr = 0x822633A0;
	sub_821AF308(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822633b4
	if (ctx.cr6.eq) goto loc_822633B4;
loc_822633AC:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822633b8
	goto loc_822633B8;
loc_822633B4:
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_822633B8:
	// rlwinm r10,r30,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 0) & 0x2;
	// clrlwi r31,r11,24
	ctx.r31.u64 = ctx.r11.u32 & 0xFF;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x822633d4
	if (ctx.cr6.eq) goto loc_822633D4;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// rlwinm r30,r30,0,31,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFD;
	// bl 0x829ff648
	ctx.lr = 0x822633D4;
	sub_829FF648(ctx, base);
loc_822633D4:
	// clrlwi r11,r30,31
	ctx.r11.u64 = ctx.r30.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x822633e8
	if (ctx.cr6.eq) goto loc_822633E8;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x829ff648
	ctx.lr = 0x822633E8;
	sub_829FF648(ctx, base);
loc_822633E8:
	// clrlwi r11,r31,24
	ctx.r11.u64 = ctx.r31.u32 & 0xFF;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822633fc
	if (!ctx.cr6.eq) goto loc_822633FC;
loc_822633F8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822633FC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82263404"))) PPC_WEAK_FUNC(sub_82263404);
PPC_FUNC_IMPL(__imp__sub_82263404) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82263408"))) PPC_WEAK_FUNC(sub_82263408);
PPC_FUNC_IMPL(__imp__sub_82263408) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lbz r11,21(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 21);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82263434
	if (ctx.cr6.eq) goto loc_82263434;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r11,21(r31)
	PPC_STORE_U8(ctx.r31.u32 + 21, ctx.r11.u8);
	// bl 0x82456428
	ctx.lr = 0x82263434;
	sub_82456428(ctx, base);
loc_82263434:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82263450
	ctx.lr = 0x8226343C;
	sub_82263450(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82263450"))) PPC_WEAK_FUNC(sub_82263450);
PPC_FUNC_IMPL(__imp__sub_82263450) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// addi r31,r3,36
	ctx.r31.s64 = ctx.r3.s64 + 36;
	// mr r8,r31
	ctx.r8.u64 = ctx.r31.u64;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
loc_82263474:
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82263478:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8226348c
	if (ctx.cr6.eq) goto loc_8226348C;
	// cmplw cr6,r8,r31
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r31.u32, ctx.xer);
	// beq cr6,0x82263490
	if (ctx.cr6.eq) goto loc_82263490;
loc_8226348C:
	// twi 31,r0,22
loc_82263490:
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226351c
	if (ctx.cr6.eq) goto loc_8226351C;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x822634a4
	if (!ctx.cr6.eq) goto loc_822634A4;
	// twi 31,r0,22
loc_822634A4:
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x822634b4
	if (!ctx.cr6.eq) goto loc_822634B4;
	// twi 31,r0,22
loc_822634B4:
	// lwz r11,12(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// addi r3,r10,8
	ctx.r3.s64 = ctx.r10.s64 + 8;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822634dc
	if (ctx.cr6.eq) goto loc_822634DC;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82263500
	if (ctx.cr6.eq) goto loc_82263500;
	// rotlwi r11,r7,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82263508
	if (!ctx.cr6.eq) goto loc_82263508;
loc_822634DC:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82954458
	ctx.lr = 0x822634EC;
	sub_82954458(ctx, base);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x82263478
	goto loc_82263478;
loc_82263500:
	// bl 0x821940c8
	ctx.lr = 0x82263504;
	sub_821940C8(ctx, base);
	// b 0x822634dc
	goto loc_822634DC;
loc_82263508:
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// bne cr6,0x82263514
	if (!ctx.cr6.eq) goto loc_82263514;
	// twi 31,r0,22
loc_82263514:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// b 0x82263474
	goto loc_82263474;
loc_8226351C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82263530"))) PPC_WEAK_FUNC(sub_82263530);
PPC_FUNC_IMPL(__imp__sub_82263530) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bc0
	ctx.lr = 0x82263538;
	sub_82CA2BC0(ctx, base);
	// stfd f29,-144(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f29.u64);
	// stfd f30,-136(r1)
	PPC_STORE_U64(ctx.r1.u32 + -136, ctx.f30.u64);
	// stfd f31,-128(r1)
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f31.u64);
	// li r12,-192
	ctx.r12.s64 = -192;
	// stvx128 v125,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-176
	ctx.r12.s64 = -176;
	// stvx128 v126,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r12,-160
	ctx.r12.s64 = -160;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-640(r1)
	ea = -640 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r23,128
	ctx.r23.s64 = 128;
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// lwz r28,20(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// lvx128 v0,r28,r23
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// bl 0x82295020
	ctx.lr = 0x82263584;
	sub_82295020(ctx, base);
	// bl 0x82239e88
	ctx.lr = 0x82263588;
	sub_82239E88(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// lfd f31,3552(r10)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r10.u32 + 3552);
	// lfd f30,3824(r9)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r9.u32 + 3824);
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x821fe378
	ctx.lr = 0x822635A4;
	sub_821FE378(ctx, base);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// fmr f2,f31
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f31.f64;
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lis r30,-31950
	ctx.r30.s64 = -2093875200;
	// addi r29,r30,5256
	ctx.r29.s64 = ctx.r30.s64 + 5256;
	// lfd f29,2560(r8)
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r8.u32 + 2560);
	// fmul f13,f1,f29
	ctx.f13.f64 = ctx.f1.f64 * ctx.f29.f64;
	// lfd f0,2552(r7)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 2552);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// fmul f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 * ctx.f0.f64;
	// stfd f0,5256(r30)
	PPC_STORE_U64(ctx.r30.u32 + 5256, ctx.f0.u64);
	// bl 0x821fe378
	ctx.lr = 0x822635D4;
	sub_821FE378(ctx, base);
	// fmul f12,f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = ctx.f1.f64 * ctx.f29.f64;
	// lis r6,-32240
	ctx.r6.s64 = -2112880640;
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lfd f0,2544(r6)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r6.u32 + 2544);
	// fmul f0,f12,f0
	ctx.f0.f64 = ctx.f12.f64 * ctx.f0.f64;
	// stfd f0,8(r29)
	PPC_STORE_U64(ctx.r29.u32 + 8, ctx.f0.u64);
	// bl 0x821fe378
	ctx.lr = 0x822635F4;
	sub_821FE378(ctx, base);
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfd f0,8(r29)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r29.u32 + 8);
	// lfd f13,5256(r30)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r30.u32 + 5256);
	// lis r5,-31950
	ctx.r5.s64 = -2093875200;
	// addi r30,r11,-27852
	ctx.r30.s64 = ctx.r11.s64 + -27852;
	// frsp f11,f0
	ctx.f11.f64 = double(float(ctx.f0.f64));
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// lvlx v13,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// frsp f10,f13
	ctx.f10.f64 = double(float(ctx.f13.f64));
	// addi r3,r5,5280
	ctx.r3.s64 = ctx.r5.s64 + 5280;
	// stfs f10,144(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lfs f31,384(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 384);
	ctx.f31.f64 = double(temp.f32);
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lfd f12,2536(r4)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r4.u32 + 2536);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// lfd f0,2528(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 2528);
	// lfd f13,2520(r10)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r10.u32 + 2520);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// stfd f12,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, ctx.f12.u64);
	// li r24,144
	ctx.r24.s64 = 144;
	// lfs f11,2516(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2516);
	ctx.f11.f64 = double(temp.f32);
	// li r26,160
	ctx.r26.s64 = 160;
	// lfs f10,2512(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 2512);
	ctx.f10.f64 = double(temp.f32);
	// fmul f9,f1,f29
	ctx.f9.f64 = ctx.f1.f64 * ctx.f29.f64;
	// stfd f0,5280(r5)
	PPC_STORE_U64(ctx.r5.u32 + 5280, ctx.f0.u64);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stfd f13,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, ctx.f13.u64);
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lfs f12,11224(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 11224);
	ctx.f12.f64 = double(temp.f32);
	// addi r10,r1,100
	ctx.r10.s64 = ctx.r1.s64 + 100;
	// lfd f0,2504(r7)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 2504);
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// stfs f11,96(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// stfs f10,100(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// stfs f31,92(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// fmul f0,f9,f0
	ctx.f0.f64 = ctx.f9.f64 * ctx.f0.f64;
	// stfd f0,16(r29)
	PPC_STORE_U64(ctx.r29.u32 + 16, ctx.f0.u64);
	// lvx128 v12,r28,r24
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// stvx128 v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r29,r1,80
	ctx.r29.s64 = ctx.r1.s64 + 80;
	// lfs f13,2496(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 2496);
	ctx.f13.f64 = double(temp.f32);
	// frsp f8,f0
	ctx.f8.f64 = double(float(ctx.f0.f64));
	// stfs f8,80(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r27,-32256
	ctx.r27.s64 = -2113929216;
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f7,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// stfs f6,80(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v9,0,r29
	temp.u32 = ctx.r29.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvx128 v8,r28,r26
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v8,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v7,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v6,v9,0
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// lvlx v5,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v7,v13,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvlx v4,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v3,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v4,v5,4,3
	_mm_store_ps(ctx.v4.f32, _mm_blend_ps(_mm_load_ps(ctx.v4.f32), _mm_permute_ps(_mm_load_ps(ctx.v5.f32), 57), 4));
	// lvlx v2,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v3,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v3.f32), 57), 4));
	// vrlimi128 v10,v2,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v2.f32), 57), 4));
	// lvlx v1,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v31,v1,0
	_mm_store_si128((__m128i*)ctx.v31.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), 0xFF));
	// lfd f30,3736(r27)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r27.u32 + 3736);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// vrlimi128 v4,v11,3,2
	_mm_store_ps(ctx.v4.f32, _mm_blend_ps(_mm_load_ps(ctx.v4.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vrlimi128 v7,v10,3,2
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 78), 3));
	// vmulfp128 v126,v4,v31
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v126.f32, _mm_mul_ps(_mm_load_ps(ctx.v4.f32), _mm_load_ps(ctx.v31.f32)));
	// vmulfp128 v125,v7,v6
	_mm_store_ps(ctx.v125.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v6.f32)));
	// bl 0x821fde30
	ctx.lr = 0x82263744;
	sub_821FDE30(ctx, base);
	// lis r4,-31950
	ctx.r4.s64 = -2093875200;
	// lfd f0,5304(r4)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 5304);
	// fmul f5,f1,f0
	ctx.f5.f64 = ctx.f1.f64 * ctx.f0.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// stfs f4,76(r31)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r31.u32 + 76, temp.u32);
	// bl 0x821fde30
	ctx.lr = 0x82263760;
	sub_821FDE30(ctx, base);
	// lis r3,-31950
	ctx.r3.s64 = -2093875200;
	// li r10,240
	ctx.r10.s64 = 240;
	// stfs f31,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lfs f0,396(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lfd f13,5312(r3)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r3.u32 + 5312);
	// addi r29,r11,3008
	ctx.r29.s64 = ctx.r11.s64 + 3008;
	// fmul f3,f1,f13
	ctx.f3.f64 = ctx.f1.f64 * ctx.f13.f64;
	// addi r25,r31,96
	ctx.r25.s64 = ctx.r31.s64 + 96;
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r27,r28,16
	ctx.r27.s64 = ctx.r28.s64 + 16;
	// stfs f0,100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// li r7,304
	ctx.r7.s64 = 304;
	// lvlx v30,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,164
	ctx.r6.s64 = ctx.r1.s64 + 164;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,120
	ctx.r4.s64 = ctx.r1.s64 + 120;
	// li r3,336
	ctx.r3.s64 = 336;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// lis r22,-32256
	ctx.r22.s64 = -2113929216;
	// li r11,32
	ctx.r11.s64 = 32;
	// frsp f2,f3
	ctx.f2.f64 = double(float(ctx.f3.f64));
	// stfs f2,80(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + 80, temp.u32);
	// lvx128 v29,r28,r10
	_mm_store_si128((__m128i*)ctx.v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,16
	ctx.r10.s64 = 16;
	// stvx128 v29,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lfs f13,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,48(r29)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r29.u32 + 48, temp.u32);
	// lvx128 v0,r0,r25
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,3752(r22)
	temp.u32 = PPC_LOAD_U32(ctx.r22.u32 + 3752);
	ctx.f12.f64 = double(temp.f32);
	// addi r20,r1,112
	ctx.r20.s64 = ctx.r1.s64 + 112;
	// stvx128 v126,r29,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r22,r1,160
	ctx.r22.s64 = ctx.r1.s64 + 160;
	// stfs f12,52(r29)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r29.u32 + 52, temp.u32);
	// li r21,64
	ctx.r21.s64 = 64;
	// stvx128 v125,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// stvx128 v0,r29,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32 + ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// lvlx v28,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r19,r1,92
	ctx.r19.s64 = ctx.r1.s64 + 92;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// lvlx128 v127,r0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v127,v28,4,3
	_mm_store_ps(ctx.v127.f32, _mm_blend_ps(_mm_load_ps(ctx.v127.f32), _mm_permute_ps(_mm_load_ps(ctx.v28.f32), 57), 4));
	// lvlx v27,0,r19
	temp.u32 = ctx.r19.u32;
	_mm_store_si128((__m128i*)ctx.v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v26,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v26,v27,4,3
	_mm_store_ps(ctx.v26.f32, _mm_blend_ps(_mm_load_ps(ctx.v26.f32), _mm_permute_ps(_mm_load_ps(ctx.v27.f32), 57), 4));
	// vrlimi128 v127,v26,3,2
	_mm_store_ps(ctx.v127.f32, _mm_blend_ps(_mm_load_ps(ctx.v127.f32), _mm_permute_ps(_mm_load_ps(ctx.v26.f32), 78), 3));
	// lvx128 v25,r27,r7
	_mm_store_si128((__m128i*)ctx.v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r27.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v25,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v23,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stvx128 v25,r0,r22
	_mm_store_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v25,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v24,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v0,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v24,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v24.f32), 57), 4));
	// vrlimi128 v23,v30,4,3
	_mm_store_ps(ctx.v23.f32, _mm_blend_ps(_mm_load_ps(ctx.v23.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// vrlimi128 v0,v23,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v23.f32), 78), 3));
	// stvx128 v0,r29,r21
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32 + ctx.r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v22,r28,r3
	_mm_store_si128((__m128i*)ctx.v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v22,r0,r20
	_mm_store_si128((__m128i*)(base + ((ctx.r20.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v22.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f31
	ctx.cr6.compare(ctx.f13.f64, ctx.f31.f64);
	// ble cr6,0x82263878
	if (!ctx.cr6.gt) goto loc_82263878;
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
loc_82263878:
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// stfs f0,56(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + 56, temp.u32);
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lfs f0,2492(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2492);
	ctx.f0.f64 = double(temp.f32);
	// addi r6,r1,304
	ctx.r6.s64 = ctx.r1.s64 + 304;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r5,r1,320
	ctx.r5.s64 = ctx.r1.s64 + 320;
	// lfs f0,2488(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2488);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lfs f13,2696(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2696);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,60(r29)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r29.u32 + 60, temp.u32);
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v10,v126,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v126.f32), _mm_load_ps(ctx.v13.f32)));
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v9,v125,v11
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v125.f32), _mm_load_ps(ctx.v11.f32)));
	// stvx128 v10,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v9,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82295020
	ctx.lr = 0x822638D8;
	sub_82295020(ctx, base);
	// bl 0x82239e88
	ctx.lr = 0x822638DC;
	sub_82239E88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,336(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 336, temp.u32);
	// bl 0x82295020
	ctx.lr = 0x822638EC;
	sub_82295020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lfs f0,11056(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 11056);
	ctx.f0.f64 = double(temp.f32);
	// lis r3,-32240
	ctx.r3.s64 = -2112880640;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r10,r1,84
	ctx.r10.s64 = ctx.r1.s64 + 84;
	// lis r9,-32240
	ctx.r9.s64 = -2112880640;
	// lfs f11,8988(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8988);
	ctx.f11.f64 = double(temp.f32);
	// lvlx v8,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r8,-31950
	ctx.r8.s64 = -2093875200;
	// lfs f0,2484(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 2484);
	ctx.f0.f64 = double(temp.f32);
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// fmuls f10,f1,f0
	ctx.f10.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// lfs f12,2764(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2764);
	ctx.f12.f64 = double(temp.f32);
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v5,v6,0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), 0xFF));
	// fmsubs f8,f1,f12,f10
	ctx.f8.f64 = double(float(ctx.f1.f64 * ctx.f12.f64 - ctx.f10.f64));
	// lfs f0,5320(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 5320);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,2480(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2480);
	ctx.f13.f64 = double(temp.f32);
	// vmulfp128 v4,v126,v5
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v126.f32), _mm_load_ps(ctx.v5.f32)));
	// fnmsubs f9,f1,f13,f0
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f9.f64 = double(float(-(ctx.f1.f64 * ctx.f13.f64 - ctx.f0.f64)));
	// vspltw v3,v8,0
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v8.u32), 0xFF));
	// vspltisw v7,-1
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_set1_epi32(int(0xFFFFFFFF)));
	// lfd f0,3368(r7)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 3368);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// vslw v2,v7,v7
	ctx.v2.u32[0] = ctx.v7.u32[0] << (ctx.v7.u8[0] & 0x1F);
	ctx.v2.u32[1] = ctx.v7.u32[1] << (ctx.v7.u8[4] & 0x1F);
	ctx.v2.u32[2] = ctx.v7.u32[2] << (ctx.v7.u8[8] & 0x1F);
	ctx.v2.u32[3] = ctx.v7.u32[3] << (ctx.v7.u8[12] & 0x1F);
	// fadds f7,f8,f11
	ctx.f7.f64 = double(float(ctx.f8.f64 + ctx.f11.f64));
	// vmaddfp128 v4,v125,v3,v4
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v4.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v125.f32), _mm_load_ps(ctx.v3.f32)), _mm_load_ps(ctx.v4.f32)));
	// fsubs f6,f9,f7
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f6.f64 = double(float(ctx.f9.f64 - ctx.f7.f64));
	// vxor128 v126,v4,v2
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v2.u8)));
	// stvx128 v126,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmadd f5,f6,f0,f7
	ctx.f5.f64 = ctx.f6.f64 * ctx.f0.f64 + ctx.f7.f64;
	// lfs f1,112(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	ctx.f1.f64 = double(temp.f32);
	// frsp f30,f5
	ctx.f30.f64 = double(float(ctx.f5.f64));
	// bl 0x821fde30
	ctx.lr = 0x8226397C;
	sub_821FDE30(ctx, base);
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// stvx128 v126,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,116(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821fde30
	ctx.lr = 0x8226398C;
	sub_821FDE30(ctx, base);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// stvx128 v126,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,120(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821fde30
	ctx.lr = 0x8226399C;
	sub_821FDE30(ctx, base);
	// lfs f0,8620(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8620);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,27812(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 27812);
	// lwz r8,27816(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 27816);
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// fmuls f4,f30,f30
	ctx.f4.f64 = double(float(ctx.f30.f64 * ctx.f30.f64));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// fmuls f3,f30,f0
	ctx.f3.f64 = double(float(ctx.f30.f64 * ctx.f0.f64));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// stw r3,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r3.u32);
	// lfs f0,396(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 396);
	ctx.f0.f64 = double(temp.f32);
	// stw r8,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r8.u32);
	// fadds f2,f4,f0
	ctx.f2.f64 = double(float(ctx.f4.f64 + ctx.f0.f64));
	// stfs f2,80(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stfs f3,88(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// lvx128 v1,r27,r26
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r27.u32 + ctx.r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// stfs f31,92(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// stfs f31,100(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stvx128 v127,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// stvx128 v127,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// stvx128 v1,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,256
	ctx.r4.s64 = ctx.r1.s64 + 256;
	// stvx128 v1,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r5,176
	ctx.r5.s64 = 176;
	// li r10,208
	ctx.r10.s64 = 208;
	// lvlx v31,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v30,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// addi r6,r1,120
	ctx.r6.s64 = ctx.r1.s64 + 120;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stvx128 v127,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,352
	ctx.r3.s64 = ctx.r1.s64 + 352;
	// lvx128 v28,r27,r5
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r27.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r22,r1,112
	ctx.r22.s64 = ctx.r1.s64 + 112;
	// lvx128 v27,r28,r10
	_mm_store_si128((__m128i*)ctx.v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// stvx128 v28,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,208
	ctx.r7.s64 = ctx.r1.s64 + 208;
	// lvx128 v29,r27,r10
	_mm_store_si128((__m128i*)ctx.v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r27.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,160
	ctx.r28.s64 = ctx.r1.s64 + 160;
	// addi r4,r1,240
	ctx.r4.s64 = ctx.r1.s64 + 240;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r21,r1,192
	ctx.r21.s64 = ctx.r1.s64 + 192;
	// stfs f0,176(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// lfd f13,144(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 144);
	// addi r27,r1,272
	ctx.r27.s64 = ctx.r1.s64 + 272;
	// fsub f1,f13,f4
	ctx.f1.f64 = ctx.f13.f64 - ctx.f4.f64;
	// lvlx v26,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v25,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v25.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,164
	ctx.r11.s64 = ctx.r1.s64 + 164;
	// vrlimi128 v25,v31,4,3
	_mm_store_ps(ctx.v25.f32, _mm_blend_ps(_mm_load_ps(ctx.v25.f32), _mm_permute_ps(_mm_load_ps(ctx.v31.f32), 57), 4));
	// addi r8,r1,196
	ctx.r8.s64 = ctx.r1.s64 + 196;
	// addi r20,r1,288
	ctx.r20.s64 = ctx.r1.s64 + 288;
	// stvx128 v29,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f0,128(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// addi r19,r1,256
	ctx.r19.s64 = ctx.r1.s64 + 256;
	// lvlx v24,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v24.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r18,r1,228
	ctx.r18.s64 = ctx.r1.s64 + 228;
	// lvlx v23,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v23.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvlx v21,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v21.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v24,v30,4,3
	_mm_store_ps(ctx.v24.f32, _mm_blend_ps(_mm_load_ps(ctx.v24.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// lvlx v22,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v22.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,196
	ctx.r11.s64 = ctx.r1.s64 + 196;
	// stvx128 v28,r0,r22
	_mm_store_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,208
	ctx.r8.s64 = ctx.r1.s64 + 208;
	// frsp f13,f1
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v20,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v20.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v20,v26,4,3
	_mm_store_ps(ctx.v20.f32, _mm_blend_ps(_mm_load_ps(ctx.v20.f32), _mm_permute_ps(_mm_load_ps(ctx.v26.f32), 57), 4));
	// vrlimi128 v20,v25,3,2
	_mm_store_ps(ctx.v20.f32, _mm_blend_ps(_mm_load_ps(ctx.v20.f32), _mm_permute_ps(_mm_load_ps(ctx.v25.f32), 78), 3));
	// stvx128 v29,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v1,r0,r28
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// stvx128 v29,r0,r21
	_mm_store_si128((__m128i*)(base + ((ctx.r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r22,r1,168
	ctx.r22.s64 = ctx.r1.s64 + 168;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// stvx128 v27,r0,r27
	_mm_store_si128((__m128i*)(base + ((ctx.r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v20,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v20.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r1,296
	ctx.r28.s64 = ctx.r1.s64 + 296;
	// stvx128 v28,r0,r20
	_mm_store_si128((__m128i*)(base + ((ctx.r20.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r27,r1,100
	ctx.r27.s64 = ctx.r1.s64 + 100;
	// lvlx v15,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v15.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r21,r1,248
	ctx.r21.s64 = ctx.r1.s64 + 248;
	// lvlx128 v62,r0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v62.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v21,v22,4,3
	_mm_store_ps(ctx.v21.f32, _mm_blend_ps(_mm_load_ps(ctx.v21.f32), _mm_permute_ps(_mm_load_ps(ctx.v22.f32), 57), 4));
	// lvlx128 v61,r0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v61.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,384
	ctx.r7.s64 = ctx.r1.s64 + 384;
	// lvlx128 v59,r0,r28
	temp.u32 = ctx.r28.u32;
	_mm_store_si128((__m128i*)ctx.v59.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,400
	ctx.r6.s64 = ctx.r1.s64 + 400;
	// lvlx v19,0,r18
	temp.u32 = ctx.r18.u32;
	_mm_store_si128((__m128i*)ctx.v19.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v59,v61,4,3
	_mm_store_ps(ctx.v59.f32, _mm_blend_ps(_mm_load_ps(ctx.v59.f32), _mm_permute_ps(_mm_load_ps(ctx.v61.f32), 57), 4));
	// lvlx128 v60,r0,r22
	temp.u32 = ctx.r22.u32;
	_mm_store_si128((__m128i*)ctx.v60.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v15,v19,4,3
	_mm_store_ps(ctx.v15.f32, _mm_blend_ps(_mm_load_ps(ctx.v15.f32), _mm_permute_ps(_mm_load_ps(ctx.v19.f32), 57), 4));
	// lvlx v18,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v18.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lvlx v14,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v14.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v60,v62,4,3
	_mm_store_ps(ctx.v60.f32, _mm_blend_ps(_mm_load_ps(ctx.v60.f32), _mm_permute_ps(_mm_load_ps(ctx.v62.f32), 57), 4));
	// lvlx128 v63,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v63.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,368
	ctx.r8.s64 = ctx.r1.s64 + 368;
	// vrlimi128 v63,v14,4,3
	_mm_store_ps(ctx.v63.f32, _mm_blend_ps(_mm_load_ps(ctx.v63.f32), _mm_permute_ps(_mm_load_ps(ctx.v14.f32), 57), 4));
	// lvlx128 v57,r0,r27
	temp.u32 = ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v57.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx128 v56,r0,r21
	temp.u32 = ctx.r21.u32;
	_mm_store_si128((__m128i*)ctx.v56.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v15,v59,3,2
	_mm_store_ps(ctx.v15.f32, _mm_blend_ps(_mm_load_ps(ctx.v15.f32), _mm_permute_ps(_mm_load_ps(ctx.v59.f32), 78), 3));
	// stfs f0,144(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// vrlimi128 v21,v60,3,2
	_mm_store_ps(ctx.v21.f32, _mm_blend_ps(_mm_load_ps(ctx.v21.f32), _mm_permute_ps(_mm_load_ps(ctx.v60.f32), 78), 3));
	// lvlx v0,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v18,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v18.f32), 57), 4));
	// lvlx v17,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v17.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lvlx v16,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v16.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,176
	ctx.r9.s64 = ctx.r1.s64 + 176;
	// vor128 v13,v63,v63
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_load_si128((__m128i*)ctx.v63.u8));
	// lvlx128 v58,r0,r19
	temp.u32 = ctx.r19.u32;
	_mm_store_si128((__m128i*)ctx.v58.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v56,v57,4,3
	_mm_store_ps(ctx.v56.f32, _mm_blend_ps(_mm_load_ps(ctx.v56.f32), _mm_permute_ps(_mm_load_ps(ctx.v57.f32), 57), 4));
	// addi r5,r1,432
	ctx.r5.s64 = ctx.r1.s64 + 432;
	// vrlimi128 v16,v17,4,3
	_mm_store_ps(ctx.v16.f32, _mm_blend_ps(_mm_load_ps(ctx.v16.f32), _mm_permute_ps(_mm_load_ps(ctx.v17.f32), 57), 4));
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// lvlx128 v55,r0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v55.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v58,v23,4,3
	_mm_store_ps(ctx.v58.f32, _mm_blend_ps(_mm_load_ps(ctx.v58.f32), _mm_permute_ps(_mm_load_ps(ctx.v23.f32), 57), 4));
	// lvlx128 v54,r0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v54.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw128 v53,v55,0
	_mm_store_si128((__m128i*)ctx.v53.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v55.u32), 0xFF));
	// vspltw128 v52,v54,0
	_mm_store_si128((__m128i*)ctx.v52.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v54.u32), 0xFF));
	// vrlimi128 v13,v56,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v56.f32), 78), 3));
	// vrlimi128 v0,v16,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v16.f32), 78), 3));
	// lfs f13,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// vrlimi128 v58,v24,3,2
	_mm_store_ps(ctx.v58.f32, _mm_blend_ps(_mm_load_ps(ctx.v58.f32), _mm_permute_ps(_mm_load_ps(ctx.v24.f32), 78), 3));
	// lfs f12,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f12.f64 = double(temp.f32);
	// vmulfp128 v51,v21,v53
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v51.f32, _mm_mul_ps(_mm_load_ps(ctx.v21.f32), _mm_load_ps(ctx.v53.f32)));
	// fadds f11,f12,f13
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// vmulfp128 v50,v15,v52
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v50.f32, _mm_mul_ps(_mm_load_ps(ctx.v15.f32), _mm_load_ps(ctx.v52.f32)));
	// lis r28,-31950
	ctx.r28.s64 = -2093875200;
	// vsubfp v10,v13,v0
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// li r3,112
	ctx.r3.s64 = 112;
	// stvx128 v58,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v58.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f11,416(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 416, temp.u32);
	// stvx128 v13,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r11,-31700(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + -31700);
	// lvx128 v49,r0,r25
	_mm_store_si128((__m128i*)ctx.v49.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r31,r3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v49,r31,r26
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v49.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// stvx128 v11,r31,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v51,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v51.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v50,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v50.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx128 v48,r0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v48.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw128 v12,v48,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v48.u32), 0xFF));
	// vmaddfp v0,v10,v12,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v0,r31,r23
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r23.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r31,r24
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r24.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x82263c6c
	if (!ctx.cr6.eq) goto loc_82263C6C;
	// li r30,0
	ctx.r30.s64 = 0;
loc_82263C28:
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// addi r6,r1,304
	ctx.r6.s64 = ctx.r1.s64 + 304;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821f71f0
	ctx.lr = 0x82263C3C;
	sub_821F71F0(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// bl 0x821fdf10
	ctx.lr = 0x82263C4C;
	sub_821FDF10(ctx, base);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r30,3
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 3, ctx.xer);
	// blt cr6,0x82263c28
	if (ctx.cr6.lt) goto loc_82263C28;
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,-31704(r10)
	PPC_STORE_U32(ctx.r10.u32 + -31704, ctx.r11.u32);
	// b 0x82263c98
	goto loc_82263C98;
loc_82263C6C:
	// lis r10,21845
	ctx.r10.s64 = 1431633920;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// ori r7,r10,21846
	ctx.r7.u64 = ctx.r10.u64 | 21846;
	// lis r6,-31950
	ctx.r6.s64 = -2093875200;
	// mulhw r10,r8,r7
	ctx.r10.s64 = (int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32)) >> 32;
	// stw r11,-31704(r6)
	PPC_STORE_U32(ctx.r6.u32 + -31704, ctx.r11.u32);
	// rlwinm r9,r10,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// subf r4,r5,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r5.s64;
loc_82263C98:
	// stw r4,-31700(r28)
	PPC_STORE_U32(ctx.r28.u32 + -31700, ctx.r4.u32);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// addi r6,r1,304
	ctx.r6.s64 = ctx.r1.s64 + 304;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821f71f0
	ctx.lr = 0x82263CAC;
	sub_821F71F0(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// lwz r3,-31700(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + -31700);
	// lwz r4,20(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// bl 0x821fdf10
	ctx.lr = 0x82263CBC;
	sub_821FDF10(ctx, base);
	// addi r1,r1,640
	ctx.r1.s64 = ctx.r1.s64 + 640;
	// li r0,-192
	ctx.r0.s64 = -192;
	// lvx128 v125,r1,r0
	_mm_store_si128((__m128i*)ctx.v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r0,-176
	ctx.r0.s64 = -176;
	// lvx128 v126,r1,r0
	_mm_store_si128((__m128i*)ctx.v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r0,-160
	ctx.r0.s64 = -160;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f29,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// lfd f30,-136(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -136);
	// lfd f31,-128(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x82ca2c10
	// ERROR 82CA2C10
	return;
}

__attribute__((alias("__imp__sub_82263CE8"))) PPC_WEAK_FUNC(sub_82263CE8);
PPC_FUNC_IMPL(__imp__sub_82263CE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd8
	ctx.lr = 0x82263CF0;
	sub_82CA2BD8(ctx, base);
	// stfd f29,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f29.u64);
	// stfd f30,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f30.u64);
	// stfd f31,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.f31.u64);
	// stwu r1,-384(r1)
	ea = -384 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// addi r31,r11,-32624
	ctx.r31.s64 = ctx.r11.s64 + -32624;
	// addi r8,r10,-9360
	ctx.r8.s64 = ctx.r10.s64 + -9360;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// addi r7,r31,8200
	ctx.r7.s64 = ctx.r31.s64 + 8200;
	// li r26,1
	ctx.r26.s64 = 1;
	// lwz r11,8196(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8196);
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// stb r26,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, ctx.r26.u8);
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// stwx r8,r6,r31
	PPC_STORE_U32(ctx.r6.u32 + ctx.r31.u32, ctx.r8.u32);
	// lwz r10,8196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8196);
	// lwz r9,8192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8192);
	// lwz r11,16396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16396);
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r10,1
	ctx.r11.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,8196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8196, ctx.r11.u32);
	// stw r10,8192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8192, ctx.r10.u32);
	// stwx r8,r5,r7
	PPC_STORE_U32(ctx.r5.u32 + ctx.r7.u32, ctx.r8.u32);
	// lwz r11,16396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16396);
	// lwz r10,16392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16392);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r10,16392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16392, ctx.r10.u32);
	// stw r11,16396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16396, ctx.r11.u32);
	// beq cr6,0x82263d7c
	if (ctx.cr6.eq) goto loc_82263D7C;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x821e13b8
	ctx.lr = 0x82263D7C;
	sub_821E13B8(ctx, base);
loc_82263D7C:
	// bl 0x822421d8
	ctx.lr = 0x82263D80;
	sub_822421D8(ctx, base);
	// li r28,0
	ctx.r28.s64 = 0;
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// stb r28,122(r1)
	PPC_STORE_U8(ctx.r1.u32 + 122, ctx.r28.u8);
	// stb r28,121(r1)
	PPC_STORE_U8(ctx.r1.u32 + 121, ctx.r28.u8);
	// stb r28,120(r1)
	PPC_STORE_U8(ctx.r1.u32 + 120, ctx.r28.u8);
	// stb r28,123(r1)
	PPC_STORE_U8(ctx.r1.u32 + 123, ctx.r28.u8);
	// bl 0x821adf68
	ctx.lr = 0x82263D9C;
	sub_821ADF68(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// addi r5,r10,3056
	ctx.r5.s64 = ctx.r10.s64 + 3056;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// addi r11,r11,-16224
	ctx.r11.s64 = ctx.r11.s64 + -16224;
	// addi r9,r10,28404
	ctx.r9.s64 = ctx.r10.s64 + 28404;
	// lis r6,-31943
	ctx.r6.s64 = -2093416448;
	// lbz r7,141(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 141);
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mulli r9,r7,8200
	ctx.r9.s64 = ctx.r7.s64 * 8200;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// lwz r10,124(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// addi r10,r9,-8200
	ctx.r10.s64 = ctx.r9.s64 + -8200;
	// beq cr6,0x82263e58
	if (ctx.cr6.eq) goto loc_82263E58;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r7,128(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 128);
	// and r4,r9,r7
	ctx.r4.u64 = ctx.r9.u64 & ctx.r7.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x82263e24
	if (!ctx.cr6.eq) goto loc_82263E24;
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 | ctx.r7.u64;
	// addi r3,r11,120
	ctx.r3.s64 = ctx.r11.s64 + 120;
	// rlwinm r7,r4,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,128(r11)
	PPC_STORE_U32(ctx.r11.u32 + 128, ctx.r9.u32);
	// stwx r3,r7,r10
	PPC_STORE_U32(ctx.r7.u32 + ctx.r10.u32, ctx.r3.u32);
	// lwz r9,124(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r7,r4,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r7,r10
	ctx.r3.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r9.u32);
loc_82263E24:
	// lbz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 140);
	// stw r8,124(r11)
	PPC_STORE_U32(ctx.r11.u32 + 124, ctx.r8.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82263e58
	if (!ctx.cr6.eq) goto loc_82263E58;
	// lwz r10,1000(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1000);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// addi r8,r11,120
	ctx.r8.s64 = ctx.r11.s64 + 120;
	// rlwinm r4,r10,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r9,140(r11)
	PPC_STORE_U8(ctx.r11.u32 + 140, ctx.r9.u8);
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// stw r7,1000(r6)
	PPC_STORE_U32(ctx.r6.u32 + 1000, ctx.r7.u32);
	// stwx r8,r4,r5
	PPC_STORE_U32(ctx.r4.u32 + ctx.r5.u32, ctx.r8.u32);
	// b 0x82263e5c
	goto loc_82263E5C;
loc_82263E58:
	// lwz r7,1000(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + 1000);
loc_82263E5C:
	// lbz r10,69(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 69);
	// lwz r9,52(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82263ef4
	if (ctx.cr6.eq) goto loc_82263EF4;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,56(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	// and r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 & ctx.r8.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x82263ec4
	if (!ctx.cr6.eq) goto loc_82263EC4;
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// addi r3,r11,48
	ctx.r3.s64 = ctx.r11.s64 + 48;
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r9.u32);
	// stwx r3,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r3.u32);
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,52(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// add r3,r8,r10
	ctx.r3.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r9.u32);
loc_82263EC4:
	// lbz r10,68(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 68);
	// mr r9,r28
	ctx.r9.u64 = ctx.r28.u64;
	// stw r9,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r9.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82263ef4
	if (!ctx.cr6.eq) goto loc_82263EF4;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// addi r8,r11,48
	ctx.r8.s64 = ctx.r11.s64 + 48;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// stb r10,68(r11)
	PPC_STORE_U8(ctx.r11.u32 + 68, ctx.r10.u8);
	// stw r7,1000(r6)
	PPC_STORE_U32(ctx.r6.u32 + 1000, ctx.r7.u32);
	// stwx r8,r9,r5
	PPC_STORE_U32(ctx.r9.u32 + ctx.r5.u32, ctx.r8.u32);
loc_82263EF4:
	// lbz r10,93(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 93);
	// lwz r9,76(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x82263f8c
	if (ctx.cr6.eq) goto loc_82263F8C;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,80(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	// and r4,r9,r8
	ctx.r4.u64 = ctx.r9.u64 & ctx.r8.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x82263f5c
	if (!ctx.cr6.eq) goto loc_82263F5C;
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// addi r3,r11,72
	ctx.r3.s64 = ctx.r11.s64 + 72;
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,80(r11)
	PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r9.u32);
	// stwx r3,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r3.u32);
	// lwz r9,76(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// lwz r4,8196(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r8,r4,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r8,r10
	ctx.r3.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r9.u32);
loc_82263F5C:
	// lbz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 92);
	// mr r9,r28
	ctx.r9.u64 = ctx.r28.u64;
	// stw r9,76(r11)
	PPC_STORE_U32(ctx.r11.u32 + 76, ctx.r9.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82263f8c
	if (!ctx.cr6.eq) goto loc_82263F8C;
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// addi r4,r11,72
	ctx.r4.s64 = ctx.r11.s64 + 72;
	// addi r9,r7,1
	ctx.r9.s64 = ctx.r7.s64 + 1;
	// stb r10,92(r11)
	PPC_STORE_U8(ctx.r11.u32 + 92, ctx.r10.u8);
	// stw r9,1000(r6)
	PPC_STORE_U32(ctx.r6.u32 + 1000, ctx.r9.u32);
	// stwx r4,r8,r5
	PPC_STORE_U32(ctx.r8.u32 + ctx.r5.u32, ctx.r4.u32);
loc_82263F8C:
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lis r30,-31946
	ctx.r30.s64 = -2093613056;
	// addi r10,r11,-9176
	ctx.r10.s64 = ctx.r11.s64 + -9176;
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// li r3,130
	ctx.r3.s64 = 130;
	// lwz r11,412(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 412);
	// lwz r10,60(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 60);
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// stw r10,11820(r11)
	PPC_STORE_U32(ctx.r11.u32 + 11820, ctx.r10.u32);
	// stw r10,28388(r9)
	PPC_STORE_U32(ctx.r9.u32 + 28388, ctx.r10.u32);
	// ld r7,16(r8)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + 16);
	// oris r6,r7,8
	ctx.r6.u64 = ctx.r7.u64 | 524288;
	// std r6,16(r8)
	PPC_STORE_U64(ctx.r8.u32 + 16, ctx.r6.u64);
	// bl 0x8222c268
	ctx.lr = 0x82263FC4;
	sub_8222C268(ctx, base);
	// li r3,131
	ctx.r3.s64 = 131;
	// bl 0x82208c48
	ctx.lr = 0x82263FCC;
	sub_82208C48(ctx, base);
	// lis r5,-31924
	ctx.r5.s64 = -2092171264;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r5,-9280
	ctx.r3.s64 = ctx.r5.s64 + -9280;
	// rldicr r29,r4,63,63
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r4.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r31,3024(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3024);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt cr6,0x82264008
	if (ctx.cr6.lt) goto loc_82264008;
	// addi r11,r31,32
	ctx.r11.s64 = ctx.r31.s64 + 32;
	// lwz r3,412(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 412);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r5,4(r25)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r25.u32 + 4);
	// clrldi r10,r11,32
	ctx.r10.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r6,r29,r10
	ctx.r6.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r29.u64 >> (ctx.r10.u8 & 0x7F));
	// bl 0x821b7020
	ctx.lr = 0x82264008;
	sub_821B7020(ctx, base);
loc_82264008:
	// addi r5,r1,256
	ctx.r5.s64 = ctx.r1.s64 + 256;
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x821faaa0
	ctx.lr = 0x82264018;
	sub_821FAAA0(ctx, base);
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// bl 0x821faaa0
	ctx.lr = 0x82264028;
	sub_821FAAA0(ctx, base);
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r8,248(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// lis r25,-32246
	ctx.r25.s64 = -2113273856;
	// li r3,67
	ctx.r3.s64 = 67;
	// addi r24,r25,-27468
	ctx.r24.s64 = ctx.r25.s64 + -27468;
	// std r10,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r10.u64);
	// std r8,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r8.u64);
	// lfd f10,136(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// lfd f0,120(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfs f31,12(r24)
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 12);
	ctx.f31.f64 = double(temp.f32);
	// fdivs f7,f31,f8
	ctx.f7.f64 = double(float(ctx.f31.f64 / ctx.f8.f64));
	// stfs f7,120(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// fdivs f11,f31,f12
	ctx.f11.f64 = double(float(ctx.f31.f64 / ctx.f12.f64));
	// stfs f11,124(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// ld r4,120(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// bl 0x82205a50
	ctx.lr = 0x82264078;
	sub_82205A50(ctx, base);
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x821faaa0
	ctx.lr = 0x82264088;
	sub_821FAAA0(ctx, base);
	// lwz r6,248(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	// lfs f0,18768(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 18768);
	ctx.f0.f64 = double(temp.f32);
	// addi r5,r1,256
	ctx.r5.s64 = ctx.r1.s64 + 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// std r6,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r6.u64);
	// lfd f6,136(r1)
	ctx.f6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f5,f6
	ctx.f5.f64 = double(ctx.f6.s64);
	// frsp f4,f5
	ctx.f4.f64 = double(float(ctx.f5.f64));
	// fdivs f29,f0,f4
	ctx.f29.f64 = double(float(ctx.f0.f64 / ctx.f4.f64));
	// bl 0x821faaa0
	ctx.lr = 0x822640B4;
	sub_821FAAA0(ctx, base);
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r11,3
	ctx.r11.s64 = 3;
	// lfs f30,-27468(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + -27468);
	ctx.f30.f64 = double(temp.f32);
	// fsubs f3,f29,f31
	ctx.f3.f64 = double(float(ctx.f29.f64 - ctx.f31.f64));
	// sth r5,128(r1)
	PPC_STORE_U16(ctx.r1.u32 + 128, ctx.r5.u16);
	// fadds f2,f29,f31
	ctx.f2.f64 = double(float(ctx.f29.f64 + ctx.f31.f64));
	// stfs f30,152(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// stfs f30,172(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// sth r28,120(r1)
	PPC_STORE_U16(ctx.r1.u32 + 120, ctx.r28.u16);
	// std r10,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, ctx.r10.u64);
	// lfd f1,136(r1)
	ctx.f1.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// fcfid f0,f1
	ctx.f0.f64 = double(ctx.f1.s64);
	// stfs f30,192(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 192, temp.u32);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// stfs f30,212(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 212, temp.u32);
	// stfs f30,156(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 156, temp.u32);
	// sth r26,122(r1)
	PPC_STORE_U16(ctx.r1.u32 + 122, ctx.r26.u16);
	// stfs f30,160(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 160, temp.u32);
	// sth r11,124(r1)
	PPC_STORE_U16(ctx.r1.u32 + 124, ctx.r11.u16);
	// stfs f31,176(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 176, temp.u32);
	// sth r28,126(r1)
	PPC_STORE_U16(ctx.r1.u32 + 126, ctx.r28.u16);
	// stfs f30,180(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 180, temp.u32);
	// sth r11,130(r1)
	PPC_STORE_U16(ctx.r1.u32 + 130, ctx.r11.u16);
	// stfs f3,144(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// li r7,20
	ctx.r7.s64 = 20;
	// stfs f2,164(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 164, temp.u32);
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// stfs f3,184(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 184, temp.u32);
	// addi r5,r1,120
	ctx.r5.s64 = ctx.r1.s64 + 120;
	// stfs f2,204(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 204, temp.u32);
	// li r4,2
	ctx.r4.s64 = 2;
	// stfs f30,196(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 196, temp.u32);
	// li r3,4
	ctx.r3.s64 = 4;
	// stfs f31,200(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 200, temp.u32);
	// fdivs f12,f31,f13
	ctx.f12.f64 = double(float(ctx.f31.f64 / ctx.f13.f64));
	// stfs f31,216(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 216, temp.u32);
	// stfs f31,220(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 220, temp.u32);
	// fadds f11,f12,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f31.f64));
	// stfs f11,148(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// fsubs f10,f12,f31
	ctx.f10.f64 = double(float(ctx.f12.f64 - ctx.f31.f64));
	// stfs f11,168(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 168, temp.u32);
	// stfs f10,188(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 188, temp.u32);
	// stfs f10,208(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 208, temp.u32);
	// bl 0x8220a528
	ctx.lr = 0x82264168;
	sub_8220A528(ctx, base);
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822641a8
	if (ctx.cr6.eq) goto loc_822641A8;
	// bl 0x82232298
	ctx.lr = 0x82264178;
	sub_82232298(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r3,412(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 412);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r6,4(r27)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// li r7,0
	ctx.r7.s64 = 0;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r28.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r28.u32);
	// bl 0x822069c0
	ctx.lr = 0x822641A8;
	sub_822069C0(ctx, base);
loc_822641A8:
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt cr6,0x822641d4
	if (ctx.cr6.lt) goto loc_822641D4;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lwz r3,412(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 412);
	// addi r10,r31,32
	ctx.r10.s64 = ctx.r31.s64 + 32;
	// addi r9,r11,-20628
	ctx.r9.s64 = ctx.r11.s64 + -20628;
	// clrldi r8,r10,32
	ctx.r8.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// srd r6,r29,r8
	ctx.r6.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r29.u64 >> (ctx.r8.u8 & 0x7F));
	// lwz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// bl 0x821b7020
	ctx.lr = 0x822641D4;
	sub_821B7020(ctx, base);
loc_822641D4:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8221f410
	ctx.lr = 0x822641DC;
	sub_8221F410(ctx, base);
	// addi r1,r1,384
	ctx.r1.s64 = ctx.r1.s64 + 384;
	// lfd f29,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f30,-88(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// lfd f31,-80(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// b 0x82ca2c28
	// ERROR 82CA2C28
	return;
}

__attribute__((alias("__imp__sub_822641F0"))) PPC_WEAK_FUNC(sub_822641F0);
PPC_FUNC_IMPL(__imp__sub_822641F0) {
	PPC_FUNC_PROLOGUE();
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r11,12
	ctx.r11.s64 = 12;
	// lwz r6,12(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// li r10,0
	ctx.r10.s64 = 0;
	// subf r8,r9,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r9.s64;
	// stw r10,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r10.u32);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// divw. r11,r8,r11
	ctx.r11.s32 = ctx.r8.s32 / ctx.r11.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226426c
	if (!ctx.cr0.gt) goto loc_8226426C;
loc_82264214:
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpw cr6,r7,r4
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r4.s32, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226423c
	if (ctx.cr6.lt) goto loc_8226423C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8226423C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82264258
	if (ctx.cr6.eq) goto loc_82264258;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82264260
	goto loc_82264260;
loc_82264258:
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_82264260:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82264214
	if (ctx.cr6.gt) goto loc_82264214;
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
loc_8226426C:
	// cmplw cr6,r9,r6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x822642a4
	if (ctx.cr6.eq) goto loc_822642A4;
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpw cr6,r4,r11
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r11.s32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// blt cr6,0x82264288
	if (ctx.cr6.lt) goto loc_82264288;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82264288:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822642a4
	if (!ctx.cr6.eq) goto loc_822642A4;
	// ld r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r11.u64);
	// lwz r11,-12(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// b 0x822642a8
	goto loc_822642A8;
loc_822642A4:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_822642A8:
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x822642bc
	if (ctx.cr6.eq) goto loc_822642BC;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// blr 
	return;
loc_822642BC:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822642C4"))) PPC_WEAK_FUNC(sub_822642C4);
PPC_FUNC_IMPL(__imp__sub_822642C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822642C8"))) PPC_WEAK_FUNC(sub_822642C8);
PPC_FUNC_IMPL(__imp__sub_822642C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x822642D0;
	sub_82CA2BE8(ctx, base);
	// li r12,-64
	ctx.r12.s64 = -64;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// vor128 v127,v1,v1
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// li r28,12
	ctx.r28.s64 = 12;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r31,r29,68
	ctx.r31.s64 = ctx.r29.s64 + 68;
	// lwz r11,76(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 76);
	// lwz r10,72(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 72);
	// subf r9,r10,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r10.s64;
	// divw. r8,r9,r28
	ctx.r8.s32 = ctx.r9.s32 / ctx.r28.s32;
	ctx.cr0.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq 0x82264398
	if (ctx.cr0.eq) goto loc_82264398;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264354
	if (!ctx.cr6.eq) goto loc_82264354;
	// li r3,64
	ctx.r3.s64 = 64;
	// bl 0x8221f388
	ctx.lr = 0x82264318;
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226434c
	if (ctx.cr6.eq) goto loc_8226434C;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r10,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r10.u32);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// stw r9,48(r3)
	PPC_STORE_U32(ctx.r3.u32 + 48, ctx.r9.u32);
	// li r10,6
	ctx.r10.s64 = 6;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_8226433C:
	// std r9,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x8226433c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_8226433C;
	// b 0x82264350
	goto loc_82264350;
loc_8226434C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82264350:
	// stw r3,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r3.u32);
loc_82264354:
	// li r11,224
	ctx.r11.s64 = 224;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// stvx128 v127,r29,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32 + ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// subf r8,r9,r10
	ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
	// divw. r7,r8,r28
	ctx.r7.s32 = ctx.r8.s32 / ctx.r28.s32;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// beq 0x8226437c
	if (ctx.cr0.eq) goto loc_8226437C;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// bl 0x821f3e10
	ctx.lr = 0x8226437C;
	sub_821F3E10(ctx, base);
loc_8226437C:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// li r10,30
	ctx.r10.s64 = 30;
	// stw r10,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r10.u32);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
	// stw r9,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r9.u32);
loc_82264398:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_822643A8"))) PPC_WEAK_FUNC(sub_822643A8);
PPC_FUNC_IMPL(__imp__sub_822643A8) {
	PPC_FUNC_PROLOGUE();
	// lbz r3,384(r3)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r3.u32 + 384);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822643B0"))) PPC_WEAK_FUNC(sub_822643B0);
PPC_FUNC_IMPL(__imp__sub_822643B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lbz r10,-29012(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -29012);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822643e4
	if (!ctx.cr6.eq) goto loc_822643E4;
loc_822643DC:
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x8226442c
	goto loc_8226442C;
loc_822643E4:
	// lwz r3,88(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x82264400
	if (ctx.cr6.eq) goto loc_82264400;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82264400;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82264400:
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226441c
	if (ctx.cr6.eq) goto loc_8226441C;
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplw cr6,r10,r30
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r30.u32, ctx.xer);
	// bge cr6,0x822643dc
	if (!ctx.cr6.lt) goto loc_822643DC;
loc_8226441C:
	// lwz r11,104(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 104);
	// subfc r10,r11,r30
	ctx.xer.ca = ctx.r30.u32 >= ctx.r11.u32;
	ctx.r10.s64 = ctx.r30.s64 - ctx.r11.s64;
	// subfe r11,r10,r10
	temp.u8 = (~ctx.r10.u32 + ctx.r10.u32 < ~ctx.r10.u32) | (~ctx.r10.u32 + ctx.r10.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r10.u64 + ctx.r10.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r3,r11,1
	ctx.r3.s64 = ctx.r11.s64 + 1;
loc_8226442C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82264444"))) PPC_WEAK_FUNC(sub_82264444);
PPC_FUNC_IMPL(__imp__sub_82264444) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82264448"))) PPC_WEAK_FUNC(sub_82264448);
PPC_FUNC_IMPL(__imp__sub_82264448) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,136
	ctx.r31.s64 = ctx.r3.s64 + 136;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82233348
	ctx.lr = 0x8226446C;
	sub_82233348(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// bne cr6,0x8226447c
	if (!ctx.cr6.eq) goto loc_8226447C;
	// twi 31,r0,22
loc_8226447C:
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r31.u32);
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822644d0
	if (ctx.cr6.eq) goto loc_822644D0;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// rldicr r7,r10,32,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// lwz r6,16(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// rldicr r5,r9,32,63
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u64, 32) & 0xFFFFFFFFFFFFFFFF;
	// or r4,r7,r8
	ctx.r4.u64 = ctx.r7.u64 | ctx.r8.u64;
	// or r3,r6,r5
	ctx.r3.u64 = ctx.r6.u64 | ctx.r5.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmpld cr6,r4,r3
	ctx.cr6.compare<uint64_t>(ctx.r4.u64, ctx.r3.u64, ctx.xer);
	// blt cr6,0x822644bc
	if (ctx.cr6.lt) goto loc_822644BC;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822644BC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822644d0
	if (!ctx.cr6.eq) goto loc_822644D0;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// b 0x822644dc
	goto loc_822644DC;
loc_822644D0:
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// addi r11,r1,88
	ctx.r11.s64 = ctx.r1.s64 + 88;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r31.u32);
loc_822644DC:
	// ld r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822644fc
	if (ctx.cr6.eq) goto loc_822644FC;
	// cmplw cr6,r11,r31
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r31.u32, ctx.xer);
	// beq cr6,0x82264500
	if (ctx.cr6.eq) goto loc_82264500;
loc_822644FC:
	// twi 31,r0,22
loc_82264500:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x82264530
	if (ctx.cr6.eq) goto loc_82264530;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264518
	if (!ctx.cr6.eq) goto loc_82264518;
	// twi 31,r0,22
loc_82264518:
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82264528
	if (!ctx.cr6.eq) goto loc_82264528;
	// twi 31,r0,22
loc_82264528:
	// lwz r3,20(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// b 0x82264534
	goto loc_82264534;
loc_82264530:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82264534:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226454C"))) PPC_WEAK_FUNC(sub_8226454C);
PPC_FUNC_IMPL(__imp__sub_8226454C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82264550"))) PPC_WEAK_FUNC(sub_82264550);
PPC_FUNC_IMPL(__imp__sub_82264550) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,12456(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12456, ctx.r30.u32);
	// beq cr6,0x82264694
	if (ctx.cr6.eq) goto loc_82264694;
	// lwz r11,12440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12440);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264588
	if (!ctx.cr6.eq) goto loc_82264588;
	// bl 0x82286850
	ctx.lr = 0x82264588;
	sub_82286850(ctx, base);
loc_82264588:
	// lwz r11,28(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 28);
	// stw r11,10376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10376, ctx.r11.u32);
	// lbz r10,10942(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10942);
	// lbz r9,10943(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10943);
	// rlwinm. r9,r9,0,26,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lwz r11,32(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 32);
	// stw r11,10560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10560, ctx.r11.u32);
	// lwz r11,32(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 32);
	// rlwimi r10,r11,5,26,26
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 5) & 0x20) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFDF);
	// stb r10,10942(r31)
	PPC_STORE_U8(ctx.r31.u32 + 10942, ctx.r10.u8);
	// beq 0x82264670
	if (ctx.cr0.eq) goto loc_82264670;
	// lbz r11,10940(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10940);
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822645c8
	if (ctx.cr0.eq) goto loc_822645C8;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82264658
	goto loc_82264658;
loc_822645C8:
	// rlwinm. r11,r11,0,26,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82264650
	if (ctx.cr0.eq) goto loc_82264650;
	// lwz r11,12440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12440);
	// lwz r10,12728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12728);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822645e8
	if (ctx.cr6.eq) goto loc_822645E8;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264650
	if (!ctx.cr6.eq) goto loc_82264650;
loc_822645E8:
	// lwz r11,12444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12444);
	// lwz r10,12732(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12732);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264600
	if (ctx.cr6.eq) goto loc_82264600;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264650
	if (!ctx.cr6.eq) goto loc_82264650;
loc_82264600:
	// lwz r11,12448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12448);
	// lwz r10,12736(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12736);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264618
	if (ctx.cr6.eq) goto loc_82264618;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264650
	if (!ctx.cr6.eq) goto loc_82264650;
loc_82264618:
	// lwz r11,12452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12452);
	// lwz r10,12740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12740);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264630
	if (ctx.cr6.eq) goto loc_82264630;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264650
	if (!ctx.cr6.eq) goto loc_82264650;
loc_82264630:
	// lwz r11,12456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12456);
	// lwz r10,12744(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12744);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264648
	if (ctx.cr6.eq) goto loc_82264648;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264650
	if (!ctx.cr6.eq) goto loc_82264650;
loc_82264648:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82264654
	goto loc_82264654;
loc_82264650:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82264654:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
loc_82264658:
	// clrlwi. r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82264670
	if (ctx.cr0.eq) goto loc_82264670;
	// lwz r11,10368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10368);
	// lwz r10,13172(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 13172);
	// rlwimi r11,r10,18,0,13
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 18) & 0xFFFC0000) | (ctx.r11.u64 & 0xFFFFFFFF0003FFFF);
	// stw r11,10368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10368, ctx.r11.u32);
loc_82264670:
	// li r12,1
	ctx.r12.s64 = 1;
	// ld r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 16);
	// rldicr r12,r12,55,63
	ctx.r12.u64 = __builtin_rotateleft64(ctx.r12.u64, 55) & 0xFFFFFFFFFFFFFFFF;
	// or r11,r11,r12
	ctx.r11.u64 = ctx.r11.u64 | ctx.r12.u64;
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// ori r11,r11,256
	ctx.r11.u64 = ctx.r11.u64 | 256;
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// oris r11,r11,2
	ctx.r11.u64 = ctx.r11.u64 | 131072;
	// b 0x822646b4
	goto loc_822646B4;
loc_82264694:
	// lwz r11,10560(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10560);
	// lbz r10,10942(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10942);
	// rlwinm r11,r11,0,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// andi. r10,r10,223
	ctx.r10.u64 = ctx.r10.u64 & 223;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,10560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10560, ctx.r11.u32);
	// stb r10,10942(r31)
	PPC_STORE_U8(ctx.r31.u32 + 10942, ctx.r10.u8);
	// ld r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 16);
	// ori r11,r11,256
	ctx.r11.u64 = ctx.r11.u64 | 256;
loc_822646B4:
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// lwz r11,12716(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12716);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264790
	if (!ctx.cr6.eq) goto loc_82264790;
	// lbz r11,10940(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10940);
	// rlwinm. r10,r11,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x8;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne 0x82264790
	if (!ctx.cr0.eq) goto loc_82264790;
	// rlwinm. r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne 0x82264790
	if (!ctx.cr0.eq) goto loc_82264790;
	// lbz r10,12187(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 12187);
	// cmplwi r10,0
	ctx.cr0.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne 0x82264790
	if (!ctx.cr0.eq) goto loc_82264790;
	// rlwinm. r10,r11,0,27,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x10;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x822646f4
	if (ctx.cr0.eq) goto loc_822646F4;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82264784
	goto loc_82264784;
loc_822646F4:
	// rlwinm. r11,r11,0,26,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8226477c
	if (ctx.cr0.eq) goto loc_8226477C;
	// lwz r11,12440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12440);
	// lwz r10,12728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12728);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264714
	if (ctx.cr6.eq) goto loc_82264714;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226477c
	if (!ctx.cr6.eq) goto loc_8226477C;
loc_82264714:
	// lwz r11,12444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12444);
	// lwz r10,12732(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12732);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226472c
	if (ctx.cr6.eq) goto loc_8226472C;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226477c
	if (!ctx.cr6.eq) goto loc_8226477C;
loc_8226472C:
	// lwz r11,12448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12448);
	// lwz r10,12736(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12736);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264744
	if (ctx.cr6.eq) goto loc_82264744;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226477c
	if (!ctx.cr6.eq) goto loc_8226477C;
loc_82264744:
	// lwz r11,12452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12452);
	// lwz r10,12740(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12740);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226475c
	if (ctx.cr6.eq) goto loc_8226475C;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226477c
	if (!ctx.cr6.eq) goto loc_8226477C;
loc_8226475C:
	// lwz r11,12456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12456);
	// lwz r10,12744(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12744);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82264774
	if (ctx.cr6.eq) goto loc_82264774;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226477c
	if (!ctx.cr6.eq) goto loc_8226477C;
loc_82264774:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82264780
	goto loc_82264780;
loc_8226477C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82264780:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
loc_82264784:
	// clrlwi. r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bne 0x82264794
	if (!ctx.cr0.eq) goto loc_82264794;
loc_82264790:
	// li r10,0
	ctx.r10.s64 = 0;
loc_82264794:
	// lbz r9,10940(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10940);
	// lwz r11,11876(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 11876);
	// rlwimi r9,r10,0,31,31
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x1) | (ctx.r9.u64 & 0xFFFFFFFFFFFFFFFE);
	// stb r9,10940(r31)
	PPC_STORE_U8(ctx.r31.u32 + 10940, ctx.r9.u8);
	// lwz r10,12456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12456);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r11,11876(r31)
	PPC_STORE_U32(ctx.r31.u32 + 11876, ctx.r11.u32);
	// bne cr6,0x822647b8
	if (!ctx.cr6.eq) goto loc_822647B8;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822647B8:
	// lwz r10,10548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10548);
	// rlwimi r10,r11,1,30,30
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 1) & 0x2) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFFD);
	// stw r10,10548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10548, ctx.r10.u32);
	// ld r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 16);
	// ori r11,r11,2048
	ctx.r11.u64 = ctx.r11.u64 | 2048;
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// oris r11,r11,2
	ctx.r11.u64 = ctx.r11.u64 | 131072;
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// lwz r10,12456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12456);
	// lwz r11,11880(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 11880);
	// stw r11,11880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 11880, ctx.r11.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822647f0
	if (!ctx.cr6.eq) goto loc_822647F0;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822647F0:
	// lwz r10,10548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 10548);
	// rlwimi r11,r10,0,0,30
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFFFFFFFE) | (ctx.r11.u64 & 0xFFFFFFFF00000001);
	// stw r11,10548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10548, ctx.r11.u32);
	// ld r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 16);
	// ori r11,r11,2048
	ctx.r11.u64 = ctx.r11.u64 | 2048;
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// oris r11,r11,2
	ctx.r11.u64 = ctx.r11.u64 | 131072;
	// std r11,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r11.u64);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82264828"))) PPC_WEAK_FUNC(sub_82264828);
PPC_FUNC_IMPL(__imp__sub_82264828) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82264830;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r10,1648
	ctx.r10.s64 = 1648;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// lfs f0,-27468(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -27468);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bne cr6,0x82264ce0
	if (!ctx.cr6.eq) goto loc_82264CE0;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lbz r11,27854(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 27854);
	// extsb r10,r11
	ctx.r10.s64 = ctx.r11.s8;
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bge cr6,0x82264ce0
	if (!ctx.cr6.lt) goto loc_82264CE0;
	// li r3,153
	ctx.r3.s64 = 153;
	// bl 0x8222c268
	ctx.lr = 0x82264880;
	sub_8222C268(ctx, base);
	// bl 0x822209a0
	ctx.lr = 0x82264884;
	sub_822209A0(ctx, base);
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// ld r11,5528(r11)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r11.u32 + 5528);
	// rldicr r10,r11,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u64, 0) & 0x8000000000000000;
	// cmpldi cr6,r10,0
	ctx.cr6.compare<uint64_t>(ctx.r10.u64, 0, ctx.xer);
	// beq cr6,0x822648b8
	if (ctx.cr6.eq) goto loc_822648B8;
	// lis r11,-31946
	ctx.r11.s64 = -2093613056;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,4
	ctx.r6.s64 = 4;
	// rldicr r7,r7,63,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// addi r5,r30,1712
	ctx.r5.s64 = ctx.r30.s64 + 1712;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 412);
	// bl 0x8221a9b0
	ctx.lr = 0x822648B8;
	sub_8221A9B0(ctx, base);
loc_822648B8:
	// li r3,412
	ctx.r3.s64 = 412;
	// lfs f1,76(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82223b18
	ctx.lr = 0x822648C4;
	sub_82223B18(ctx, base);
	// li r10,1
	ctx.r10.s64 = 1;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// stb r10,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r10.u8);
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// addi r30,r11,-32624
	ctx.r30.s64 = ctx.r11.s64 + -32624;
	// addi r9,r10,-9360
	ctx.r9.s64 = ctx.r10.s64 + -9360;
	// addi r11,r30,8196
	ctx.r11.s64 = ctx.r30.s64 + 8196;
loc_822648E0:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r8,r30,24596
	ctx.r8.s64 = ctx.r30.s64 + 24596;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r9,-8196(r7)
	PPC_STORE_U32(ctx.r7.u32 + -8196, ctx.r9.u32);
	// lwz r5,-4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r6,r10,1
	ctx.r6.s64 = ctx.r10.s64 + 1;
	// rlwinm r4,r5,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// stw r4,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r4.u32);
	// addi r11,r11,8200
	ctx.r11.s64 = ctx.r11.s64 + 8200;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x822648e0
	if (ctx.cr6.lt) goto loc_822648E0;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// addi r31,r11,-16224
	ctx.r31.s64 = ctx.r11.s64 + -16224;
	// addi r9,r10,28404
	ctx.r9.s64 = ctx.r10.s64 + 28404;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lis r29,-31943
	ctx.r29.s64 = -2093416448;
	// addi r28,r10,3056
	ctx.r28.s64 = ctx.r10.s64 + 3056;
	// lbz r7,141(r31)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r31.u32 + 141);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mulli r11,r7,8200
	ctx.r11.s64 = ctx.r7.s64 * 8200;
	// lwz r10,124(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// cmplw cr6,r10,r8
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, ctx.xer);
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x822649d4
	if (ctx.cr6.eq) goto loc_822649D4;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,128(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 128);
	// and r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x822649a0
	if (!ctx.cr6.eq) goto loc_822649A0;
	// lwz r7,8196(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r31,120
	ctx.r6.s64 = ctx.r31.s64 + 120;
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 128, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,124(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 124);
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_822649A0:
	// lbz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 140);
	// stw r8,124(r31)
	PPC_STORE_U32(ctx.r31.u32 + 124, ctx.r8.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822649d4
	if (!ctx.cr6.eq) goto loc_822649D4;
	// lwz r11,1000(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r9,r31,120
	ctx.r9.s64 = ctx.r31.s64 + 120;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r10,140(r31)
	PPC_STORE_U8(ctx.r31.u32 + 140, ctx.r10.u8);
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// stw r8,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r8.u32);
	// stwx r9,r7,r28
	PPC_STORE_U32(ctx.r7.u32 + ctx.r28.u32, ctx.r9.u32);
	// b 0x822649d8
	goto loc_822649D8;
loc_822649D4:
	// lwz r8,1000(r29)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
loc_822649D8:
	// lbz r11,69(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 69);
	// lwz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	// mulli r11,r11,8200
	ctx.r11.s64 = ctx.r11.s64 * 8200;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x82264a70
	if (ctx.cr6.eq) goto loc_82264A70;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,56(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	// and r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x82264a40
	if (!ctx.cr6.eq) goto loc_82264A40;
	// lwz r7,8196(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r31,48
	ctx.r6.s64 = ctx.r31.s64 + 48;
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,56(r31)
	PPC_STORE_U32(ctx.r31.u32 + 56, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r10,52(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_82264A40:
	// lbz r11,68(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 68);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,52(r31)
	PPC_STORE_U32(ctx.r31.u32 + 52, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264a70
	if (!ctx.cr6.eq) goto loc_82264A70;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r9,r31,48
	ctx.r9.s64 = ctx.r31.s64 + 48;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r11,68(r31)
	PPC_STORE_U8(ctx.r31.u32 + 68, ctx.r11.u8);
	// stw r8,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r8.u32);
	// stwx r9,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r9.u32);
loc_82264A70:
	// lbz r7,21(r31)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r31.u32 + 21);
	// lis r9,-31951
	ctx.r9.s64 = -2093940736;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mulli r11,r7,8200
	ctx.r11.s64 = ctx.r7.s64 * 8200;
	// lwz r7,28392(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28392);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// cmplw cr6,r10,r7
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, ctx.xer);
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x82264b04
	if (ctx.cr6.eq) goto loc_82264B04;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// and r6,r10,r9
	ctx.r6.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x82264adc
	if (!ctx.cr6.eq) goto loc_82264ADC;
	// lwz r6,8196(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// rlwinm r5,r6,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// stwx r31,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r31.u32);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_82264ADC:
	// lbz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 20);
	// stw r7,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264b04
	if (!ctx.cr6.eq) goto loc_82264B04;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r11,20(r31)
	PPC_STORE_U8(ctx.r31.u32 + 20, ctx.r11.u8);
	// stw r8,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r8.u32);
	// stwx r31,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r31.u32);
loc_82264B04:
	// lbz r11,45(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 45);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// mulli r11,r11,8200
	ctx.r11.s64 = ctx.r11.s64 * 8200;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x82264b9c
	if (ctx.cr6.eq) goto loc_82264B9C;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// and r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x82264b6c
	if (!ctx.cr6.eq) goto loc_82264B6C;
	// lwz r7,8196(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r31,24
	ctx.r6.s64 = ctx.r31.s64 + 24;
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_82264B6C:
	// lbz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 44);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264b9c
	if (!ctx.cr6.eq) goto loc_82264B9C;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r9,r31,24
	ctx.r9.s64 = ctx.r31.s64 + 24;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r11,44(r31)
	PPC_STORE_U8(ctx.r31.u32 + 44, ctx.r11.u8);
	// stw r8,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r8.u32);
	// stwx r9,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r9.u32);
loc_82264B9C:
	// lbz r11,213(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 213);
	// lwz r10,196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 196);
	// mulli r11,r11,8200
	ctx.r11.s64 = ctx.r11.s64 * 8200;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x82264c34
	if (ctx.cr6.eq) goto loc_82264C34;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// and r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x82264c04
	if (!ctx.cr6.eq) goto loc_82264C04;
	// lwz r7,8196(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r31,192
	ctx.r6.s64 = ctx.r31.s64 + 192;
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 200, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 196);
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_82264C04:
	// lbz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 212);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r10,196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 196, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264c34
	if (!ctx.cr6.eq) goto loc_82264C34;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r7,r31,192
	ctx.r7.s64 = ctx.r31.s64 + 192;
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// stb r11,212(r31)
	PPC_STORE_U8(ctx.r31.u32 + 212, ctx.r11.u8);
	// stw r10,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r10.u32);
	// stwx r7,r9,r28
	PPC_STORE_U32(ctx.r9.u32 + ctx.r28.u32, ctx.r7.u32);
loc_82264C34:
	// li r3,59
	ctx.r3.s64 = 59;
	// bl 0x821d71e8
	ctx.lr = 0x82264C3C;
	sub_821D71E8(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// lbz r10,45(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 45);
	// cmplwi cr6,r11,15
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 15, ctx.xer);
	// mulli r11,r10,8200
	ctx.r11.s64 = ctx.r10.s64 * 8200;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x82264cd8
	if (ctx.cr6.eq) goto loc_82264CD8;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// and r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x82264ca4
	if (!ctx.cr6.eq) goto loc_82264CA4;
	// lwz r8,8196(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r7,r31,24
	ctx.r7.s64 = ctx.r31.s64 + 24;
	// rlwinm r6,r8,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r10.u32);
	// stwx r7,r6,r11
	PPC_STORE_U32(ctx.r6.u32 + ctx.r11.u32, ctx.r7.u32);
	// lwz r5,8196(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r5,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// add r4,r9,r11
	ctx.r4.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r3,r10,1
	ctx.r3.s64 = ctx.r10.s64 + 1;
	// stw r3,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r3.u32);
loc_82264CA4:
	// lbz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 44);
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r10,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82264cd8
	if (!ctx.cr6.eq) goto loc_82264CD8;
	// lwz r11,1000(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r9,r31,24
	ctx.r9.s64 = ctx.r31.s64 + 24;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r10,44(r31)
	PPC_STORE_U8(ctx.r31.u32 + 44, ctx.r10.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r11.u32);
	// stwx r9,r8,r28
	PPC_STORE_U32(ctx.r8.u32 + ctx.r28.u32, ctx.r9.u32);
loc_82264CD8:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8221f410
	ctx.lr = 0x82264CE0;
	sub_8221F410(ctx, base);
loc_82264CE0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82264CE8"))) PPC_WEAK_FUNC(sub_82264CE8);
PPC_FUNC_IMPL(__imp__sub_82264CE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r10,208
	ctx.r10.s64 = 208;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// lfs f1,232(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 232);
	ctx.f1.f64 = double(temp.f32);
	// lvx128 v1,r11,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x821ff058
	sub_821FF058(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82264D00"))) PPC_WEAK_FUNC(sub_82264D00);
PPC_FUNC_IMPL(__imp__sub_82264D00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x82264D08;
	sub_82CA2BE4(ctx, base);
	// stfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f30.u64);
	// stfd f31,-56(r1)
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f31.u64);
	// li r12,-80
	ctx.r12.s64 = -80;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r6,r7,-28240
	ctx.r6.s64 = ctx.r7.s64 + -28240;
	// lfs f30,-27456(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -27456);
	ctx.f30.f64 = double(temp.f32);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// stfs f30,80(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// vand128 v127,v13,v0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lis r27,-32246
	ctx.r27.s64 = -2113273856;
	// addi r29,r11,-28480
	ctx.r29.s64 = ctx.r11.s64 + -28480;
	// cmpwi cr6,r9,-1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, -1, ctx.xer);
	// addi r28,r10,-28224
	ctx.r28.s64 = ctx.r10.s64 + -28224;
	// beq cr6,0x82264e34
	if (ctx.cr6.eq) goto loc_82264E34;
	// lfs f0,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f30,f31
	ctx.f13.f64 = double(float(ctx.f30.f64 - ctx.f31.f64));
	// fmuls f12,f0,f31
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// lfs f11,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f11.f64 = double(temp.f32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f2,-28492(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f1,f13,f11,f12
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bl 0x8223a048
	ctx.lr = 0x82264D94;
	sub_8223A048(ctx, base);
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v12,r0,r28
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r5,r11,r10
	ctx.r5.u64 = ctx.r11.u64 + ctx.r10.u64;
	// vand v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvx128 v0,r0,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// rlwinm r11,r5,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// vpermwi128 v12,v0,24
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xE7));
	// vpermwi128 v11,v0,97
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9E));
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// vspltw v10,v0,3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// vpermwi128 v9,v0,134
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x79));
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// std r10,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.r10.u64);
	// std r9,8(r7)
	PPC_STORE_U64(ctx.r7.u32 + 8, ctx.r9.u64);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v8,v0,252
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// vmulfp128 v7,v12,v8
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32)));
	// vpermwi128 v6,v0,133
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// vpermwi128 v5,v0,98
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vmulfp128 v4,v11,v6
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v3,v9,v5
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v5.f32)));
	// vxor v2,v7,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmaddfp v1,v10,v0,v2
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v2.f32)));
	// vxor v31,v4,v13
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddfp v30,v1,v31
	_mm_store_ps(ctx.v30.f32, _mm_add_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v31.f32)));
	// vsubfp v29,v30,v3
	_mm_store_ps(ctx.v29.f32, _mm_sub_ps(_mm_load_ps(ctx.v30.f32), _mm_load_ps(ctx.v3.f32)));
	// stvx128 v29,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
	// ld r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// std r8,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r8.u64);
	// std r7,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r7.u64);
loc_82264E34:
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// beq cr6,0x82264f04
	if (ctx.cr6.eq) goto loc_82264F04;
	// lfs f0,32(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f30,f31
	ctx.f13.f64 = double(float(ctx.f30.f64 - ctx.f31.f64));
	// fmuls f12,f0,f31
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// lfs f11,36(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	ctx.f11.f64 = double(temp.f32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f2,-28492(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f1,f13,f11,f12
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bl 0x8223a048
	ctx.lr = 0x82264E64;
	sub_8223A048(ctx, base);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v12,r0,r28
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r5,r11,r10
	ctx.r5.u64 = ctx.r11.u64 + ctx.r10.u64;
	// vand v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// lvx128 v0,r0,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// rlwinm r11,r5,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// vpermwi128 v12,v0,24
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xE7));
	// vpermwi128 v11,v0,97
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9E));
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// add r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
	// vspltw v10,v0,3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// vpermwi128 v9,v0,134
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x79));
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// std r10,0(r7)
	PPC_STORE_U64(ctx.r7.u32 + 0, ctx.r10.u64);
	// std r9,8(r7)
	PPC_STORE_U64(ctx.r7.u32 + 8, ctx.r9.u64);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v8,v0,252
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// vmulfp128 v7,v12,v8
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32)));
	// vpermwi128 v6,v0,133
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// vpermwi128 v5,v0,98
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vmulfp128 v4,v11,v6
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v6.f32)));
	// vmulfp128 v3,v9,v5
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v5.f32)));
	// vxor v2,v7,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmaddfp v1,v10,v0,v2
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v2.f32)));
	// vxor v31,v4,v13
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vaddfp v30,v1,v31
	_mm_store_ps(ctx.v30.f32, _mm_add_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v31.f32)));
	// vsubfp v29,v30,v3
	_mm_store_ps(ctx.v29.f32, _mm_sub_ps(_mm_load_ps(ctx.v30.f32), _mm_load_ps(ctx.v3.f32)));
	// stvx128 v29,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
	// ld r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// std r8,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r8.u64);
	// std r7,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r7.u64);
loc_82264F04:
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// beq cr6,0x82264fd4
	if (ctx.cr6.eq) goto loc_82264FD4;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f13,f30,f31
	ctx.f13.f64 = double(float(ctx.f30.f64 - ctx.f31.f64));
	// fmuls f12,f0,f31
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// lfs f11,48(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	ctx.f11.f64 = double(temp.f32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f2,-28492(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + -28492);
	ctx.f2.f64 = double(temp.f32);
	// fmadds f1,f13,f11,f12
	ctx.f1.f64 = double(float(ctx.f13.f64 * ctx.f11.f64 + ctx.f12.f64));
	// bl 0x8223a048
	ctx.lr = 0x82264F34;
	sub_8223A048(ctx, base);
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lvx128 v12,r0,r28
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// add r5,r11,r9
	ctx.r5.u64 = ctx.r11.u64 + ctx.r9.u64;
	// vand v13,v13,v12
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// rlwinm r11,r5,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0xFFFFFFF0;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v12,v0,24
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xE7));
	// vpermwi128 v11,v0,97
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9E));
	// vspltw v10,v0,3
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x0));
	// vpermwi128 v9,v0,134
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x79));
	// ld r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r11.u32 + 0);
	// ld r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + 8);
	// std r10,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r10.u64);
	// std r9,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r9.u64);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v8,v0,252
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x3));
	// vpermwi128 v7,v0,133
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x7A));
	// vpermwi128 v6,v0,98
	_mm_store_si128((__m128i*)ctx.v6.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0x9D));
	// vmulfp128 v5,v12,v8
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v8.f32)));
	// vmulfp128 v4,v11,v7
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v7.f32)));
	// vmulfp128 v3,v9,v6
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v6.f32)));
	// vxor v2,v5,v13
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vxor v1,v4,v13
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// vmaddfp v31,v10,v0,v2
	_mm_store_ps(ctx.v31.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v0.f32)), _mm_load_ps(ctx.v2.f32)));
	// vaddfp v30,v31,v1
	_mm_store_ps(ctx.v30.f32, _mm_add_ps(_mm_load_ps(ctx.v31.f32), _mm_load_ps(ctx.v1.f32)));
	// vsubfp v29,v30,v3
	_mm_store_ps(ctx.v29.f32, _mm_sub_ps(_mm_load_ps(ctx.v30.f32), _mm_load_ps(ctx.v3.f32)));
	// stvx128 v29,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// ld r7,8(r3)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
	// std r8,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r8.u64);
	// std r7,8(r11)
	PPC_STORE_U64(ctx.r11.u32 + 8, ctx.r7.u64);
loc_82264FD4:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// li r0,-80
	ctx.r0.s64 = -80;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f30,-64(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// lfd f31,-56(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_82264FEC"))) PPC_WEAK_FUNC(sub_82264FEC);
PPC_FUNC_IMPL(__imp__sub_82264FEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82264FF0"))) PPC_WEAK_FUNC(sub_82264FF0);
PPC_FUNC_IMPL(__imp__sub_82264FF0) {
	PPC_FUNC_PROLOGUE();
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// li r11,80
	ctx.r11.s64 = 80;
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// li r8,0
	ctx.r8.s64 = 0;
	// subf r7,r9,r10
	ctx.r7.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// divw. r10,r7,r11
	ctx.r10.s32 = ctx.r7.s32 / ctx.r11.s32;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// blelr 
	if (!ctx.cr0.gt) return;
	// lwz r7,0(r5)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
loc_82265018:
	// srawi r11,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r10.s32 >> 1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r6,r7
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, ctx.xer);
	// bge cr6,0x82265048
	if (!ctx.cr6.lt) goto loc_82265048;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// addi r9,r9,80
	ctx.r9.s64 = ctx.r9.s64 + 80;
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// b 0x82265050
	goto loc_82265050;
loc_82265048:
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
loc_82265050:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bgt cr6,0x82265018
	if (ctx.cr6.gt) goto loc_82265018;
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265060"))) PPC_WEAK_FUNC(sub_82265060);
PPC_FUNC_IMPL(__imp__sub_82265060) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82265068;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// bl 0x823db930
	ctx.lr = 0x82265080;
	sub_823DB930(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r29.u32);
	// addi r4,r30,40
	ctx.r4.s64 = ctx.r30.s64 + 40;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82264ff0
	ctx.lr = 0x82265098;
	sub_82264FF0(ctx, base);
	// lwz r11,48(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x822650f0
	if (ctx.cr6.eq) goto loc_822650F0;
	// li r9,80
	ctx.r9.s64 = 80;
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// divw r7,r8,r9
	ctx.r7.s32 = ctx.r8.s32 / ctx.r9.s32;
	// cmpw cr6,r7,r31
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r31.s32, ctx.xer);
	// ble cr6,0x822650f0
	if (!ctx.cr6.gt) goto loc_822650F0;
	// rlwinm r11,r31,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r31,r11
	ctx.r11.u64 = ctx.r31.u64 + ctx.r11.u64;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r10,r29
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r29.u32, ctx.xer);
	// bne cr6,0x822650f0
	if (!ctx.cr6.eq) goto loc_822650F0;
	// addi r4,r11,16
	ctx.r4.s64 = ctx.r11.s64 + 16;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82265100
	ctx.lr = 0x822650E4;
	sub_82265100(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_822650F0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_822650FC"))) PPC_WEAK_FUNC(sub_822650FC);
PPC_FUNC_IMPL(__imp__sub_822650FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265100"))) PPC_WEAK_FUNC(sub_82265100);
PPC_FUNC_IMPL(__imp__sub_82265100) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// bl 0x82265160
	ctx.lr = 0x82265120;
	sub_82265160(ctx, base);
	// ld r10,16(r30)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r30.u32 + 16);
	// li r11,32
	ctx.r11.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// std r10,16(r31)
	PPC_STORE_U64(ctx.r31.u32 + 16, ctx.r10.u64);
	// ld r9,24(r30)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r30.u32 + 24);
	// std r9,24(r31)
	PPC_STORE_U64(ctx.r31.u32 + 24, ctx.r9.u64);
	// lvx128 v0,r30,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r31,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r8,48(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// stw r8,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r8.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265160"))) PPC_WEAK_FUNC(sub_82265160);
PPC_FUNC_IMPL(__imp__sub_82265160) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// cmplw cr6,r30,r31
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r31.u32, ctx.xer);
	// beq cr6,0x822651c4
	if (ctx.cr6.eq) goto loc_822651C4;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82265194
	if (ctx.cr6.eq) goto loc_82265194;
	// bl 0x821c67d8
	ctx.lr = 0x82265194;
	sub_821C67D8(ctx, base);
loc_82265194:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822651c4
	if (ctx.cr6.eq) goto loc_822651C4;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// addi r11,r11,12
	ctx.r11.s64 = ctx.r11.s64 + 12;
loc_822651A8:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r10,0,r11
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r11.u32);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r11.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x822651a8
	if (!ctx.cr0.eq) goto loc_822651A8;
loc_822651C4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822651E0"))) PPC_WEAK_FUNC(sub_822651E0);
PPC_FUNC_IMPL(__imp__sub_822651E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r10,3
	ctx.r10.s64 = 3;
	// addi r3,r31,4
	ctx.r3.s64 = ctx.r31.s64 + 4;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwimi r11,r10,0,29,31
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x7) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFF8);
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// bl 0x82265160
	ctx.lr = 0x82265214;
	sub_82265160(ctx, base);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226522c
	if (!ctx.cr6.eq) goto loc_8226522C;
	// lis r11,-32247
	ctx.r11.s64 = -2113339392;
	// addi r4,r11,63
	ctx.r4.s64 = ctx.r11.s64 + 63;
	// b 0x82265230
	goto loc_82265230;
loc_8226522C:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
loc_82265230:
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226523C;
	sub_8222CF18(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8228a4c0
	ctx.lr = 0x82265244;
	sub_8228A4C0(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r3.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x82265250;
	sub_82214F08(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265274"))) PPC_WEAK_FUNC(sub_82265274);
PPC_FUNC_IMPL(__imp__sub_82265274) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265278"))) PPC_WEAK_FUNC(sub_82265278);
PPC_FUNC_IMPL(__imp__sub_82265278) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82265280;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// bl 0x821eea00
	ctx.lr = 0x82265294;
	sub_821EEA00(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x821eea00
	ctx.lr = 0x822652A0;
	sub_821EEA00(ctx, base);
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// li r30,0
	ctx.r30.s64 = 0;
	// addi r29,r11,28344
	ctx.r29.s64 = ctx.r11.s64 + 28344;
loc_822652AC:
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82265338
	if (ctx.cr6.eq) goto loc_82265338;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x82265338
	if (!ctx.cr6.gt) goto loc_82265338;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lbz r8,-1(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + -1);
	// cmplwi cr6,r8,92
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 92, ctx.xer);
	// beq cr6,0x822652e8
	if (ctx.cr6.eq) goto loc_822652E8;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// cmplwi cr6,r9,47
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 47, ctx.xer);
	// bne cr6,0x82265338
	if (!ctx.cr6.eq) goto loc_82265338;
loc_822652E8:
	// addi r6,r11,-1
	ctx.r6.s64 = ctx.r11.s64 + -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x821e3950
	ctx.lr = 0x822652FC;
	sub_821E3950(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82265160
	ctx.lr = 0x82265308;
	sub_82265160(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x821c67d8
	ctx.lr = 0x82265310;
	sub_821C67D8(ctx, base);
	// mr r9,r29
	ctx.r9.u64 = ctx.r29.u64;
loc_82265314:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r9
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stwcx. r11,0,r9
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x82265314
	if (!ctx.cr0.eq) goto loc_82265314;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r30.u32);
	// b 0x822652ac
	goto loc_822652AC;
loc_82265338:
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x821f0108
	ctx.lr = 0x82265344;
	sub_821F0108(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82281ee0
	ctx.lr = 0x8226534C;
	sub_82281EE0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82265160
	ctx.lr = 0x82265358;
	sub_82265160(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82214f08
	ctx.lr = 0x82265360;
	sub_82214F08(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r31,r11,3060
	ctx.r31.s64 = ctx.r11.s64 + 3060;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bl 0x8222cf18
	ctx.lr = 0x82265378;
	sub_8222CF18(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,3064
	ctx.r4.s64 = ctx.r10.s64 + 3064;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x8226538C;
	sub_8222CF18(ctx, base);
	// addi r5,r1,100
	ctx.r5.s64 = ctx.r1.s64 + 100;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e6408
	ctx.lr = 0x8226539C;
	sub_821E6408(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x822653A4;
	sub_82214F08(ctx, base);
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x82214f08
	ctx.lr = 0x822653AC;
	sub_82214F08(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	ctx.lr = 0x822653BC;
	sub_8222CF18(ctx, base);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r9,3072
	ctx.r4.s64 = ctx.r9.s64 + 3072;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x8222cf18
	ctx.lr = 0x822653D0;
	sub_8222CF18(ctx, base);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e6408
	ctx.lr = 0x822653E0;
	sub_821E6408(ctx, base);
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x82214f08
	ctx.lr = 0x822653E8;
	sub_82214F08(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	ctx.lr = 0x822653F0;
	sub_82214F08(ctx, base);
	// lwz r31,84(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x82265408
	if (!ctx.cr6.eq) goto loc_82265408;
	// lis r11,-32247
	ctx.r11.s64 = -2113339392;
	// addi r4,r11,63
	ctx.r4.s64 = ctx.r11.s64 + 63;
	// b 0x8226540c
	goto loc_8226540C;
loc_82265408:
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
loc_8226540C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e6308
	ctx.lr = 0x82265414;
	sub_821E6308(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x822654d8
	if (ctx.cr6.lt) goto loc_822654D8;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82265430
	if (ctx.cr6.eq) goto loc_82265430;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_82265430:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// beq cr6,0x82265440
	if (ctx.cr6.eq) goto loc_82265440;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
loc_82265440:
	// add r5,r11,r3
	ctx.r5.u64 = ctx.r11.u64 + ctx.r3.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x821e3950
	ctx.lr = 0x82265450;
	sub_821E3950(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82265160
	ctx.lr = 0x8226545C;
	sub_82265160(ctx, base);
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x82214f08
	ctx.lr = 0x82265464;
	sub_82214F08(ctx, base);
loc_82265464:
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822654d8
	if (ctx.cr6.eq) goto loc_822654D8;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// ble cr6,0x822654d8
	if (!ctx.cr6.gt) goto loc_822654D8;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,92
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 92, ctx.xer);
	// bne cr6,0x822654d8
	if (!ctx.cr6.eq) goto loc_822654D8;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821e3950
	ctx.lr = 0x8226549C;
	sub_821E3950(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82265160
	ctx.lr = 0x822654A8;
	sub_82265160(ctx, base);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x821c67d8
	ctx.lr = 0x822654B0;
	sub_821C67D8(ctx, base);
	// mr r9,r29
	ctx.r9.u64 = ctx.r29.u64;
loc_822654B4:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r9
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stwcx. r11,0,r9
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x822654b4
	if (!ctx.cr0.eq) goto loc_822654B4;
	// stw r30,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r30.u32);
	// b 0x82265464
	goto loc_82265464;
loc_822654D8:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x821f0108
	ctx.lr = 0x822654E4;
	sub_821F0108(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82214f08
	ctx.lr = 0x822654EC;
	sub_82214F08(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x822654F4;
	sub_82214F08(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82265500"))) PPC_WEAK_FUNC(sub_82265500);
PPC_FUNC_IMPL(__imp__sub_82265500) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x82265508;
	sub_82CA2BE4(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// bl 0x821c7460
	ctx.lr = 0x82265520;
	sub_821C7460(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,-1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, -1, ctx.xer);
	// bne cr6,0x82265540
	if (!ctx.cr6.eq) goto loc_82265540;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82265160
	ctx.lr = 0x82265538;
	sub_82265160(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
loc_82265540:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82265554
	if (ctx.cr6.eq) goto loc_82265554;
	// lwz r29,4(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_82265554:
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e3950
	ctx.lr = 0x82265568;
	sub_821E3950(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82265160
	ctx.lr = 0x82265574;
	sub_82265160(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8226557C;
	sub_82214F08(ctx, base);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// addi r5,r30,1
	ctx.r5.s64 = ctx.r30.s64 + 1;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e3950
	ctx.lr = 0x82265590;
	sub_821E3950(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x82265160
	ctx.lr = 0x8226559C;
	sub_82265160(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x822655A4;
	sub_82214F08(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_822655AC"))) PPC_WEAK_FUNC(sub_822655AC);
PPC_FUNC_IMPL(__imp__sub_822655AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822655B0"))) PPC_WEAK_FUNC(sub_822655B0);
PPC_FUNC_IMPL(__imp__sub_822655B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// li r9,0
	ctx.r9.s64 = 0;
	// bne cr6,0x822656b4
	if (!ctx.cr6.eq) goto loc_822656B4;
	// lbz r10,10940(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10940);
	// stw r9,12716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12716, ctx.r9.u32);
	// rlwinm. r11,r10,0,28,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822656a0
	if (!ctx.cr0.eq) goto loc_822656A0;
	// rlwinm. r11,r10,0,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822656a0
	if (!ctx.cr0.eq) goto loc_822656A0;
	// lbz r11,12187(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 12187);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne 0x822656a0
	if (!ctx.cr0.eq) goto loc_822656A0;
	// rlwinm. r11,r10,0,27,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x10;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82265604
	if (ctx.cr0.eq) goto loc_82265604;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82265694
	goto loc_82265694;
loc_82265604:
	// rlwinm. r11,r10,0,26,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8226568c
	if (ctx.cr0.eq) goto loc_8226568C;
	// lwz r11,12440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12440);
	// lwz r8,12728(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12728);
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82265624
	if (ctx.cr6.eq) goto loc_82265624;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226568c
	if (!ctx.cr6.eq) goto loc_8226568C;
loc_82265624:
	// lwz r11,12444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12444);
	// lwz r8,12732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12732);
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226563c
	if (ctx.cr6.eq) goto loc_8226563C;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226568c
	if (!ctx.cr6.eq) goto loc_8226568C;
loc_8226563C:
	// lwz r11,12448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12448);
	// lwz r8,12736(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12736);
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82265654
	if (ctx.cr6.eq) goto loc_82265654;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226568c
	if (!ctx.cr6.eq) goto loc_8226568C;
loc_82265654:
	// lwz r11,12452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12452);
	// lwz r8,12740(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12740);
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226566c
	if (ctx.cr6.eq) goto loc_8226566C;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226568c
	if (!ctx.cr6.eq) goto loc_8226568C;
loc_8226566C:
	// lwz r11,12456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12456);
	// lwz r8,12744(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12744);
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82265684
	if (ctx.cr6.eq) goto loc_82265684;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226568c
	if (!ctx.cr6.eq) goto loc_8226568C;
loc_82265684:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82265690
	goto loc_82265690;
loc_8226568C:
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82265690:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
loc_82265694:
	// clrlwi. r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne 0x822656a4
	if (!ctx.cr0.eq) goto loc_822656A4;
loc_822656A0:
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_822656A4:
	// rlwimi r11,r10,0,24,30
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0xFE) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFF01);
	// li r10,-1
	ctx.r10.s64 = -1;
	// stw r10,12708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12708, ctx.r10.u32);
	// b 0x822656c8
	goto loc_822656C8;
loc_822656B4:
	// lbz r11,10940(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 10940);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r4,12708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12708, ctx.r4.u32);
	// rlwinm r11,r11,0,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r10,12716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12716, ctx.r10.u32);
loc_822656C8:
	// stb r11,10940(r31)
	PPC_STORE_U8(ctx.r31.u32 + 10940, ctx.r11.u8);
	// lwz r11,56(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// stw r9,12712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12712, ctx.r9.u32);
	// stw r4,10932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10932, ctx.r4.u32);
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// stw r9,10936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 10936, ctx.r9.u32);
	// ble cr6,0x822656f0
	if (!ctx.cr6.gt) goto loc_822656F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821e8d20
	ctx.lr = 0x822656F0;
	sub_821E8D20(ctx, base);
loc_822656F0:
	// lis r11,-16384
	ctx.r11.s64 = -1073741824;
	// ori r11,r11,24576
	ctx.r11.u64 = ctx.r11.u64 | 24576;
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r11.u32);
	ctx.r3.u32 = ea;
	// lwz r11,12708(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12708);
	// stwu r11,4(r3)
	ea = 4 + ctx.r3.u32;
	PPC_STORE_U32(ea, ctx.r11.u32);
	ctx.r3.u32 = ea;
	// stw r3,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r3.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226571C"))) PPC_WEAK_FUNC(sub_8226571C);
PPC_FUNC_IMPL(__imp__sub_8226571C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265720"))) PPC_WEAK_FUNC(sub_82265720);
PPC_FUNC_IMPL(__imp__sub_82265720) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82265728;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// std r4,288(r1)
	PPC_STORE_U64(ctx.r1.u32 + 288, ctx.r4.u64);
	// std r5,296(r1)
	PPC_STORE_U64(ctx.r1.u32 + 296, ctx.r5.u64);
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// addi r6,r1,128
	ctx.r6.s64 = ctx.r1.s64 + 128;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// stvx128 v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f12,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f12.f64 = double(temp.f32);
	// lwz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// stvx128 v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,16
	ctx.r10.s64 = 16;
	// stvx128 v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r11,160
	ctx.r11.s64 = ctx.r11.s64 + 160;
	// stvx128 v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r9,32
	ctx.r9.s64 = 32;
	// lfs f13,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// li r8,48
	ctx.r8.s64 = 48;
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// addi r30,r1,160
	ctx.r30.s64 = ctx.r1.s64 + 160;
	// lfs f0,116(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// addi r29,r1,176
	ctx.r29.s64 = ctx.r1.s64 + 176;
	// lfs f11,136(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	ctx.f11.f64 = double(temp.f32);
	// addi r6,r1,192
	ctx.r6.s64 = ctx.r1.s64 + 192;
	// stfs f11,104(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// addi r28,r1,112
	ctx.r28.s64 = ctx.r1.s64 + 112;
	// stfs f12,108(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f0,100(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r11,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v13,r11,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r11,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v11,r0,r11
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v10,r0,r28
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82d50780
	ctx.lr = 0x822657E0;
	sub_82D50780(ctx, base);
	// lbz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x82265830
	if (!ctx.cr6.eq) goto loc_82265830;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v13,v0,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xFF));
	// lfs f0,-27456(r9)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27456);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f13.f64 = double(temp.f32);
	// fsqrts f12,f13
	ctx.f12.f64 = double(float(sqrt(ctx.f13.f64)));
	// fdivs f11,f0,f12
	ctx.f11.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
	// stfs f11,128(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// lvx128 v12,r0,r8
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v10,v0,v11
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v11.f32)));
	// stvx128 v10,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82265830:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x82d51008
	ctx.lr = 0x8226583C;
	sub_82D51008(ctx, base);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// bl 0x82d7a848
	ctx.lr = 0x82265848;
	sub_82D7A848(ctx, base);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82265850"))) PPC_WEAK_FUNC(sub_82265850);
PPC_FUNC_IMPL(__imp__sub_82265850) {
	PPC_FUNC_PROLOGUE();
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// cmplw cr6,r3,r4
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// blelr cr6
	if (!ctx.cr6.gt) return;
	// li r9,0
	ctx.r9.s64 = 0;
loc_82265878:
	// lwz r8,8(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r11,8(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addi r7,r11,8
	ctx.r7.s64 = ctx.r11.s64 + 8;
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// stw r7,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r7.u32);
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stw r6,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r6.u32);
	// lwz r5,4(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r5,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r5.u32);
	// bne 0x82265878
	if (!ctx.cr0.eq) goto loc_82265878;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822658AC"))) PPC_WEAK_FUNC(sub_822658AC);
PPC_FUNC_IMPL(__imp__sub_822658AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822658B0"))) PPC_WEAK_FUNC(sub_822658B0);
PPC_FUNC_IMPL(__imp__sub_822658B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lfs f0,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 0, temp.u32);
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r30)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r30.u32 + 4, temp.u32);
	// lfs f12,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r30)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r30.u32 + 8, temp.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r10,56(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 56);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x822658F4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,12(r30)
	PPC_STORE_U32(ctx.r30.u32 + 12, ctx.r3.u32);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r8,100(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 100);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8226590C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,16(r30)
	PPC_STORE_U32(ctx.r30.u32 + 16, ctx.r3.u32);
	// lwz r7,108(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 108);
	// stw r7,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r7.u32);
	// lfs f11,52(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 52);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,24(r30)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r30.u32 + 24, temp.u32);
	// lfs f10,56(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,28(r30)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r30.u32 + 28, temp.u32);
	// lfs f9,60(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	ctx.f9.f64 = double(temp.f32);
	// stfs f9,32(r30)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r30.u32 + 32, temp.u32);
	// lfs f8,64(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 64);
	ctx.f8.f64 = double(temp.f32);
	// stfs f8,36(r30)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r30.u32 + 36, temp.u32);
	// lfs f7,68(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 68);
	ctx.f7.f64 = double(temp.f32);
	// stfs f7,40(r30)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r30.u32 + 40, temp.u32);
	// lfs f6,72(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	ctx.f6.f64 = double(temp.f32);
	// stfs f6,44(r30)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r30.u32 + 44, temp.u32);
	// lfs f5,76(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,48(r30)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r30.u32 + 48, temp.u32);
	// lfs f4,80(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	ctx.f4.f64 = double(temp.f32);
	// stfs f4,52(r30)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r30.u32 + 52, temp.u32);
	// lwz r6,84(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// stw r6,56(r30)
	PPC_STORE_U32(ctx.r30.u32 + 56, ctx.r6.u32);
	// lfs f3,88(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	ctx.f3.f64 = double(temp.f32);
	// stfs f3,60(r30)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r30.u32 + 60, temp.u32);
	// lfs f2,92(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 92);
	ctx.f2.f64 = double(temp.f32);
	// stfs f2,64(r30)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r30.u32 + 64, temp.u32);
	// lfs f1,96(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 96);
	ctx.f1.f64 = double(temp.f32);
	// stfs f1,68(r30)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r30.u32 + 68, temp.u32);
	// lfs f0,100(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 100);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,72(r30)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r30.u32 + 72, temp.u32);
	// lbz r5,104(r31)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r31.u32 + 104);
	// stb r5,76(r30)
	PPC_STORE_U8(ctx.r30.u32 + 76, ctx.r5.u8);
	// lbz r4,105(r31)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r31.u32 + 105);
	// stb r4,77(r30)
	PPC_STORE_U8(ctx.r30.u32 + 77, ctx.r4.u8);
	// lwz r3,156(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// stw r3,80(r30)
	PPC_STORE_U32(ctx.r30.u32 + 80, ctx.r3.u32);
	// lwz r11,160(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// stw r11,84(r30)
	PPC_STORE_U32(ctx.r30.u32 + 84, ctx.r11.u32);
	// lwz r10,164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	// stw r10,88(r30)
	PPC_STORE_U32(ctx.r30.u32 + 88, ctx.r10.u32);
	// lwz r9,168(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	// stw r9,92(r30)
	PPC_STORE_U32(ctx.r30.u32 + 92, ctx.r9.u32);
	// lwz r8,172(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	// stw r8,96(r30)
	PPC_STORE_U32(ctx.r30.u32 + 96, ctx.r8.u32);
	// lwz r7,176(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	// stw r7,100(r30)
	PPC_STORE_U32(ctx.r30.u32 + 100, ctx.r7.u32);
	// lwz r6,180(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// stw r6,104(r30)
	PPC_STORE_U32(ctx.r30.u32 + 104, ctx.r6.u32);
	// lwz r5,184(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	// stw r5,108(r30)
	PPC_STORE_U32(ctx.r30.u32 + 108, ctx.r5.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822659E8"))) PPC_WEAK_FUNC(sub_822659E8);
PPC_FUNC_IMPL(__imp__sub_822659E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r10,4(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// lwz r9,8(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// lwz r8,12(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// addi r4,r31,36
	ctx.r4.s64 = ctx.r31.s64 + 36;
	// stw r11,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r11.u32);
	// stw r10,4(r30)
	PPC_STORE_U32(ctx.r30.u32 + 4, ctx.r10.u32);
	// stw r9,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r9.u32);
	// stw r8,12(r30)
	PPC_STORE_U32(ctx.r30.u32 + 12, ctx.r8.u32);
	// bl 0x82265a80
	ctx.lr = 0x82265A2C;
	sub_82265A80(ctx, base);
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lwz r6,8(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// subf r10,r3,r7
	ctx.r10.s64 = ctx.r7.s64 - ctx.r3.s64;
	// lwz r4,12(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	// subf r9,r3,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r3.s64;
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r10,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r10.u32);
	// subf r8,r11,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r11.s64;
	// stw r9,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r9.u32);
	// subf r7,r11,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r11.s64;
	// stw r8,4(r30)
	PPC_STORE_U32(ctx.r30.u32 + 4, ctx.r8.u32);
	// stw r7,12(r30)
	PPC_STORE_U32(ctx.r30.u32 + 12, ctx.r7.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265A80"))) PPC_WEAK_FUNC(sub_82265A80);
PPC_FUNC_IMPL(__imp__sub_82265A80) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,8(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// srawi r7,r8,31
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// subf r11,r6,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r6.s64;
	// subf r5,r11,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r11.s64;
	// srawi r10,r5,31
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r5.s32 >> 31;
	// and r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 & ctx.r5.u64;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// lwz r8,12(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r7,4(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r6,4(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// subf r5,r6,r7
	ctx.r5.s64 = ctx.r7.s64 - ctx.r6.s64;
	// srawi r11,r5,31
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x7FFFFFFF) != 0);
	ctx.r11.s64 = ctx.r5.s32 >> 31;
	// and r10,r11,r5
	ctx.r10.u64 = ctx.r11.u64 & ctx.r5.u64;
	// subf r11,r10,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r10.s64;
	// subf r9,r11,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r11.s64;
	// srawi r8,r9,31
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 31;
	// and r10,r8,r9
	ctx.r10.u64 = ctx.r8.u64 & ctx.r9.u64;
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r5,8(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// subf r10,r11,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r11.s64;
	// srawi r9,r10,31
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// and r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 & ctx.r10.u64;
	// subf r11,r8,r5
	ctx.r11.s64 = ctx.r5.s64 - ctx.r8.s64;
	// subf r7,r11,r6
	ctx.r7.s64 = ctx.r6.s64 - ctx.r11.s64;
	// srawi r6,r7,31
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// and r10,r6,r7
	ctx.r10.u64 = ctx.r6.u64 & ctx.r7.u64;
	// add r5,r10,r11
	ctx.r5.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r5,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r5.u32);
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r9,12(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r8,4(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// subf r7,r8,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r8.s64;
	// srawi r6,r7,31
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 31;
	// and r5,r6,r7
	ctx.r5.u64 = ctx.r6.u64 & ctx.r7.u64;
	// subf r11,r5,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r5.s64;
	// subf r4,r11,r10
	ctx.r4.s64 = ctx.r10.s64 - ctx.r11.s64;
	// srawi r10,r4,31
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r4.s32 >> 31;
	// and r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 & ctx.r4.u64;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r9,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265B44"))) PPC_WEAK_FUNC(sub_82265B44);
PPC_FUNC_IMPL(__imp__sub_82265B44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265B48"))) PPC_WEAK_FUNC(sub_82265B48);
PPC_FUNC_IMPL(__imp__sub_82265B48) {
	PPC_FUNC_PROLOGUE();
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v13,v0,v1
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v1.f32)));
	// stvx128 v13,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265B58"))) PPC_WEAK_FUNC(sub_82265B58);
PPC_FUNC_IMPL(__imp__sub_82265B58) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r4,44(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 44);
	// bl 0x82d7c158
	ctx.lr = 0x82265B74;
	sub_82D7C158(ctx, base);
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// extsb r9,r10
	ctx.r9.s64 = ctx.r10.s8;
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r3,r7,1
	ctx.r3.u64 = ctx.r7.u64 ^ 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265B98"))) PPC_WEAK_FUNC(sub_82265B98);
PPC_FUNC_IMPL(__imp__sub_82265B98) {
	PPC_FUNC_PROLOGUE();
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// li r11,4
	ctx.r11.s64 = 4;
	// add r10,r4,r10
	ctx.r10.u64 = ctx.r4.u64 + ctx.r10.u64;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// ble cr6,0x82265bb8
	if (!ctx.cr6.gt) goto loc_82265BB8;
loc_82265BAC:
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x82265bac
	if (ctx.cr6.lt) goto loc_82265BAC;
loc_82265BB8:
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// b 0x82266f88
	sub_82266F88(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82265BC0"))) PPC_WEAK_FUNC(sub_82265BC0);
PPC_FUNC_IMPL(__imp__sub_82265BC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82265BC8;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x82265bf8
	if (!ctx.cr6.gt) goto loc_82265BF8;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r4,r11,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82266f88
	ctx.lr = 0x82265BF8;
	sub_82266F88(ctx, base);
loc_82265BF8:
	// lis r10,-25033
	ctx.r10.s64 = -1640562688;
	// lwz r8,8(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rlwinm r11,r30,28,4,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 28) & 0xFFFFFFF;
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// ori r10,r10,31153
	ctx.r10.u64 = ctx.r10.u64 | 31153;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// and r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 & ctx.r8.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r9,r10
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmpwi cr6,r7,-1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, -1, ctx.xer);
	// beq cr6,0x82265c4c
	if (ctx.cr6.eq) goto loc_82265C4C;
loc_82265C24:
	// lwzx r10,r9,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// cmplw cr6,r10,r30
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x82265c4c
	if (ctx.cr6.eq) goto loc_82265C4C;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// and r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 & ctx.r8.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r7,r10,r7
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r7.u32);
	// cmpwi cr6,r7,-1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, -1, ctx.xer);
	// bne cr6,0x82265c24
	if (!ctx.cr6.eq) goto loc_82265C24;
loc_82265C4C:
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// subf r8,r8,r30
	ctx.r8.s64 = ctx.r30.s64 - ctx.r8.s64;
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r8,r8,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r8,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r8.u32);
	// stwx r30,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r30.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r29,r11,r9
	PPC_STORE_U32(ctx.r11.u32 + ctx.r9.u32, ctx.r29.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82265C94"))) PPC_WEAK_FUNC(sub_82265C94);
PPC_FUNC_IMPL(__imp__sub_82265C94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265C98"))) PPC_WEAK_FUNC(sub_82265C98);
PPC_FUNC_IMPL(__imp__sub_82265C98) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82265CA0;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// bl 0x822060a0
	ctx.lr = 0x82265CBC;
	sub_822060A0(ctx, base);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x82265cd8
	if (ctx.cr0.eq) goto loc_82265CD8;
	// mullw r5,r30,r29
	ctx.r5.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r29.s32);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x82ca2c60
	ctx.lr = 0x82265CD0;
	sub_82CA2C60(ctx, base);
	// lwz r11,13444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 13444);
	// stw r11,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r11.u32);
loc_82265CD8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82265CE0"))) PPC_WEAK_FUNC(sub_82265CE0);
PPC_FUNC_IMPL(__imp__sub_82265CE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f30.u64);
	// stfd f31,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r30,r11,-8700
	ctx.r30.s64 = ctx.r11.s64 + -8700;
	// lfs f0,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f30,f1,f0
	ctx.f30.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
	// stfs f1,16(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + 16, temp.u32);
	// lfs f31,-18768(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + -18768);
	ctx.f31.f64 = double(temp.f32);
	// fcmpu cr6,f30,f31
	ctx.cr6.compare(ctx.f30.f64, ctx.f31.f64);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,128(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 128);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// rotlwi r10,r7,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lwz r11,4(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x82265d50
	if (!ctx.cr6.eq) goto loc_82265D50;
	// twi 31,r0,22
	// twi 31,r0,22
loc_82265D50:
	// lwz r3,8(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82265D64;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lbz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82265d88
	if (ctx.cr6.eq) goto loc_82265D88;
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
	// cmpwi cr6,r3,8
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 8, ctx.xer);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
loc_82265D88:
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x82265df4
	if (ctx.cr6.lt) goto loc_82265DF4;
	// bne cr6,0x82265e50
	if (!ctx.cr6.eq) goto loc_82265E50;
	// lfs f13,24(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f31
	ctx.cr6.compare(ctx.f13.f64, ctx.f31.f64);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
	// lfs f0,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f0,f13,f30,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f30.f64 + ctx.f0.f64));
	// lfs f12,-8100(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + -8100);
	ctx.f12.f64 = double(temp.f32);
	// stfs f0,20(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 20, temp.u32);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x82265dd4
	if (!ctx.cr6.gt) goto loc_82265DD4;
	// lfs f0,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmr f1,f12
	ctx.f1.f64 = ctx.f12.f64;
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f12,20(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 20, temp.u32);
	// stfs f0,24(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 24, temp.u32);
	// b 0x82265e48
	goto loc_82265E48;
loc_82265DD4:
	// fcmpu cr6,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bge cr6,0x82265dec
	if (!ctx.cr6.lt) goto loc_82265DEC;
	// lfs f0,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f31,20(r31)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 20, temp.u32);
	// stfs f0,24(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 24, temp.u32);
loc_82265DEC:
	// lfs f1,20(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f1.f64 = double(temp.f32);
	// b 0x82265e48
	goto loc_82265E48;
loc_82265DF4:
	// lfs f12,28(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f12.f64 = double(temp.f32);
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	ctx.cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
	// lfs f0,24(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// beq cr6,0x82265e38
	if (ctx.cr6.eq) goto loc_82265E38;
	// fmadds f1,f0,f30,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 * ctx.f30.f64 + ctx.f13.f64));
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// ble cr6,0x82265e24
	if (!ctx.cr6.gt) goto loc_82265E24;
	// fcmpu cr6,f1,f12
	ctx.cr6.compare(ctx.f1.f64, ctx.f12.f64);
	// bgt cr6,0x82265e34
	if (ctx.cr6.gt) goto loc_82265E34;
loc_82265E24:
	// fcmpu cr6,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bge cr6,0x82265e3c
	if (!ctx.cr6.lt) goto loc_82265E3C;
	// fcmpu cr6,f1,f12
	ctx.cr6.compare(ctx.f1.f64, ctx.f12.f64);
	// bge cr6,0x82265e3c
	if (!ctx.cr6.lt) goto loc_82265E3C;
loc_82265E34:
	// stfs f31,24(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 24, temp.u32);
loc_82265E38:
	// fmr f1,f12
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f12.f64;
loc_82265E3C:
	// lfs f0,20(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	ctx.cr6.compare(ctx.f1.f64, ctx.f0.f64);
	// beq cr6,0x82265e50
	if (ctx.cr6.eq) goto loc_82265E50;
loc_82265E48:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8257c130
	ctx.lr = 0x82265E50;
	sub_8257C130(ctx, base);
loc_82265E50:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f30,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// lfd f31,-32(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265E70"))) PPC_WEAK_FUNC(sub_82265E70);
PPC_FUNC_IMPL(__imp__sub_82265E70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,16
	ctx.r11.s64 = 16;
	// lvx128 v13,r0,r4
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// vsubfp v11,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// vsubfp v0,v0,v13
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v12,r3,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,-27468(r9)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27468);
	ctx.f13.f64 = double(temp.f32);
	// vmsum3fp128 v10,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx128 v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// stfs f0,-32(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// ble cr6,0x82265eec
	if (!ctx.cr6.gt) goto loc_82265EEC;
	// addi r11,r1,-32
	ctx.r11.s64 = ctx.r1.s64 + -32;
	// lfs f13,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f12,16(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f13,f0,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64 + ctx.f12.f64));
	// lvlx v13,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v13,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vmaddfp v0,v12,v11,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmsum3fp128 v10,v0,v0
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx128 v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f10,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f10.f64 = double(temp.f32);
	// fsqrts f9,f10
	ctx.f9.f64 = double(float(sqrt(ctx.f10.f64)));
	// fcmpu cr6,f9,f11
	ctx.cr6.compare(ctx.f9.f64, ctx.f11.f64);
	// bge cr6,0x82265f24
	if (!ctx.cr6.lt) goto loc_82265F24;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_82265EEC:
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// lfs f0,16(r4)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f0,f0
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r11,1
	ctx.r11.s64 = 1;
	// lfs f12,-16(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	ctx.cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// blt cr6,0x82265f14
	if (ctx.cr6.lt) goto loc_82265F14;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82265F14:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
loc_82265F24:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265F2C"))) PPC_WEAK_FUNC(sub_82265F2C);
PPC_FUNC_IMPL(__imp__sub_82265F2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265F30"))) PPC_WEAK_FUNC(sub_82265F30);
PPC_FUNC_IMPL(__imp__sub_82265F30) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,12(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// addi r11,r4,1
	ctx.r11.s64 = ctx.r4.s64 + 1;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// bltlr cr6
	if (ctx.cr6.lt) return;
	// li r3,-1
	ctx.r3.s64 = -1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82265F4C"))) PPC_WEAK_FUNC(sub_82265F4C);
PPC_FUNC_IMPL(__imp__sub_82265F4C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82265F50"))) PPC_WEAK_FUNC(sub_82265F50);
PPC_FUNC_IMPL(__imp__sub_82265F50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,284(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 284);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x82265fc4
	if (!ctx.cr6.eq) goto loc_82265FC4;
	// lwz r11,140(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// lbz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 52);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82265f9c
	if (ctx.cr6.eq) goto loc_82265F9C;
	// lbz r11,54(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 54);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x82265fa0
	if (!ctx.cr6.eq) goto loc_82265FA0;
loc_82265F9C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82265FA0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82265fc4
	if (ctx.cr6.eq) goto loc_82265FC4;
	// addi r3,r31,304
	ctx.r3.s64 = ctx.r31.s64 + 304;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_82265FC4:
	// bl 0x82266070
	ctx.lr = 0x82265FC8;
	sub_82266070(ctx, base);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82266054
	if (ctx.cr6.eq) goto loc_82266054;
	// lwz r4,60(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 60);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x82265fe8
	if (!ctx.cr6.eq) goto loc_82265FE8;
	// lwz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// b 0x82265fec
	goto loc_82265FEC;
loc_82265FE8:
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_82265FEC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226600c
	if (ctx.cr6.eq) goto loc_8226600C;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x82266000
	if (!ctx.cr6.eq) goto loc_82266000;
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
loc_82266000:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821d3068
	ctx.lr = 0x82266008;
	sub_821D3068(ctx, base);
	// b 0x82266014
	goto loc_82266014;
loc_8226600C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82422090
	ctx.lr = 0x82266014;
	sub_82422090(ctx, base);
loc_82266014:
	// addi r10,r31,144
	ctx.r10.s64 = ctx.r31.s64 + 144;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// addi r8,r10,16
	ctx.r8.s64 = ctx.r10.s64 + 16;
	// li r11,16
	ctx.r11.s64 = 16;
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r9,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r8,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32 + ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,48(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,192(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 192, temp.u32);
	// lbz r11,52(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 52);
	// stb r11,196(r31)
	PPC_STORE_U8(ctx.r31.u32 + 196, ctx.r11.u8);
	// bl 0x82266070
	ctx.lr = 0x82266050;
	sub_82266070(ctx, base);
	// stw r3,208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 208, ctx.r3.u32);
loc_82266054:
	// addi r3,r31,144
	ctx.r3.s64 = ctx.r31.s64 + 144;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226606C"))) PPC_WEAK_FUNC(sub_8226606C);
PPC_FUNC_IMPL(__imp__sub_8226606C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82266070"))) PPC_WEAK_FUNC(sub_82266070);
PPC_FUNC_IMPL(__imp__sub_82266070) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lwz r11,2312(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 2312);
	// lwz r3,16(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82266080"))) PPC_WEAK_FUNC(sub_82266080);
PPC_FUNC_IMPL(__imp__sub_82266080) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f1,28(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821ee000
	ctx.lr = 0x8226609C;
	sub_821EE000(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822660fc
	if (ctx.cr6.eq) goto loc_822660FC;
	// lbz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 40);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266110
	if (ctx.cr6.eq) goto loc_82266110;
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x822660e0
	if (ctx.cr6.lt) goto loc_822660E0;
	// bne cr6,0x82266110
	if (!ctx.cr6.eq) goto loc_82266110;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82266128
	ctx.lr = 0x822660CC;
	sub_82266128(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822660E0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8299e6f0
	ctx.lr = 0x822660E8;
	sub_8299E6F0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822660FC:
	// lbz r11,41(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 41);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266110
	if (!ctx.cr6.eq) goto loc_82266110;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82274ed0
	ctx.lr = 0x82266110;
	sub_82274ED0(ctx, base);
loc_82266110:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82266124"))) PPC_WEAK_FUNC(sub_82266124);
PPC_FUNC_IMPL(__imp__sub_82266124) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82266128"))) PPC_WEAK_FUNC(sub_82266128);
PPC_FUNC_IMPL(__imp__sub_82266128) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82266130;
	sub_82CA2BE8(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// li r12,-64
	ctx.r12.s64 = -64;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// addi r31,r30,80
	ctx.r31.s64 = ctx.r30.s64 + 80;
	// lwz r11,84(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266188
	if (ctx.cr6.eq) goto loc_82266188;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266244
	if (ctx.cr6.eq) goto loc_82266244;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266188
	if (ctx.cr6.eq) goto loc_82266188;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825575c8
	ctx.lr = 0x82266174;
	sub_825575C8(ctx, base);
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226618c
	if (!ctx.cr6.eq) goto loc_8226618C;
loc_82266188:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226618C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822664c4
	if (ctx.cr6.eq) goto loc_822664C4;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lfs f31,-27468(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27468);
	ctx.f31.f64 = double(temp.f32);
	// addi r29,r30,48
	ctx.r29.s64 = ctx.r30.s64 + 48;
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r28,r11,-27468
	ctx.r28.s64 = ctx.r11.s64 + -27468;
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v11,v13,3,2
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// lvx128 v10,r0,r29
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vpermwi128 v9,v10,24
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xE7));
	// vpermwi128 v8,v11,24
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xE7));
	// vcmpeqfp. v7,v9,v8
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v7.f32, _mm_cmpeq_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v8.f32)));
	ctx.cr6.setFromMask(_mm_load_ps(ctx.v7.f32), 0xF);
	// mfocrf r6,2
	ctx.r6.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// rlwinm r5,r6,25,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 25) & 0x1;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x822663c8
	if (ctx.cr6.eq) goto loc_822663C8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// bl 0x825575c8
	ctx.lr = 0x82266210;
	sub_825575C8(ctx, base);
	// lwz r11,60(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// rlwinm r10,r11,29,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 29) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266300
	if (ctx.cr6.eq) goto loc_82266300;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266250
	if (ctx.cr6.eq) goto loc_82266250;
	// lbz r10,195(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 195);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x82266300
	goto loc_82266300;
loc_82266244:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821940c8
	ctx.lr = 0x8226624C;
	sub_821940C8(ctx, base);
	// b 0x82266188
	goto loc_82266188;
loc_82266250:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r11.u32);
	// subf r9,r10,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// srawi. r11,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x822662c0
	if (!ctx.cr0.gt) goto loc_822662C0;
loc_82266270:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,195
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 195, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82266290
	if (ctx.cr6.lt) goto loc_82266290;
	// li r7,0
	ctx.r7.s64 = 0;
loc_82266290:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822662ac
	if (ctx.cr6.eq) goto loc_822662AC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x822662b4
	goto loc_822662B4;
loc_822662AC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_822662B4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82266270
	if (ctx.cr6.gt) goto loc_82266270;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
loc_822662C0:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x822662f8
	if (ctx.cr6.eq) goto loc_822662F8;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,195
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 195, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x822662dc
	if (ctx.cr6.gt) goto loc_822662DC;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822662DC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822662f8
	if (!ctx.cr6.eq) goto loc_822662F8;
	// ld r11,96(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x822662fc
	goto loc_822662FC;
loc_822662F8:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_822662FC:
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_82266300:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82266590
	ctx.lr = 0x8226630C;
	sub_82266590(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r4,124(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 124);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,64(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82266334;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r30,64
	ctx.r3.s64 = ctx.r30.s64 + 64;
	// lis r31,-32246
	ctx.r31.s64 = -2113273856;
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v13,v12
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// lfs f1,-25888(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v11,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82260bc8
	ctx.lr = 0x82266358;
	sub_82260BC8(ctx, base);
	// lwz r6,4(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r4,124(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 124);
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r11,64(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 64);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82266374;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lvx128 v10,r0,r29
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f1,-25888(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// lvx128 v9,r0,r10
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v8,v9,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v10.f32)));
	// stvx128 v8,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82260bc8
	ctx.lr = 0x82266398;
	sub_82260BC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r6,r8,-28176
	ctx.r6.s64 = ctx.r8.s64 + -28176;
	// stfs f31,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// lvx128 v6,r0,r7
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r3,124(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 124);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v7,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v1,v6,v7,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// bl 0x822664d8
	ctx.lr = 0x822663C8;
	sub_822664D8(ctx, base);
loc_822663C8:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x822663E4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lvx128 v0,r0,r29
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// vsubfp v12,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// addi r7,r1,96
	ctx.r7.s64 = ctx.r1.s64 + 96;
	// addi r6,r8,-28336
	ctx.r6.s64 = ctx.r8.s64 + -28336;
	// lfs f0,12(r28)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v10,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx128 v10,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bgt cr6,0x82266470
	if (ctx.cr6.gt) goto loc_82266470;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// stfs f31,96(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// addi r11,r1,84
	ctx.r11.s64 = ctx.r1.s64 + 84;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v11,v13,3,2
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// stvx128 v11,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_82266470:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// li r10,64
	ctx.r10.s64 = 64;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lvx128 v127,r30,r10
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,64(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x82266494;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// li r7,16
	ctx.r7.s64 = 16;
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// lwz r3,124(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 124);
	// lvlx v0,r30,r7
	temp.u32 = ctx.r30.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// vmaddfp128 v1,v127,v13,v1
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v127.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(ctx.v1.f32)));
	// lwz r11,100(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 100);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822664C4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_822664C4:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_822664D8"))) PPC_WEAK_FUNC(sub_822664D8);
PPC_FUNC_IMPL(__imp__sub_822664D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// addi r9,r11,-28384
	ctx.r9.s64 = ctx.r11.s64 + -28384;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v0,v1,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lfs f1,-25888(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821ee7c8
	ctx.lr = 0x82266514;
	sub_821EE7C8(ctx, base);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r11,r11,-27456
	ctx.r11.s64 = ctx.r11.s64 + -27456;
	// lfs f0,-12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	ctx.cr6.compare(ctx.f1.f64, ctx.f0.f64);
	// beq cr6,0x82266578
	if (ctx.cr6.eq) goto loc_82266578;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lfs f0,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r7,r9,-28240
	ctx.r7.s64 = ctx.r9.s64 + -28240;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v1,v13,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v2,r0,r8
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82267578
	ctx.lr = 0x8226655C;
	sub_82267578(ctx, base);
	// lwz r6,0(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// ld r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// ld r5,120(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// lwz r11,104(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 104);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82266578;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82266578:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226658C"))) PPC_WEAK_FUNC(sub_8226658C);
PPC_FUNC_IMPL(__imp__sub_8226658C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82266590"))) PPC_WEAK_FUNC(sub_82266590);
PPC_FUNC_IMPL(__imp__sub_82266590) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82266598;
	sub_82CA2BE8(ctx, base);
	// stfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r29,-31927
	ctx.r29.s64 = -2092367872;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,26912(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 26912);
	// lfs f1,12(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// addi r4,r11,120
	ctx.r4.s64 = ctx.r11.s64 + 120;
	// bl 0x821f0890
	ctx.lr = 0x822665CC;
	sub_821F0890(ctx, base);
	// lwz r11,26912(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 26912);
	// lis r9,27670
	ctx.r9.s64 = 1813381120;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// ori r8,r9,49517
	ctx.r8.u64 = ctx.r9.u64 | 49517;
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r6,88(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 88);
	// lwz r11,4(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// lwz r5,120(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 120);
	// mulli r10,r5,9377
	ctx.r10.s64 = ctx.r5.s64 * 9377;
	// addi r4,r10,9439
	ctx.r4.s64 = ctx.r10.s64 + 9439;
	// rotlwi r10,r4,19
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r4.u32, 19);
	// stw r10,120(r11)
	PPC_STORE_U32(ctx.r11.u32 + 120, ctx.r10.u32);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// mulhwu r11,r6,r8
	ctx.r11.u64 = (uint64_t(ctx.r6.u32) * uint64_t(ctx.r8.u32)) >> 32;
	// subf r5,r11,r6
	ctx.r5.s64 = ctx.r6.s64 - ctx.r11.s64;
	// rlwinm r10,r5,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 31) & 0x7FFFFFFF;
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r4,25,7,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 25) & 0x1FFFFFF;
	// mulli r10,r11,180
	ctx.r10.s64 = ctx.r11.s64 * 180;
	// subf r28,r10,r9
	ctx.r28.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r4,124(r7)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r7.u32 + 124);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,64(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 64);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8226663C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrldi r7,r28,32
	ctx.r7.u64 = ctx.r28.u64 & 0xFFFFFFFF;
	// lis r28,-32246
	ctx.r28.s64 = -2113273856;
	// std r7,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r7.u64);
	// lfd f0,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// addi r6,r28,-27468
	ctx.r6.s64 = ctx.r28.s64 + -27468;
	// frsp f12,f13
	ctx.f12.f64 = double(float(ctx.f13.f64));
	// lfs f0,8232(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 8232);
	ctx.f0.f64 = double(temp.f32);
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f13,476(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 476);
	ctx.f13.f64 = double(temp.f32);
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r3
	_mm_store_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fsubs f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// fmuls f30,f11,f13
	ctx.f30.f64 = double(float(ctx.f11.f64 * ctx.f13.f64));
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x82239e88
	ctx.lr = 0x82266684;
	sub_82239E88(ctx, base);
	// frsp f10,f1
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f1.f64));
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// lfs f9,96(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmadds f8,f10,f31,f9
	ctx.f8.f64 = double(float(ctx.f10.f64 * ctx.f31.f64 + ctx.f9.f64));
	// stfs f8,84(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// bl 0x82239f68
	ctx.lr = 0x822666AC;
	sub_82239F68(ctx, base);
	// lwz r11,26912(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 26912);
	// frsp f7,f1
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = double(float(ctx.f1.f64));
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lfs f6,100(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	ctx.f6.f64 = double(temp.f32);
	// lfs f0,-27468(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + -27468);
	ctx.f0.f64 = double(temp.f32);
	// lfs f1,16(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 16);
	ctx.f1.f64 = double(temp.f32);
	// lwz r7,12(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvx128 v12,r0,r9
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r6,88(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 88);
	// fmadds f5,f7,f31,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 * ctx.f31.f64 + ctx.f6.f64));
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lwz r11,4(r6)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// addi r4,r11,120
	ctx.r4.s64 = ctx.r11.s64 + 120;
	// bl 0x821f0890
	ctx.lr = 0x822666F0;
	sub_821F0890(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lfs f4,104(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	ctx.f4.f64 = double(temp.f32);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// fadds f3,f1,f4
	ctx.f3.f64 = double(float(ctx.f1.f64 + ctx.f4.f64));
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// stfs f3,96(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v9,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lvlx v11,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v8,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vrlimi128 v8,v9,4,3
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 57), 4));
	// vrlimi128 v10,v8,3,2
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 78), 3));
	// stvx128 v10,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-56(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82266740"))) PPC_WEAK_FUNC(sub_82266740);
PPC_FUNC_IMPL(__imp__sub_82266740) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd0
	ctx.lr = 0x82266748;
	sub_82CA2BD0(ctx, base);
	// stfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// mr r24,r3
	ctx.r24.u64 = ctx.r3.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r3,r11,104
	ctx.r3.s64 = ctx.r11.s64 + 104;
	// lwz r11,108(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226686c
	if (ctx.cr6.eq) goto loc_8226686C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266868
	if (ctx.cr6.eq) goto loc_82266868;
	// rotlwi r30,r10,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x822667ac
	if (ctx.cr6.eq) goto loc_822667AC;
	// lbz r11,144(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x822667b0
	if (!ctx.cr6.eq) goto loc_822667B0;
loc_822667AC:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_822667B0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266eb8
	if (ctx.cr6.eq) goto loc_82266EB8;
	// lwz r11,4(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
	// bctrl 
	ctx.lr = 0x822667D8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r4,124(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r7,64(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 64);
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x822667F0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r23,r11,-27456
	ctx.r23.s64 = ctx.r11.s64 + -27456;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r5
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// lfs f0,11504(r23)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 11504);
	ctx.f0.f64 = double(temp.f32);
	// vmsum3fp128 v11,v12,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx128 v11,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,96(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bge cr6,0x82266eb8
	if (!ctx.cr6.lt) goto loc_82266EB8;
	// lwz r10,40(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// rlwinm r9,r10,31,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82266930
	if (ctx.cr6.eq) goto loc_82266930;
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266874
	if (ctx.cr6.eq) goto loc_82266874;
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 33);
	// lwz r11,72(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x82266934
	goto loc_82266934;
loc_82266868:
	// bl 0x821940c8
	ctx.lr = 0x8226686C;
	sub_821940C8(ctx, base);
loc_8226686C:
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// b 0x822667ac
	goto loc_822667AC;
loc_82266874:
	// lwz r10,72(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// lwz r6,76(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x822668e0
	if (!ctx.cr0.gt) goto loc_822668E0;
loc_82266890:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,33
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 33, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822668b0
	if (ctx.cr6.lt) goto loc_822668B0;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_822668B0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822668cc
	if (ctx.cr6.eq) goto loc_822668CC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x822668d4
	goto loc_822668D4;
loc_822668CC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_822668D4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82266890
	if (ctx.cr6.gt) goto loc_82266890;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822668E0:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82266920
	if (ctx.cr6.eq) goto loc_82266920;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,33
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 33, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x822668fc
	if (ctx.cr6.gt) goto loc_822668FC;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_822668FC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266920
	if (!ctx.cr6.eq) goto loc_82266920;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lwz r11,92(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x82266934
	goto loc_82266934;
loc_82266920:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x82266934
	goto loc_82266934;
loc_82266930:
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
loc_82266934:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266944
	if (ctx.cr6.eq) goto loc_82266944;
	// lbz r4,140(r11)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + 140);
loc_82266944:
	// lwz r5,36(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// rlwinm r10,r5,4,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 4) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266a3c
	if (ctx.cr6.eq) goto loc_82266A3C;
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266980
	if (ctx.cr6.eq) goto loc_82266980;
	// lbz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 28);
	// lwz r11,72(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x82266a40
	goto loc_82266A40;
loc_82266980:
	// lwz r10,72(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// lwz r6,76(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x822669ec
	if (!ctx.cr0.gt) goto loc_822669EC;
loc_8226699C:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,28
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 28, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822669bc
	if (ctx.cr6.lt) goto loc_822669BC;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_822669BC:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822669d8
	if (ctx.cr6.eq) goto loc_822669D8;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x822669e0
	goto loc_822669E0;
loc_822669D8:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_822669E0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226699c
	if (ctx.cr6.gt) goto loc_8226699C;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822669EC:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82266a2c
	if (ctx.cr6.eq) goto loc_82266A2C;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,28
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 28, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82266a08
	if (ctx.cr6.gt) goto loc_82266A08;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266A08:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266a2c
	if (!ctx.cr6.eq) goto loc_82266A2C;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lwz r11,92(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x82266a40
	goto loc_82266A40;
loc_82266A2C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// b 0x82266a40
	goto loc_82266A40;
loc_82266A3C:
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
loc_82266A40:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266eb8
	if (ctx.cr6.eq) goto loc_82266EB8;
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x82266ab8
	if (!ctx.cr6.eq) goto loc_82266AB8;
	// lwz r11,704(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 704);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bgt cr6,0x82266a74
	if (ctx.cr6.gt) goto loc_82266A74;
	// lwz r11,708(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 708);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// ble cr6,0x82266a78
	if (!ctx.cr6.gt) goto loc_82266A78;
loc_82266A74:
	// li r11,1
	ctx.r11.s64 = 1;
loc_82266A78:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266ab8
	if (ctx.cr6.eq) goto loc_82266AB8;
	// stw r28,716(r24)
	PPC_STORE_U32(ctx.r24.u32 + 716, ctx.r28.u32);
	// addi r11,r24,716
	ctx.r11.s64 = ctx.r24.s64 + 716;
	// stw r28,720(r24)
	PPC_STORE_U32(ctx.r24.u32 + 720, ctx.r28.u32);
	// stw r28,724(r24)
	PPC_STORE_U32(ctx.r24.u32 + 724, ctx.r28.u32);
	// stw r28,728(r24)
	PPC_STORE_U32(ctx.r24.u32 + 728, ctx.r28.u32);
	// stw r28,732(r24)
	PPC_STORE_U32(ctx.r24.u32 + 732, ctx.r28.u32);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r28.u32);
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,704(r24)
	PPC_STORE_U64(ctx.r24.u32 + 704, ctx.r11.u64);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x82ca2c20
	// ERROR 82CA2C20
	return;
loc_82266AB8:
	// clrlwi r11,r4,24
	ctx.r11.u64 = ctx.r4.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266eb8
	if (!ctx.cr6.eq) goto loc_82266EB8;
	// rlwinm r11,r5,8,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 8) & 0x1;
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266bc4
	if (ctx.cr6.eq) goto loc_82266BC4;
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266b00
	if (ctx.cr6.eq) goto loc_82266B00;
	// lbz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 24);
	// lwz r11,72(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82266bc8
	goto loc_82266BC8;
loc_82266B00:
	// lwz r10,72(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// lwz r6,76(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82266b6c
	if (!ctx.cr0.gt) goto loc_82266B6C;
loc_82266B1C:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,24
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 24, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82266b3c
	if (ctx.cr6.lt) goto loc_82266B3C;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_82266B3C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82266b58
	if (ctx.cr6.eq) goto loc_82266B58;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82266b60
	goto loc_82266B60;
loc_82266B58:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82266B60:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82266b1c
	if (ctx.cr6.gt) goto loc_82266B1C;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82266B6C:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82266bb0
	if (ctx.cr6.eq) goto loc_82266BB0;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,24
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 24, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82266b88
	if (ctx.cr6.gt) goto loc_82266B88;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266B88:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266bb0
	if (!ctx.cr6.eq) goto loc_82266BB0;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lwz r11,92(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82266bc8
	goto loc_82266BC8;
loc_82266BB0:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82266bc8
	goto loc_82266BC8;
loc_82266BC4:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266BC8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266eb8
	if (ctx.cr6.eq) goto loc_82266EB8;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// addi r3,r11,-20096
	ctx.r3.s64 = ctx.r11.s64 + -20096;
	// bl 0x823db930
	ctx.lr = 0x82266BE0;
	sub_823DB930(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82500cb0
	ctx.lr = 0x82266BEC;
	sub_82500CB0(ctx, base);
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x82266c0c
	if (ctx.cr6.eq) goto loc_82266C0C;
	// lbz r11,144(r25)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r25.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82266c10
	if (!ctx.cr6.eq) goto loc_82266C10;
loc_82266C0C:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266C10:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266eb8
	if (ctx.cr6.eq) goto loc_82266EB8;
	// lwz r11,704(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 704);
	// lwz r10,20(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 20);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82266c40
	if (!ctx.cr6.eq) goto loc_82266C40;
	// lwz r11,24(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 24);
	// lwz r10,708(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 708);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x82266c44
	if (ctx.cr6.eq) goto loc_82266C44;
loc_82266C40:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266C44:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266eb8
	if (!ctx.cr6.eq) goto loc_82266EB8;
	// mr r27,r28
	ctx.r27.u64 = ctx.r28.u64;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x82266eb8
	if (ctx.cr6.eq) goto loc_82266EB8;
	// lwz r11,52(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 52);
	// rlwinm r10,r11,15,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 15) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82266d58
	if (ctx.cr6.eq) goto loc_82266D58;
	// lwz r11,140(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// lwz r10,72(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 72);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266c98
	if (ctx.cr6.eq) goto loc_82266C98;
	// lbz r11,145(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 145);
	// rotlwi r11,r11,3
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 3);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82266d5c
	goto loc_82266D5C;
loc_82266C98:
	// lwz r6,76(r25)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r25.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82266d00
	if (!ctx.cr0.gt) goto loc_82266D00;
loc_82266CB0:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,145
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 145, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82266cd0
	if (ctx.cr6.lt) goto loc_82266CD0;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_82266CD0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82266cec
	if (ctx.cr6.eq) goto loc_82266CEC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82266cf4
	goto loc_82266CF4;
loc_82266CEC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82266CF4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x82266cb0
	if (ctx.cr6.gt) goto loc_82266CB0;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82266D00:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82266d44
	if (ctx.cr6.eq) goto loc_82266D44;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,145
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 145, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82266d1c
	if (ctx.cr6.gt) goto loc_82266D1C;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266D1C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82266d44
	if (!ctx.cr6.eq) goto loc_82266D44;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lwz r11,92(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82266d5c
	goto loc_82266D5C;
loc_82266D44:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82266d5c
	goto loc_82266D5C;
loc_82266D58:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_82266D5C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266eb8
	if (ctx.cr6.eq) goto loc_82266EB8;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r28.u32);
	// lis r10,-32247
	ctx.r10.s64 = -2113339392;
	// stw r28,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r28.u32);
	// addi r29,r11,-26648
	ctx.r29.s64 = ctx.r11.s64 + -26648;
	// stw r28,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r28.u32);
	// addi r26,r10,63
	ctx.r26.s64 = ctx.r10.s64 + 63;
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r31,0(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
loc_82266D8C:
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82266e18
	if (ctx.cr6.eq) goto loc_82266E18;
	// addi r30,r31,8
	ctx.r30.s64 = ctx.r31.s64 + 8;
	// lwz r22,4(r24)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r24.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x821f0108
	ctx.lr = 0x82266DA8;
	sub_821F0108(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mr r5,r22
	ctx.r5.u64 = ctx.r22.u64;
	// bl 0x8299af70
	ctx.lr = 0x82266DB8;
	sub_8299AF70(ctx, base);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82266dcc
	if (!ctx.cr6.eq) goto loc_82266DCC;
	// twi 31,r0,22
loc_82266DCC:
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82266de0
	if (ctx.cr6.eq) goto loc_82266DE0;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
loc_82266DE0:
	// lis r4,-32484
	ctx.r4.s64 = -2128871424;
	// ori r4,r4,40389
	ctx.r4.u64 = ctx.r4.u64 | 40389;
	// bl 0x821f3c28
	ctx.lr = 0x82266DEC;
	sub_821F3C28(ctx, base);
	// stw r3,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r3.u32);
	// stfs f31,92(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82b30a58
	ctx.lr = 0x82266E00;
	sub_82B30A58(ctx, base);
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x82266e10
	if (!ctx.cr6.eq) goto loc_82266E10;
	// twi 31,r0,22
loc_82266E10:
	// lwz r31,0(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// b 0x82266d8c
	goto loc_82266D8C;
loc_82266E18:
	// lfs f6,-12(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + -12);
	ctx.f6.f64 = double(temp.f32);
	// addi r30,r24,716
	ctx.r30.s64 = ctx.r24.s64 + 716;
	// lfs f4,0(r23)
	temp.u32 = PPC_LOAD_U32(ctx.r23.u32 + 0);
	ctx.f4.f64 = double(temp.f32);
	// fmr f5,f6
	ctx.f5.f64 = ctx.f6.f64;
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
	// addi r29,r24,236
	ctx.r29.s64 = ctx.r24.s64 + 236;
loc_82266E30:
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// fmr f1,f4
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f4.f64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x827e0c68
	ctx.lr = 0x82266E40;
	sub_827E0C68(ctx, base);
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	ctx.cr6.compare(ctx.f1.f64, ctx.f0.f64);
	// beq cr6,0x82266e70
	if (ctx.cr6.eq) goto loc_82266E70;
	// fsubs f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
	// fcmpu cr6,f0,f6
	ctx.cr6.compare(ctx.f0.f64, ctx.f6.f64);
	// beq cr6,0x82266e70
	if (ctx.cr6.eq) goto loc_82266E70;
	// fabs f13,f0
	ctx.f13.u64 = ctx.f0.u64 & ~0x8000000000000000;
	// fabs f12,f5
	ctx.f12.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// fcmpu cr6,f13,f12
	ctx.cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// ble cr6,0x82266e70
	if (!ctx.cr6.gt) goto loc_82266E70;
	// mr r28,r31
	ctx.r28.u64 = ctx.r31.u64;
	// fmr f5,f0
	ctx.f5.f64 = ctx.f0.f64;
loc_82266E70:
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// stfs f1,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r30.u32 + 0, temp.u32);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// addi r29,r29,16
	ctx.r29.s64 = ctx.r29.s64 + 16;
	// cmplwi cr6,r31,5
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 5, ctx.xer);
	// blt cr6,0x82266e30
	if (ctx.cr6.lt) goto loc_82266E30;
	// fcmpu cr6,f5,f6
	ctx.cr6.compare(ctx.f5.f64, ctx.f6.f64);
	// beq cr6,0x82266ea0
	if (ctx.cr6.eq) goto loc_82266EA0;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// fmr f1,f5
	ctx.f1.f64 = ctx.f5.f64;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x8275d1b8
	ctx.lr = 0x82266EA0;
	sub_8275D1B8(ctx, base);
loc_82266EA0:
	// lwz r11,20(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 20);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r11,704(r24)
	PPC_STORE_U32(ctx.r24.u32 + 704, ctx.r11.u32);
	// lwz r10,24(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 24);
	// stw r10,708(r24)
	PPC_STORE_U32(ctx.r24.u32 + 708, ctx.r10.u32);
	// bl 0x823d2d28
	ctx.lr = 0x82266EB8;
	sub_823D2D28(ctx, base);
loc_82266EB8:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f31,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x82ca2c20
	// ERROR 82CA2C20
	return;
}

__attribute__((alias("__imp__sub_82266EC4"))) PPC_WEAK_FUNC(sub_82266EC4);
PPC_FUNC_IMPL(__imp__sub_82266EC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82266EC8"))) PPC_WEAK_FUNC(sub_82266EC8);
PPC_FUNC_IMPL(__imp__sub_82266EC8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r11,r11,0,0,0
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// li r9,4
	ctx.r9.s64 = 4;
	// lwz r10,0(r13)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0);
	// li r6,22
	ctx.r6.s64 = 22;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// rlwinm r5,r11,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r3,r9,r10
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// b 0x82d4eca8
	sub_82D4ECA8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82266EFC"))) PPC_WEAK_FUNC(sub_82266EFC);
PPC_FUNC_IMPL(__imp__sub_82266EFC) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82266F00"))) PPC_WEAK_FUNC(sub_82266F00);
PPC_FUNC_IMPL(__imp__sub_82266F00) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r13)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0);
	// li r10,4
	ctx.r10.s64 = 4;
	// li r5,22
	ctx.r5.s64 = 22;
	// li r4,128
	ctx.r4.s64 = 128;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// bl 0x82d4ec28
	ctx.lr = 0x82266F2C;
	sub_82D4EC28(ctx, base);
	// li r9,15
	ctx.r9.s64 = 15;
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r11,32
	ctx.r11.s64 = 32;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// li r9,-1
	ctx.r9.s64 = -1;
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
loc_82266F48:
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82266f48
	if (!ctx.cr6.eq) goto loc_82266F48;
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rlwinm r11,r11,0,0,0
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82266F84"))) PPC_WEAK_FUNC(sub_82266F84);
PPC_FUNC_IMPL(__imp__sub_82266F84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82266F88"))) PPC_WEAK_FUNC(sub_82266F88);
PPC_FUNC_IMPL(__imp__sub_82266F88) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd4
	ctx.lr = 0x82266F90;
	sub_82CA2BD4(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r24,0(r13)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r13.u32 + 0);
	// li r25,4
	ctx.r25.s64 = 4;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// li r5,22
	ctx.r5.s64 = 22;
	// rlwinm r4,r30,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwz r26,0(r31)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwinm r23,r10,0,0,0
	ctx.r23.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// lwzx r3,r25,r24
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r24.u32);
	// addi r27,r11,1
	ctx.r27.s64 = ctx.r11.s64 + 1;
	// bl 0x82d4ec28
	ctx.lr = 0x82266FC8;
	sub_82D4EC28(ctx, base);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// ble cr6,0x82266ff8
	if (!ctx.cr6.gt) goto loc_82266FF8;
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// li r9,-1
	ctx.r9.s64 = -1;
loc_82266FE0:
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bne cr6,0x82266fe0
	if (!ctx.cr6.eq) goto loc_82266FE0;
loc_82266FF8:
	// addi r11,r30,-1
	ctx.r11.s64 = ctx.r30.s64 + -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// stw r10,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r10.u32);
	// ble cr6,0x8226704c
	if (!ctx.cr6.gt) goto loc_8226704C;
	// rlwinm r11,r27,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// add r30,r11,r26
	ctx.r30.u64 = ctx.r11.u64 + ctx.r26.u64;
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
loc_82267020:
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// cmpwi cr6,r4,-1
	ctx.cr6.compare<int32_t>(ctx.r4.s32, -1, ctx.xer);
	// beq cr6,0x82267038
	if (ctx.cr6.eq) goto loc_82267038;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// bl 0x82265bc0
	ctx.lr = 0x82267038;
	sub_82265BC0(ctx, base);
loc_82267038:
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// addi r29,r29,4
	ctx.r29.s64 = ctx.r29.s64 + 4;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// bne cr6,0x82267020
	if (!ctx.cr6.eq) goto loc_82267020;
loc_8226704C:
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// bne cr6,0x82267068
	if (!ctx.cr6.eq) goto loc_82267068;
	// li r6,22
	ctx.r6.s64 = 22;
	// lwzx r3,r25,r24
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r24.u32);
	// rlwinm r5,r27,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x82d4eca8
	ctx.lr = 0x82267068;
	sub_82D4ECA8(ctx, base);
loc_82267068:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
}

__attribute__((alias("__imp__sub_82267070"))) PPC_WEAK_FUNC(sub_82267070);
PPC_FUNC_IMPL(__imp__sub_82267070) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lvx128 v0,r0,r4
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// vsubfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// addi r9,r11,-28464
	ctx.r9.s64 = ctx.r11.s64 + -28464;
	// addi r8,r1,36
	ctx.r8.s64 = ctx.r1.s64 + 36;
	// stfs f1,36(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 36, temp.u32);
	// lfs f0,-28492(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -28492);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f0,36(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 36, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vand v9,v12,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgefp. v8,v9,v10
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v8.f32, _mm_cmpge_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v10.f32)));
	ctx.cr6.setFromMask(_mm_load_ps(ctx.v8.f32), 0xF);
	// mfocrf r7,2
	ctx.r7.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// rlwinm r6,r7,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x8226715c
	if (ctx.cr6.eq) goto loc_8226715C;
	// li r11,16
	ctx.r11.s64 = 16;
	// addi r10,r1,36
	ctx.r10.s64 = ctx.r1.s64 + 36;
	// lvx128 v13,r4,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r3,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v12,v13
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// vand v8,v11,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgefp. v7,v8,v9
	_mm_store_ps(ctx.v7.f32, _mm_cmpge_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32)));
	ctx.cr6.setFromMask(_mm_load_ps(ctx.v7.f32), 0xF);
	// mfocrf r9,2
	ctx.r9.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8226715c
	if (ctx.cr6.eq) goto loc_8226715C;
	// li r11,32
	ctx.r11.s64 = 32;
	// addi r10,r1,36
	ctx.r10.s64 = ctx.r1.s64 + 36;
	// lvx128 v13,r4,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r3,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v12,v13
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// vand v8,v11,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgefp. v7,v8,v9
	_mm_store_ps(ctx.v7.f32, _mm_cmpge_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32)));
	ctx.cr6.setFromMask(_mm_load_ps(ctx.v7.f32), 0xF);
	// mfocrf r9,2
	ctx.r9.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8226715c
	if (ctx.cr6.eq) goto loc_8226715C;
	// li r11,48
	ctx.r11.s64 = 48;
	// addi r10,r1,36
	ctx.r10.s64 = ctx.r1.s64 + 36;
	// lvx128 v13,r4,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r3,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v12,v13
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// vand v8,v11,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vcmpgefp. v7,v8,v9
	_mm_store_ps(ctx.v7.f32, _mm_cmpge_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32)));
	ctx.cr6.setFromMask(_mm_load_ps(ctx.v7.f32), 0xF);
	// mfocrf r9,2
	ctx.r9.u64 = (ctx.cr6.lt << 7) | (ctx.cr6.gt << 6) | (ctx.cr6.eq << 5) | (ctx.cr6.so << 4);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x82267160
	if (!ctx.cr6.eq) goto loc_82267160;
loc_8226715C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82267160:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267168"))) PPC_WEAK_FUNC(sub_82267168);
PPC_FUNC_IMPL(__imp__sub_82267168) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// addi r3,r31,4
	ctx.r3.s64 = ctx.r31.s64 + 4;
	// bl 0x821f7798
	ctx.lr = 0x8226718C;
	sub_821F7798(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822752b0
	ctx.lr = 0x82267194;
	sub_822752B0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x822651e0
	ctx.lr = 0x822671A0;
	sub_822651E0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822671BC"))) PPC_WEAK_FUNC(sub_822671BC);
PPC_FUNC_IMPL(__imp__sub_822671BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822671C0"))) PPC_WEAK_FUNC(sub_822671C0);
PPC_FUNC_IMPL(__imp__sub_822671C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// extsw r6,r9
	ctx.r6.s64 = ctx.r9.s32;
	// addi r5,r7,-27456
	ctx.r5.s64 = ctx.r7.s64 + -27456;
	// lwz r11,-27380(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -27380);
	// lis r4,-32240
	ctx.r4.s64 = -2112880640;
	// std r6,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r6.u64);
	// lfd f0,-16(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// subf r3,r10,r11
	ctx.r3.s64 = ctx.r11.s64 - ctx.r10.s64;
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// extsw r11,r3
	ctx.r11.s64 = ctx.r3.s32;
	// frsp f11,f13
	ctx.f11.f64 = double(float(ctx.f13.f64));
	// lfs f0,-25888(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25888);
	ctx.f0.f64 = double(temp.f32);
	// addi r10,r4,368
	ctx.r10.s64 = ctx.r4.s64 + 368;
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r11.u64);
	// lfd f10,-16(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// lfs f13,-12(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -12);
	ctx.f13.f64 = double(temp.f32);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// lfs f12,-27456(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -27456);
	ctx.f12.f64 = double(temp.f32);
	// fdivs f7,f8,f11
	ctx.f7.f64 = double(float(ctx.f8.f64 / ctx.f11.f64));
	// fsubs f6,f7,f0
	ctx.f6.f64 = double(float(ctx.f7.f64 - ctx.f0.f64));
	// fcmpu cr6,f6,f13
	ctx.cr6.compare(ctx.f6.f64, ctx.f13.f64);
	// mfcr r9
	ctx.r9.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r9.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r9.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r9.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r9.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r9.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r9.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r9.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r9.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r9.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r9.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r9.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r9.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r9.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r9.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r9.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r9.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r9.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r9.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r9.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r9.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r9.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r9.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r9.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r9.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r9.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r9.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r9.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r9.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r9.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r9.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r9.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r8,r9,27,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x4;
	// rlwinm r7,r9,30,29,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x4;
	// or r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 | ctx.r7.u64;
	// lfsx f5,r10,r6
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	ctx.f5.f64 = double(temp.f32);
	// fsel f4,f5,f7,f0
	ctx.f4.f64 = ctx.f5.f64 >= 0.0 ? ctx.f7.f64 : ctx.f0.f64;
	// fsubs f3,f4,f12
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f12.f64));
	// fcmpu cr6,f3,f13
	ctx.cr6.compare(ctx.f3.f64, ctx.f13.f64);
	// mfcr r5
	ctx.r5.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r5.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r5.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r5.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r5.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r5.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r5.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r5.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r5.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r5.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r5.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r5.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r5.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r5.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r5.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r5.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r5.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r5.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r5.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r5.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r5.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r5.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r5.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r5.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r5.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r5.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r5.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r5.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r5.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r5.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r5.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r5.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r4,r5,27,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x4;
	// rlwinm r3,r5,30,29,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x4;
	// or r11,r4,r3
	ctx.r11.u64 = ctx.r4.u64 | ctx.r3.u64;
	// lfsx f2,r10,r11
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	ctx.f2.f64 = double(temp.f32);
	// fsel f1,f2,f12,f4
	ctx.f1.f64 = ctx.f2.f64 >= 0.0 ? ctx.f12.f64 : ctx.f4.f64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267264"))) PPC_WEAK_FUNC(sub_82267264);
PPC_FUNC_IMPL(__imp__sub_82267264) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267268"))) PPC_WEAK_FUNC(sub_82267268);
PPC_FUNC_IMPL(__imp__sub_82267268) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r9,40(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// rlwinm r8,r9,4,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0x1;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8226738c
	if (ctx.cr6.eq) goto loc_8226738C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822672c4
	if (ctx.cr6.eq) goto loc_822672C4;
	// lbz r10,60(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 60);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82267390
	goto loc_82267390;
loc_822672C4:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// subf r8,r10,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82267334
	if (!ctx.cr0.gt) goto loc_82267334;
loc_822672E4:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,60
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 60, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82267304
	if (ctx.cr6.lt) goto loc_82267304;
	// li r7,0
	ctx.r7.s64 = 0;
loc_82267304:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82267320
	if (ctx.cr6.eq) goto loc_82267320;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82267328
	goto loc_82267328;
loc_82267320:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82267328:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x822672e4
	if (ctx.cr6.gt) goto loc_822672E4;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82267334:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82267378
	if (ctx.cr6.eq) goto loc_82267378;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,60
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 60, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82267350
	if (ctx.cr6.gt) goto loc_82267350;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82267350:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82267378
	if (!ctx.cr6.eq) goto loc_82267378;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82267390
	goto loc_82267390;
loc_82267378:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82267390
	goto loc_82267390;
loc_8226738C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82267390:
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// clrlwi r8,r11,24
	ctx.r8.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// lfs f31,-27468(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27468);
	ctx.f31.f64 = double(temp.f32);
	// beq cr6,0x822673d0
	if (ctx.cr6.eq) goto loc_822673D0;
	// li r11,528
	ctx.r11.s64 = 528;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r10,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx128 v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// beq cr6,0x822673d0
	if (ctx.cr6.eq) goto loc_822673D0;
	// stvx128 v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x822674b4
	goto loc_822674B4;
loc_822673D0:
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// addi r4,r10,-28304
	ctx.r4.s64 = ctx.r10.s64 + -28304;
	// lwz r3,24(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// bl 0x822674d0
	ctx.lr = 0x822673E4;
	sub_822674D0(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x822674b0
	if (ctx.cr6.eq) goto loc_822674B0;
	// lwz r11,312(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 312);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822674b0
	if (ctx.cr6.eq) goto loc_822674B0;
	// lwz r11,308(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 308);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822674b0
	if (ctx.cr6.eq) goto loc_822674B0;
	// addi r11,r3,248
	ctx.r11.s64 = ctx.r3.s64 + 248;
	// stfs f31,80(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// li r10,8
	ctx.r10.s64 = 8;
	// li r9,4
	ctx.r9.s64 = 4;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lvlx v0,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,r11,r10
	temp.u32 = ctx.r11.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,r11,r9
	temp.u32 = ctx.r11.u32 + ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v0,v13,4,3
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v11,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vrlimi128 v0,v12,3,2
	_mm_store_ps(ctx.v0.f32, _mm_blend_ps(_mm_load_ps(ctx.v0.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// stvx128 v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822050e0
	ctx.lr = 0x82267448;
	sub_822050E0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x822674b0
	if (!ctx.cr6.eq) goto loc_822674B0;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822050e0
	ctx.lr = 0x82267464;
	sub_822050E0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x822674b0
	if (!ctx.cr6.eq) goto loc_822674B0;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f1,88(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822050e0
	ctx.lr = 0x82267480;
	sub_822050E0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x822674b0
	if (!ctx.cr6.eq) goto loc_822674B0;
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// vmsum3fp128 v13,v0,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// li r3,1
	ctx.r3.s64 = 1;
	// lfs f0,1204(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1204);
	ctx.f0.f64 = double(temp.f32);
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bgt cr6,0x822674b4
	if (ctx.cr6.gt) goto loc_822674B4;
loc_822674B0:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822674B4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822674CC"))) PPC_WEAK_FUNC(sub_822674CC);
PPC_FUNC_IMPL(__imp__sub_822674CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822674D0"))) PPC_WEAK_FUNC(sub_822674D0);
PPC_FUNC_IMPL(__imp__sub_822674D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x822674D8;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x82267544
	if (!ctx.cr6.gt) goto loc_82267544;
	// li r30,0
	ctx.r30.s64 = 0;
loc_822674F8:
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r30.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82267510;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82267510:
	// cmplw cr6,r28,r3
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r3.u32, ctx.xer);
	// beq cr6,0x82267550
	if (ctx.cr6.eq) goto loc_82267550;
	// lwz r3,268(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne 0x82267510
	if (!ctx.cr0.eq) goto loc_82267510;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82267528:
	// clrlwi. r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82267558
	if (!ctx.cr0.eq) goto loc_82267558;
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x822674f8
	if (ctx.cr6.lt) goto loc_822674F8;
loc_82267544:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82267548:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_82267550:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82267528
	goto loc_82267528;
loc_82267558:
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// b 0x82267548
	goto loc_82267548;
}

__attribute__((alias("__imp__sub_82267568"))) PPC_WEAK_FUNC(sub_82267568);
PPC_FUNC_IMPL(__imp__sub_82267568) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// addi r3,r11,-28304
	ctx.r3.s64 = ctx.r11.s64 + -28304;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267574"))) PPC_WEAK_FUNC(sub_82267574);
PPC_FUNC_IMPL(__imp__sub_82267574) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267578"))) PPC_WEAK_FUNC(sub_82267578);
PPC_FUNC_IMPL(__imp__sub_82267578) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stvx128 v1,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v2,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82278808
	ctx.lr = 0x822675A8;
	sub_82278808(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lfs f1,-25888(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8223b988
	ctx.lr = 0x822675BC;
	sub_8223B988(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822675D0"))) PPC_WEAK_FUNC(sub_822675D0);
PPC_FUNC_IMPL(__imp__sub_822675D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be4
	ctx.lr = 0x822675D8;
	sub_82CA2BE4(ctx, base);
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// mulli r11,r10,224
	ctx.r11.s64 = ctx.r10.s64 * 224;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// li r9,2128
	ctx.r9.s64 = 2128;
	// li r7,2160
	ctx.r7.s64 = 2160;
	// li r8,2144
	ctx.r8.s64 = 2144;
	// li r6,2176
	ctx.r6.s64 = 2176;
	// li r5,2192
	ctx.r5.s64 = 2192;
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// lvx128 v0,r11,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,224
	ctx.r9.s64 = ctx.r1.s64 + 224;
	// lvx128 v12,r11,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r11,r8
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// lvx128 v11,r11,r6
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// lvx128 v10,r11,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,240
	ctx.r6.s64 = ctx.r1.s64 + 240;
	// stvx128 v0,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r5,2320
	ctx.r5.s64 = 2320;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// stvx128 v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// stvx128 v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r10,10
	ctx.r8.s64 = ctx.r10.s64 + 10;
	// stvx128 v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r9,-27468
	ctx.r7.s64 = ctx.r9.s64 + -27468;
	// lvx128 v9,r11,r5
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r5,2224
	ctx.r5.s64 = 2224;
	// stvx128 v9,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mulli r4,r8,224
	ctx.r4.s64 = ctx.r8.s64 * 224;
	// lfs f0,-27468(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27468);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lfs f0,10908(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 10908);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,112(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lfs f0,120(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 120);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v7,r11,r5
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r4,r3
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r4.u32 + ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r6,2208
	ctx.r6.s64 = 2208;
	// li r9,2256
	ctx.r9.s64 = 2256;
	// li r8,2272
	ctx.r8.s64 = 2272;
	// li r7,2288
	ctx.r7.s64 = 2288;
	// li r5,2336
	ctx.r5.s64 = 2336;
	// lvx128 v8,r11,r6
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r6,2304
	ctx.r6.s64 = 2304;
	// lvx128 v5,r11,r9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r30,r1,240
	ctx.r30.s64 = ctx.r1.s64 + 240;
	// lvx128 v4,r11,r8
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,160
	ctx.r9.s64 = ctx.r1.s64 + 160;
	// lvx128 v3,r11,r7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,192
	ctx.r8.s64 = ctx.r1.s64 + 192;
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// lvx128 v1,r11,r5
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r5,0
	ctx.r5.s64 = 0;
	// lvx128 v2,r11,r6
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32 + ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f12.f64 = double(temp.f32);
	// addi r29,r1,208
	ctx.r29.s64 = ctx.r1.s64 + 208;
	// lfs f11,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f11.f64 = double(temp.f32);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// lfs f10,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f10.f64 = double(temp.f32);
	// addi r11,r1,256
	ctx.r11.s64 = ctx.r1.s64 + 256;
	// lfs f9,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f9.f64 = double(temp.f32);
	// addi r28,r1,272
	ctx.r28.s64 = ctx.r1.s64 + 272;
	// lfs f13,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f13.f64 = double(temp.f32);
	// addi r27,r1,176
	ctx.r27.s64 = ctx.r1.s64 + 176;
	// lfs f8,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f8.f64 = double(temp.f32);
	// li r6,1
	ctx.r6.s64 = 1;
	// stfs f13,96(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stfs f12,100(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// addi r4,r10,8
	ctx.r4.s64 = ctx.r10.s64 + 8;
	// stfs f11,104(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f10,108(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 108, temp.u32);
	// stfs f9,116(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stvx128 v8,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v7,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v6,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v5,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v3,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// stvx128 v2,r0,r28
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,256(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 256);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f12.f64 = double(temp.f32);
	// stvx128 v1,r0,r27
	_mm_store_si128((__m128i*)(base + ((ctx.r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f7,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f7.f64 = double(temp.f32);
	// fmuls f4,f7,f0
	ctx.f4.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// stvx128 v4,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f5,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f5.f64 = double(temp.f32);
	// lfs f3,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,224(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	ctx.f1.f64 = double(temp.f32);
	// lfs f0,208(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	ctx.f0.f64 = double(temp.f32);
	// stfs f5,120(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f3,124(r1)
	temp.f32 = float(ctx.f3.f64);
	PPC_STORE_U32(ctx.r1.u32 + 124, temp.u32);
	// stfs f2,128(r1)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r1.u32 + 128, temp.u32);
	// stfs f1,132(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 132, temp.u32);
	// stfs f0,136(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 136, temp.u32);
	// stfs f13,140(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 140, temp.u32);
	// stfs f12,144(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 144, temp.u32);
	// stfs f6,148(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 148, temp.u32);
	// stfs f4,152(r1)
	temp.f32 = float(ctx.f4.f64);
	PPC_STORE_U32(ctx.r1.u32 + 152, temp.u32);
	// bl 0x821d08c0
	ctx.lr = 0x82267780;
	sub_821D08C0(ctx, base);
	// ld r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r31.u32 + 52);
	// addi r11,r31,80
	ctx.r11.s64 = ctx.r31.s64 + 80;
	// lis r4,-31927
	ctx.r4.s64 = -2092367872;
	// lfd f11,72(r31)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 72);
	// lfd f0,27720(r4)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r4.u32 + 27720);
	// stfd f0,72(r31)
	PPC_STORE_U64(ctx.r31.u32 + 72, ctx.f0.u64);
	// fsub f10,f0,f11
	ctx.f10.f64 = ctx.f0.f64 - ctx.f11.f64;
	// lfs f8,80(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	ctx.f8.f64 = double(temp.f32);
	// lfs f9,84(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	ctx.f9.f64 = double(temp.f32);
	// frsp f5,f10
	ctx.f5.f64 = double(float(ctx.f10.f64));
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// std r3,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r3.u64);
	// lfs f6,84(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	ctx.f6.f64 = double(temp.f32);
	// lfs f7,80(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f7.f64 = double(temp.f32);
	// stw r11,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r11.u32);
	// fmuls f3,f7,f5
	ctx.f3.f64 = double(float(ctx.f7.f64 * ctx.f5.f64));
	// fmuls f4,f6,f5
	ctx.f4.f64 = double(float(ctx.f6.f64 * ctx.f5.f64));
	// fadds f1,f3,f8
	ctx.f1.f64 = double(float(ctx.f3.f64 + ctx.f8.f64));
	// stfs f1,80(r31)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r31.u32 + 80, temp.u32);
	// fadds f2,f9,f4
	ctx.f2.f64 = double(float(ctx.f9.f64 + ctx.f4.f64));
	// stfs f2,84(r31)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r31.u32 + 84, temp.u32);
	// bl 0x8222c3e8
	ctx.lr = 0x822677D8;
	sub_8222C3E8(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64));
	// lfs f13,80(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	ctx.f13.f64 = double(temp.f32);
	// lfs f1,84(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	ctx.f1.f64 = double(temp.f32);
	// fsubs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// stfs f12,80(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 80, temp.u32);
	// bl 0x8222c3e8
	ctx.lr = 0x822677F0;
	sub_8222C3E8(ctx, base);
	// frsp f11,f1
	ctx.fpscr.disableFlushMode();
	ctx.f11.f64 = double(float(ctx.f1.f64));
	// lfs f10,84(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	ctx.f10.f64 = double(temp.f32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// fsubs f9,f10,f11
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f11.f64));
	// stfs f9,84(r31)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r31.u32 + 84, temp.u32);
	// bl 0x8227ae18
	ctx.lr = 0x8226780C;
	sub_8227AE18(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c34
	// ERROR 82CA2C34
	return;
}

__attribute__((alias("__imp__sub_82267814"))) PPC_WEAK_FUNC(sub_82267814);
PPC_FUNC_IMPL(__imp__sub_82267814) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267818"))) PPC_WEAK_FUNC(sub_82267818);
PPC_FUNC_IMPL(__imp__sub_82267818) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bdc
	ctx.lr = 0x82267820;
	sub_82CA2BDC(ctx, base);
	// stfd f31,-72(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	ctx.r27.s64 = 0;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// mr r29,r27
	ctx.r29.u64 = ctx.r27.u64;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r27.u32);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r29.u32);
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// stw r30,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r30.u32);
	// lfs f31,-27468(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27468);
	ctx.f31.f64 = double(temp.f32);
	// addi r31,r3,256
	ctx.r31.s64 = ctx.r3.s64 + 256;
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
loc_82267858:
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// addi r4,r11,16
	ctx.r4.s64 = ctx.r11.s64 + 16;
	// bl 0x822675d0
	ctx.lr = 0x82267870;
	sub_822675D0(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x82267888
	if (!ctx.cr6.eq) goto loc_82267888;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x822678a0
	goto loc_822678A0;
loc_82267888:
	// lfs f0,0(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// bgt cr6,0x8226789c
	if (ctx.cr6.gt) goto loc_8226789C;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_8226789C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
loc_822678A0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226790c
	if (ctx.cr6.eq) goto loc_8226790C;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x822678bc
	if (!ctx.cr6.eq) goto loc_822678BC;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x822678c8
	goto loc_822678C8;
loc_822678BC:
	// lwz r11,108(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// subf r9,r29,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r29.s64;
	// srawi r11,r9,2
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 2;
loc_822678C8:
	// subf r9,r29,r30
	ctx.r9.s64 = ctx.r30.s64 - ctx.r29.s64;
	// srawi r8,r9,2
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r9.s32 >> 2;
	// cmplw cr6,r8,r11
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x822678e8
	if (!ctx.cr6.lt) goto loc_822678E8;
	// stw r10,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r10.u32);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// stw r30,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r30.u32);
	// b 0x8226790c
	goto loc_8226790C;
loc_822678E8:
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r27.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x82a734d0
	ctx.lr = 0x82267904;
	sub_82A734D0(ctx, base);
	// lwz r30,104(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r29,100(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_8226790C:
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplwi cr6,r28,4
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 4, ctx.xer);
	// blt cr6,0x82267858
	if (ctx.cr6.lt) goto loc_82267858;
	// subf r11,r29,r30
	ctx.r11.s64 = ctx.r30.s64 - ctx.r29.s64;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// lis r10,-32231
	ctx.r10.s64 = -2112290816;
	// stw r29,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r29.u32);
	// srawi r31,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r31.s64 = ctx.r11.s32 >> 2;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r27.u32);
	// addi r6,r10,-9760
	ctx.r6.s64 = ctx.r10.s64 + -9760;
	// stw r27,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r27.u32);
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x8218db30
	ctx.lr = 0x8226794C;
	sub_8218DB30(ctx, base);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x82267974
	if (ctx.cr6.eq) goto loc_82267974;
	// mr r30,r29
	ctx.r30.u64 = ctx.r29.u64;
loc_82267958:
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x821db170
	ctx.lr = 0x82267968;
	sub_821DB170(ctx, base);
	// addic. r31,r31,-1
	ctx.xer.ca = ctx.r31.u32 > 0;
	ctx.r31.s64 = ctx.r31.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// bne 0x82267958
	if (!ctx.cr0.eq) goto loc_82267958;
loc_82267974:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x82267984
	if (ctx.cr6.eq) goto loc_82267984;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8221be68
	ctx.lr = 0x82267984;
	sub_8221BE68(ctx, base);
loc_82267984:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f31,-72(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// b 0x82ca2c2c
	// ERROR 82CA2C2C
	return;
}

__attribute__((alias("__imp__sub_82267990"))) PPC_WEAK_FUNC(sub_82267990);
PPC_FUNC_IMPL(__imp__sub_82267990) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lwz r31,8(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// b 0x822679d4
	goto loc_822679D4;
loc_822679B4:
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x822679d0
	if (ctx.cr0.eq) goto loc_822679D0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x82ca6320
	ctx.lr = 0x822679C8;
	sub_82CA6320(ctx, base);
	// cmpwi r3,0
	ctx.cr0.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq 0x822679f8
	if (ctx.cr0.eq) goto loc_822679F8;
loc_822679D0:
	// lwz r31,24(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
loc_822679D4:
	// cmplwi r31,0
	ctx.cr0.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne 0x822679b4
	if (!ctx.cr0.eq) goto loc_822679B4;
	// li r3,0
	ctx.r3.s64 = 0;
loc_822679E0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822679F8:
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// b 0x822679e0
	goto loc_822679E0;
}

__attribute__((alias("__imp__sub_82267A00"))) PPC_WEAK_FUNC(sub_82267A00);
PPC_FUNC_IMPL(__imp__sub_82267A00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82267A08;
	sub_82CA2BE8(ctx, base);
	// stfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x82ffff44
	ctx.lr = 0x82267A14;
	sub_82FFFF44(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// vor128 v125,v1,v1
	_mm_store_si128((__m128i*)ctx.v125.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// vor128 v124,v2,v2
	_mm_store_si128((__m128i*)ctx.v124.u8, _mm_load_si128((__m128i*)ctx.v2.u8));
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// lwz r3,-720(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -720);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82267b90
	if (ctx.cr6.eq) goto loc_82267B90;
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// addi r4,r11,30304
	ctx.r4.s64 = ctx.r11.s64 + 30304;
	// bl 0x82267be0
	ctx.lr = 0x82267A40;
	sub_82267BE0(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82267b90
	if (ctx.cr6.eq) goto loc_82267B90;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r4,r11,4040
	ctx.r4.s64 = ctx.r11.s64 + 4040;
	// bl 0x82267990
	ctx.lr = 0x82267A54;
	sub_82267990(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82267b90
	if (ctx.cr6.eq) goto loc_82267B90;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lfs f0,-27468(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27468);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r11,r1,92
	ctx.r11.s64 = ctx.r1.s64 + 92;
	// lfs f13,-12728(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -12728);
	ctx.f13.f64 = double(temp.f32);
	// addi r28,r1,192
	ctx.r28.s64 = ctx.r1.s64 + 192;
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,92
	ctx.r10.s64 = ctx.r1.s64 + 92;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,88
	ctx.r9.s64 = ctx.r1.s64 + 88;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r31,r1,144
	ctx.r31.s64 = ctx.r1.s64 + 144;
	// stfs f13,80(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r1,192
	ctx.r7.s64 = ctx.r1.s64 + 192;
	// lvlx v11,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r30,r1,160
	ctx.r30.s64 = ctx.r1.s64 + 160;
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r29,r1,176
	ctx.r29.s64 = ctx.r1.s64 + 176;
	// lvlx v10,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// stfs f13,92(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lvlx v9,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stfs f0,92(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// stfs f0,88(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// lvlx v8,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v9,v10,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// lvlx v7,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v7,v8,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// vrlimi128 v13,v11,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vrlimi128 v7,v9,3,2
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 78), 3));
	// vaddfp128 v127,v125,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v127.f32, _mm_add_ps(_mm_load_ps(ctx.v125.f32), _mm_load_ps(ctx.v13.f32)));
	// vaddfp128 v126,v124,v7
	_mm_store_ps(ctx.v126.f32, _mm_add_ps(_mm_load_ps(ctx.v124.f32), _mm_load_ps(ctx.v7.f32)));
	// stvx128 v127,r0,r28
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f10,196(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	ctx.f10.f64 = double(temp.f32);
	// stvx128 v127,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,168(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,176(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	ctx.f11.f64 = double(temp.f32);
	// lfs f9,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f9.f64 = double(temp.f32);
	// stvx128 v127,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,128(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	ctx.f0.f64 = double(temp.f32);
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// stfs f9,100(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 100, temp.u32);
	// stfs f13,120(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 120, temp.u32);
	// stfs f12,116(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 116, temp.u32);
	// stfs f11,112(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// bl 0x821d9a88
	ctx.lr = 0x82267B5C;
	sub_821D9A88(ctx, base);
	// clrlwi r6,r3,24
	ctx.r6.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x82267ba8
	if (ctx.cr6.eq) goto loc_82267BA8;
	// vor128 v2,v126,v126
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v126.u8));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// bl 0x8263aa48
	ctx.lr = 0x82267B74;
	sub_8263AA48(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82267ba8
	if (!ctx.cr6.eq) goto loc_82267BA8;
	// vor128 v2,v124,v124
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)ctx.v124.u8));
	// vor128 v1,v125,v125
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v125.u8));
	// bl 0x8263aa48
	ctx.lr = 0x82267B8C;
	sub_8263AA48(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
loc_82267B90:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x830001dc
	ctx.lr = 0x82267BA0;
	sub_830001DC(ctx, base);
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_82267BA8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// addi r12,r1,-48
	ctx.r12.s64 = ctx.r1.s64 + -48;
	// bl 0x830001dc
	ctx.lr = 0x82267BB8;
	sub_830001DC(ctx, base);
	// lfd f31,-48(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_82267BC0"))) PPC_WEAK_FUNC(sub_82267BC0);
PPC_FUNC_IMPL(__imp__sub_82267BC0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// addi r3,r11,11360
	ctx.r3.s64 = ctx.r11.s64 + 11360;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267BCC"))) PPC_WEAK_FUNC(sub_82267BCC);
PPC_FUNC_IMPL(__imp__sub_82267BCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267BD0"))) PPC_WEAK_FUNC(sub_82267BD0);
PPC_FUNC_IMPL(__imp__sub_82267BD0) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// addi r3,r11,23744
	ctx.r3.s64 = ctx.r11.s64 + 23744;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267BDC"))) PPC_WEAK_FUNC(sub_82267BDC);
PPC_FUNC_IMPL(__imp__sub_82267BDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267BE0"))) PPC_WEAK_FUNC(sub_82267BE0);
PPC_FUNC_IMPL(__imp__sub_82267BE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x82267BE8;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x82267c64
	if (!ctx.cr6.gt) goto loc_82267C64;
	// li r30,0
	ctx.r30.s64 = 0;
loc_82267C08:
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// lwzx r11,r11,r30
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r30.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82267c50
	if (ctx.cr6.eq) goto loc_82267C50;
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// lwzx r3,r11,r30
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r30.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82267C30;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82267C30:
	// cmplw cr6,r28,r3
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r3.u32, ctx.xer);
	// beq cr6,0x82267c70
	if (ctx.cr6.eq) goto loc_82267C70;
	// lwz r3,268(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne 0x82267c30
	if (!ctx.cr0.eq) goto loc_82267C30;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82267C48:
	// clrlwi. r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82267c78
	if (!ctx.cr0.eq) goto loc_82267C78;
loc_82267C50:
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x82267c08
	if (ctx.cr6.lt) goto loc_82267C08;
loc_82267C64:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82267C68:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_82267C70:
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82267c48
	goto loc_82267C48;
loc_82267C78:
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r10,r11
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// b 0x82267c68
	goto loc_82267C68;
}

__attribute__((alias("__imp__sub_82267C88"))) PPC_WEAK_FUNC(sub_82267C88);
PPC_FUNC_IMPL(__imp__sub_82267C88) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31947
	ctx.r11.s64 = -2093678592;
	// addi r3,r11,-7468
	ctx.r3.s64 = ctx.r11.s64 + -7468;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267C94"))) PPC_WEAK_FUNC(sub_82267C94);
PPC_FUNC_IMPL(__imp__sub_82267C94) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267C98"))) PPC_WEAK_FUNC(sub_82267C98);
PPC_FUNC_IMPL(__imp__sub_82267C98) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// addi r3,r11,30304
	ctx.r3.s64 = ctx.r11.s64 + 30304;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267CA4"))) PPC_WEAK_FUNC(sub_82267CA4);
PPC_FUNC_IMPL(__imp__sub_82267CA4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267CA8"))) PPC_WEAK_FUNC(sub_82267CA8);
PPC_FUNC_IMPL(__imp__sub_82267CA8) {
	PPC_FUNC_PROLOGUE();
	// lis r11,-31946
	ctx.r11.s64 = -2093613056;
	// addi r3,r11,-30196
	ctx.r3.s64 = ctx.r11.s64 + -30196;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267CB4"))) PPC_WEAK_FUNC(sub_82267CB4);
PPC_FUNC_IMPL(__imp__sub_82267CB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267CB8"))) PPC_WEAK_FUNC(sub_82267CB8);
PPC_FUNC_IMPL(__imp__sub_82267CB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82267CC0;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r31,-31948
	ctx.r31.s64 = -2093744128;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// addi r4,r11,-28028
	ctx.r4.s64 = ctx.r11.s64 + -28028;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r3,-720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + -720);
	// bl 0x82267be0
	ctx.lr = 0x82267CE0;
	sub_82267BE0(ctx, base);
	// lwz r9,-720(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + -720);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82267d28
	if (ctx.cr6.eq) goto loc_82267D28;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x82267d28
	if (ctx.cr6.eq) goto loc_82267D28;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lwz r8,27384(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 27384);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x82267d28
	if (ctx.cr6.eq) goto loc_82267D28;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lwz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lwz r11,27388(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 27388);
	// cmplw cr6,r11,r7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r7.u32, ctx.xer);
	// bge cr6,0x82267d28
	if (!ctx.cr6.lt) goto loc_82267D28;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,27388(r10)
	PPC_STORE_U32(ctx.r10.u32 + 27388, ctx.r11.u32);
	// stwx r30,r7,r8
	PPC_STORE_U32(ctx.r7.u32 + ctx.r8.u32, ctx.r30.u32);
loc_82267D28:
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f12,8(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,464(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 464);
	ctx.f11.f64 = double(temp.f32);
	// addi r31,r30,464
	ctx.r31.s64 = ctx.r30.s64 + 464;
	// lfs f0,0(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lfs f13,472(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 472);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// lfs f10,468(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 468);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f13,f13,f12
	ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// lfs f9,4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// lfs f11,-27456(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27456);
	ctx.f11.f64 = double(temp.f32);
	// fsubs f12,f10,f9
	ctx.f12.f64 = double(float(ctx.f10.f64 - ctx.f9.f64));
	// beq cr6,0x82267d68
	if (ctx.cr6.eq) goto loc_82267D68;
	// lfs f10,12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f10.f64 = double(temp.f32);
	// b 0x82267d6c
	goto loc_82267D6C;
loc_82267D68:
	// fmr f10,f11
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = ctx.f11.f64;
loc_82267D6C:
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82267d78
	if (ctx.cr6.eq) goto loc_82267D78;
	// lfs f11,12(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	ctx.f11.f64 = double(temp.f32);
loc_82267D78:
	// fmuls f9,f0,f0
	ctx.fpscr.disableFlushMode();
	ctx.f9.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// fmuls f8,f11,f10
	ctx.f8.f64 = double(float(ctx.f11.f64 * ctx.f10.f64));
	// lfs f0,-25888(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25888);
	ctx.f0.f64 = double(temp.f32);
	// li r11,0
	ctx.r11.s64 = 0;
	// fmadds f7,f12,f12,f9
	ctx.f7.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f9.f64));
	// fmuls f6,f8,f0
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f0.f64));
	// fmadds f5,f13,f13,f7
	ctx.f5.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f7.f64));
	// fcmpu cr6,f5,f6
	ctx.cr6.compare(ctx.f5.f64, ctx.f6.f64);
	// bgt cr6,0x82267da4
	if (ctx.cr6.gt) goto loc_82267DA4;
	// li r11,1
	ctx.r11.s64 = 1;
loc_82267DA4:
	// stw r11,476(r30)
	PPC_STORE_U32(ctx.r30.u32 + 476, ctx.r11.u32);
	// addi r4,r30,484
	ctx.r4.s64 = ctx.r30.s64 + 484;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x822658b0
	ctx.lr = 0x82267DB4;
	sub_822658B0(ctx, base);
	// lfs f0,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,0(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 0, temp.u32);
	// lfs f13,4(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,4(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 4, temp.u32);
	// lfs f12,8(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// stfs f12,8(r31)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r31.u32 + 8, temp.u32);
	// lbz r3,480(r30)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r30.u32 + 480);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82267DD8"))) PPC_WEAK_FUNC(sub_82267DD8);
PPC_FUNC_IMPL(__imp__sub_82267DD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f11,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f11.f64 = double(temp.f32);
	// lfs f0,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f12,f11,f0
	ctx.f12.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// lfs f13,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f12
	ctx.cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// blt cr6,0x82267f34
	if (ctx.cr6.lt) goto loc_82267F34;
	// lfs f12,20(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	ctx.f12.f64 = double(temp.f32);
	// fadds f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fcmpu cr6,f13,f12
	ctx.cr6.compare(ctx.f13.f64, ctx.f12.f64);
	// bge cr6,0x82267f34
	if (!ctx.cr6.lt) goto loc_82267F34;
	// lfs f10,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f0.f64));
	// lfs f12,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f12,f10
	ctx.cr6.compare(ctx.f12.f64, ctx.f10.f64);
	// blt cr6,0x82267f34
	if (ctx.cr6.lt) goto loc_82267F34;
	// lfs f10,28(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	ctx.f10.f64 = double(temp.f32);
	// fadds f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 + ctx.f0.f64));
	// fcmpu cr6,f12,f10
	ctx.cr6.compare(ctx.f12.f64, ctx.f10.f64);
	// bge cr6,0x82267f34
	if (!ctx.cr6.lt) goto loc_82267F34;
	// fsubs f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 - ctx.f0.f64));
	// lfs f13,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// li r9,0
	ctx.r9.s64 = 0;
	// fsubs f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
	// fdivs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f0.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x82267e58
	if (!ctx.cr6.lt) goto loc_82267E58;
	// stw r9,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r9.u32);
	// b 0x82267e5c
	goto loc_82267E5C;
loc_82267E58:
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
loc_82267E5C:
	// lfs f13,12(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,32(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f13,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// fdivs f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
	// fctidz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f0.u32);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x82267ea0
	if (ctx.cr6.lt) goto loc_82267EA0;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r11.u32);
	// b 0x82267ea4
	goto loc_82267EA4;
loc_82267EA0:
	// stw r10,0(r6)
	PPC_STORE_U32(ctx.r6.u32 + 0, ctx.r10.u32);
loc_82267EA4:
	// lfs f13,12(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// addi r11,r1,-16
	ctx.r11.s64 = ctx.r1.s64 + -16;
	// lfs f0,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f13,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// fdivs f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r11
	PPC_STORE_U32(ctx.r11.u32, ctx.f0.u32);
	// lwz r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x82267ee0
	if (!ctx.cr6.lt) goto loc_82267EE0;
	// stw r9,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r9.u32);
	// b 0x82267ee4
	goto loc_82267EE4;
loc_82267EE0:
	// stw r11,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r11.u32);
loc_82267EE4:
	// lfs f13,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// addi r10,r1,-16
	ctx.r10.s64 = ctx.r1.s64 + -16;
	// lfs f0,12(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// lfs f13,24(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// fdivs f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
	// fctidz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(LLONG_MAX)) ? LLONG_MAX : _mm_cvttsd_si64(_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f0.u32);
	// lwz r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x82267f28
	if (ctx.cr6.lt) goto loc_82267F28;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// b 0x82267f2c
	goto loc_82267F2C;
loc_82267F28:
	// stw r10,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r10.u32);
loc_82267F2C:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_82267F34:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82267F3C"))) PPC_WEAK_FUNC(sub_82267F3C);
PPC_FUNC_IMPL(__imp__sub_82267F3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82267F40"))) PPC_WEAK_FUNC(sub_82267F40);
PPC_FUNC_IMPL(__imp__sub_82267F40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bbc
	ctx.lr = 0x82267F48;
	sub_82CA2BBC(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r19,r5
	ctx.r19.u64 = ctx.r5.u64;
	// mr r17,r6
	ctx.r17.u64 = ctx.r6.u64;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// bl 0x82267dd8
	ctx.lr = 0x82267F70;
	sub_82267DD8(ctx, base);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8226808c
	if (ctx.cr0.eq) goto loc_8226808C;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r18,84(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r22,r11
	ctx.r22.u64 = ctx.r11.u64;
	// cmplw cr6,r11,r18
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r18.u32, ctx.xer);
	// bgt cr6,0x8226808c
	if (ctx.cr6.gt) goto loc_8226808C;
	// lwz r20,88(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r21,92(r1)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
loc_82267F94:
	// mr r23,r21
	ctx.r23.u64 = ctx.r21.u64;
	// cmplw cr6,r21,r20
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, ctx.r20.u32, ctx.xer);
	// bgt cr6,0x82268080
	if (ctx.cr6.gt) goto loc_82268080;
	// rlwinm r25,r22,2,0,29
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r21,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r21.u32 | (ctx.r21.u64 << 32), 2) & 0xFFFFFFFC;
loc_82267FA8:
	// lwz r11,44(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 44);
	// li r27,0
	ctx.r27.s64 = 0;
	// lwz r10,40(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 40);
	// lwzx r11,r25,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r11.u32);
	// lwzx r10,r10,r25
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r25.u32);
	// lwzx r26,r11,r28
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r28.u32);
	// lwzx r31,r10,r28
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r28.u32);
	// cmplwi r26,0
	ctx.cr0.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq 0x82268070
	if (ctx.cr0.eq) goto loc_82268070;
loc_82267FCC:
	// lfs f0,4(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,20(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// blt cr6,0x82268060
	if (ctx.cr6.lt) goto loc_82268060;
	// lfs f13,16(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bgt cr6,0x82268060
	if (ctx.cr6.gt) goto loc_82268060;
	// mr r6,r19
	ctx.r6.u64 = ctx.r19.u64;
	// lfs f1,12(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 12);
	ctx.f1.f64 = double(temp.f32);
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822680a0
	ctx.lr = 0x82267FFC;
	sub_822680A0(ctx, base);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82268060
	if (ctx.cr0.eq) goto loc_82268060;
	// lwz r11,60(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 60);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x82268098
	if (ctx.cr0.eq) goto loc_82268098;
	// rlwinm. r10,r17,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r17.u32 | (ctx.r17.u64 << 32), 0) & 0x8;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne 0x82268098
	if (!ctx.cr0.eq) goto loc_82268098;
	// lwz r30,8(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplwi r30,0
	ctx.cr0.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq 0x82268098
	if (ctx.cr0.eq) goto loc_82268098;
loc_82268024:
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// mr r6,r24
	ctx.r6.u64 = ctx.r24.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,36(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82268044;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x82268058
	if (ctx.cr0.eq) goto loc_82268058;
	// lwz r30,12(r30)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	// cmplwi r30,0
	ctx.cr0.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne 0x82268024
	if (!ctx.cr0.eq) goto loc_82268024;
loc_82268058:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x82268098
	if (ctx.cr6.eq) goto loc_82268098;
loc_82268060:
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r31,r31,100
	ctx.r31.s64 = ctx.r31.s64 + 100;
	// cmplw cr6,r27,r26
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r26.u32, ctx.xer);
	// blt cr6,0x82267fcc
	if (ctx.cr6.lt) goto loc_82267FCC;
loc_82268070:
	// addi r23,r23,1
	ctx.r23.s64 = ctx.r23.s64 + 1;
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// cmplw cr6,r23,r20
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, ctx.r20.u32, ctx.xer);
	// ble cr6,0x82267fa8
	if (!ctx.cr6.gt) goto loc_82267FA8;
loc_82268080:
	// addi r22,r22,1
	ctx.r22.s64 = ctx.r22.s64 + 1;
	// cmplw cr6,r22,r18
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, ctx.r18.u32, ctx.xer);
	// ble cr6,0x82267f94
	if (!ctx.cr6.gt) goto loc_82267F94;
loc_8226808C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82268090:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c0c
	// ERROR 82CA2C0C
	return;
loc_82268098:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// b 0x82268090
	goto loc_82268090;
}

__attribute__((alias("__imp__sub_822680A0"))) PPC_WEAK_FUNC(sub_822680A0);
PPC_FUNC_IMPL(__imp__sub_822680A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd0
	ctx.lr = 0x822680A8;
	sub_82CA2BD0(ctx, base);
	// stfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.f30.u64);
	// stfd f31,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// fmr f30,f1
	ctx.f30.f64 = ctx.f1.f64;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r23,r4
	ctx.r23.u64 = ctx.r4.u64;
	// mr r22,r6
	ctx.r22.u64 = ctx.r6.u64;
	// lfs f0,30580(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 30580);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f13,3084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3084);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// beq cr6,0x82268110
	if (ctx.cr6.eq) goto loc_82268110;
	// lwz r11,72(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// lwz r10,36(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// fmuls f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// bl 0x82260978
	ctx.lr = 0x82268104;
	sub_82260978(ctx, base);
	// lfs f0,116(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 116);
	ctx.f0.f64 = double(temp.f32);
	// fadds f0,f0,f31
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// stfs f0,116(r3)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r3.u32 + 116, temp.u32);
loc_82268110:
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r25,72(r30)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r30.u32 + 72);
	// li r26,0
	ctx.r26.s64 = 0;
	// cmplwi r25,0
	ctx.cr0.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r11.u8);
	// beq 0x82268194
	if (ctx.cr0.eq) goto loc_82268194;
	// lwz r24,76(r30)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r30.u32 + 76);
	// li r27,0
	ctx.r27.s64 = 0;
loc_82268130:
	// lwzx r28,r27,r24
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r27.u32 + ctx.r24.u32);
	// cmplwi cr6,r28,2
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 2, ctx.xer);
	// blt cr6,0x82268184
	if (ctx.cr6.lt) goto loc_82268184;
	// lwz r11,80(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// li r29,1
	ctx.r29.s64 = 1;
	// cmplwi cr6,r28,1
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 1, ctx.xer);
	// lwzx r4,r11,r27
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	// ble cr6,0x82268184
	if (!ctx.cr6.gt) goto loc_82268184;
	// addi r31,r4,12
	ctx.r31.s64 = ctx.r4.s64 + 12;
loc_82268154:
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x82268260
	ctx.lr = 0x82268168;
	sub_82268260(ctx, base);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82268254
	if (!ctx.cr0.eq) goto loc_82268254;
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r31,r31,12
	ctx.r31.s64 = ctx.r31.s64 + 12;
	// cmplw cr6,r29,r28
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r28.u32, ctx.xer);
	// blt cr6,0x82268154
	if (ctx.cr6.lt) goto loc_82268154;
loc_82268184:
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// addi r27,r27,4
	ctx.r27.s64 = ctx.r27.s64 + 4;
	// cmplw cr6,r26,r25
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r25.u32, ctx.xer);
	// blt cr6,0x82268130
	if (ctx.cr6.lt) goto loc_82268130;
loc_82268194:
	// lwz r27,36(r30)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// li r28,0
	ctx.r28.s64 = 0;
	// cmplwi r27,0
	ctx.cr0.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq 0x822681e0
	if (ctx.cr0.eq) goto loc_822681E0;
	// lwz r31,40(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	// addi r29,r31,12
	ctx.r29.s64 = ctx.r31.s64 + 12;
loc_822681AC:
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x82268260
	ctx.lr = 0x822681C4;
	sub_82268260(ctx, base);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82268254
	if (!ctx.cr0.eq) goto loc_82268254;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// addi r29,r29,24
	ctx.r29.s64 = ctx.r29.s64 + 24;
	// cmplw cr6,r28,r27
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r27.u32, ctx.xer);
	// blt cr6,0x822681ac
	if (ctx.cr6.lt) goto loc_822681AC;
loc_822681E0:
	// clrlwi. r11,r22,24
	ctx.r11.u64 = ctx.r22.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82268234
	if (!ctx.cr0.eq) goto loc_82268234;
	// lwz r28,48(r30)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r30.u32 + 48);
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi r28,0
	ctx.cr0.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq 0x82268234
	if (ctx.cr0.eq) goto loc_82268234;
	// lwz r31,52(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 52);
	// addi r30,r31,12
	ctx.r30.s64 = ctx.r31.s64 + 12;
loc_82268200:
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x82268260
	ctx.lr = 0x82268218;
	sub_82268260(ctx, base);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x82268254
	if (!ctx.cr0.eq) goto loc_82268254;
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// addi r31,r31,24
	ctx.r31.s64 = ctx.r31.s64 + 24;
	// addi r30,r30,24
	ctx.r30.s64 = ctx.r30.s64 + 24;
	// cmplw cr6,r29,r28
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r28.u32, ctx.xer);
	// blt cr6,0x82268200
	if (ctx.cr6.lt) goto loc_82268200;
loc_82268234:
	// lbz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = ctx.r11.u64 ^ 1;
loc_82268244:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x82ca2c20
	// ERROR 82CA2C20
	return;
loc_82268254:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82268244
	goto loc_82268244;
}

__attribute__((alias("__imp__sub_8226825C"))) PPC_WEAK_FUNC(sub_8226825C);
PPC_FUNC_IMPL(__imp__sub_8226825C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82268260"))) PPC_WEAK_FUNC(sub_82268260);
PPC_FUNC_IMPL(__imp__sub_82268260) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f8,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f8.f64 = double(temp.f32);
	// lfs f10,8(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// fcmpu cr6,f8,f10
	ctx.cr6.compare(ctx.f8.f64, ctx.f10.f64);
	// bge cr6,0x82268278
	if (!ctx.cr6.lt) goto loc_82268278;
	// fmr f7,f8
	ctx.f7.f64 = ctx.f8.f64;
	// b 0x8226827c
	goto loc_8226827C;
loc_82268278:
	// fmr f7,f10
	ctx.fpscr.disableFlushMode();
	ctx.f7.f64 = ctx.f10.f64;
loc_8226827C:
	// lfs f9,8(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f9,f7
	ctx.cr6.compare(ctx.f9.f64, ctx.f7.f64);
	// blt cr6,0x82268350
	if (ctx.cr6.lt) goto loc_82268350;
	// fcmpu cr6,f8,f10
	ctx.cr6.compare(ctx.f8.f64, ctx.f10.f64);
	// ble cr6,0x82268298
	if (!ctx.cr6.gt) goto loc_82268298;
	// fmr f0,f8
	ctx.f0.f64 = ctx.f8.f64;
	// b 0x8226829c
	goto loc_8226829C;
loc_82268298:
	// fmr f0,f10
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f10.f64;
loc_8226829C:
	// fcmpu cr6,f9,f0
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f9.f64, ctx.f0.f64);
	// bgt cr6,0x82268350
	if (ctx.cr6.gt) goto loc_82268350;
	// lfs f0,0(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lfs f12,0(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// ble cr6,0x822682bc
	if (!ctx.cr6.gt) goto loc_822682BC;
	// fmr f13,f0
	ctx.f13.f64 = ctx.f0.f64;
	// b 0x822682c0
	goto loc_822682C0;
loc_822682BC:
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
loc_822682C0:
	// lfs f11,0(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f11,f13
	ctx.cr6.compare(ctx.f11.f64, ctx.f13.f64);
	// bgt cr6,0x82268350
	if (ctx.cr6.gt) goto loc_82268350;
	// fcmpu cr6,f8,f10
	ctx.cr6.compare(ctx.f8.f64, ctx.f10.f64);
	// beq cr6,0x82268374
	if (ctx.cr6.eq) goto loc_82268374;
	// fcmpu cr6,f0,f12
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// bne cr6,0x8226832c
	if (!ctx.cr6.eq) goto loc_8226832C;
loc_822682DC:
	// fcmpu cr6,f11,f0
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f11.f64, ctx.f0.f64);
	// bne cr6,0x82268358
	if (!ctx.cr6.eq) goto loc_82268358;
	// lfs f0,8(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f9,f9,f0
	ctx.f9.f64 = double(float(ctx.f9.f64 - ctx.f0.f64));
	// lfs f12,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f0,f10,f0
	ctx.f0.f64 = double(float(ctx.f10.f64 - ctx.f0.f64));
	// fsubs f12,f12,f13
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// lfs f11,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f11.f64 = double(temp.f32);
	// fmuls f12,f12,f9
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f9.f64));
	// fdivs f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 / ctx.f0.f64));
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// fsubs f0,f0,f11
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
loc_82268310:
	// fabs f0,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = ctx.f0.u64 & ~0x8000000000000000;
	// li r11,1
	ctx.r11.s64 = 1;
	// fcmpu cr6,f0,f1
	ctx.cr6.compare(ctx.f0.f64, ctx.f1.f64);
	// blt cr6,0x82268324
	if (ctx.cr6.lt) goto loc_82268324;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82268324:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr 
	return;
loc_8226832C:
	// lfs f13,8(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// fsubs f8,f9,f13
	ctx.f8.f64 = double(float(ctx.f9.f64 - ctx.f13.f64));
	// fsubs f13,f10,f13
	ctx.f13.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// fmuls f12,f12,f8
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f8.f64));
	// fdivs f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 / ctx.f13.f64));
	// fadds f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fcmpu cr6,f11,f0
	ctx.cr6.compare(ctx.f11.f64, ctx.f0.f64);
	// ble cr6,0x822682dc
	if (!ctx.cr6.gt) goto loc_822682DC;
loc_82268350:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_82268358:
	// fcmpu cr6,f9,f7
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f9.f64, ctx.f7.f64);
	// ble cr6,0x82268350
	if (!ctx.cr6.gt) goto loc_82268350;
	// lbz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stb r11,0(r6)
	PPC_STORE_U8(ctx.r6.u32 + 0, ctx.r11.u8);
	// b 0x82268350
	goto loc_82268350;
loc_82268374:
	// fcmpu cr6,f0,f12
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f0.f64, ctx.f12.f64);
	// bge cr6,0x82268384
	if (!ctx.cr6.lt) goto loc_82268384;
	// fmr f13,f0
	ctx.f13.f64 = ctx.f0.f64;
	// b 0x82268388
	goto loc_82268388;
loc_82268384:
	// fmr f13,f12
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f12.f64;
loc_82268388:
	// fcmpu cr6,f11,f13
	ctx.fpscr.disableFlushMode();
	ctx.cr6.compare(ctx.f11.f64, ctx.f13.f64);
	// blt cr6,0x82268350
	if (ctx.cr6.lt) goto loc_82268350;
	// lfs f13,4(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f11,f11,f0
	ctx.f11.f64 = double(float(ctx.f11.f64 - ctx.f0.f64));
	// lfs f10,4(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f0,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 - ctx.f0.f64));
	// fsubs f10,f10,f13
	ctx.f10.f64 = double(float(ctx.f10.f64 - ctx.f13.f64));
	// lfs f12,4(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f10,f11
	ctx.f11.f64 = double(float(ctx.f10.f64 * ctx.f11.f64));
	// fdivs f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 / ctx.f0.f64));
	// fadds f0,f0,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f13.f64));
	// fsubs f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f12.f64));
	// b 0x82268310
	goto loc_82268310;
}

__attribute__((alias("__imp__sub_822683BC"))) PPC_WEAK_FUNC(sub_822683BC);
PPC_FUNC_IMPL(__imp__sub_822683BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822683C0"))) PPC_WEAK_FUNC(sub_822683C0);
PPC_FUNC_IMPL(__imp__sub_822683C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt cr6,0x8226844c
	if (ctx.cr6.lt) goto loc_8226844C;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r7
	ctx.r3.u64 = ctx.r7.u64;
	// bl 0x8226a9f0
	ctx.lr = 0x822683F0;
	sub_8226A9F0(ctx, base);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x82268424
	if (!ctx.cr6.eq) goto loc_82268424;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// addi r9,r11,-20628
	ctx.r9.s64 = ctx.r11.s64 + -20628;
	// addi r8,r31,32
	ctx.r8.s64 = ctx.r31.s64 + 32;
	// li r7,1
	ctx.r7.s64 = 1;
	// clrldi r6,r8,32
	ctx.r6.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// rldicr r11,r7,63,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// lwz r3,412(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 412);
	// lwz r5,4(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// srd r6,r11,r6
	ctx.r6.u64 = ctx.r6.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r6.u8 & 0x7F));
	// b 0x82268444
	goto loc_82268444;
loc_82268424:
	// lis r11,-31946
	ctx.r11.s64 = -2093613056;
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// addi r10,r31,32
	ctx.r10.s64 = ctx.r31.s64 + 32;
	// li r9,1
	ctx.r9.s64 = 1;
	// clrldi r8,r10,32
	ctx.r8.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// rldicr r7,r9,63,63
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// lwz r3,412(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 412);
	// srd r6,r7,r8
	ctx.r6.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r7.u64 >> (ctx.r8.u8 & 0x7F));
loc_82268444:
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// bl 0x821b7020
	ctx.lr = 0x8226844C;
	sub_821B7020(ctx, base);
loc_8226844C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82268464"))) PPC_WEAK_FUNC(sub_82268464);
PPC_FUNC_IMPL(__imp__sub_82268464) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82268468"))) PPC_WEAK_FUNC(sub_82268468);
PPC_FUNC_IMPL(__imp__sub_82268468) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r3,r11,104
	ctx.r3.s64 = ctx.r11.s64 + 104;
	// lwz r11,108(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268524
	if (ctx.cr6.eq) goto loc_82268524;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82268520
	if (ctx.cr6.eq) goto loc_82268520;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822684d0
	if (ctx.cr6.eq) goto loc_822684D0;
	// lbz r10,144(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 144);
	// rlwinm r9,r10,0,25,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822684d4
	if (!ctx.cr6.eq) goto loc_822684D4;
loc_822684D0:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822684D4:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226861c
	if (ctx.cr6.eq) goto loc_8226861C;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r9,r10,24,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822685f4
	if (ctx.cr6.eq) goto loc_822685F4;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226852c
	if (ctx.cr6.eq) goto loc_8226852C;
	// lbz r10,136(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 136);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822685f8
	goto loc_822685F8;
loc_82268520:
	// bl 0x821940c8
	ctx.lr = 0x82268524;
	sub_821940C8(ctx, base);
loc_82268524:
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x822684d0
	goto loc_822684D0;
loc_8226852C:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// subf r8,r10,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226859c
	if (!ctx.cr0.gt) goto loc_8226859C;
loc_8226854C:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,136
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 136, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226856c
	if (ctx.cr6.lt) goto loc_8226856C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8226856C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82268588
	if (ctx.cr6.eq) goto loc_82268588;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82268590
	goto loc_82268590;
loc_82268588:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82268590:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226854c
	if (ctx.cr6.gt) goto loc_8226854C;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226859C:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x822685e0
	if (ctx.cr6.eq) goto loc_822685E0;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,136
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 136, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x822685b8
	if (ctx.cr6.gt) goto loc_822685B8;
	// li r11,0
	ctx.r11.s64 = 0;
loc_822685B8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822685e0
	if (!ctx.cr6.eq) goto loc_822685E0;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822685f8
	goto loc_822685F8;
loc_822685E0:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822685f8
	goto loc_822685F8;
loc_822685F4:
	// li r11,0
	ctx.r11.s64 = 0;
loc_822685F8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226861c
	if (ctx.cr6.eq) goto loc_8226861C;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x821c6e08
	ctx.lr = 0x8226860C;
	sub_821C6E08(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82268620
	if (!ctx.cr6.eq) goto loc_82268620;
loc_8226861C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_82268620:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82268634"))) PPC_WEAK_FUNC(sub_82268634);
PPC_FUNC_IMPL(__imp__sub_82268634) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82268638"))) PPC_WEAK_FUNC(sub_82268638);
PPC_FUNC_IMPL(__imp__sub_82268638) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r3,r11,104
	ctx.r3.s64 = ctx.r11.s64 + 104;
	// lwz r11,108(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822686f4
	if (ctx.cr6.eq) goto loc_822686F4;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822686f0
	if (ctx.cr6.eq) goto loc_822686F0;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822686a0
	if (ctx.cr6.eq) goto loc_822686A0;
	// lbz r10,144(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 144);
	// rlwinm r9,r10,0,25,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x40;
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822686a4
	if (!ctx.cr6.eq) goto loc_822686A4;
loc_822686A0:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822686A4:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822687ec
	if (ctx.cr6.eq) goto loc_822687EC;
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 52);
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r9,r10,24,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x822687c4
	if (ctx.cr6.eq) goto loc_822687C4;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822686fc
	if (ctx.cr6.eq) goto loc_822686FC;
	// lbz r10,136(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 136);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822687c8
	goto loc_822687C8;
loc_822686F0:
	// bl 0x821940c8
	ctx.lr = 0x822686F4;
	sub_821940C8(ctx, base);
loc_822686F4:
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x822686a0
	goto loc_822686A0;
loc_822686FC:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// subf r8,r10,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226876c
	if (!ctx.cr0.gt) goto loc_8226876C;
loc_8226871C:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,136
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 136, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226873c
	if (ctx.cr6.lt) goto loc_8226873C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8226873C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82268758
	if (ctx.cr6.eq) goto loc_82268758;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82268760
	goto loc_82268760;
loc_82268758:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82268760:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226871c
	if (ctx.cr6.gt) goto loc_8226871C;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226876C:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x822687b0
	if (ctx.cr6.eq) goto loc_822687B0;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,136
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 136, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82268788
	if (ctx.cr6.gt) goto loc_82268788;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82268788:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822687b0
	if (!ctx.cr6.eq) goto loc_822687B0;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822687c8
	goto loc_822687C8;
loc_822687B0:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x822687c8
	goto loc_822687C8;
loc_822687C4:
	// li r11,0
	ctx.r11.s64 = 0;
loc_822687C8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822687ec
	if (ctx.cr6.eq) goto loc_822687EC;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// bl 0x82285670
	ctx.lr = 0x822687DC;
	sub_82285670(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x822687f0
	if (!ctx.cr6.eq) goto loc_822687F0;
loc_822687EC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822687F0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82268804"))) PPC_WEAK_FUNC(sub_82268804);
PPC_FUNC_IMPL(__imp__sub_82268804) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82268808"))) PPC_WEAK_FUNC(sub_82268808);
PPC_FUNC_IMPL(__imp__sub_82268808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x82268810;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r30,-31927
	ctx.r30.s64 = -2092367872;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,26912(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26912);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r3,r11,104
	ctx.r3.s64 = ctx.r11.s64 + 104;
	// lwz r11,108(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226886c
	if (ctx.cr6.eq) goto loc_8226886C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x822688b4
	if (ctx.cr6.eq) goto loc_822688B4;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226886c
	if (ctx.cr6.eq) goto loc_8226886C;
	// lbz r11,144(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82268870
	if (!ctx.cr6.eq) goto loc_82268870;
loc_8226886C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82268870:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268bd4
	if (ctx.cr6.eq) goto loc_82268BD4;
	// lbz r11,776(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 776);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268bc4
	if (ctx.cr6.eq) goto loc_82268BC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82268468
	ctx.lr = 0x82268890;
	sub_82268468(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822688bc
	if (ctx.cr6.eq) goto loc_822688BC;
	// li r4,4
	ctx.r4.s64 = 4;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x821ad680
	ctx.lr = 0x822688AC;
	sub_821AD680(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_822688B4:
	// bl 0x821940c8
	ctx.lr = 0x822688B8;
	sub_821940C8(ctx, base);
	// b 0x8226886c
	goto loc_8226886C;
loc_822688BC:
	// bl 0x82268638
	ctx.lr = 0x822688C0;
	sub_82268638(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822688e4
	if (ctx.cr6.eq) goto loc_822688E4;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x821ad680
	ctx.lr = 0x822688DC;
	sub_821AD680(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_822688E4:
	// lbz r11,771(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 771);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268908
	if (ctx.cr6.eq) goto loc_82268908;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x821ad680
	ctx.lr = 0x82268900;
	sub_821AD680(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82268908:
	// bl 0x8226b098
	ctx.lr = 0x8226890C;
	sub_8226B098(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268bc4
	if (ctx.cr6.eq) goto loc_82268BC4;
	// lfs f0,44(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,116(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 116);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bgt cr6,0x82268b80
	if (ctx.cr6.gt) goto loc_82268B80;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821ad680
	ctx.lr = 0x82268938;
	sub_821AD680(ctx, base);
	// lbz r11,778(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 778);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82268bd4
	if (!ctx.cr6.eq) goto loc_82268BD4;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,26912(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26912);
	// stb r10,778(r31)
	PPC_STORE_U8(ctx.r31.u32 + 778, ctx.r10.u8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// bl 0x8222c210
	ctx.lr = 0x82268964;
	sub_8222C210(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82268980
	if (ctx.cr6.eq) goto loc_82268980;
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x82268984
	if (!ctx.cr6.eq) goto loc_82268984;
loc_82268980:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82268984:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268bd4
	if (ctx.cr6.eq) goto loc_82268BD4;
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// li r31,0
	ctx.r31.s64 = 0;
	// rlwinm r10,r11,7,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82268a8c
	if (ctx.cr6.eq) goto loc_82268A8C;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822689cc
	if (ctx.cr6.eq) goto loc_822689CC;
	// lbz r10,57(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 57);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x82268a90
	goto loc_82268A90;
loc_822689CC:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// subf r9,r10,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x82268a3c
	if (!ctx.cr0.gt) goto loc_82268A3C;
loc_822689EC:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,57
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 57, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x82268a0c
	if (ctx.cr6.lt) goto loc_82268A0C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_82268A0C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x82268a28
	if (ctx.cr6.eq) goto loc_82268A28;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x82268a30
	goto loc_82268A30;
loc_82268A28:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_82268A30:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x822689ec
	if (ctx.cr6.gt) goto loc_822689EC;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_82268A3C:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x82268a7c
	if (ctx.cr6.eq) goto loc_82268A7C;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,57
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 57, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x82268a58
	if (ctx.cr6.gt) goto loc_82268A58;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82268A58:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82268a7c
	if (!ctx.cr6.eq) goto loc_82268A7C;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82268a90
	goto loc_82268A90;
loc_82268A7C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x82268a90
	goto loc_82268A90;
loc_82268A8C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82268A90:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268bd4
	if (ctx.cr6.eq) goto loc_82268BD4;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r4,54
	ctx.r4.s64 = 54;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x822d9508
	ctx.lr = 0x82268AAC;
	sub_822D9508(ctx, base);
	// lis r11,-32243
	ctx.r11.s64 = -2113077248;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r30,r11,12572
	ctx.r30.s64 = ctx.r11.s64 + 12572;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8222cf18
	ctx.lr = 0x82268AC4;
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8249fb20
	ctx.lr = 0x82268AD0;
	sub_8249FB20(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r29,r10,27,31,31
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// bl 0x82214f08
	ctx.lr = 0x82268AE4;
	sub_82214F08(ctx, base);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x82268bd4
	if (ctx.cr6.eq) goto loc_82268BD4;
	// lis r11,-32245
	ctx.r11.s64 = -2113208320;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,23684
	ctx.r4.s64 = ctx.r11.s64 + 23684;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x82268B00;
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x8219c690
	ctx.lr = 0x82268B0C;
	sub_8219C690(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x82268B14;
	sub_82214F08(ctx, base);
	// lis r10,-32243
	ctx.r10.s64 = -2113077248;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r7,r10,12608
	ctx.r7.s64 = ctx.r10.s64 + 12608;
	// bl 0x821b2710
	ctx.lr = 0x82268B24;
	sub_821B2710(ctx, base);
	// mr r4,r7
	ctx.r4.u64 = ctx.r7.u64;
	// bl 0x823b12e8
	ctx.lr = 0x82268B2C;
	sub_823B12E8(ctx, base);
	// lwz r6,12(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r11,r31,12
	ctx.r11.s64 = ctx.r31.s64 + 12;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r4,54
	ctx.r4.s64 = 54;
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// lwz r5,0(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// bctrl 
	ctx.lr = 0x82268B4C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpw cr6,r3,r29
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r29.s32, ctx.xer);
	// blt cr6,0x82268bd4
	if (ctx.cr6.lt) goto loc_82268BD4;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x82268B64;
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8249f8a0
	ctx.lr = 0x82268B70;
	sub_8249F8A0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x82268B78;
	sub_82214F08(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82268B80:
	// lfs f13,112(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 112);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bgt cr6,0x82268bc4
	if (ctx.cr6.gt) goto loc_82268BC4;
	// lbz r11,770(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 770);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268bc4
	if (ctx.cr6.eq) goto loc_82268BC4;
	// lfs f13,108(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 108);
	ctx.f13.f64 = double(temp.f32);
	// li r5,0
	ctx.r5.s64 = 0;
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// beq cr6,0x82268bb0
	if (ctx.cr6.eq) goto loc_82268BB0;
	// stfs f0,108(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 108, temp.u32);
	// li r5,1
	ctx.r5.s64 = 1;
loc_82268BB0:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821ad680
	ctx.lr = 0x82268BBC;
	sub_821AD680(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_82268BC4:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x821ad680
	ctx.lr = 0x82268BD4;
	sub_821AD680(ctx, base);
loc_82268BD4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_82268BDC"))) PPC_WEAK_FUNC(sub_82268BDC);
PPC_FUNC_IMPL(__imp__sub_82268BDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82268BE0"))) PPC_WEAK_FUNC(sub_82268BE0);
PPC_FUNC_IMPL(__imp__sub_82268BE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,76(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x82268C08;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82268C2C"))) PPC_WEAK_FUNC(sub_82268C2C);
PPC_FUNC_IMPL(__imp__sub_82268C2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82268C30"))) PPC_WEAK_FUNC(sub_82268C30);
PPC_FUNC_IMPL(__imp__sub_82268C30) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bb0
	ctx.lr = 0x82268C38;
	sub_82CA2BB0(ctx, base);
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f31.u64);
	// stwu r1,-416(r1)
	ea = -416 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r14,r3
	ctx.r14.u64 = ctx.r3.u64;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// bl 0x821fbbb8
	ctx.lr = 0x82268C4C;
	sub_821FBBB8(ctx, base);
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r14
	ctx.r4.u64 = ctx.r14.u64;
	// addi r3,r1,136
	ctx.r3.s64 = ctx.r1.s64 + 136;
	// bl 0x822b6550
	ctx.lr = 0x82268C5C;
	sub_822B6550(ctx, base);
	// li r20,1
	ctx.r20.s64 = 1;
	// addi r31,r14,64
	ctx.r31.s64 = ctx.r14.s64 + 64;
	// mr r26,r20
	ctx.r26.u64 = ctx.r20.u64;
	// li r30,2
	ctx.r30.s64 = 2;
	// li r28,16
	ctx.r28.s64 = 16;
	// li r29,58
	ctx.r29.s64 = 58;
	// lis r23,-31926
	ctx.r23.s64 = -2092302336;
	// lis r27,-31924
	ctx.r27.s64 = -2092171264;
loc_82268C7C:
	// lbz r11,-5771(r27)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + -5771);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r4,-2368(r23)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r23.u32 + -2368);
	// li r8,0
	ctx.r8.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// beq cr6,0x82268ccc
	if (ctx.cr6.eq) goto loc_82268CCC;
	// addi r11,r1,124
	ctx.r11.s64 = ctx.r1.s64 + 124;
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r28.u32);
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// stw r29,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r29.u32);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x821fbc60
	ctx.lr = 0x82268CB8;
	sub_821FBC60(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821fbab8
	ctx.lr = 0x82268CC4;
	sub_821FBAB8(ctx, base);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// b 0x82268cec
	goto loc_82268CEC;
loc_82268CCC:
	// li r6,2
	ctx.r6.s64 = 2;
	// li r7,2
	ctx.r7.s64 = 2;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x821fbc60
	ctx.lr = 0x82268CDC;
	sub_821FBC60(ctx, base);
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821fbab8
	ctx.lr = 0x82268CE8;
	sub_821FBAB8(ctx, base);
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
loc_82268CEC:
	// bl 0x821fbe60
	ctx.lr = 0x82268CF0;
	sub_821FBE60(ctx, base);
	// lwz r11,4(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// addic. r30,r30,-1
	ctx.xer.ca = ctx.r30.u32 > 0;
	ctx.r30.s64 = ctx.r30.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// addi r31,r31,36
	ctx.r31.s64 = ctx.r31.s64 + 36;
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 ^ 1;
	// and r26,r8,r26
	ctx.r26.u64 = ctx.r8.u64 & ctx.r26.u64;
	// bne 0x82268c7c
	if (!ctx.cr0.eq) goto loc_82268C7C;
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// lbz r11,-5771(r27)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + -5771);
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// lis r7,-32241
	ctx.r7.s64 = -2112946176;
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// addi r5,r9,24220
	ctx.r5.s64 = ctx.r9.s64 + 24220;
	// addi r4,r8,24232
	ctx.r4.s64 = ctx.r8.s64 + 24232;
	// addi r3,r7,24236
	ctx.r3.s64 = ctx.r7.s64 + 24236;
	// stw r5,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r5.u32);
	// li r25,0
	ctx.r25.s64 = 0;
	// stw r4,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r4.u32);
	// addi r6,r10,24204
	ctx.r6.s64 = ctx.r10.s64 + 24204;
	// stw r3,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r3.u32);
	// lwz r4,-2368(r23)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r23.u32 + -2368);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r25,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r25.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r6,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r6.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r25,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r25.u32);
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// stw r25,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r25.u32);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// beq cr6,0x82268d94
	if (ctx.cr6.eq) goto loc_82268D94;
	// addi r11,r1,124
	ctx.r11.s64 = ctx.r1.s64 + 124;
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r28.u32);
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// stw r29,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r29.u32);
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x821fbc60
	ctx.lr = 0x82268D8C;
	sub_821FBC60(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x82268da4
	goto loc_82268DA4;
loc_82268D94:
	// li r6,2
	ctx.r6.s64 = 2;
	// li r7,2
	ctx.r7.s64 = 2;
	// bl 0x821fbc60
	ctx.lr = 0x82268DA0;
	sub_821FBC60(ctx, base);
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
loc_82268DA4:
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x821fbab8
	ctx.lr = 0x82268DAC;
	sub_821FBAB8(ctx, base);
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// bl 0x821fbe60
	ctx.lr = 0x82268DB4;
	sub_821FBE60(ctx, base);
	// lwz r29,164(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// clrlwi r11,r26,24
	ctx.r11.u64 = ctx.r26.u32 & 0xFF;
	// cntlzw r10,r29
	ctx.r10.u64 = ctx.r29.u32 == 0 ? 32 : __builtin_clz(ctx.r29.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 ^ 1;
	// and r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 & ctx.r11.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x822692b0
	if (ctx.cr6.eq) goto loc_822692B0;
	// lwz r11,4(r14)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r14.u32 + 4);
	// lwz r3,20(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// bl 0x82a84608
	ctx.lr = 0x82268DE0;
	sub_82A84608(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r31,r11,-27468
	ctx.r31.s64 = ctx.r11.s64 + -27468;
	// lfs f0,-18828(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -18828);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f13,f1,f0
	ctx.f13.f64 = double(float(ctx.f1.f64 * ctx.f0.f64));
	// stfs f13,136(r14)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r14.u32 + 136, temp.u32);
	// lfs f0,12(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f1,f0,f13
	ctx.f1.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// bl 0x821b1b70
	ctx.lr = 0x82268E04;
	sub_821B1B70(ctx, base);
	// lis r11,-32241
	ctx.r11.s64 = -2112946176;
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// lfs f31,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// lis r8,-31951
	ctx.r8.s64 = -2093940736;
	// lis r7,-31924
	ctx.r7.s64 = -2092171264;
	// lis r6,-31924
	ctx.r6.s64 = -2092171264;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// addi r11,r11,7532
	ctx.r11.s64 = ctx.r11.s64 + 7532;
	// mr r19,r25
	ctx.r19.u64 = ctx.r25.u64;
	// lis r21,-31946
	ctx.r21.s64 = -2093613056;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r11.u32);
	// lis r26,-31943
	ctx.r26.s64 = -2093416448;
	// lis r23,-31927
	ctx.r23.s64 = -2092367872;
	// lis r15,-31927
	ctx.r15.s64 = -2092367872;
	// lis r18,-31943
	ctx.r18.s64 = -2093416448;
	// addi r16,r9,3056
	ctx.r16.s64 = ctx.r9.s64 + 3056;
	// addi r17,r8,28404
	ctx.r17.s64 = ctx.r8.s64 + 28404;
	// addi r30,r7,-16224
	ctx.r30.s64 = ctx.r7.s64 + -16224;
	// addi r22,r6,-9360
	ctx.r22.s64 = ctx.r6.s64 + -9360;
	// addi r31,r10,-32624
	ctx.r31.s64 = ctx.r10.s64 + -32624;
loc_82268E54:
	// lwz r3,180(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r11,128(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r10,184(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r3.u32);
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r11.u32);
	// stw r10,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r10.u32);
	// beq cr6,0x82268e78
	if (ctx.cr6.eq) goto loc_82268E78;
	// bl 0x821fc048
	ctx.lr = 0x82268E78;
	sub_821FC048(ctx, base);
loc_82268E78:
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// bl 0x821fc270
	ctx.lr = 0x82268E80;
	sub_821FC270(ctx, base);
	// bl 0x822421d8
	ctx.lr = 0x82268E84;
	sub_822421D8(ctx, base);
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// lbz r11,-5770(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + -5770);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82268ed8
	if (ctx.cr6.eq) goto loc_82268ED8;
	// cmplwi cr6,r19,0
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 0, ctx.xer);
	// li r11,255
	ctx.r11.s64 = 255;
	// bne cr6,0x82268ebc
	if (!ctx.cr6.eq) goto loc_82268EBC;
	// stb r25,117(r1)
	PPC_STORE_U8(ctx.r1.u32 + 117, ctx.r25.u8);
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// stb r25,116(r1)
	PPC_STORE_U8(ctx.r1.u32 + 116, ctx.r25.u8);
	// stb r11,118(r1)
	PPC_STORE_U8(ctx.r1.u32 + 118, ctx.r11.u8);
	// stb r11,119(r1)
	PPC_STORE_U8(ctx.r1.u32 + 119, ctx.r11.u8);
	// bl 0x821adf68
	ctx.lr = 0x82268EB8;
	sub_821ADF68(ctx, base);
	// b 0x82269238
	goto loc_82269238;
loc_82268EBC:
	// stb r25,122(r1)
	PPC_STORE_U8(ctx.r1.u32 + 122, ctx.r25.u8);
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// stb r25,120(r1)
	PPC_STORE_U8(ctx.r1.u32 + 120, ctx.r25.u8);
	// stb r11,121(r1)
	PPC_STORE_U8(ctx.r1.u32 + 121, ctx.r11.u8);
	// stb r11,123(r1)
	PPC_STORE_U8(ctx.r1.u32 + 123, ctx.r11.u8);
	// bl 0x821adf68
	ctx.lr = 0x82268ED4;
	sub_821ADF68(ctx, base);
	// b 0x82269238
	goto loc_82269238;
loc_82268ED8:
	// stb r25,126(r1)
	PPC_STORE_U8(ctx.r1.u32 + 126, ctx.r25.u8);
	// addi r3,r1,124
	ctx.r3.s64 = ctx.r1.s64 + 124;
	// stb r25,125(r1)
	PPC_STORE_U8(ctx.r1.u32 + 125, ctx.r25.u8);
	// stb r25,124(r1)
	PPC_STORE_U8(ctx.r1.u32 + 124, ctx.r25.u8);
	// stb r25,127(r1)
	PPC_STORE_U8(ctx.r1.u32 + 127, ctx.r25.u8);
	// bl 0x821adf68
	ctx.lr = 0x82268EF0;
	sub_821ADF68(ctx, base);
	// lwz r11,8196(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8196);
	// addi r8,r31,8200
	ctx.r8.s64 = ctx.r31.s64 + 8200;
	// stb r20,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, ctx.r20.u8);
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// stwx r22,r7,r31
	PPC_STORE_U32(ctx.r7.u32 + ctx.r31.u32, ctx.r22.u32);
	// lwz r10,8196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8196);
	// lwz r9,8192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8192);
	// lwz r11,16396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16396);
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r10,1
	ctx.r11.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,8196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8196, ctx.r11.u32);
	// stw r10,8192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8192, ctx.r10.u32);
	// stwx r22,r6,r8
	PPC_STORE_U32(ctx.r6.u32 + ctx.r8.u32, ctx.r22.u32);
	// lbz r5,141(r30)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r30.u32 + 141);
	// lwz r9,16392(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16392);
	// lwz r7,8(r17)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r17.u32 + 8);
	// lwz r8,124(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	// lwz r10,16396(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16396);
	// addi r11,r10,1
	ctx.r11.s64 = ctx.r10.s64 + 1;
	// mulli r10,r5,8200
	ctx.r10.s64 = ctx.r5.s64 * 8200;
	// stw r11,16396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16396, ctx.r11.u32);
	// add r11,r10,r31
	ctx.r11.u64 = ctx.r10.u64 + ctx.r31.u64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// stw r10,16392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16392, ctx.r10.u32);
	// cmplw cr6,r8,r7
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, ctx.xer);
	// beq cr6,0x82268fe0
	if (ctx.cr6.eq) goto loc_82268FE0;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,128(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 128);
	// and r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x82268fac
	if (!ctx.cr6.eq) goto loc_82268FAC;
	// lwz r8,8196(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r30,120
	ctx.r6.s64 = ctx.r30.s64 + 120;
	// rlwinm r5,r8,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,128(r30)
	PPC_STORE_U32(ctx.r30.u32 + 128, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r10,124(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_82268FAC:
	// lbz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 140);
	// stw r7,124(r30)
	PPC_STORE_U32(ctx.r30.u32 + 124, ctx.r7.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82268fe0
	if (!ctx.cr6.eq) goto loc_82268FE0;
	// lwz r11,1000(r18)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r18.u32 + 1000);
	// mr r10,r20
	ctx.r10.u64 = ctx.r20.u64;
	// addi r9,r30,120
	ctx.r9.s64 = ctx.r30.s64 + 120;
	// rlwinm r7,r11,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r10,140(r30)
	PPC_STORE_U8(ctx.r30.u32 + 140, ctx.r10.u8);
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// stw r8,1000(r18)
	PPC_STORE_U32(ctx.r18.u32 + 1000, ctx.r8.u32);
	// stwx r9,r7,r16
	PPC_STORE_U32(ctx.r7.u32 + ctx.r16.u32, ctx.r9.u32);
	// b 0x82268fe4
	goto loc_82268FE4;
loc_82268FE0:
	// lwz r8,1000(r18)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r18.u32 + 1000);
loc_82268FE4:
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,1776(r24)
	PPC_STORE_U32(ctx.r24.u32 + 1776, ctx.r11.u32);
	// lwz r10,52(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 52);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lbz r9,69(r30)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r30.u32 + 69);
	// mulli r11,r9,8200
	ctx.r11.s64 = ctx.r9.s64 * 8200;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x82269084
	if (ctx.cr6.eq) goto loc_82269084;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,56(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 56);
	// and r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x82269054
	if (!ctx.cr6.eq) goto loc_82269054;
	// lwz r7,8196(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r30,48
	ctx.r6.s64 = ctx.r30.s64 + 48;
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,56(r30)
	PPC_STORE_U32(ctx.r30.u32 + 56, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r10,52(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 52);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_82269054:
	// lbz r11,68(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 68);
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// stw r10,52(r30)
	PPC_STORE_U32(ctx.r30.u32 + 52, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x82269084
	if (!ctx.cr6.eq) goto loc_82269084;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r20
	ctx.r11.u64 = ctx.r20.u64;
	// addi r9,r30,48
	ctx.r9.s64 = ctx.r30.s64 + 48;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stb r11,68(r30)
	PPC_STORE_U8(ctx.r30.u32 + 68, ctx.r11.u8);
	// stw r8,1000(r18)
	PPC_STORE_U32(ctx.r18.u32 + 1000, ctx.r8.u32);
	// stwx r9,r10,r16
	PPC_STORE_U32(ctx.r10.u32 + ctx.r16.u32, ctx.r9.u32);
loc_82269084:
	// lbz r11,93(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 93);
	// lwz r10,76(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 76);
	// mulli r11,r11,8200
	ctx.r11.s64 = ctx.r11.s64 * 8200;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// addi r11,r11,-8200
	ctx.r11.s64 = ctx.r11.s64 + -8200;
	// beq cr6,0x8226911c
	if (ctx.cr6.eq) goto loc_8226911C;
	// lwz r10,8192(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8192);
	// lwz r9,80(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// and r7,r10,r9
	ctx.r7.u64 = ctx.r10.u64 & ctx.r9.u64;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// bne cr6,0x822690ec
	if (!ctx.cr6.eq) goto loc_822690EC;
	// lwz r7,8196(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// addi r6,r30,72
	ctx.r6.s64 = ctx.r30.s64 + 72;
	// rlwinm r5,r7,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,80(r30)
	PPC_STORE_U32(ctx.r30.u32 + 80, ctx.r10.u32);
	// stwx r6,r5,r11
	PPC_STORE_U32(ctx.r5.u32 + ctx.r11.u32, ctx.r6.u32);
	// lwz r4,8196(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,76(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 76);
	// add r3,r9,r11
	ctx.r3.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// lwz r10,8196(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8196);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,8196(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8196, ctx.r10.u32);
loc_822690EC:
	// lbz r11,92(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 92);
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// stw r10,76(r30)
	PPC_STORE_U32(ctx.r30.u32 + 76, ctx.r10.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226911c
	if (!ctx.cr6.eq) goto loc_8226911C;
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r20
	ctx.r11.u64 = ctx.r20.u64;
	// addi r7,r30,72
	ctx.r7.s64 = ctx.r30.s64 + 72;
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// stb r11,92(r30)
	PPC_STORE_U8(ctx.r30.u32 + 92, ctx.r11.u8);
	// stw r10,1000(r18)
	PPC_STORE_U32(ctx.r18.u32 + 1000, ctx.r10.u32);
	// stwx r7,r9,r16
	PPC_STORE_U32(ctx.r9.u32 + ctx.r16.u32, ctx.r7.u32);
loc_8226911C:
	// lbz r11,28435(r15)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r15.u32 + 28435);
	// lbz r27,1795(r24)
	ctx.r27.u64 = PPC_LOAD_U8(ctx.r24.u32 + 1795);
	// stw r20,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r20.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r20,1795(r24)
	PPC_STORE_U8(ctx.r24.u32 + 1795, ctx.r20.u8);
	// beq cr6,0x82269140
	if (ctx.cr6.eq) goto loc_82269140;
	// bl 0x822228f0
	ctx.lr = 0x82269138;
	sub_822228F0(ctx, base);
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
	// stb r11,28435(r15)
	PPC_STORE_U8(ctx.r15.u32 + 28435, ctx.r11.u8);
loc_82269140:
	// lwz r11,5360(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 5360);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r10,1004(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1004);
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// lwz r3,412(r21)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r21.u32 + 412);
	// rlwinm r11,r11,0,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 | 1;
	// stw r11,5360(r23)
	PPC_STORE_U32(ctx.r23.u32 + 5360, ctx.r11.u32);
	// xor r8,r11,r9
	ctx.r8.u64 = ctx.r11.u64 ^ ctx.r9.u64;
	// or r11,r8,r10
	ctx.r11.u64 = ctx.r8.u64 | ctx.r10.u64;
	// stw r11,1004(r26)
	PPC_STORE_U32(ctx.r26.u32 + 1004, ctx.r11.u32);
	// bl 0x821c6778
	ctx.lr = 0x82269178;
	sub_821C6778(ctx, base);
	// cmplwi cr6,r19,1
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 1, ctx.xer);
	// lwz r28,1784(r24)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1784);
	// stw r20,1784(r24)
	PPC_STORE_U32(ctx.r24.u32 + 1784, ctx.r20.u32);
	// lwz r11,60(r14)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r14.u32 + 60);
	// lwz r10,52(r14)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r14.u32 + 52);
	// bne cr6,0x82269198
	if (!ctx.cr6.eq) goto loc_82269198;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// b 0x8226919c
	goto loc_8226919C;
loc_82269198:
	// lwz r10,16(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 16);
loc_8226919C:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// blt cr6,0x822691b8
	if (ctx.cr6.lt) goto loc_822691B8;
	// lwz r11,28(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,88(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwzx r4,r9,r10
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// b 0x822691c0
	goto loc_822691C0;
loc_822691B8:
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// lwz r4,21784(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21784);
loc_822691C0:
	// mr r6,r19
	ctx.r6.u64 = ctx.r19.u64;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r3,r14
	ctx.r3.u64 = ctx.r14.u64;
	// bl 0x822695d8
	ctx.lr = 0x822691D0;
	sub_822695D8(ctx, base);
	// lbz r11,28435(r15)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r15.u32 + 28435);
	// clrlwi r29,r27,24
	ctx.r29.u64 = ctx.r27.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r29,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r29.u32);
	// beq cr6,0x822691f0
	if (ctx.cr6.eq) goto loc_822691F0;
	// bl 0x822228f0
	ctx.lr = 0x822691E8;
	sub_822228F0(ctx, base);
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
	// stb r11,28435(r15)
	PPC_STORE_U8(ctx.r15.u32 + 28435, ctx.r11.u8);
loc_822691F0:
	// lwz r11,5360(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 5360);
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r10,1004(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1004);
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// lwz r3,412(r21)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r21.u32 + 412);
	// rlwimi r11,r29,0,31,31
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r29.u32, 0) & 0x1) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFFE);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,5360(r23)
	PPC_STORE_U32(ctx.r23.u32 + 5360, ctx.r11.u32);
	// xor r8,r11,r9
	ctx.r8.u64 = ctx.r11.u64 ^ ctx.r9.u64;
	// or r11,r8,r10
	ctx.r11.u64 = ctx.r8.u64 | ctx.r10.u64;
	// stw r11,1004(r26)
	PPC_STORE_U32(ctx.r26.u32 + 1004, ctx.r11.u32);
	// bl 0x821c6778
	ctx.lr = 0x82269224;
	sub_821C6778(ctx, base);
	// stb r27,1795(r24)
	PPC_STORE_U8(ctx.r24.u32 + 1795, ctx.r27.u8);
	// stw r28,1784(r24)
	PPC_STORE_U32(ctx.r24.u32 + 1784, ctx.r28.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8221f410
	ctx.lr = 0x82269234;
	sub_8221F410(ctx, base);
	// lwz r29,164(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
loc_82269238:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x82269274
	if (ctx.cr6.eq) goto loc_82269274;
	// bl 0x82232298
	ctx.lr = 0x82269244;
	sub_82232298(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r3,412(r21)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r21.u32 + 412);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r25.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r25.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x822069c0
	ctx.lr = 0x82269274;
	sub_822069C0(ctx, base);
loc_82269274:
	// rlwinm r11,r19,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r19.u32 | (ctx.r19.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// add r11,r19,r11
	ctx.r11.u64 = ctx.r19.u64 + ctx.r11.u64;
	// mr r3,r14
	ctx.r3.u64 = ctx.r14.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r14
	ctx.r11.u64 = ctx.r11.u64 + ctx.r14.u64;
	// addi r5,r11,64
	ctx.r5.s64 = ctx.r11.s64 + 64;
	// bl 0x82263ce8
	ctx.lr = 0x82269294;
	sub_82263CE8(ctx, base);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// cmplwi cr6,r19,2
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 2, ctx.xer);
	// blt cr6,0x82268e54
	if (ctx.cr6.lt) goto loc_82268E54;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// lwz r11,-2744(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -2744);
	// lfs f1,16812(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 16812);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821b1b70
	ctx.lr = 0x822692B0;
	sub_821B1B70(ctx, base);
loc_822692B0:
	// bl 0x821fc320
	ctx.lr = 0x822692B4;
	sub_821FC320(ctx, base);
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// lwz r3,-2368(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -2368);
	// bl 0x82237060
	ctx.lr = 0x822692C4;
	sub_82237060(ctx, base);
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x821fbe60
	ctx.lr = 0x822692CC;
	sub_821FBE60(ctx, base);
	// addi r1,r1,416
	ctx.r1.s64 = ctx.r1.s64 + 416;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	sub_82CA2C00(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822692D8"))) PPC_WEAK_FUNC(sub_822692D8);
PPC_FUNC_IMPL(__imp__sub_822692D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f0,556(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 556);
	ctx.f0.f64 = double(temp.f32);
	// addi r11,r11,-27456
	ctx.r11.s64 = ctx.r11.s64 + -27456;
	// fsubs f0,f1,f0
	ctx.f0.f64 = double(float(ctx.f1.f64 - ctx.f0.f64));
	// lfs f13,28(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 28);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,24(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f12,f13,f12
	ctx.f12.f64 = double(float(ctx.f13.f64 - ctx.f12.f64));
	// lfs f13,-404(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -404);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f12,f13
	ctx.cr6.compare(ctx.f12.f64, ctx.f13.f64);
	// bge cr6,0x82269308
	if (!ctx.cr6.lt) goto loc_82269308;
	// fmr f12,f13
	ctx.f12.f64 = ctx.f13.f64;
loc_82269308:
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lfs f11,-27632(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -27632);
	ctx.f11.f64 = double(temp.f32);
	// fcmpu cr6,f0,f11
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// bgt cr6,0x82269320
	if (ctx.cr6.gt) goto loc_82269320;
loc_82269318:
	// lfs f1,-12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -12);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
loc_82269320:
	// lfs f13,8592(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8592);
	ctx.f13.f64 = double(temp.f32);
	// fadds f10,f12,f13
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f13.f64));
	// fcmpu cr6,f0,f10
	ctx.cr6.compare(ctx.f0.f64, ctx.f10.f64);
	// bge cr6,0x82269318
	if (!ctx.cr6.lt) goto loc_82269318;
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bgt cr6,0x82269360
	if (ctx.cr6.gt) goto loc_82269360;
	// fsubs f11,f0,f11
	ctx.f11.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
	// lfs f0,9704(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 9704);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,8224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8224);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,9544(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 9544);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f9,f10,f10
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f10.f64));
	// fmuls f8,f9,f10
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f10.f64));
	// fmuls f7,f8,f13
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// fmsubs f1,f9,f12,f7
	ctx.f1.f64 = double(float(ctx.f9.f64 * ctx.f12.f64 - ctx.f7.f64));
	// blr 
	return;
loc_82269360:
	// fsubs f13,f12,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f12.f64 - ctx.f13.f64));
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// blt cr6,0x82269398
	if (ctx.cr6.lt) goto loc_82269398;
	// fsubs f10,f0,f13
	ctx.f10.f64 = double(float(ctx.f0.f64 - ctx.f13.f64));
	// lfs f0,9704(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 9704);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,0(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,8224(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 8224);
	ctx.f12.f64 = double(temp.f32);
	// lfs f11,9544(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 9544);
	ctx.f11.f64 = double(temp.f32);
	// fnmsubs f9,f10,f0,f13
	ctx.f9.f64 = double(float(-(ctx.f10.f64 * ctx.f0.f64 - ctx.f13.f64)));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fmuls f7,f8,f9
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f9.f64));
	// fmuls f6,f7,f12
	ctx.f6.f64 = double(float(ctx.f7.f64 * ctx.f12.f64));
	// fmsubs f1,f8,f11,f6
	ctx.f1.f64 = double(float(ctx.f8.f64 * ctx.f11.f64 - ctx.f6.f64));
	// blr 
	return;
loc_82269398:
	// lfs f1,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822693A0"))) PPC_WEAK_FUNC(sub_822693A0);
PPC_FUNC_IMPL(__imp__sub_822693A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x82269520
	ctx.lr = 0x822693BC;
	sub_82269520(ctx, base);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lfs f1,32(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,44(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 44);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822693D4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r30,r31,20
	ctx.r30.s64 = ctx.r31.s64 + 20;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,40(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822693F0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x822694e0
	if (ctx.cr0.eq) goto loc_822694E0;
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82269414;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x822694d8
	if (ctx.cr6.eq) goto loc_822694D8;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,64(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 64);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82269434;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi. r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne 0x822694d8
	if (!ctx.cr0.eq) goto loc_822694D8;
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lwz r11,-720(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -720);
	// lfs f0,3080(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 3080);
	ctx.f0.f64 = double(temp.f32);
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x8226945c
	if (ctx.cr0.eq) goto loc_8226945C;
	// lfs f13,12(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// b 0x82269460
	goto loc_82269460;
loc_8226945C:
	// fmr f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f0.f64;
loc_82269460:
	// cmplwi r11,0
	ctx.cr0.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq 0x8226946c
	if (ctx.cr0.eq) goto loc_8226946C;
	// lfs f0,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
loc_8226946C:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x822694d8
	if (ctx.cr0.eq) goto loc_822694D8;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// fmuls f13,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f12,8(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	ctx.f12.f64 = double(temp.f32);
	// lwz r4,20(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// lfs f0,2720(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2720);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f11,f13,f0
	ctx.f11.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// lfs f10,52(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	ctx.f10.f64 = double(temp.f32);
	// lfs f0,0(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fsubs f0,f0,f10
	ctx.f0.f64 = double(float(ctx.f0.f64 - ctx.f10.f64));
	// lfs f13,4(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// lfs f9,56(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 56);
	ctx.f9.f64 = double(temp.f32);
	// fsubs f13,f13,f9
	ctx.f13.f64 = double(float(ctx.f13.f64 - ctx.f9.f64));
	// lfs f10,60(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 60);
	ctx.f10.f64 = double(temp.f32);
	// fsubs f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 - ctx.f10.f64));
	// fmuls f0,f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f0.f64));
	// fmadds f0,f13,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f13.f64 + ctx.f0.f64));
	// fmadds f0,f12,f12,f0
	ctx.f0.f64 = double(float(ctx.f12.f64 * ctx.f12.f64 + ctx.f0.f64));
	// fcmpu cr6,f0,f11
	ctx.cr6.compare(ctx.f0.f64, ctx.f11.f64);
	// bge cr6,0x822694d8
	if (!ctx.cr6.lt) goto loc_822694D8;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lfs f1,3084(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 3084);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822a8fd8
	ctx.lr = 0x822694D8;
	sub_822A8FD8(ctx, base);
loc_822694D8:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x82269504
	goto loc_82269504;
loc_822694E0:
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x822694F4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r11,r3,-2
	ctx.r11.s64 = ctx.r3.s64 + -2;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = ctx.r11.u64 ^ 1;
loc_82269504:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226951C"))) PPC_WEAK_FUNC(sub_8226951C);
PPC_FUNC_IMPL(__imp__sub_8226951C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82269520"))) PPC_WEAK_FUNC(sub_82269520);
PPC_FUNC_IMPL(__imp__sub_82269520) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r11,24(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x82269554
	if (ctx.cr6.eq) goto loc_82269554;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x82269554;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_82269554:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82269568"))) PPC_WEAK_FUNC(sub_82269568);
PPC_FUNC_IMPL(__imp__sub_82269568) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.f31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lis r11,-31948
	ctx.r11.s64 = -2093744128;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r11,-16196
	ctx.r4.s64 = ctx.r11.s64 + -16196;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r3,20(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// bl 0x821e8238
	ctx.lr = 0x8226959C;
	sub_821E8238(ctx, base);
	// cmplwi r3,0
	ctx.cr0.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq 0x822695b8
	if (ctx.cr0.eq) goto loc_822695B8;
	// lfs f0,4(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	ctx.cr6.compare(ctx.f31.f64, ctx.f0.f64);
	// ble cr6,0x822695b8
	if (!ctx.cr6.gt) goto loc_822695B8;
	// stfs f0,16(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 16, temp.u32);
	// b 0x822695bc
	goto loc_822695BC;
loc_822695B8:
	// stfs f31,16(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 16, temp.u32);
loc_822695BC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// lfd f31,-24(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822695D4"))) PPC_WEAK_FUNC(sub_822695D4);
PPC_FUNC_IMPL(__imp__sub_822695D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822695D8"))) PPC_WEAK_FUNC(sub_822695D8);
PPC_FUNC_IMPL(__imp__sub_822695D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd4
	ctx.lr = 0x822695E0;
	sub_82CA2BD4(ctx, base);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// addi r31,r10,-32624
	ctx.r31.s64 = ctx.r10.s64 + -32624;
	// addi r8,r11,-9360
	ctx.r8.s64 = ctx.r11.s64 + -9360;
	// li r11,-1
	ctx.r11.s64 = -1;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r11.u32);
	// lwz r10,8196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8196);
	// addi r7,r31,8200
	ctx.r7.s64 = ctx.r31.s64 + 8200;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r11.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r11.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r11.u32);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// stb r6,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r6.u8);
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// stwx r8,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + ctx.r31.u32, ctx.r8.u32);
	// lwz r10,8196(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8196);
	// lwz r9,8192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8192);
	// lwz r11,16396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16396);
	// rlwinm r6,r11,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r10,1
	ctx.r11.s64 = ctx.r10.s64 + 1;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,8196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8196, ctx.r11.u32);
	// stw r10,8192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8192, ctx.r10.u32);
	// stwx r8,r6,r7
	PPC_STORE_U32(ctx.r6.u32 + ctx.r7.u32, ctx.r8.u32);
	// lwz r11,16396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16396);
	// lwz r10,16392(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16392);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r10,16392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16392, ctx.r10.u32);
	// stw r11,16396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16396, ctx.r11.u32);
	// bl 0x8221a8d0
	ctx.lr = 0x8226967C;
	sub_8221A8D0(ctx, base);
	// lis r5,-31950
	ctx.r5.s64 = -2093875200;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r26,1
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 1, ctx.xer);
	// stb r11,5390(r5)
	PPC_STORE_U8(ctx.r5.u32 + 5390, ctx.r11.u8);
	// lwz r29,60(r30)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r30.u32 + 60);
	// lwz r28,52(r29)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r29.u32 + 52);
	// lwz r9,120(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 120);
	// bne cr6,0x822696a4
	if (!ctx.cr6.eq) goto loc_822696A4;
	// lwz r11,36(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// b 0x822696a8
	goto loc_822696A8;
loc_822696A4:
	// lwz r11,36(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
loc_822696A8:
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// mr r24,r11
	ctx.r24.u64 = ctx.r11.u64;
	// lwz r7,20(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// lbz r6,5389(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 5389);
	// cmplwi cr6,r6,1
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 1, ctx.xer);
	// lwz r11,3408(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 3408);
	// bne cr6,0x822696e8
	if (!ctx.cr6.eq) goto loc_822696E8;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// li r7,12
	ctx.r7.s64 = 12;
	// lwz r6,72(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// subf r5,r6,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r6.s64;
	// divw. r4,r5,r7
	ctx.r4.s32 = ctx.r5.s32 / ctx.r7.s32;
	ctx.cr0.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// bne 0x822696fc
	if (!ctx.cr0.eq) goto loc_822696FC;
	// cmpwi cr6,r24,2
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 2, ctx.xer);
	// bne cr6,0x822696fc
	if (!ctx.cr6.eq) goto loc_822696FC;
loc_822696E8:
	// cmplwi cr6,r26,1
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 1, ctx.xer);
	// bne cr6,0x822696f8
	if (!ctx.cr6.eq) goto loc_822696F8;
	// lwz r24,36(r29)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// b 0x822696fc
	goto loc_822696FC;
loc_822696F8:
	// lwz r24,36(r30)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
loc_822696FC:
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// li r25,0
	ctx.r25.s64 = 0;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r25,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r25.u32);
	// cmplwi cr6,r24,2
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 2, ctx.xer);
	// stb r11,5389(r10)
	PPC_STORE_U8(ctx.r10.u32 + 5389, ctx.r11.u8);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r25,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r25.u32);
	// stw r25,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r25.u32);
	// stw r25,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r25.u32);
	// stw r25,12(r8)
	PPC_STORE_U32(ctx.r8.u32 + 12, ctx.r25.u32);
	// blt cr6,0x82269764
	if (ctx.cr6.lt) goto loc_82269764;
	// bne cr6,0x82269764
	if (!ctx.cr6.eq) goto loc_82269764;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r10,16376
	ctx.r11.s64 = ctx.r10.s64 + 16376;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r4,r10,r11
	ctx.r4.u64 = ctx.r10.u64 + ctx.r11.u64;
	// bl 0x822a25e8
	ctx.lr = 0x8226974C;
	sub_822A25E8(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r28,96
	ctx.r4.s64 = ctx.r28.s64 + 96;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82220d38
	ctx.lr = 0x82269760;
	sub_82220D38(ctx, base);
	// b 0x82269770
	goto loc_82269770;
loc_82269764:
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// addi r4,r11,16376
	ctx.r4.s64 = ctx.r11.s64 + 16376;
	// bl 0x822a25e8
	ctx.lr = 0x82269770;
	sub_822A25E8(ctx, base);
loc_82269770:
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// bne cr6,0x82269790
	if (!ctx.cr6.eq) goto loc_82269790;
	// lwz r11,36(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 36);
	// lwz r10,216(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 216);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r9,r10
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// stw r8,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r8.u32);
	// b 0x822697f4
	goto loc_822697F4;
loc_82269790:
	// lwz r11,48(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r9,r10,31,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x822697b0
	if (!ctx.cr6.eq) goto loc_822697B0;
	// mr r9,r25
	ctx.r9.u64 = ctx.r25.u64;
	// b 0x822697c4
	goto loc_822697C4;
loc_822697B0:
	// lbz r11,155(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 155);
	// cntlzw r9,r11
	ctx.r9.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r11,r8,1
	ctx.r11.u64 = ctx.r8.u64 ^ 1;
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
loc_822697C4:
	// lwz r11,36(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 36);
	// rlwinm r10,r10,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x1;
	// lwz r8,52(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 52);
	// addi r7,r11,20
	ctx.r7.s64 = ctx.r11.s64 + 20;
	// rlwinm r11,r7,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r6,r11,r9
	ctx.r6.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r4,r5,r8
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r8.u32);
	// stw r4,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r4.u32);
loc_822697F4:
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// li r9,1
	ctx.r9.s64 = 1;
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// addi r8,r11,80
	ctx.r8.s64 = ctx.r11.s64 + 80;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8220bc30
	ctx.lr = 0x82269818;
	sub_8220BC30(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8220bbc0
	ctx.lr = 0x82269820;
	sub_8220BBC0(ctx, base);
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x821e7648
	ctx.lr = 0x82269830;
	sub_821E7648(ctx, base);
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x82269840
	if (ctx.cr6.eq) goto loc_82269840;
	// bl 0x821fc1f0
	ctx.lr = 0x82269840;
	sub_821FC1F0(ctx, base);
loc_82269840:
	// lbz r10,265(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 265);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82269858
	if (ctx.cr6.eq) goto loc_82269858;
	// stb r25,265(r1)
	PPC_STORE_U8(ctx.r1.u32 + 265, ctx.r25.u8);
	// addi r3,r31,8200
	ctx.r3.s64 = ctx.r31.s64 + 8200;
	// bl 0x8221f478
	ctx.lr = 0x82269858;
	sub_8221F478(ctx, base);
loc_82269858:
	// lbz r10,264(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 264);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82269870
	if (ctx.cr6.eq) goto loc_82269870;
	// stb r25,264(r1)
	PPC_STORE_U8(ctx.r1.u32 + 264, ctx.r25.u8);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8221f478
	ctx.lr = 0x82269870;
	sub_8221F478(ctx, base);
loc_82269870:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8221f410
	ctx.lr = 0x82269878;
	sub_8221F410(ctx, base);
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
}

__attribute__((alias("__imp__sub_82269880"))) PPC_WEAK_FUNC(sub_82269880);
PPC_FUNC_IMPL(__imp__sub_82269880) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r4,256
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 256, ctx.xer);
	// bge cr6,0x822698ac
	if (!ctx.cr6.lt) goto loc_822698AC;
	// clrlwi r10,r4,27
	ctx.r10.u64 = ctx.r4.u32 & 0x1F;
	// li r9,1
	ctx.r9.s64 = 1;
	// rlwinm r11,r4,27,5,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 27) & 0x7FFFFFF;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r3
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r3.u32);
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// and r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 & ctx.r11.u64;
	// blr 
	return;
loc_822698AC:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822698B4"))) PPC_WEAK_FUNC(sub_822698B4);
PPC_FUNC_IMPL(__imp__sub_822698B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_822698B8"))) PPC_WEAK_FUNC(sub_822698B8);
PPC_FUNC_IMPL(__imp__sub_822698B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bcc
	ctx.lr = 0x822698C0;
	sub_82CA2BCC(ctx, base);
	// stfd f29,-120(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -120, ctx.f29.u64);
	// stfd f30,-112(r1)
	PPC_STORE_U64(ctx.r1.u32 + -112, ctx.f30.u64);
	// stfd f31,-104(r1)
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.f31.u64);
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r22,1
	ctx.r22.s64 = 1;
	// lwz r11,196(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 196);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x82269908
	if (ctx.cr6.eq) goto loc_82269908;
	// lbz r11,260(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 260);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269900
	if (ctx.cr6.eq) goto loc_82269900;
	// lfs f0,252(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,264(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bgt cr6,0x82269908
	if (ctx.cr6.gt) goto loc_82269908;
loc_82269900:
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
	// b 0x8226990c
	goto loc_8226990C;
loc_82269908:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226990C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269ba0
	if (ctx.cr6.eq) goto loc_82269BA0;
	// lbz r11,425(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 425);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269ba0
	if (ctx.cr6.eq) goto loc_82269BA0;
	// lwz r11,368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269ba0
	if (ctx.cr6.eq) goto loc_82269BA0;
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// lwz r24,360(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r23,r11,-7536
	ctx.r23.s64 = ctx.r11.s64 + -7536;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// lfs f1,-25888(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8223b988
	ctx.lr = 0x82269950;
	sub_8223B988(ctx, base);
	// lbz r9,424(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 424);
	// li r21,272
	ctx.r21.s64 = 272;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x82269974
	if (ctx.cr6.eq) goto loc_82269974;
	// li r11,48
	ctx.r11.s64 = 48;
	// li r10,0
	ctx.r10.s64 = 0;
	// lvx128 v0,r23,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r23.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r10,424(r31)
	PPC_STORE_U8(ctx.r31.u32 + 424, ctx.r10.u8);
	// stvx128 v0,r31,r21
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82269974:
	// lbz r10,432(r31)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r31.u32 + 432);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// addi r29,r11,-27456
	ctx.r29.s64 = ctx.r11.s64 + -27456;
	// beq cr6,0x82269ae0
	if (ctx.cr6.eq) goto loc_82269AE0;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lfs f0,252(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// li r4,3
	ctx.r4.s64 = 3;
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// addi r30,r10,-4108
	ctx.r30.s64 = ctx.r10.s64 + -4108;
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r3,472(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 472);
	// lwz r11,27648(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 27648);
	// addi r9,r30,24
	ctx.r9.s64 = ctx.r30.s64 + 24;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f29,r8,r9
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	ctx.f29.f64 = double(temp.f32);
	// bl 0x8321e6a0
	ctx.lr = 0x822699C0;
	sub_8321E6A0(ctx, base);
	// fmuls f12,f1,f29
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64 * ctx.f29.f64));
	// lis r7,-31950
	ctx.r7.s64 = -2093875200;
	// lfs f30,40(r30)
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 40);
	ctx.f30.f64 = double(temp.f32);
	// lwz r11,-31652(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + -31652);
	// cmpwi cr6,r11,267
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 267, ctx.xer);
	// addi r8,r11,-267
	ctx.r8.s64 = ctx.r11.s64 + -267;
	// fmuls f13,f12,f30
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// bge cr6,0x822699e4
	if (!ctx.cr6.lt) goto loc_822699E4;
	// addi r8,r11,103
	ctx.r8.s64 = ctx.r11.s64 + 103;
loc_822699E4:
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,-4048
	ctx.r10.s64 = ctx.r10.s64 + -4048;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpwi cr6,r11,370
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 370, ctx.xer);
	// lwzx r6,r9,r10
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// lwzx r5,r8,r10
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// xor r8,r5,r6
	ctx.r8.u64 = ctx.r5.u64 ^ ctx.r6.u64;
	// stwx r8,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, ctx.r8.u32);
	// blt cr6,0x82269a14
	if (ctx.cr6.lt) goto loc_82269A14;
	// li r11,0
	ctx.r11.s64 = 0;
loc_82269A14:
	// stw r11,-31652(r7)
	PPC_STORE_U32(ctx.r7.u32 + -31652, ctx.r11.u32);
	// clrlwi r11,r8,8
	ctx.r11.u64 = ctx.r8.u32 & 0xFFFFFF;
	// lis r10,-31955
	ctx.r10.s64 = -2094202880;
	// lwz r30,472(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 472);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lfs f12,252(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 252);
	ctx.f12.f64 = double(temp.f32);
	// fctiwz f8,f12
	ctx.f8.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// lfs f0,24080(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24080);
	ctx.f0.f64 = double(temp.f32);
	// lfd f11,96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// stfd f8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f8.u64);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f7,f9,f13
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fmuls f31,f7,f0
	ctx.f31.f64 = double(float(ctx.f7.f64 * ctx.f0.f64));
	// bl 0x8321e6a0
	ctx.lr = 0x82269A5C;
	sub_8321E6A0(ctx, base);
	// lwz r9,368(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// fmuls f0,f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f30.f64));
	// lwz r8,32(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// cmpwi cr6,r8,3
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 3, ctx.xer);
	// beq cr6,0x82269a78
	if (ctx.cr6.eq) goto loc_82269A78;
	// fmuls f31,f29,f31
	ctx.f31.f64 = double(float(ctx.f29.f64 * ctx.f31.f64));
	// fmuls f0,f29,f0
	ctx.f0.f64 = double(float(ctx.f29.f64 * ctx.f0.f64));
loc_82269A78:
	// fadds f0,f0,f31
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(float(ctx.f0.f64 + ctx.f31.f64));
	// lwz r11,232(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 232);
	// lfs f13,428(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 428);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,340(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	// lfs f12,68(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 68);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f0,f0,f12,f13
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f12.f64 + ctx.f13.f64));
	// stfs f0,428(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 428, temp.u32);
	// fctiwz f11,f0
	ctx.f11.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// stfd f11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f11.u64);
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// add r8,r10,r9
	ctx.r8.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r11,196(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 196);
	// cmpw cr6,r8,r11
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x82269ae8
	if (!ctx.cr6.gt) goto loc_82269AE8;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x82269ae8
	if (ctx.cr6.lt) goto loc_82269AE8;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsw r10,r11
	ctx.r10.s64 = ctx.r11.s32;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f13,96(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f12,f13
	ctx.f12.f64 = double(ctx.f13.s64);
	// frsp f11,f12
	ctx.f11.f64 = double(float(ctx.f12.f64));
	// fsubs f10,f0,f11
	ctx.f10.f64 = double(float(ctx.f0.f64 - ctx.f11.f64));
	// stfs f10,428(r31)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r31.u32 + 428, temp.u32);
	// b 0x82269ae8
	goto loc_82269AE8;
loc_82269AE0:
	// lfs f0,-12(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + -12);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,428(r31)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r31.u32 + 428, temp.u32);
loc_82269AE8:
	// lwz r11,368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// lfs f31,0(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// lfs f0,428(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 428);
	ctx.f0.f64 = double(temp.f32);
	// addi r30,r24,4
	ctx.r30.s64 = ctx.r24.s64 + 4;
	// fdivs f29,f31,f0
	ctx.f29.f64 = double(float(ctx.f31.f64 / ctx.f0.f64));
	// addi r29,r24,64
	ctx.r29.s64 = ctx.r24.s64 + 64;
	// fmr f30,f31
	ctx.f30.f64 = ctx.f31.f64;
	// fcmpu cr6,f0,f31
	ctx.cr6.compare(ctx.f0.f64, ctx.f31.f64);
	// lwz r11,36(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// addi r28,r11,48
	ctx.r28.s64 = ctx.r11.s64 + 48;
	// lwz r27,20(r11)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// blt cr6,0x82269b94
	if (ctx.cr6.lt) goto loc_82269B94;
	// ld r26,120(r1)
	ctx.r26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// ld r25,112(r1)
	ctx.r25.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
loc_82269B20:
	// lfs f0,428(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 428);
	ctx.f0.f64 = double(temp.f32);
	// stb r22,476(r31)
	PPC_STORE_U8(ctx.r31.u32 + 476, ctx.r22.u8);
	// fsubs f13,f0,f31
	ctx.f13.f64 = double(float(ctx.f0.f64 - ctx.f31.f64));
	// stfs f13,428(r31)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r31.u32 + 428, temp.u32);
	// mr r6,r23
	ctx.r6.u64 = ctx.r23.u64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x83228e80
	ctx.lr = 0x82269B48;
	sub_83228E80(ctx, base);
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
	// mr r9,r27
	ctx.r9.u64 = ctx.r27.u64;
	// lwz r8,368(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r7,r26
	ctx.r7.u64 = ctx.r26.u64;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x8321b5d8
	ctx.lr = 0x82269B6C;
	sub_8321B5D8(ctx, base);
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x832294f0
	ctx.lr = 0x82269B84;
	sub_832294F0(ctx, base);
	// lfs f12,428(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 428);
	ctx.f12.f64 = double(temp.f32);
	// fsubs f30,f30,f29
	ctx.f30.f64 = double(float(ctx.f30.f64 - ctx.f29.f64));
	// fcmpu cr6,f12,f31
	ctx.cr6.compare(ctx.f12.f64, ctx.f31.f64);
	// bge cr6,0x82269b20
	if (!ctx.cr6.lt) goto loc_82269B20;
loc_82269B94:
	// li r11,48
	ctx.r11.s64 = 48;
	// lvx128 v0,r23,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r23.u32 + ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r31,r21
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r21.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_82269BA0:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// lfd f29,-120(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -120);
	// lfd f30,-112(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -112);
	// lfd f31,-104(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// b 0x82ca2c1c
	// ERROR 82CA2C1C
	return;
}

__attribute__((alias("__imp__sub_82269BB4"))) PPC_WEAK_FUNC(sub_82269BB4);
PPC_FUNC_IMPL(__imp__sub_82269BB4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82269BB8"))) PPC_WEAK_FUNC(sub_82269BB8);
PPC_FUNC_IMPL(__imp__sub_82269BB8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bb0
	ctx.lr = 0x82269BC0;
	sub_82CA2BB0(ctx, base);
	// stfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -168, ctx.f30.u64);
	// stfd f31,-160(r1)
	PPC_STORE_U64(ctx.r1.u32 + -160, ctx.f31.u64);
	// li r12,-192
	ctx.r12.s64 = -192;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-704(r1)
	ea = -704 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stb r4,735(r1)
	PPC_STORE_U8(ctx.r1.u32 + 735, ctx.r4.u8);
	// lwz r22,368(r30)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r30.u32 + 368);
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// beq cr6,0x82269f84
	if (ctx.cr6.eq) goto loc_82269F84;
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// lwz r10,36(r22)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r22.u32 + 36);
	// lis r5,-31950
	ctx.r5.s64 = -2093875200;
	// lwz r7,340(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 340);
	// addi r4,r6,-27348
	ctx.r4.s64 = ctx.r6.s64 + -27348;
	// lwz r23,360(r30)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r30.u32 + 360);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r11,r1,112
	ctx.r11.s64 = ctx.r1.s64 + 112;
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// lwz r24,20(r10)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// addi r31,r1,96
	ctx.r31.s64 = ctx.r1.s64 + 96;
	// lfs f0,-22556(r5)
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + -22556);
	ctx.f0.f64 = double(temp.f32);
	// lfs f31,-120(r4)
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + -120);
	ctx.f31.f64 = double(temp.f32);
	// fneg f0,f0
	ctx.f0.u64 = ctx.f0.u64 ^ 0x8000000000000000;
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r5,r5,-22556
	ctx.r5.s64 = ctx.r5.s64 + -22556;
	// stfs f0,96(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lis r29,-31927
	ctx.r29.s64 = -2092367872;
	// stfs f31,112(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 112, temp.u32);
	// lis r8,-31924
	ctx.r8.s64 = -2092171264;
	// lvlx v0,0,r3
	temp.u32 = ctx.r3.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r3,-31946
	ctx.r3.s64 = -2093613056;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r14,r30,304
	ctx.r14.s64 = ctx.r30.s64 + 304;
	// lvlx v8,0,r31
	temp.u32 = ctx.r31.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r15,r30,320
	ctx.r15.s64 = ctx.r30.s64 + 320;
	// lvlx v13,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r9,27648(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 27648);
	// lvlx v11,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r3,-4108
	ctx.r11.s64 = ctx.r3.s64 + -4108;
	// lvlx v10,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r7,r7,3
	ctx.r7.s64 = ctx.r7.s64 + 3;
	// lvlx v7,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// stfs f0,84(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r3,r11,24
	ctx.r3.s64 = ctx.r11.s64 + 24;
	// lvlx v9,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// rlwinm r4,r9,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// vrlimi128 v8,v9,4,3
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v9.f32), 57), 4));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vrlimi128 v7,v0,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lfs f0,-27348(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + -27348);
	ctx.f0.f64 = double(temp.f32);
	// vrlimi128 v12,v13,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lwz r9,14360(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 14360);
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lis r6,-31924
	ctx.r6.s64 = -2092171264;
	// clrlwi r29,r9,31
	ctx.r29.u64 = ctx.r9.u32 & 0x1;
	// vrlimi128 v10,v7,3,2
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v7.f32), 78), 3));
	// srawi r7,r7,2
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 2;
	// vrlimi128 v8,v12,3,2
	_mm_store_ps(ctx.v8.f32, _mm_blend_ps(_mm_load_ps(ctx.v8.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 78), 3));
	// addi r18,r10,48
	ctx.r18.s64 = ctx.r10.s64 + 48;
	// addi r17,r23,4
	ctx.r17.s64 = ctx.r23.s64 + 4;
	// addi r16,r23,64
	ctx.r16.s64 = ctx.r23.s64 + 64;
	// stvx128 v10,r0,r14
	_mm_store_si128((__m128i*)(base + ((ctx.r14.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addze r31,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r31.s64 = temp.s64;
	// stvx128 v8,r0,r15
	_mm_store_si128((__m128i*)(base + ((ctx.r15.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// lfs f0,40(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	ctx.f0.f64 = double(temp.f32);
	// addi r27,r6,6000
	ctx.r27.s64 = ctx.r6.s64 + 6000;
	// lfsx f13,r4,r3
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r3.u32);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfs f12,96(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v6,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw128 v127,v6,0
	_mm_store_si128((__m128i*)ctx.v127.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), 0xFF));
	// bne cr6,0x82269d20
	if (!ctx.cr6.eq) goto loc_82269D20;
	// ori r11,r9,1
	ctx.r11.u64 = ctx.r9.u64 | 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,14360(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14360, ctx.r11.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,4(r27)
	PPC_STORE_U32(ctx.r27.u32 + 4, ctx.r10.u32);
	// stw r9,8(r27)
	PPC_STORE_U32(ctx.r27.u32 + 8, ctx.r9.u32);
	// lis r10,-31957
	ctx.r10.s64 = -2094333952;
	// stw r11,12(r27)
	PPC_STORE_U32(ctx.r27.u32 + 12, ctx.r11.u32);
	// addi r3,r10,-24424
	ctx.r3.s64 = ctx.r10.s64 + -24424;
	// bl 0x82ca3700
	ctx.lr = 0x82269D20;
	sub_82CA3700(ctx, base);
loc_82269D20:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// li r9,352
	ctx.r9.s64 = 352;
	// lwz r11,8(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 8);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// divw r10,r8,r9
	ctx.r10.s32 = ctx.r8.s32 / ctx.r9.s32;
	// cmplw cr6,r10,r31
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r31.u32, ctx.xer);
	// bge cr6,0x82269d5c
	if (!ctx.cr6.lt) goto loc_82269D5C;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// subf r5,r10,r31
	ctx.r5.s64 = ctx.r31.s64 - ctx.r10.s64;
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x82a02528
	ctx.lr = 0x82269D5C;
	sub_82A02528(ctx, base);
loc_82269D5C:
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r6,r16
	ctx.r6.u64 = ctx.r16.u64;
	// mr r5,r17
	ctx.r5.u64 = ctx.r17.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x821cf480
	ctx.lr = 0x82269D70;
	sub_821CF480(ctx, base);
	// lis r11,-31926
	ctx.r11.s64 = -2092302336;
	// lwz r10,472(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 472);
	// mr r19,r3
	ctx.r19.u64 = ctx.r3.u64;
	// addi r8,r11,-7776
	ctx.r8.s64 = ctx.r11.s64 + -7776;
	// lwz r7,224(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 224);
	// lwz r6,220(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 220);
	// addi r5,r19,3
	ctx.r5.s64 = ctx.r19.s64 + 3;
	// subf r4,r6,r7
	ctx.r4.s64 = ctx.r7.s64 - ctx.r6.s64;
	// lbz r3,126(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 126);
	// rlwinm r26,r5,30,2,31
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r10,8(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// srawi r9,r4,2
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r4.s32 >> 2;
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// extsb r8,r3
	ctx.r8.s64 = ctx.r3.s8;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// addi r10,r8,1
	ctx.r10.s64 = ctx.r8.s64 + 1;
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// cntlzw r8,r10
	ctx.r8.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + ctx.r11.u64;
	// rlwinm r6,r8,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// subfic r5,r7,0
	ctx.xer.ca = ctx.r7.u32 <= 0;
	ctx.r5.s64 = 0 - ctx.r7.s64;
	// xori r20,r6,1
	ctx.r20.u64 = ctx.r6.u64 ^ 1;
	// subfe r4,r5,r5
	temp.u8 = (~ctx.r5.u32 + ctx.r5.u32 < ~ctx.r5.u32) | (~ctx.r5.u32 + ctx.r5.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r4.u64 = ~ctx.r5.u64 + ctx.r5.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// clrlwi r21,r4,31
	ctx.r21.u64 = ctx.r4.u32 & 0x1;
	// beq cr6,0x82269e6c
	if (ctx.cr6.eq) goto loc_82269E6C;
	// li r25,0
	ctx.r25.s64 = 0;
	// addi r29,r19,-4
	ctx.r29.s64 = ctx.r19.s64 + -4;
loc_82269DE0:
	// srawi r10,r29,31
	ctx.xer.ca = (ctx.r29.s32 < 0) & ((ctx.r29.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r29.s32 >> 31;
	// lwz r11,4(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// and r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 & ctx.r29.u64;
	// add r31,r25,r11
	ctx.r31.u64 = ctx.r25.u64 + ctx.r11.u64;
	// addi r28,r10,4
	ctx.r28.s64 = ctx.r10.s64 + 4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// bl 0x8226a338
	ctx.lr = 0x82269E08;
	sub_8226A338(ctx, base);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r7,r20
	ctx.r7.u64 = ctx.r20.u64;
	// mr r6,r21
	ctx.r6.u64 = ctx.r21.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x82269fa0
	ctx.lr = 0x82269E20;
	sub_82269FA0(ctx, base);
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v127.u8));
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// bl 0x821dcaf0
	ctx.lr = 0x82269E38;
	sub_821DCAF0(ctx, base);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// mr r8,r28
	ctx.r8.u64 = ctx.r28.u64;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// mr r6,r18
	ctx.r6.u64 = ctx.r18.u64;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x8321ba10
	ctx.lr = 0x82269E5C;
	sub_8321BA10(ctx, base);
	// addic. r26,r26,-1
	ctx.xer.ca = ctx.r26.u32 > 0;
	ctx.r26.s64 = ctx.r26.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// addi r25,r25,352
	ctx.r25.s64 = ctx.r25.s64 + 352;
	// addi r29,r29,-4
	ctx.r29.s64 = ctx.r29.s64 + -4;
	// bne 0x82269de0
	if (!ctx.cr0.eq) goto loc_82269DE0;
loc_82269E6C:
	// li r31,0
	ctx.r31.s64 = 0;
	// cmplwi cr6,r19,0
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 0, ctx.xer);
	// beq cr6,0x82269ebc
	if (ctx.cr6.eq) goto loc_82269EBC;
	// li r28,0
	ctx.r28.s64 = 0;
	// li r29,0
	ctx.r29.s64 = 0;
loc_82269E80:
	// rlwinm r8,r31,30,2,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 30) & 0x3FFFFFFF;
	// lwz r9,4(r17)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r17.u32 + 4);
	// lwz r10,4(r16)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r16.u32 + 4);
	// clrlwi r3,r31,30
	ctx.r3.u64 = ctx.r31.u32 & 0x3;
	// lwz r11,4(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 4);
	// mulli r8,r8,352
	ctx.r8.s64 = ctx.r8.s64 * 352;
	// add r6,r8,r11
	ctx.r6.u64 = ctx.r8.u64 + ctx.r11.u64;
	// add r5,r9,r29
	ctx.r5.u64 = ctx.r9.u64 + ctx.r29.u64;
	// add r4,r28,r10
	ctx.r4.u64 = ctx.r28.u64 + ctx.r10.u64;
	// bl 0x8321c518
	ctx.lr = 0x82269EA8;
	sub_8321C518(ctx, base);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// addi r29,r29,48
	ctx.r29.s64 = ctx.r29.s64 + 48;
	// addi r28,r28,36
	ctx.r28.s64 = ctx.r28.s64 + 36;
	// cmplw cr6,r31,r19
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r19.u32, ctx.xer);
	// blt cr6,0x82269e80
	if (ctx.cr6.lt) goto loc_82269E80;
loc_82269EBC:
	// lbz r11,476(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 476);
	// stw r19,340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 340, ctx.r19.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269f84
	if (ctx.cr6.eq) goto loc_82269F84;
	// lwz r11,232(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 232);
	// li r31,0
	ctx.r31.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269ef0
	if (ctx.cr6.eq) goto loc_82269EF0;
	// lbz r11,56(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 56);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x82269ef0
	if (ctx.cr6.eq) goto loc_82269EF0;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// lwz r31,27644(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 27644);
loc_82269EF0:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x832222d0
	ctx.lr = 0x82269EF8;
	sub_832222D0(ctx, base);
	// fmr f30,f1
	ctx.fpscr.disableFlushMode();
	ctx.f30.f64 = ctx.f1.f64;
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// bne cr6,0x82269f0c
	if (!ctx.cr6.eq) goto loc_82269F0C;
	// li r5,0
	ctx.r5.s64 = 0;
	// b 0x82269f70
	goto loc_82269F70;
loc_82269F0C:
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// stfs f31,96(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r14
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r14.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lvlx v13,0,r11
	temp.u32 = ctx.r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v12,v13,4,3
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v10,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 57), 4));
	// vrlimi128 v12,v11,3,2
	_mm_store_ps(ctx.v12.f32, _mm_blend_ps(_mm_load_ps(ctx.v12.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vsubfp v9,v0,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32)));
	// stvx128 v9,r0,r14
	_mm_store_si128((__m128i*)(base + ((ctx.r14.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v8,r0,r15
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r15.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v7,v8,v12
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v12.f32)));
	// stvx128 v7,r0,r15
	_mm_store_si128((__m128i*)(base + ((ctx.r15.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r6,32(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 32);
	// mtctr r6
	ctx.ctr.u64 = ctx.r6.u64;
	// bctrl 
	ctx.lr = 0x82269F6C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
loc_82269F70:
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// lbz r4,735(r1)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r1.u32 + 735);
	// addi r3,r30,336
	ctx.r3.s64 = ctx.r30.s64 + 336;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x83221fc8
	ctx.lr = 0x82269F84;
	sub_83221FC8(ctx, base);
loc_82269F84:
	// addi r1,r1,704
	ctx.r1.s64 = ctx.r1.s64 + 704;
	// li r0,-192
	ctx.r0.s64 = -192;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f30,-168(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -168);
	// lfd f31,-160(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	sub_82CA2C00(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_82269F9C"))) PPC_WEAK_FUNC(sub_82269F9C);
PPC_FUNC_IMPL(__imp__sub_82269F9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_82269FA0"))) PPC_WEAK_FUNC(sub_82269FA0);
PPC_FUNC_IMPL(__imp__sub_82269FA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bb8
	ctx.lr = 0x82269FA8;
	sub_82CA2BB8(ctx, base);
	// stfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -144, ctx.f31.u64);
	// li r12,-160
	ctx.r12.s64 = -160;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r24,r3
	ctx.r24.u64 = ctx.r3.u64;
	// mr r21,r5
	ctx.r21.u64 = ctx.r5.u64;
	// mr r18,r6
	ctx.r18.u64 = ctx.r6.u64;
	// mr r17,r7
	ctx.r17.u64 = ctx.r7.u64;
	// lwz r11,472(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 472);
	// lis r16,-31926
	ctx.r16.s64 = -2092302336;
	// lbz r10,225(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 225);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x82269ff0
	if (ctx.cr6.eq) goto loc_82269FF0;
	// lwz r11,-7712(r16)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r16.u32 + -7712);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x82269ff4
	if (!ctx.cr6.eq) goto loc_82269FF4;
loc_82269FF0:
	// li r11,0
	ctx.r11.s64 = 0;
loc_82269FF4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// li r27,0
	ctx.r27.s64 = 0;
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// beq cr6,0x8226a164
	if (ctx.cr6.eq) goto loc_8226A164;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// clrlwi r23,r11,24
	ctx.r23.u64 = ctx.r11.u32 & 0xFF;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// addi r26,r30,304
	ctx.r26.s64 = ctx.r30.s64 + 304;
	// addi r25,r30,320
	ctx.r25.s64 = ctx.r30.s64 + 320;
	// lfs f31,-19232(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19232);
	ctx.f31.f64 = double(temp.f32);
	// mr r31,r24
	ctx.r31.u64 = ctx.r24.u64;
	// addi r28,r24,256
	ctx.r28.s64 = ctx.r24.s64 + 256;
	// li r19,228
	ctx.r19.s64 = 228;
	// lis r22,-31924
	ctx.r22.s64 = -2092171264;
	// addi r20,r11,14208
	ctx.r20.s64 = ctx.r11.s64 + 14208;
loc_8226A030:
	// addi r29,r31,64
	ctx.r29.s64 = ctx.r31.s64 + 64;
	// lfs f0,0(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// mr r8,r17
	ctx.r8.u64 = ctx.r17.u64;
	// mr r7,r18
	ctx.r7.u64 = ctx.r18.u64;
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// lvx128 v127,r0,r31
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x83229378
	ctx.lr = 0x8226A060;
	sub_83229378(ctx, base);
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// beq cr6,0x8226a114
	if (ctx.cr6.eq) goto loc_8226A114;
	// lwz r3,-7712(r16)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r16.u32 + -7712);
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vor v1,v0,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// vsubfp v2,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v2.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226A094;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226a114
	if (ctx.cr6.eq) goto loc_8226A114;
	// lwz r11,14204(r22)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r22.u32 + 14204);
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v12,v127,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v127.f32), _mm_load_ps(ctx.v0.f32)));
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226a0d8
	if (!ctx.cr6.eq) goto loc_8226A0D8;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f31,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// ori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 | 1;
	// stw r11,14204(r22)
	PPC_STORE_U32(ctx.r22.u32 + 14204, ctx.r11.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// stvx128 v13,r0,r20
	_mm_store_si128((__m128i*)(base + ((ctx.r20.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8226a0dc
	goto loc_8226A0DC;
loc_8226A0D8:
	// lvx128 v13,r0,r20
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r20.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8226A0DC:
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lwz r10,472(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 472);
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v12,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// lvlx v10,r10,r19
	temp.u32 = ctx.r10.u32 + ctx.r19.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v8,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v7,v13,v11
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v6,v7,v0
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v5,v6,v12
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v4,v5,v9
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v9.f32)));
	// stvx128 v4,r0,r29
	_mm_store_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8226A114:
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// lvx128 v13,r0,r26
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// vcmpgefp v11,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_cmpge_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v10,r0,r25
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtfp v9,v0,v10
	_mm_store_ps(ctx.v9.f32, _mm_cmpgt_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32)));
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// vnor v8,v11,v11
	// vsel v7,v13,v0,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8))));
	// stvx128 v7,r0,r26
	_mm_store_si128((__m128i*)(base + ((ctx.r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsel v5,v10,v6,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8))));
	// stvx128 v5,r0,r25
	_mm_store_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8321cb98
	ctx.lr = 0x8226A150;
	sub_8321CB98(ctx, base);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// cmplw cr6,r27,r21
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r21.u32, ctx.xer);
	// blt cr6,0x8226a030
	if (ctx.cr6.lt) goto loc_8226A030;
loc_8226A164:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// li r0,-160
	ctx.r0.s64 = -160;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-144(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -144);
	// b 0x82ca2c08
	// ERROR 82CA2C08
	return;
}

__attribute__((alias("__imp__sub_8226A178"))) PPC_WEAK_FUNC(sub_8226A178);
PPC_FUNC_IMPL(__imp__sub_8226A178) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bc0
	ctx.lr = 0x8226A180;
	sub_82CA2BC0(ctx, base);
	// stfd f31,-128(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -128, ctx.f31.u64);
	// li r12,-144
	ctx.r12.s64 = -144;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r20,r6
	ctx.r20.u64 = ctx.r6.u64;
	// mr r19,r7
	ctx.r19.u64 = ctx.r7.u64;
	// lis r18,-31926
	ctx.r18.s64 = -2092302336;
	// lwz r11,472(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 472);
	// lbz r10,225(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 225);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226a1c0
	if (ctx.cr6.eq) goto loc_8226A1C0;
	// lwz r11,-7712(r18)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7712);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x8226a1c4
	if (!ctx.cr6.eq) goto loc_8226A1C4;
loc_8226A1C0:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226A1C4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x8226a320
	if (ctx.cr6.eq) goto loc_8226A320;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// clrlwi r24,r11,24
	ctx.r24.u64 = ctx.r11.u32 & 0xFF;
	// lis r11,-31924
	ctx.r11.s64 = -2092171264;
	// addi r26,r29,304
	ctx.r26.s64 = ctx.r29.s64 + 304;
	// addi r25,r29,320
	ctx.r25.s64 = ctx.r29.s64 + 320;
	// lfs f31,-19232(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -19232);
	ctx.f31.f64 = double(temp.f32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r28,r3,160
	ctx.r28.s64 = ctx.r3.s64 + 160;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// li r21,228
	ctx.r21.s64 = 228;
	// lis r23,-31924
	ctx.r23.s64 = -2092171264;
	// addi r22,r11,13984
	ctx.r22.s64 = ctx.r11.s64 + 13984;
loc_8226A200:
	// addi r30,r31,64
	ctx.r30.s64 = ctx.r31.s64 + 64;
	// lfs f0,0(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// mr r8,r19
	ctx.r8.u64 = ctx.r19.u64;
	// mr r7,r20
	ctx.r7.u64 = ctx.r20.u64;
	// stfd f13,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f13.u64);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lvx128 v127,r0,r31
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x83229378
	ctx.lr = 0x8226A230;
	sub_83229378(ctx, base);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x8226a2e4
	if (ctx.cr6.eq) goto loc_8226A2E4;
	// lwz r3,-7712(r18)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r18.u32 + -7712);
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r30
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// vor v1,v0,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)ctx.v0.u8));
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// vsubfp v2,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v2.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226A264;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226a2e4
	if (ctx.cr6.eq) goto loc_8226A2E4;
	// lwz r11,13968(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 13968);
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v12,v127,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v127.f32), _mm_load_ps(ctx.v0.f32)));
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226a2a8
	if (!ctx.cr6.eq) goto loc_8226A2A8;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// stfs f31,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// ori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 | 1;
	// stw r11,13968(r23)
	PPC_STORE_U32(ctx.r23.u32 + 13968, ctx.r11.u32);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// stvx128 v13,r0,r22
	_mm_store_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8226a2ac
	goto loc_8226A2AC;
loc_8226A2A8:
	// lvx128 v13,r0,r22
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r22.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8226A2AC:
	// addi r11,r1,96
	ctx.r11.s64 = ctx.r1.s64 + 96;
	// lwz r10,472(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 472);
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v11,v12,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// lvlx v10,r10,r21
	temp.u32 = ctx.r10.u32 + ctx.r21.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// lvx128 v8,r0,r9
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v8,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v7,v13,v11
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32)));
	// vmulfp128 v6,v7,v0
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v0.f32)));
	// vsubfp v5,v6,v12
	_mm_store_ps(ctx.v5.f32, _mm_sub_ps(_mm_load_ps(ctx.v6.f32), _mm_load_ps(ctx.v12.f32)));
	// vmulfp128 v4,v5,v9
	_mm_store_ps(ctx.v4.f32, _mm_mul_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v9.f32)));
	// stvx128 v4,r0,r30
	_mm_store_si128((__m128i*)(base + ((ctx.r30.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8226A2E4:
	// lvx128 v0,r0,r31
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addic. r27,r27,-1
	ctx.xer.ca = ctx.r27.u32 > 0;
	ctx.r27.s64 = ctx.r27.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// lvx128 v13,r0,r26
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r28,r28,4
	ctx.r28.s64 = ctx.r28.s64 + 4;
	// vcmpgefp v11,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_cmpge_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v10,r0,r25
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vcmpgtfp v9,v0,v10
	_mm_store_ps(ctx.v9.f32, _mm_cmpgt_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v10.f32)));
	// vnor v8,v11,v11
	// vsel v7,v13,v0,v8
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v13.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8))));
	// stvx128 v7,r0,r26
	_mm_store_si128((__m128i*)(base + ((ctx.r26.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v6,r0,r31
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsel v5,v10,v6,v9
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_or_si128(_mm_andnot_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v10.u8)), _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v6.u8))));
	// stvx128 v5,r0,r25
	_mm_store_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// bne 0x8226a200
	if (!ctx.cr0.eq) goto loc_8226A200;
loc_8226A320:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// li r0,-144
	ctx.r0.s64 = -144;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-128(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -128);
	// b 0x82ca2c10
	// ERROR 82CA2C10
	return;
}

__attribute__((alias("__imp__sub_8226A334"))) PPC_WEAK_FUNC(sub_8226A334);
PPC_FUNC_IMPL(__imp__sub_8226A334) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226A338"))) PPC_WEAK_FUNC(sub_8226A338);
PPC_FUNC_IMPL(__imp__sub_8226A338) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bdc
	ctx.lr = 0x8226A340;
	sub_82CA2BDC(ctx, base);
	// stfd f31,-72(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.f31.u64);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// addi r8,r29,44
	ctx.r8.s64 = ctx.r29.s64 + 44;
	// li r25,0
	ctx.r25.s64 = 0;
	// lwz r6,52(r29)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r29.u32 + 52);
	// lfs f0,48(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	ctx.f0.f64 = double(temp.f32);
	// li r27,1
	ctx.r27.s64 = 1;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x8226a3e8
	if (ctx.cr6.eq) goto loc_8226A3E8;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r11,r3,336
	ctx.r11.s64 = ctx.r3.s64 + 336;
	// addi r9,r10,-28144
	ctx.r9.s64 = ctx.r10.s64 + -28144;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_8226A380:
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x8226a3d8
	if (ctx.cr6.eq) goto loc_8226A3D8;
	// lfs f13,-80(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -80);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bgt cr6,0x8226a3d8
	if (ctx.cr6.gt) goto loc_8226A3D8;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// blt cr6,0x8226a3b0
	if (ctx.cr6.lt) goto loc_8226A3B0;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// blt cr6,0x8226a3b0
	if (ctx.cr6.lt) goto loc_8226A3B0;
	// stw r25,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r25.u32);
	// b 0x8226a3d8
	goto loc_8226A3D8;
loc_8226A3B0:
	// addi r10,r3,256
	ctx.r10.s64 = ctx.r3.s64 + 256;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v12,v13,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r27,-32(r11)
	PPC_STORE_U32(ctx.r11.u32 + -32, ctx.r27.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
loc_8226A3D8:
	// addic. r7,r7,-1
	ctx.xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// bne 0x8226a380
	if (!ctx.cr0.eq) goto loc_8226A380;
loc_8226A3E8:
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x8226a4d4
	if (ctx.cr6.eq) goto loc_8226A4D4;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r31,r3,256
	ctx.r31.s64 = ctx.r3.s64 + 256;
	// lfs f31,-27852(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27852);
	ctx.f31.f64 = double(temp.f32);
loc_8226A400:
	// lwz r11,48(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// lwz r9,16(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 16);
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// srawi r8,r11,31
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 31;
	// and r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 & ctx.r11.u64;
	// subf r6,r7,r11
	ctx.r6.s64 = ctx.r11.s64 - ctx.r7.s64;
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r5,r9
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f31
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r11,r4
	ctx.r11.s64 = ctx.r4.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f10,88(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fcmpu cr6,f0,f8
	ctx.cr6.compare(ctx.f0.f64, ctx.f8.f64);
	// blt cr6,0x8226a4ac
	if (ctx.cr6.lt) goto loc_8226A4AC;
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
loc_8226A454:
	// lwz r11,48(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r11.u32);
	// rotlwi r8,r11,0
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// lwz r9,16(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 16);
	// lfs f0,0(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// srawi r7,r8,31
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// subf r5,r6,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r6.s64;
	// rlwinm r4,r5,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f13,r4,r9
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r9.u32);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f31
	ctx.f12.f64 = double(float(ctx.f13.f64 + ctx.f31.f64));
	// fctiwz f11,f12
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfd f11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.f11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fcmpu cr6,f0,f8
	ctx.cr6.compare(ctx.f0.f64, ctx.f8.f64);
	// bge cr6,0x8226a454
	if (!ctx.cr6.lt) goto loc_8226A454;
loc_8226A4AC:
	// clrlwi r11,r10,24
	ctx.r11.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226a4c4
	if (ctx.cr6.eq) goto loc_8226A4C4;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8321b498
	ctx.lr = 0x8226A4C4;
	sub_8321B498(ctx, base);
loc_8226A4C4:
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// cmplw cr6,r30,r28
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r28.u32, ctx.xer);
	// blt cr6,0x8226a400
	if (ctx.cr6.lt) goto loc_8226A400;
loc_8226A4D4:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// lfd f31,-72(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// b 0x82ca2c2c
	// ERROR 82CA2C2C
	return;
}

__attribute__((alias("__imp__sub_8226A4E0"))) PPC_WEAK_FUNC(sub_8226A4E0);
PPC_FUNC_IMPL(__imp__sub_8226A4E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// std r30,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r30.u64);
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, ctx.r31.u64);
	// lwz r31,52(r4)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r4.u32 + 52);
	// lfs f0,48(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 48);
	ctx.f0.f64 = double(temp.f32);
	// addi r8,r4,44
	ctx.r8.s64 = ctx.r4.s64 + 44;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x8226a644
	if (ctx.cr6.eq) goto loc_8226A644;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r11,r3,192
	ctx.r11.s64 = ctx.r3.s64 + 192;
	// addi r9,r10,-28144
	ctx.r9.s64 = ctx.r10.s64 + -28144;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
loc_8226A514:
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// beq cr6,0x8226a56c
	if (ctx.cr6.eq) goto loc_8226A56C;
	// lfs f13,-32(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -32);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bgt cr6,0x8226a56c
	if (ctx.cr6.gt) goto loc_8226A56C;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// blt cr6,0x8226a544
	if (ctx.cr6.lt) goto loc_8226A544;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r10,r31
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r31.u32, ctx.xer);
	// blt cr6,0x8226a544
	if (ctx.cr6.lt) goto loc_8226A544;
	// stw r30,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r30.u32);
	// b 0x8226a56c
	goto loc_8226A56C;
loc_8226A544:
	// addi r10,r3,160
	ctx.r10.s64 = ctx.r3.s64 + 160;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v12,v13,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v11,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r5,-16(r11)
	PPC_STORE_U32(ctx.r11.u32 + -16, ctx.r5.u32);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
loc_8226A56C:
	// addic. r7,r7,-1
	ctx.xer.ca = ctx.r7.u32 > 0;
	ctx.r7.s64 = ctx.r7.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// addi r9,r9,16
	ctx.r9.s64 = ctx.r9.s64 + 16;
	// bne 0x8226a514
	if (!ctx.cr0.eq) goto loc_8226A514;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x8226a644
	if (ctx.cr6.eq) goto loc_8226A644;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lwz r10,16(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// addi r11,r3,160
	ctx.r11.s64 = ctx.r3.s64 + 160;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// lfs f0,-27852(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -27852);
	ctx.f0.f64 = double(temp.f32);
loc_8226A598:
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// srawi r7,r8,31
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// subf r5,r6,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r6.s64;
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r3,r10
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fctiwz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfd f10,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f10.u64);
	// lwz r8,-44(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// std r7,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.r7.u64);
	// lfd f9,-40(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// fcmpu cr6,f13,f7
	ctx.cr6.compare(ctx.f13.f64, ctx.f7.f64);
	// blt cr6,0x8226a638
	if (ctx.cr6.lt) goto loc_8226A638;
loc_8226A5E0:
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r10.u32);
	// lfs f13,0(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,16(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 16);
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// srawi r7,r8,31
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 31;
	// and r6,r7,r8
	ctx.r6.u64 = ctx.r7.u64 & ctx.r8.u64;
	// subf r5,r6,r8
	ctx.r5.s64 = ctx.r8.s64 - ctx.r6.s64;
	// rlwinm r3,r5,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// lfsx f12,r3,r10
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r10.u32);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f12,f0
	ctx.f11.f64 = double(float(ctx.f12.f64 + ctx.f0.f64));
	// fctiwz f10,f11
	ctx.f10.s64 = (ctx.f11.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f11.f64));
	// stfd f10,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f10.u64);
	// lwz r8,-44(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + -44);
	// extsw r7,r8
	ctx.r7.s64 = ctx.r8.s32;
	// std r7,-32(r1)
	PPC_STORE_U64(ctx.r1.u32 + -32, ctx.r7.u64);
	// lfd f9,-32(r1)
	ctx.f9.u64 = PPC_LOAD_U64(ctx.r1.u32 + -32);
	// fcfid f8,f9
	ctx.f8.f64 = double(ctx.f9.s64);
	// frsp f7,f8
	ctx.f7.f64 = double(float(ctx.f8.f64));
	// fcmpu cr6,f13,f7
	ctx.cr6.compare(ctx.f13.f64, ctx.f7.f64);
	// bge cr6,0x8226a5e0
	if (!ctx.cr6.lt) goto loc_8226A5E0;
loc_8226A638:
	// addic. r9,r9,-1
	ctx.xer.ca = ctx.r9.u32 > 0;
	ctx.r9.s64 = ctx.r9.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bne 0x8226a598
	if (!ctx.cr0.eq) goto loc_8226A598;
loc_8226A644:
	// ld r30,-16(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// ld r31,-8(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226A650"))) PPC_WEAK_FUNC(sub_8226A650);
PPC_FUNC_IMPL(__imp__sub_8226A650) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd8
	ctx.lr = 0x8226A658;
	sub_82CA2BD8(ctx, base);
	// stfd f31,-80(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.f31.u64);
	// li r12,-96
	ctx.r12.s64 = -96;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stfs f1,300(r1)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r1.u32 + 300, temp.u32);
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// lfs f0,252(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// lwz r3,480(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 480);
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// bl 0x8321e6a0
	ctx.lr = 0x8226A69C;
	sub_8321E6A0(ctx, base);
	// lis r7,-31950
	ctx.r7.s64 = -2093875200;
	// lwz r11,-31652(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + -31652);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// cmpwi cr6,r11,267
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 267, ctx.xer);
	// addi r8,r11,-267
	ctx.r8.s64 = ctx.r11.s64 + -267;
	// bge cr6,0x8226a6b8
	if (!ctx.cr6.lt) goto loc_8226A6B8;
	// addi r8,r11,103
	ctx.r8.s64 = ctx.r11.s64 + 103;
loc_8226A6B8:
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,-4048
	ctx.r10.s64 = ctx.r10.s64 + -4048;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// li r24,0
	ctx.r24.s64 = 0;
	// cmpwi cr6,r11,370
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 370, ctx.xer);
	// lwzx r6,r9,r10
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// lwzx r5,r8,r10
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// xor r29,r5,r6
	ctx.r29.u64 = ctx.r5.u64 ^ ctx.r6.u64;
	// stwx r29,r9,r10
	PPC_STORE_U32(ctx.r9.u32 + ctx.r10.u32, ctx.r29.u32);
	// blt cr6,0x8226a6ec
	if (ctx.cr6.lt) goto loc_8226A6EC;
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
loc_8226A6EC:
	// stw r11,-31652(r7)
	PPC_STORE_U32(ctx.r7.u32 + -31652, ctx.r11.u32);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// lfs f1,-25888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8223b988
	ctx.lr = 0x8226A704;
	sub_8223B988(ctx, base);
	// li r10,32
	ctx.r10.s64 = 32;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lvx128 v0,r30,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32 + ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x83228f28
	ctx.lr = 0x8226A720;
	sub_83228F28(ctx, base);
	// li r8,272
	ctx.r8.s64 = 272;
	// li r7,48
	ctx.r7.s64 = 48;
	// vspltisw128 v127,0
	_mm_store_si128((__m128i*)ctx.v127.u32, _mm_set1_epi32(int(0x0)));
	// addi r6,r1,300
	ctx.r6.s64 = ctx.r1.s64 + 300;
	// addi r27,r31,412
	ctx.r27.s64 = ctx.r31.s64 + 412;
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// lvx128 v0,r31,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r4,4
	ctx.r4.s64 = 4;
	// lvx128 v11,r30,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r30.u32 + ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v10,v11,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v0.f32)));
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v13,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// lvlx v9,0,r27
	temp.u32 = ctx.r27.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v8,v9,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v9.u32), 0xFF));
	// vmaddfp v0,v10,v12,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(ctx.v0.f32)));
	// vmaddfp128 v0,v127,v8,v0
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v127.f32), _mm_load_ps(ctx.v8.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v0,r0,r25
	_mm_store_si128((__m128i*)(base + ((ctx.r25.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r24,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r24.u32);
	// lwz r3,480(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 480);
	// lfs f0,252(r31)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 252);
	ctx.f0.f64 = double(temp.f32);
	// fctiwz f13,f0
	ctx.f13.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f0.f64));
	// stfd f13,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.f13.u64);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8321e6a0
	ctx.lr = 0x8226A784;
	sub_8321E6A0(ctx, base);
	// clrlwi r10,r29,8
	ctx.r10.u64 = ctx.r29.u32 & 0xFFFFFF;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f12,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r9,-31955
	ctx.r9.s64 = -2094202880;
	// frsp f10,f11
	ctx.f10.f64 = double(float(ctx.f11.f64));
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lfs f0,24080(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 24080);
	ctx.f0.f64 = double(temp.f32);
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// lis r5,-31946
	ctx.r5.s64 = -2093613056;
	// lis r4,-31926
	ctx.r4.s64 = -2092302336;
	// addi r10,r5,-4108
	ctx.r10.s64 = ctx.r5.s64 + -4108;
	// lwz r11,27648(r7)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 27648);
	// addi r9,r4,-7776
	ctx.r9.s64 = ctx.r4.s64 + -7776;
	// lvx128 v7,r0,r6
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r10,16
	ctx.r7.s64 = ctx.r10.s64 + 16;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// li r6,0
	ctx.r6.s64 = 0;
	// fmuls f9,f10,f31
	ctx.f9.f64 = double(float(ctx.f10.f64 * ctx.f31.f64));
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lvlx v6,r11,r7
	temp.u32 = ctx.r11.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v5,v6,0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), 0xFF));
	// fmadds f8,f9,f0,f1
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f0.f64 + ctx.f1.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// lvlx v4,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v3,v4,0
	_mm_store_si128((__m128i*)ctx.v3.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v4.u32), 0xFF));
	// vmulfp128 v2,v7,v3
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v2.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v3.f32)));
	// vmulfp128 v1,v2,v5
	_mm_store_ps(ctx.v1.f32, _mm_mul_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(ctx.v5.f32)));
	// stvx128 v1,r0,r28
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r8,480(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 480);
	// lwz r10,8(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// lbz r8,126(r8)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + 126);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// subf r7,r9,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r9.s64;
	// srawi r9,r7,2
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 2;
	// srawi r10,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 3;
	// extsb r11,r8
	ctx.r11.s64 = ctx.r8.s8;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// subfic r8,r10,0
	ctx.xer.ca = ctx.r10.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r10.s64;
	// cntlzw r7,r9
	ctx.r7.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// subfe r11,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r8.u64 + ctx.r8.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// rlwinm r10,r7,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// clrlwi r7,r11,31
	ctx.r7.u64 = ctx.r11.u32 & 0x1;
	// xori r8,r10,1
	ctx.r8.u64 = ctx.r10.u64 ^ 1;
	// bl 0x83229378
	ctx.lr = 0x8226A854;
	sub_83229378(ctx, base);
	// lwz r9,480(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 480);
	// lbz r8,224(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 224);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x8226a9dc
	if (ctx.cr6.eq) goto loc_8226A9DC;
	// lwz r11,448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 448);
	// addi r25,r31,448
	ctx.r25.s64 = ctx.r31.s64 + 448;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226a9dc
	if (ctx.cr6.eq) goto loc_8226A9DC;
	// lwz r11,232(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 232);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226a9dc
	if (ctx.cr6.eq) goto loc_8226A9DC;
	// lwz r11,456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 456);
	// addi r30,r31,456
	ctx.r30.s64 = ctx.r31.s64 + 456;
	// addi r29,r31,460
	ctx.r29.s64 = ctx.r31.s64 + 460;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// stw r11,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r11.u32);
	// bl 0x825cb448
	ctx.lr = 0x8226A8A0;
	sub_825CB448(ctx, base);
	// lwz r11,464(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 464);
	// lwz r26,96(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x8226a8b8
	if (ctx.cr6.eq) goto loc_8226A8B8;
	// cmplw cr6,r26,r29
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r29.u32, ctx.xer);
	// beq cr6,0x8226a8bc
	if (ctx.cr6.eq) goto loc_8226A8BC;
loc_8226A8B8:
	// twi 31,r0,22
loc_8226A8BC:
	// lwz r28,100(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226a918
	if (ctx.cr6.eq) goto loc_8226A918;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// bne cr6,0x8226a8d4
	if (!ctx.cr6.eq) goto loc_8226A8D4;
	// twi 31,r0,22
loc_8226A8D4:
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226a8e4
	if (!ctx.cr6.eq) goto loc_8226A8E4;
	// twi 31,r0,22
loc_8226A8E4:
	// lwz r3,16(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 16);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226a904
	if (ctx.cr6.eq) goto loc_8226A904;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226A904;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8226A904:
	// lwz r11,4(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226a914
	if (!ctx.cr6.eq) goto loc_8226A914;
	// twi 31,r0,22
loc_8226A914:
	// stw r24,16(r28)
	PPC_STORE_U32(ctx.r28.u32 + 16, ctx.r24.u32);
loc_8226A918:
	// li r3,128
	ctx.r3.s64 = 128;
	// bl 0x8221f388
	ctx.lr = 0x8226A920;
	sub_8221F388(ctx, base);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x8226a980
	if (ctx.cr6.eq) goto loc_8226A980;
	// lwz r11,480(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 480);
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// lwz r9,232(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 232);
	// addi r3,r28,16
	ctx.r3.s64 = ctx.r28.s64 + 16;
	// addi r8,r10,6200
	ctx.r8.s64 = ctx.r10.s64 + 6200;
	// lfs f7,0(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,408(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 408);
	ctx.f6.f64 = double(temp.f32);
	// lbz r7,221(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 221);
	// lfs f5,216(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 216);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,212(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 212);
	ctx.f4.f64 = double(temp.f32);
	// stw r8,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r8.u32);
	// lfs f3,208(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 208);
	ctx.f3.f64 = double(temp.f32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// lfs f2,204(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 204);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,168(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 168);
	ctx.f1.f64 = double(temp.f32);
	// stb r7,87(r1)
	PPC_STORE_U8(ctx.r1.u32 + 87, ctx.r7.u8);
	// bl 0x83221d10
	ctx.lr = 0x8226A970;
	sub_83221D10(ctx, base);
	// li r6,112
	ctx.r6.s64 = 112;
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
	// stvx128 v127,r28,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r28.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x8226a984
	goto loc_8226A984;
loc_8226A980:
	// mr r31,r24
	ctx.r31.u64 = ctx.r24.u64;
loc_8226A984:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82a00620
	ctx.lr = 0x8226A990;
	sub_82A00620(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r31,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r31.u32);
	// bl 0x82a00620
	ctx.lr = 0x8226A9A4;
	sub_82A00620(ctx, base);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// addi r3,r11,16
	ctx.r3.s64 = ctx.r11.s64 + 16;
	// bl 0x83222238
	ctx.lr = 0x8226A9B4;
	sub_83222238(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82a00620
	ctx.lr = 0x8226A9C0;
	sub_82A00620(ctx, base);
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8321e160
	ctx.lr = 0x8226A9D0;
	sub_8321E160(ctx, base);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// stw r9,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r9.u32);
loc_8226A9DC:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// li r0,-96
	ctx.r0.s64 = -96;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// b 0x82ca2c28
	// ERROR 82CA2C28
	return;
}

__attribute__((alias("__imp__sub_8226A9F0"))) PPC_WEAK_FUNC(sub_8226A9F0);
PPC_FUNC_IMPL(__imp__sub_8226A9F0) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be0
	ctx.lr = 0x8226A9F8;
	sub_82CA2BE0(ctx, base);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// blt cr6,0x8226adf8
	if (ctx.cr6.lt) goto loc_8226ADF8;
	// rlwinm r11,r4,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// add r8,r4,r11
	ctx.r8.u64 = ctx.r4.u64 + ctx.r11.u64;
	// addi r5,r10,-16224
	ctx.r5.s64 = ctx.r10.s64 + -16224;
	// rlwinm r4,r8,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r5,2736
	ctx.r11.s64 = ctx.r5.s64 + 2736;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// addi r7,r10,28104
	ctx.r7.s64 = ctx.r10.s64 + 28104;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r31,r10,-32624
	ctx.r31.s64 = ctx.r10.s64 + -32624;
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// lwz r30,4(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r27,1
	ctx.r27.s64 = 1;
	// mulli r10,r10,8200
	ctx.r10.s64 = ctx.r10.s64 * 8200;
	// lwzx r8,r6,r7
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r7.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lis r29,-31943
	ctx.r29.s64 = -2093416448;
	// cmplw cr6,r30,r8
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r8.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// addi r28,r9,3056
	ctx.r28.s64 = ctx.r9.s64 + 3056;
	// beq cr6,0x8226aad8
	if (ctx.cr6.eq) goto loc_8226AAD8;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r30,r6,r9
	ctx.r30.u64 = ctx.r6.u64 & ctx.r9.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x8226aaac
	if (!ctx.cr6.eq) goto loc_8226AAAC;
	// lwz r30,8196(r10)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 | ctx.r9.u64;
	// rlwinm r6,r30,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// stwx r11,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r6,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r6.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r6,r9,1
	ctx.r6.s64 = ctx.r9.s64 + 1;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
loc_8226AAAC:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r8,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226aad8
	if (!ctx.cr6.eq) goto loc_8226AAD8;
	// lwz r10,1000(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r30,r10,1
	ctx.r30.s64 = ctx.r10.s64 + 1;
	// stw r30,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r30.u32);
	// stwx r11,r9,r28
	PPC_STORE_U32(ctx.r9.u32 + ctx.r28.u32, ctx.r11.u32);
	// b 0x8226aadc
	goto loc_8226AADC;
loc_8226AAD8:
	// lwz r30,1000(r29)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r29.u32 + 1000);
loc_8226AADC:
	// addi r11,r5,3216
	ctx.r11.s64 = ctx.r5.s64 + 3216;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r8,21(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwzx r6,r9,r7
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
	// mulli r10,r8,8200
	ctx.r10.s64 = ctx.r8.s64 * 8200;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r9,r6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226ab78
	if (ctx.cr6.eq) goto loc_8226AB78;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r26,r8,r9
	ctx.r26.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// bne cr6,0x8226ab54
	if (!ctx.cr6.eq) goto loc_8226AB54;
	// lwz r26,8196(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r8,r26,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// stwx r11,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r8,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r8.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// stw r8,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r8.u32);
loc_8226AB54:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r6,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r6.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226ab78
	if (!ctx.cr6.eq) goto loc_8226AB78;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// stw r30,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r30.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226AB78:
	// addi r11,r5,3696
	ctx.r11.s64 = ctx.r5.s64 + 3696;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r8,21(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwzx r7,r9,r7
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r7.u32);
	// mulli r10,r8,8200
	ctx.r10.s64 = ctx.r8.s64 * 8200;
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r6,r7
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226ac14
	if (ctx.cr6.eq) goto loc_8226AC14;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x8226abf0
	if (!ctx.cr6.eq) goto loc_8226ABF0;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r8,r6,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// stwx r11,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r6,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r6,r9,1
	ctx.r6.s64 = ctx.r9.s64 + 1;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
loc_8226ABF0:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226ac14
	if (!ctx.cr6.eq) goto loc_8226AC14;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// stw r30,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r30.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226AC14:
	// addi r11,r5,4656
	ctx.r11.s64 = ctx.r5.s64 + 4656;
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// addi r9,r10,28240
	ctx.r9.s64 = ctx.r10.s64 + 28240;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,21(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r26,4(r11)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mulli r10,r6,8200
	ctx.r10.s64 = ctx.r6.s64 * 8200;
	// lwzx r8,r7,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r26,r8
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r8.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226acb8
	if (ctx.cr6.eq) goto loc_8226ACB8;
	// lwz r7,8192(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r26,r6,r7
	ctx.r26.u64 = ctx.r6.u64 & ctx.r7.u64;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// bne cr6,0x8226ac94
	if (!ctx.cr6.eq) goto loc_8226AC94;
	// lwz r26,8196(r10)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 | ctx.r7.u64;
	// rlwinm r6,r26,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r7,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r7.u32);
	// stwx r11,r6,r10
	PPC_STORE_U32(ctx.r6.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r7,r7,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r6,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, ctx.r6.u32);
	// lwz r7,8196(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r6,r7,1
	ctx.r6.s64 = ctx.r7.s64 + 1;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
loc_8226AC94:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r8,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226acb8
	if (!ctx.cr6.eq) goto loc_8226ACB8;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// stw r30,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r30.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226ACB8:
	// addi r11,r5,4176
	ctx.r11.s64 = ctx.r5.s64 + 4176;
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r6,21(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwzx r7,r8,r9
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r9.u32);
	// mulli r10,r6,8200
	ctx.r10.s64 = ctx.r6.s64 * 8200;
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r9,r7
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226ad54
	if (ctx.cr6.eq) goto loc_8226AD54;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x8226ad30
	if (!ctx.cr6.eq) goto loc_8226AD30;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r8,r6,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r9,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r9.u32);
	// stwx r11,r8,r10
	PPC_STORE_U32(ctx.r8.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r6,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, ctx.r6.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r6,r9,1
	ctx.r6.s64 = ctx.r9.s64 + 1;
	// stw r6,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r6.u32);
loc_8226AD30:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226ad54
	if (!ctx.cr6.eq) goto loc_8226AD54;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// stw r30,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r30.u32);
	// stwx r11,r10,r28
	PPC_STORE_U32(ctx.r10.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226AD54:
	// addi r11,r5,2256
	ctx.r11.s64 = ctx.r5.s64 + 2256;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// lis r9,-31951
	ctx.r9.s64 = -2093940736;
	// add r11,r4,r11
	ctx.r11.u64 = ctx.r4.u64 + ctx.r11.u64;
	// rlwinm r7,r10,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r9,28252
	ctx.r8.s64 = ctx.r9.s64 + 28252;
	// lbz r6,21(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mulli r10,r6,8200
	ctx.r10.s64 = ctx.r6.s64 * 8200;
	// lwzx r7,r7,r8
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplw cr6,r5,r7
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r7.u32, ctx.xer);
	// addi r10,r10,-8200
	ctx.r10.s64 = ctx.r10.s64 + -8200;
	// beq cr6,0x8226adf8
	if (ctx.cr6.eq) goto loc_8226ADF8;
	// lwz r9,8192(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8192);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// and r6,r8,r9
	ctx.r6.u64 = ctx.r8.u64 & ctx.r9.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x8226add4
	if (!ctx.cr6.eq) goto loc_8226ADD4;
	// lwz r6,8196(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// or r5,r8,r9
	ctx.r5.u64 = ctx.r8.u64 | ctx.r9.u64;
	// rlwinm r4,r6,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r5,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r5.u32);
	// stwx r11,r4,r10
	PPC_STORE_U32(ctx.r4.u32 + ctx.r10.u32, ctx.r11.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r3,8196(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// rlwinm r9,r3,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 3) & 0xFFFFFFF8;
	// add r6,r9,r10
	ctx.r6.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r8,4(r6)
	PPC_STORE_U32(ctx.r6.u32 + 4, ctx.r8.u32);
	// lwz r9,8196(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8196);
	// addi r5,r9,1
	ctx.r5.s64 = ctx.r9.s64 + 1;
	// stw r5,8196(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8196, ctx.r5.u32);
loc_8226ADD4:
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// stw r7,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r7.u32);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226adf8
	if (!ctx.cr6.eq) goto loc_8226ADF8;
	// rlwinm r9,r30,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// stb r27,20(r11)
	PPC_STORE_U8(ctx.r11.u32 + 20, ctx.r27.u8);
	// addi r10,r30,1
	ctx.r10.s64 = ctx.r30.s64 + 1;
	// stw r10,1000(r29)
	PPC_STORE_U32(ctx.r29.u32 + 1000, ctx.r10.u32);
	// stwx r11,r9,r28
	PPC_STORE_U32(ctx.r9.u32 + ctx.r28.u32, ctx.r11.u32);
loc_8226ADF8:
	// b 0x82ca2c30
	// ERROR 82CA2C30
	return;
}

__attribute__((alias("__imp__sub_8226ADFC"))) PPC_WEAK_FUNC(sub_8226ADFC);
PPC_FUNC_IMPL(__imp__sub_8226ADFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226AE00"))) PPC_WEAK_FUNC(sub_8226AE00);
PPC_FUNC_IMPL(__imp__sub_8226AE00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lfs f0,12(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// li r11,1
	ctx.r11.s64 = 1;
	// lfs f13,16(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// bge cr6,0x8226ae18
	if (!ctx.cr6.lt) goto loc_8226AE18;
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226AE18:
	// clrlwi r3,r11,24
	ctx.r3.u64 = ctx.r11.u32 & 0xFF;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226AE20"))) PPC_WEAK_FUNC(sub_8226AE20);
PPC_FUNC_IMPL(__imp__sub_8226AE20) {
	PPC_FUNC_PROLOGUE();
	// lwz r3,52(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 52);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	return;
}

__attribute__((alias("__imp__sub_8226AE34"))) PPC_WEAK_FUNC(sub_8226AE34);
PPC_FUNC_IMPL(__imp__sub_8226AE34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226AE38"))) PPC_WEAK_FUNC(sub_8226AE38);
PPC_FUNC_IMPL(__imp__sub_8226AE38) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,10560(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 10560);
	// rlwimi r11,r4,2,29,29
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r4.u32, 2) & 0x4) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFFB);
	// stw r11,10560(r3)
	PPC_STORE_U32(ctx.r3.u32 + 10560, ctx.r11.u32);
	// ld r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 16);
	// ori r11,r11,256
	ctx.r11.u64 = ctx.r11.u64 | 256;
	// std r11,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, ctx.r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226AE54"))) PPC_WEAK_FUNC(sub_8226AE54);
PPC_FUNC_IMPL(__imp__sub_8226AE54) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226AE58"))) PPC_WEAK_FUNC(sub_8226AE58);
PPC_FUNC_IMPL(__imp__sub_8226AE58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f1,-25888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821ee7c8
	ctx.lr = 0x8226AE78;
	sub_821EE7C8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r8,r10,-28176
	ctx.r8.s64 = ctx.r10.s64 + -28176;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// lfs f0,-27468(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -27468);
	ctx.f0.f64 = double(temp.f32);
	// lvx128 v0,r0,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f0,80(r1)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v11,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226AEB8"))) PPC_WEAK_FUNC(sub_8226AEB8);
PPC_FUNC_IMPL(__imp__sub_8226AEB8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,32(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226aef4
	if (ctx.cr6.eq) goto loc_8226AEF4;
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226aee0
	if (!ctx.cr6.eq) goto loc_8226AEE0;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r10,r11,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r3,r10,1
	ctx.r3.u64 = ctx.r10.u64 ^ 1;
	// blr 
	return;
loc_8226AEE0:
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r10,r11,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r3,r10,1
	ctx.r3.u64 = ctx.r10.u64 ^ 1;
	// blr 
	return;
loc_8226AEF4:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226AEFC"))) PPC_WEAK_FUNC(sub_8226AEFC);
PPC_FUNC_IMPL(__imp__sub_8226AEFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226AF00"))) PPC_WEAK_FUNC(sub_8226AF00);
PPC_FUNC_IMPL(__imp__sub_8226AF00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lvx128 v1,r0,r31
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8256a030
	ctx.lr = 0x8226AF28;
	sub_8256A030(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226af6c
	if (ctx.cr6.eq) goto loc_8226AF6C;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lvx128 v13,r0,r31
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r9,r11,-28176
	ctx.r9.s64 = ctx.r11.s64 + -28176;
	// li r3,1
	ctx.r3.s64 = 1;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v11,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v11,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_8226AF6C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226AF84"))) PPC_WEAK_FUNC(sub_8226AF84);
PPC_FUNC_IMPL(__imp__sub_8226AF84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226AF88"))) PPC_WEAK_FUNC(sub_8226AF88);
PPC_FUNC_IMPL(__imp__sub_8226AF88) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// vspltisw v0,0
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_set1_epi32(int(0x0)));
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// lwz r11,172(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 172);
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x8226afdc
	if (ctx.cr6.eq) goto loc_8226AFDC;
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// beq cr6,0x8226afdc
	if (ctx.cr6.eq) goto loc_8226AFDC;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x8226afdc
	if (ctx.cr6.eq) goto loc_8226AFDC;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// beq cr6,0x8226afdc
	if (ctx.cr6.eq) goto loc_8226AFDC;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// bne cr6,0x8226afe0
	if (!ctx.cr6.eq) goto loc_8226AFE0;
loc_8226AFDC:
	// li r11,1
	ctx.r11.s64 = 1;
loc_8226AFE0:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226affc
	if (ctx.cr6.eq) goto loc_8226AFFC;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x821e7c38
	ctx.lr = 0x8226AFF4;
	sub_821E7C38(ctx, base);
	// addi r11,r1,80
	ctx.r11.s64 = ctx.r1.s64 + 80;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_8226AFFC:
	// stvx128 v0,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226B018"))) PPC_WEAK_FUNC(sub_8226B018);
PPC_FUNC_IMPL(__imp__sub_8226B018) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,10548(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 10548);
	// rlwimi r11,r4,2,29,29
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r4.u32, 2) & 0x4) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFFB);
	// stw r11,10548(r3)
	PPC_STORE_U32(ctx.r3.u32 + 10548, ctx.r11.u32);
	// ld r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 16);
	// ori r11,r11,2048
	ctx.r11.u64 = ctx.r11.u64 | 2048;
	// std r11,16(r3)
	PPC_STORE_U64(ctx.r3.u32 + 16, ctx.r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226B034"))) PPC_WEAK_FUNC(sub_8226B034);
PPC_FUNC_IMPL(__imp__sub_8226B034) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226B038"))) PPC_WEAK_FUNC(sub_8226B038);
PPC_FUNC_IMPL(__imp__sub_8226B038) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r11,-20516
	ctx.r3.s64 = ctx.r11.s64 + -20516;
	// bl 0x821fbfc8
	ctx.lr = 0x8226B05C;
	sub_821FBFC8(ctx, base);
	// lis r10,-31946
	ctx.r10.s64 = -2093613056;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// lwz r3,412(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 412);
	// bl 0x82264550
	ctx.lr = 0x8226B06C;
	sub_82264550(ctx, base);
	// lis r9,-31926
	ctx.r9.s64 = -2092302336;
	// addi r8,r9,23928
	ctx.r8.s64 = ctx.r9.s64 + 23928;
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226b084
	if (!ctx.cr6.eq) goto loc_8226B084;
	// bl 0x821fa028
	ctx.lr = 0x8226B084;
	sub_821FA028(ctx, base);
loc_8226B084:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226B098"))) PPC_WEAK_FUNC(sub_8226B098);
PPC_FUNC_IMPL(__imp__sub_8226B098) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x8226B0A0;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	ctx.r11.s64 = -2092367872;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// lwz r11,26912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r9,88(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r3,r11,104
	ctx.r3.s64 = ctx.r11.s64 + 104;
	// lwz r11,108(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 108);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b15c
	if (ctx.cr6.eq) goto loc_8226B15C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226b158
	if (ctx.cr6.eq) goto loc_8226B158;
	// rotlwi r31,r10,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x8226b100
	if (ctx.cr6.eq) goto loc_8226B100;
	// lbz r11,144(r31)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// li r11,1
	ctx.r11.s64 = 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226b104
	if (!ctx.cr6.eq) goto loc_8226B104;
loc_8226B100:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B104:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b364
	if (ctx.cr6.eq) goto loc_8226B364;
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 48);
	// rlwinm r9,r10,6,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226b228
	if (ctx.cr6.eq) goto loc_8226B228;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226b164
	if (ctx.cr6.eq) goto loc_8226B164;
	// lbz r10,122(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 122);
	// lwz r11,72(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b22c
	goto loc_8226B22C;
loc_8226B158:
	// bl 0x821940c8
	ctx.lr = 0x8226B15C;
	sub_821940C8(ctx, base);
loc_8226B15C:
	// mr r31,r30
	ctx.r31.u64 = ctx.r30.u64;
	// b 0x8226b100
	goto loc_8226B100;
loc_8226B164:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226b1d0
	if (!ctx.cr0.gt) goto loc_8226B1D0;
loc_8226B180:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,122
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 122, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226b1a0
	if (ctx.cr6.lt) goto loc_8226B1A0;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
loc_8226B1A0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226b1bc
	if (ctx.cr6.eq) goto loc_8226B1BC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226b1c4
	goto loc_8226B1C4;
loc_8226B1BC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226B1C4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226b180
	if (ctx.cr6.gt) goto loc_8226B180;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226B1D0:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226b214
	if (ctx.cr6.eq) goto loc_8226B214;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,122
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 122, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226b1ec
	if (ctx.cr6.gt) goto loc_8226B1EC;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B1EC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226b214
	if (!ctx.cr6.eq) goto loc_8226B214;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b22c
	goto loc_8226B22C;
loc_8226B214:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b22c
	goto loc_8226B22C;
loc_8226B228:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B22C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b364
	if (ctx.cr6.eq) goto loc_8226B364;
	// lwz r11,48(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// rlwinm r10,r11,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226b338
	if (ctx.cr6.eq) goto loc_8226B338;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b274
	if (ctx.cr6.eq) goto loc_8226B274;
	// lbz r10,122(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 122);
	// lwz r11,72(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b33c
	goto loc_8226B33C;
loc_8226B274:
	// lwz r10,72(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// lwz r6,76(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226b2e0
	if (!ctx.cr0.gt) goto loc_8226B2E0;
loc_8226B290:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,122
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 122, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226b2b0
	if (ctx.cr6.lt) goto loc_8226B2B0;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
loc_8226B2B0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226b2cc
	if (ctx.cr6.eq) goto loc_8226B2CC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226b2d4
	goto loc_8226B2D4;
loc_8226B2CC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226B2D4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226b290
	if (ctx.cr6.gt) goto loc_8226B290;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226B2E0:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226b324
	if (ctx.cr6.eq) goto loc_8226B324;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,122
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 122, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226b2fc
	if (ctx.cr6.gt) goto loc_8226B2FC;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B2FC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226b324
	if (!ctx.cr6.eq) goto loc_8226B324;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b33c
	goto loc_8226B33C;
loc_8226B324:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b33c
	goto loc_8226B33C;
loc_8226B338:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B33C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b364
	if (ctx.cr6.eq) goto loc_8226B364;
	// bl 0x821c2c18
	ctx.lr = 0x8226B34C;
	sub_821C2C18(ctx, base);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x8226b580
	if (ctx.cr6.eq) goto loc_8226B580;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// bl 0x821c2c18
	ctx.lr = 0x8226B35C;
	sub_821C2C18(ctx, base);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// beq cr6,0x8226b580
	if (ctx.cr6.eq) goto loc_8226B580;
loc_8226B364:
	// lwz r11,4(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 36);
	// rlwinm r9,r10,20,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226b46c
	if (ctx.cr6.eq) goto loc_8226B46C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226b3a8
	if (ctx.cr6.eq) goto loc_8226B3A8;
	// lbz r9,12(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 12);
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r11,r9,3
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b470
	goto loc_8226B470;
loc_8226B3A8:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226b414
	if (!ctx.cr0.gt) goto loc_8226B414;
loc_8226B3C4:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,12
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 12, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226b3e4
	if (ctx.cr6.lt) goto loc_8226B3E4;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
loc_8226B3E4:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226b400
	if (ctx.cr6.eq) goto loc_8226B400;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226b408
	goto loc_8226B408;
loc_8226B400:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226B408:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226b3c4
	if (ctx.cr6.gt) goto loc_8226B3C4;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226B414:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226b458
	if (ctx.cr6.eq) goto loc_8226B458;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,12
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 12, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226b430
	if (ctx.cr6.gt) goto loc_8226B430;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B430:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226b458
	if (!ctx.cr6.eq) goto loc_8226B458;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b470
	goto loc_8226B470;
loc_8226B458:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b470
	goto loc_8226B470;
loc_8226B46C:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B470:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b5a4
	if (ctx.cr6.eq) goto loc_8226B5A4;
	// lwz r5,512(r29)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r29.u32 + 512);
	// cmpwi cr6,r5,3
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 3, ctx.xer);
	// beq cr6,0x8226b580
	if (ctx.cr6.eq) goto loc_8226B580;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// rlwinm r9,r10,20,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226b568
	if (ctx.cr6.eq) goto loc_8226B568;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// lwz r10,72(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b4c0
	if (ctx.cr6.eq) goto loc_8226B4C0;
	// lbz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 12);
	// rotlwi r11,r11,3
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 3);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// b 0x8226b568
	goto loc_8226B568;
loc_8226B4C0:
	// lwz r6,76(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 76);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226b528
	if (!ctx.cr0.gt) goto loc_8226B528;
loc_8226B4D8:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,12
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 12, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226b4f8
	if (ctx.cr6.lt) goto loc_8226B4F8;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
loc_8226B4F8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226b514
	if (ctx.cr6.eq) goto loc_8226B514;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226b51c
	goto loc_8226B51C;
loc_8226B514:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226B51C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226b4d8
	if (ctx.cr6.gt) goto loc_8226B4D8;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226B528:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226b560
	if (ctx.cr6.eq) goto loc_8226B560;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,12
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 12, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226b544
	if (ctx.cr6.gt) goto loc_8226B544;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_8226B544:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226b560
	if (!ctx.cr6.eq) goto loc_8226B560;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// b 0x8226b564
	goto loc_8226B564;
loc_8226B560:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_8226B564:
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_8226B568:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bne cr6,0x8226b58c
	if (!ctx.cr6.eq) goto loc_8226B58C;
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// bne cr6,0x8226b5a4
	if (!ctx.cr6.eq) goto loc_8226B5A4;
loc_8226B580:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_8226B58C:
	// cmpwi cr6,r5,1
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 1, ctx.xer);
	// bne cr6,0x8226b5a4
	if (!ctx.cr6.eq) goto loc_8226B5A4;
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// bne cr6,0x8226b580
	if (!ctx.cr6.eq) goto loc_8226B580;
loc_8226B5A4:
	// lbz r11,777(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 777);
	// cntlzw r10,r11
	ctx.r10.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r3,r9,1
	ctx.r3.u64 = ctx.r9.u64 ^ 1;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8226B5BC"))) PPC_WEAK_FUNC(sub_8226B5BC);
PPC_FUNC_IMPL(__imp__sub_8226B5BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226B5C0"))) PPC_WEAK_FUNC(sub_8226B5C0);
PPC_FUNC_IMPL(__imp__sub_8226B5C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x8226B5C8;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// addi r3,r29,4
	ctx.r3.s64 = ctx.r29.s64 + 4;
	// lwz r11,8(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b5fc
	if (ctx.cr6.eq) goto loc_8226B5FC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226b5f8
	if (ctx.cr6.eq) goto loc_8226B5F8;
	// rotlwi r11,r10,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// b 0x8226b600
	goto loc_8226B600;
loc_8226B5F8:
	// bl 0x821940c8
	ctx.lr = 0x8226B5FC;
	sub_821940C8(ctx, base);
loc_8226B5FC:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226B600:
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 40);
	// rlwinm r9,r10,20,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 20) & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226b704
	if (ctx.cr6.eq) goto loc_8226B704;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 140);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226b63c
	if (ctx.cr6.eq) goto loc_8226B63C;
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// rotlwi r11,r9,3
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r11,r10
	ctx.r8.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b708
	goto loc_8226B708;
loc_8226B63C:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 72);
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// subf r8,r10,r6
	ctx.r8.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226b6ac
	if (!ctx.cr0.gt) goto loc_8226B6AC;
loc_8226B65C:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,44
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 44, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226b67c
	if (ctx.cr6.lt) goto loc_8226B67C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_8226B67C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226b698
	if (ctx.cr6.eq) goto loc_8226B698;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226b6a0
	goto loc_8226B6A0;
loc_8226B698:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226B6A0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226b65c
	if (ctx.cr6.gt) goto loc_8226B65C;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226B6AC:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226b6f0
	if (ctx.cr6.eq) goto loc_8226B6F0;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,44
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 44, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226b6c8
	if (ctx.cr6.gt) goto loc_8226B6C8;
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226B6C8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226b6f0
	if (!ctx.cr6.eq) goto loc_8226B6F0;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b708
	goto loc_8226B708;
loc_8226B6F0:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226b708
	goto loc_8226B708;
loc_8226B704:
	// li r11,0
	ctx.r11.s64 = 0;
loc_8226B708:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b734
	if (ctx.cr6.eq) goto loc_8226B734;
	// lbz r11,80(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 80);
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 100);
	// subfic r9,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r9.s64 = 0 - ctx.r11.s64;
	// subfe r8,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r9.u64 + ctx.r9.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// clrlwi r11,r8,30
	ctx.r11.u64 = ctx.r8.u32 & 0x3;
	// addi r7,r11,12
	ctx.r7.s64 = ctx.r11.s64 + 12;
	// or r6,r7,r10
	ctx.r6.u64 = ctx.r7.u64 | ctx.r10.u64;
	// stw r6,100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 100, ctx.r6.u32);
loc_8226B734:
	// lbz r11,72(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 72);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b7e0
	if (ctx.cr6.eq) goto loc_8226B7E0;
	// lwz r11,28(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 28);
	// li r10,12
	ctx.r10.s64 = 12;
	// lwz r9,24(r29)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r29.u32 + 24);
	// subf r8,r9,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r9.s64;
	// divw. r11,r8,r10
	ctx.r11.s32 = ctx.r8.s32 / ctx.r10.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x8226b7c4
	if (ctx.cr0.eq) goto loc_8226B7C4;
	// li r31,0
	ctx.r31.s64 = 0;
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
loc_8226B760:
	// lwz r11,24(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 24);
	// add r11,r31,r11
	ctx.r11.u64 = ctx.r31.u64 + ctx.r11.u64;
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// beq cr6,0x8226b7a0
	if (ctx.cr6.eq) goto loc_8226B7A0;
loc_8226B784:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r10
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r10.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stwcx. r11,0,r10
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r10.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226b784
	if (!ctx.cr0.eq) goto loc_8226B784;
loc_8226B7A0:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226B7B0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x829ff648
	ctx.lr = 0x8226B7B8;
	sub_829FF648(ctx, base);
	// addic. r30,r30,-1
	ctx.xer.ca = ctx.r30.u32 > 0;
	ctx.r30.s64 = ctx.r30.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// addi r31,r31,12
	ctx.r31.s64 = ctx.r31.s64 + 12;
	// bne 0x8226b760
	if (!ctx.cr0.eq) goto loc_8226B760;
loc_8226B7C4:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 64);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226B7D8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
loc_8226B7E0:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x826df670
	ctx.lr = 0x8226B7E8;
	sub_826DF670(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226b7fc
	if (ctx.cr6.eq) goto loc_8226B7FC;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x821b6348
	ctx.lr = 0x8226B7FC;
	sub_821B6348(ctx, base);
loc_8226B7FC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8226B804"))) PPC_WEAK_FUNC(sub_8226B804);
PPC_FUNC_IMPL(__imp__sub_8226B804) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226B808"))) PPC_WEAK_FUNC(sub_8226B808);
PPC_FUNC_IMPL(__imp__sub_8226B808) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r4,48
	ctx.r11.s64 = ctx.r4.s64 + 48;
	// cntlzw r10,r5
	ctx.r10.u64 = ctx.r5.u32 == 0 ? 32 : __builtin_clz(ctx.r5.u32);
	// mulli r11,r11,24
	ctx.r11.s64 = ctx.r11.s64 * 24;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// addi r9,r4,32
	ctx.r9.s64 = ctx.r4.s64 + 32;
	// li r8,1
	ctx.r8.s64 = 1;
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// lwz r7,20(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// rldicr r8,r8,63,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 63) & 0xFFFFFFFFFFFFFFFF;
	// rlwinm r7,r7,0,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFC;
	// srd r9,r8,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r8.u64 >> (ctx.r9.u8 & 0x7F));
	// or r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 | ctx.r7.u64;
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// ld r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 24);
	// or r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 | ctx.r11.u64;
	// std r11,24(r3)
	PPC_STORE_U64(ctx.r3.u32 + 24, ctx.r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226B854"))) PPC_WEAK_FUNC(sub_8226B854);
PPC_FUNC_IMPL(__imp__sub_8226B854) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226B858"))) PPC_WEAK_FUNC(sub_8226B858);
PPC_FUNC_IMPL(__imp__sub_8226B858) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x8226B860;
	sub_82CA2BEC(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	ctx.r11.s64 = 0;
	// lis r31,-31927
	ctx.r31.s64 = -2092367872;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// addi r4,r30,496
	ctx.r4.s64 = ctx.r30.s64 + 496;
	// lwz r11,26912(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 26912);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r3,28(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// bl 0x8229c288
	ctx.lr = 0x8226B89C;
	sub_8229C288(ctx, base);
	// lwz r10,26912(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 26912);
	// lwz r8,12(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// lwz r7,88(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 88);
	// lwz r6,4(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 4);
	// lwz r29,0(r6)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// ble cr6,0x8226b8c4
	if (!ctx.cr6.gt) goto loc_8226B8C4;
	// twi 31,r0,22
loc_8226B8C4:
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
loc_8226B8C8:
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x8226b8d4
	if (!ctx.cr6.gt) goto loc_8226B8D4;
	// twi 31,r0,22
loc_8226B8D4:
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226b910
	if (ctx.cr6.eq) goto loc_8226B910;
	// blt cr6,0x8226b8e4
	if (ctx.cr6.lt) goto loc_8226B8E4;
	// twi 31,r0,22
loc_8226B8E4:
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x82759570
	ctx.lr = 0x8226B8F4;
	sub_82759570(ctx, base);
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x8226b904
	if (ctx.cr6.lt) goto loc_8226B904;
	// twi 31,r0,22
loc_8226B904:
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// b 0x8226b8c8
	goto loc_8226B8C8;
loc_8226B910:
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226b91c
	if (ctx.cr6.eq) goto loc_8226B91C;
	// bl 0x8221be68
	ctx.lr = 0x8226B91C;
	sub_8221BE68(ctx, base);
loc_8226B91C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8226B924"))) PPC_WEAK_FUNC(sub_8226B924);
PPC_FUNC_IMPL(__imp__sub_8226B924) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226B928"))) PPC_WEAK_FUNC(sub_8226B928);
PPC_FUNC_IMPL(__imp__sub_8226B928) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bec
	ctx.lr = 0x8226B930;
	sub_82CA2BEC(ctx, base);
	// stfd f30,-48(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f30.u64);
	// stfd f31,-40(r1)
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f31.u64);
	// li r12,-64
	ctx.r12.s64 = -64;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r9,r11,-27456
	ctx.r9.s64 = ctx.r11.s64 + -27456;
	// addi r10,r1,84
	ctx.r10.s64 = ctx.r1.s64 + 84;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// lfs f30,-27456(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27456);
	ctx.f30.f64 = double(temp.f32);
	// addi r7,r1,84
	ctx.r7.s64 = ctx.r1.s64 + 84;
	// stfs f30,80(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// lvlx v13,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lfs f31,-12(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -12);
	ctx.f31.f64 = double(temp.f32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r4,124(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f31,84(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// stfs f31,80(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r5,0(r4)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lvlx v11,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v11,v13,3,2
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// lwz r11,68(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 68);
	// vor128 v127,v11,v11
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_load_si128((__m128i*)ctx.v11.u8));
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x8226B9B4;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// ld r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r10.u32 + 0);
	// ld r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r10.u32 + 8);
	// bl 0x821f5a28
	ctx.lr = 0x8226B9C4;
	sub_821F5A28(ctx, base);
	// vpermwi128 v10,v127,99
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x9C));
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// vpermwi128 v9,v127,135
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v127.u32), 0x78));
	// lwz r4,124(r30)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	// vpermwi128 v8,v1,135
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), 0x78));
	// addi r29,r9,-28400
	ctx.r29.s64 = ctx.r9.s64 + -28400;
	// vpermwi128 v7,v1,99
	_mm_store_si128((__m128i*)ctx.v7.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), 0x9C));
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// vmulfp128 v6,v8,v10
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v6.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v10.f32)));
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// vmulfp128 v5,v7,v9
	_mm_store_ps(ctx.v5.f32, _mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v9.f32)));
	// lvx128 v0,r0,r29
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r7,68(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 68);
	// vsubfp v4,v5,v6
	_mm_store_ps(ctx.v4.f32, _mm_sub_ps(_mm_load_ps(ctx.v5.f32), _mm_load_ps(ctx.v6.f32)));
	// vand v3,v4,v0
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v3,r0,r31
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mtctr r7
	ctx.ctr.u64 = ctx.r7.u64;
	// bctrl 
	ctx.lr = 0x8226BA0C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// ld r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r6.u32 + 0);
	// ld r4,8(r6)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r6.u32 + 8);
	// bl 0x821f5a28
	ctx.lr = 0x8226BA1C;
	sub_821F5A28(ctx, base);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// stfs f31,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// addi r10,r1,88
	ctx.r10.s64 = ctx.r1.s64 + 88;
	// stfs f31,88(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// stfs f31,92(r1)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stfs f30,80(r1)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lwz r11,124(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 124);
	// lvx128 v0,r0,r29
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v2,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r8,16
	ctx.r8.s64 = 16;
	// lvlx v30,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v30.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// li r7,32
	ctx.r7.s64 = 32;
	// lvlx v29,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v29.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vand v1,v1,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvlx v31,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// vrlimi128 v29,v30,4,3
	_mm_store_ps(ctx.v29.f32, _mm_blend_ps(_mm_load_ps(ctx.v29.f32), _mm_permute_ps(_mm_load_ps(ctx.v30.f32), 57), 4));
	// lwz r5,64(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 64);
	// vrlimi128 v31,v2,4,3
	_mm_store_ps(ctx.v31.f32, _mm_blend_ps(_mm_load_ps(ctx.v31.f32), _mm_permute_ps(_mm_load_ps(ctx.v2.f32), 57), 4));
	// stvx128 v1,r31,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v29,v31,3,2
	_mm_store_ps(ctx.v29.f32, _mm_blend_ps(_mm_load_ps(ctx.v29.f32), _mm_permute_ps(_mm_load_ps(ctx.v31.f32), 78), 3));
	// vand v28,v29,v0
	_mm_store_si128((__m128i*)ctx.v28.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v29.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v28,r31,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v28.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mtctr r5
	ctx.ctr.u64 = ctx.r5.u64;
	// bctrl 
	ctx.lr = 0x8226BA90;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// stfs f30,92(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r11,r4,-28160
	ctx.r11.s64 = ctx.r4.s64 + -28160;
	// addi r10,r1,92
	ctx.r10.s64 = ctx.r1.s64 + 92;
	// li r9,48
	ctx.r9.s64 = 48;
	// lvx128 v26,r0,r3
	_mm_store_si128((__m128i*)ctx.v26.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v27,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v27.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v25,v26,v27,v0
	_mm_store_si128((__m128i*)ctx.v25.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v26.u8), _mm_load_si128((__m128i*)ctx.v27.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v25,r31,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r31.u32 + ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v25.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// li r0,-64
	ctx.r0.s64 = -64;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)ctx.v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + ctx.r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f30,-48(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// lfd f31,-40(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x82ca2c3c
	// ERROR 82CA2C3C
	return;
}

__attribute__((alias("__imp__sub_8226BAD8"))) PPC_WEAK_FUNC(sub_8226BAD8);
PPC_FUNC_IMPL(__imp__sub_8226BAD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bd4
	ctx.lr = 0x8226BAE0;
	sub_82CA2BD4(ctx, base);
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x82ca7504
	ctx.lr = 0x8226BAE8;
	sub_82CA7504(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// lwz r11,8(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226be18
	if (ctx.cr6.eq) goto loc_8226BE18;
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4);
	// lwz r30,0(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226bb14
	if (!ctx.cr6.eq) goto loc_8226BB14;
	// twi 31,r0,22
loc_8226BB14:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// li r28,0
	ctx.r28.s64 = 0;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// beq cr6,0x8226bba0
	if (ctx.cr6.eq) goto loc_8226BBA0;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226bb70
	if (ctx.cr6.eq) goto loc_8226BB70;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_8226BB38:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r9
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stwcx. r11,0,r9
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bb38
	if (!ctx.cr0.eq) goto loc_8226BB38;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8226bb6c
	if (!ctx.cr6.eq) goto loc_8226BB6C;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BB6C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8226BB6C:
	// stw r28,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r28.u32);
loc_8226BB70:
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r11.u32);
	// beq cr6,0x8226bba0
	if (ctx.cr6.eq) goto loc_8226BBA0;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
loc_8226BB84:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r10,0,r11
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r11.u32);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r11.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bb84
	if (!ctx.cr0.eq) goto loc_8226BB84;
loc_8226BBA0:
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226bbb0
	if (!ctx.cr6.eq) goto loc_8226BBB0;
	// twi 31,r0,22
loc_8226BBB0:
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f0,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// addi r25,r11,-27456
	ctx.r25.s64 = ctx.r11.s64 + -27456;
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// li r27,1
	ctx.r27.s64 = 1;
	// lfs f27,2952(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2952);
	ctx.f27.f64 = double(temp.f32);
	// addi r24,r11,368
	ctx.r24.s64 = ctx.r11.s64 + 368;
	// lfs f31,-12(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + -12);
	ctx.f31.f64 = double(temp.f32);
	// addi r26,r10,-15064
	ctx.r26.s64 = ctx.r10.s64 + -15064;
	// lfs f29,0(r25)
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	ctx.f29.f64 = double(temp.f32);
loc_8226BBE0:
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226bbf0
	if (!ctx.cr6.eq) goto loc_8226BBF0;
	// twi 31,r0,22
loc_8226BBF0:
	// lwz r30,0(r30)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x8226be18
	if (ctx.cr6.eq) goto loc_8226BE18;
	// lfs f13,12(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + 12);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// fadds f28,f13,f0
	ctx.f28.f64 = double(float(ctx.f13.f64 + ctx.f0.f64));
	// fcmpu cr6,f12,f31
	ctx.cr6.compare(ctx.f12.f64, ctx.f31.f64);
	// mfcr r10
	ctx.r10.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r10.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r10.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r10.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r10.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r10.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r10.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r10.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r10.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r10.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r10.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r10.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r10.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r10.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r10.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r10.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r10.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r10.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r10.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r10.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r10.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r10.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r10.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r10.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r10.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r10.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r10.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r10.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r10.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r10.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r10.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r10.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r9,r10,27,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x4;
	// rlwinm r8,r10,30,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x4;
	// or r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 | ctx.r8.u64;
	// lfsx f11,r24,r7
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r7.u32);
	ctx.f11.f64 = double(temp.f32);
	// fsel f10,f11,f12,f31
	ctx.f10.f64 = ctx.f11.f64 >= 0.0 ? ctx.f12.f64 : ctx.f31.f64;
	// fsubs f9,f10,f29
	ctx.f9.f64 = double(float(ctx.f10.f64 - ctx.f29.f64));
	// fcmpu cr6,f9,f31
	ctx.cr6.compare(ctx.f9.f64, ctx.f31.f64);
	// mfcr r6
	ctx.r6.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r6.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r6.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r6.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r6.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r6.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r6.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r6.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r6.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r6.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r6.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r6.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r6.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r6.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r6.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r6.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r6.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r6.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r6.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r6.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r6.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r6.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r6.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r6.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r6.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r6.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r6.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r6.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r6.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r6.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r6.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r6.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r5,r6,27,29,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x4;
	// rlwinm r4,r6,30,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 30) & 0x4;
	// or r3,r5,r4
	ctx.r3.u64 = ctx.r5.u64 | ctx.r4.u64;
	// lfsx f8,r24,r3
	temp.u32 = PPC_LOAD_U32(ctx.r24.u32 + ctx.r3.u32);
	ctx.f8.f64 = double(temp.f32);
	// fsel f30,f8,f29,f10
	ctx.f30.f64 = ctx.f8.f64 >= 0.0 ? ctx.f29.f64 : ctx.f10.f64;
	// fcmpu cr6,f30,f27
	ctx.cr6.compare(ctx.f30.f64, ctx.f27.f64);
	// blt cr6,0x8226bce4
	if (ctx.cr6.lt) goto loc_8226BCE4;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226bc58
	if (!ctx.cr6.eq) goto loc_8226BC58;
	// twi 31,r0,22
loc_8226BC58:
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x8226be10
	if (ctx.cr6.eq) goto loc_8226BE10;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226bcac
	if (ctx.cr6.eq) goto loc_8226BCAC;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_8226BC74:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r9
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stwcx. r11,0,r9
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bc74
	if (!ctx.cr0.eq) goto loc_8226BC74;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8226bca8
	if (!ctx.cr6.eq) goto loc_8226BCA8;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BCA8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8226BCA8:
	// stw r28,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r28.u32);
loc_8226BCAC:
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r11.u32);
	// beq cr6,0x8226be10
	if (ctx.cr6.eq) goto loc_8226BE10;
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
loc_8226BCC0:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r10,0,r11
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r11.u32);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r11.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bcc0
	if (!ctx.cr0.eq) goto loc_8226BCC0;
	// fmr f0,f28
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f28.f64;
	// b 0x8226bbe0
	goto loc_8226BBE0;
loc_8226BCE4:
	// lfs f0,20044(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r25.u32 + 20044);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f30,f0
	ctx.cr6.compare(ctx.f30.f64, ctx.f0.f64);
	// ble cr6,0x8226be10
	if (!ctx.cr6.gt) goto loc_8226BE10;
	// li r3,36
	ctx.r3.s64 = 36;
	// bl 0x8221f388
	ctx.lr = 0x8226BCF8;
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226bd2c
	if (ctx.cr6.eq) goto loc_8226BD2C;
	// stw r27,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r27.u32);
	// stfs f29,16(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f29.f64);
	PPC_STORE_U32(ctx.r3.u32 + 16, temp.u32);
	// stw r28,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r28.u32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// stw r28,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r28.u32);
	// stw r27,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r27.u32);
	// stw r26,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r26.u32);
	// stw r28,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r28.u32);
	// stw r28,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r28.u32);
	// stfs f31,32(r3)
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
	// b 0x8226bd30
	goto loc_8226BD30;
loc_8226BD2C:
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
loc_8226BD30:
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8228ea30
	ctx.lr = 0x8226BD3C;
	sub_8228EA30(ctx, base);
	// lwz r11,4(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 4);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x8226bd4c
	if (!ctx.cr6.eq) goto loc_8226BD4C;
	// twi 31,r0,22
loc_8226BD4C:
	// addi r4,r30,8
	ctx.r4.s64 = ctx.r30.s64 + 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821b8fe0
	ctx.lr = 0x8226BD58;
	sub_821B8FE0(ctx, base);
	// stfs f30,32(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r31.u32 + 32, temp.u32);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// cmplw cr6,r3,r31
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r31.u32, ctx.xer);
	// beq cr6,0x8226bdd0
	if (ctx.cr6.eq) goto loc_8226BDD0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226bdac
	if (ctx.cr6.eq) goto loc_8226BDAC;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_8226BD74:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r9
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stwcx. r11,0,r9
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bd74
	if (!ctx.cr0.eq) goto loc_8226BD74;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8226bda8
	if (!ctx.cr6.eq) goto loc_8226BDA8;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BDA8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8226BDA8:
	// stw r28,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r28.u32);
loc_8226BDAC:
	// stw r31,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r31.u32);
	// addi r11,r31,4
	ctx.r11.s64 = ctx.r31.s64 + 4;
loc_8226BDB4:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r10,0,r11
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r11.u32);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r11.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bdb4
	if (!ctx.cr0.eq) goto loc_8226BDB4;
loc_8226BDD0:
	// addi r11,r31,4
	ctx.r11.s64 = ctx.r31.s64 + 4;
loc_8226BDD4:
	// mfmsr r9
	ctx.r9.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r10,0,r11
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r11.u32);
	ctx.r10.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r11
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r11.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r10.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r9,1
	ctx.msr = (ctx.r9.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bdd4
	if (!ctx.cr0.eq) goto loc_8226BDD4;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x8226be10
	if (!ctx.cr6.eq) goto loc_8226BE10;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BE10;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8226BE10:
	// fmr f0,f28
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = ctx.f28.f64;
	// b 0x8226bbe0
	goto loc_8226BBE0;
loc_8226BE18:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// addi r12,r1,-80
	ctx.r12.s64 = ctx.r1.s64 + -80;
	// bl 0x82ca7550
	ctx.lr = 0x8226BE24;
	sub_82CA7550(ctx, base);
	// b 0x82ca2c24
	// ERROR 82CA2C24
	return;
}

__attribute__((alias("__imp__sub_8226BE28"))) PPC_WEAK_FUNC(sub_8226BE28);
PPC_FUNC_IMPL(__imp__sub_8226BE28) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x8226BE30;
	sub_82CA2BE8(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BE50;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x821dabf0
	ctx.lr = 0x8226BE5C;
	sub_821DABF0(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x8226bf64
	if (!ctx.cr6.eq) goto loc_8226BF64;
	// lwz r31,4(r29)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r31.u32);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// beq cr6,0x8226bea0
	if (ctx.cr6.eq) goto loc_8226BEA0;
loc_8226BE84:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r31
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r31.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stwcx. r11,0,r31
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r31.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226be84
	if (!ctx.cr0.eq) goto loc_8226BE84;
loc_8226BEA0:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BEB0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r3,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r3.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r31.u32);
	// beq cr6,0x8226bee0
	if (ctx.cr6.eq) goto loc_8226BEE0;
loc_8226BEC4:
	// mfmsr r10
	ctx.r10.u64 = ctx.msr;
	// mtmsrd r13,1
	ctx.msr = (ctx.r13.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// lwarx r11,0,r31
	ctx.reserved.u32 = *(uint32_t*)(base + ctx.r31.u32);
	ctx.r11.u64 = __builtin_bswap32(ctx.reserved.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stwcx. r11,0,r31
	ctx.cr0.lt = 0;
	ctx.cr0.gt = 0;
	ctx.cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r31.u32), ctx.reserved.s32, __builtin_bswap32(ctx.r11.s32));
	ctx.cr0.so = ctx.xer.so;
	// mtmsrd r10,1
	ctx.msr = (ctx.r10.u32 & 0x8020) | (ctx.msr & ~0x8020);
	// bne 0x8226bec4
	if (!ctx.cr0.eq) goto loc_8226BEC4;
loc_8226BEE0:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x829ff648
	ctx.lr = 0x8226BEE8;
	sub_829FF648(ctx, base);
	// addi r31,r28,20
	ctx.r31.s64 = ctx.r28.s64 + 20;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x826016b0
	ctx.lr = 0x8226BEF8;
	sub_826016B0(ctx, base);
	// li r11,1
	ctx.r11.s64 = 1;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// stb r11,37(r28)
	PPC_STORE_U8(ctx.r28.u32 + 37, ctx.r11.u8);
	// bl 0x829ff648
	ctx.lr = 0x8226BF08;
	sub_829FF648(ctx, base);
	// lbz r10,37(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + 37);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226bf50
	if (ctx.cr6.eq) goto loc_8226BF50;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// li r9,12
	ctx.r9.s64 = 12;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// li r30,0
	ctx.r30.s64 = 0;
	// lbz r6,16(r31)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r31.u32 + 16);
	// subf r8,r10,r11
	ctx.r8.s64 = ctx.r11.s64 - ctx.r10.s64;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r30.u32);
	// divw r5,r8,r9
	ctx.r5.s32 = ctx.r8.s32 / ctx.r9.s32;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x82297350
	ctx.lr = 0x8226BF4C;
	sub_82297350(ctx, base);
	// stb r30,17(r31)
	PPC_STORE_U8(ctx.r31.u32 + 17, ctx.r30.u8);
loc_8226BF50:
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BF64;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_8226BF64:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_8226BF6C"))) PPC_WEAK_FUNC(sub_8226BF6C);
PPC_FUNC_IMPL(__imp__sub_8226BF6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226BF70"))) PPC_WEAK_FUNC(sub_8226BF70);
PPC_FUNC_IMPL(__imp__sub_8226BF70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be0
	ctx.lr = 0x8226BF78;
	sub_82CA2BE0(ctx, base);
	// stfd f30,-72(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.f30.u64);
	// stfd f31,-64(r1)
	PPC_STORE_U64(ctx.r1.u32 + -64, ctx.f31.u64);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// li r27,1
	ctx.r27.s64 = 1;
	// li r28,0
	ctx.r28.s64 = 0;
	// lis r30,-31927
	ctx.r30.s64 = -2092367872;
	// lbz r10,65(r29)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r29.u32 + 65);
	// addi r26,r11,-27852
	ctx.r26.s64 = ctx.r11.s64 + -27852;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226c308
	if (!ctx.cr6.eq) goto loc_8226C308;
	// bl 0x8226c6c0
	ctx.lr = 0x8226BFAC;
	sub_8226C6C0(ctx, base);
	// lwz r3,124(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,92(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 92);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226BFC0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226c024
	if (ctx.cr6.eq) goto loc_8226C024;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x8218dd60
	ctx.lr = 0x8226BFD4;
	sub_8218DD60(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c024
	if (ctx.cr6.eq) goto loc_8226C024;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	ctx.lr = 0x8226BFE8;
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226c008
	if (ctx.cr6.eq) goto loc_8226C008;
	// lis r11,-32245
	ctx.r11.s64 = -2113208320;
	// stw r28,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r28.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r10,r11,-32268
	ctx.r10.s64 = ctx.r11.s64 + -32268;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x8226c00c
	goto loc_8226C00C;
loc_8226C008:
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
loc_8226C00C:
	// addi r3,r29,72
	ctx.r3.s64 = ctx.r29.s64 + 72;
	// bl 0x8238a848
	ctx.lr = 0x8226C014;
	sub_8238A848(ctx, base);
	// li r5,13
	ctx.r5.s64 = 13;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r4,72(r29)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r29.u32 + 72);
	// bl 0x821da6d8
	ctx.lr = 0x8226C024;
	sub_821DA6D8(ctx, base);
loc_8226C024:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226C02C;
	sub_8226C6C0(ctx, base);
	// lwz r11,48(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// rlwinm r10,r11,31,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226c308
	if (!ctx.cr6.eq) goto loc_8226C308;
	// lwz r11,26912(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26912);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r31,0(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// bl 0x8226c6c0
	ctx.lr = 0x8226C058;
	sub_8226C6C0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82232748
	ctx.lr = 0x8226C064;
	sub_82232748(ctx, base);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x8226c308
	if (!ctx.cr6.eq) goto loc_8226C308;
	// stb r27,67(r29)
	PPC_STORE_U8(ctx.r29.u32 + 67, ctx.r27.u8);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226C080;
	sub_8226C6C0(ctx, base);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r10,r11,11,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 11) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226c180
	if (ctx.cr6.eq) goto loc_8226C180;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c0bc
	if (ctx.cr6.eq) goto loc_8226C0BC;
	// lbz r10,21(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 21);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c184
	goto loc_8226C184;
loc_8226C0BC:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226c128
	if (!ctx.cr0.gt) goto loc_8226C128;
loc_8226C0D8:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,21
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 21, ctx.xer);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// blt cr6,0x8226c0f8
	if (ctx.cr6.lt) goto loc_8226C0F8;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_8226C0F8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226c114
	if (ctx.cr6.eq) goto loc_8226C114;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226c11c
	goto loc_8226C11C;
loc_8226C114:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226C11C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226c0d8
	if (ctx.cr6.gt) goto loc_8226C0D8;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226C128:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226c16c
	if (ctx.cr6.eq) goto loc_8226C16C;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,21
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 21, ctx.xer);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// bgt cr6,0x8226c144
	if (ctx.cr6.gt) goto loc_8226C144;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C144:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c16c
	if (!ctx.cr6.eq) goto loc_8226C16C;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c184
	goto loc_8226C184;
loc_8226C16C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c184
	goto loc_8226C184;
loc_8226C180:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C184:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c1dc
	if (ctx.cr6.eq) goto loc_8226C1DC;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821ff4e0
	ctx.lr = 0x8226C19C;
	sub_821FF4E0(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c1d8
	if (!ctx.cr6.eq) goto loc_8226C1D8;
	// li r4,18
	ctx.r4.s64 = 18;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821ff4e0
	ctx.lr = 0x8226C1B4;
	sub_821FF4E0(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c1d8
	if (!ctx.cr6.eq) goto loc_8226C1D8;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821ff4e0
	ctx.lr = 0x8226C1CC;
	sub_821FF4E0(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c1dc
	if (ctx.cr6.eq) goto loc_8226C1DC;
loc_8226C1D8:
	// stb r28,67(r29)
	PPC_STORE_U8(ctx.r29.u32 + 67, ctx.r28.u8);
loc_8226C1DC:
	// lfs f0,408(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 408);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stfs f0,68(r29)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + 68, temp.u32);
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226C1F0;
	sub_8226C6C0(ctx, base);
	// lwz r11,36(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r10,r11,12,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 12) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226c2f0
	if (ctx.cr6.eq) goto loc_8226C2F0;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c22c
	if (ctx.cr6.eq) goto loc_8226C22C;
	// lbz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 20);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c2f4
	goto loc_8226C2F4;
loc_8226C22C:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226c298
	if (!ctx.cr0.gt) goto loc_8226C298;
loc_8226C248:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,20
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 20, ctx.xer);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// blt cr6,0x8226c268
	if (ctx.cr6.lt) goto loc_8226C268;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_8226C268:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226c284
	if (ctx.cr6.eq) goto loc_8226C284;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226c28c
	goto loc_8226C28C;
loc_8226C284:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226C28C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226c248
	if (ctx.cr6.gt) goto loc_8226C248;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226C298:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226c2dc
	if (ctx.cr6.eq) goto loc_8226C2DC;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,20
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 20, ctx.xer);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// bgt cr6,0x8226c2b4
	if (ctx.cr6.gt) goto loc_8226C2B4;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C2B4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c2dc
	if (!ctx.cr6.eq) goto loc_8226C2DC;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c2f4
	goto loc_8226C2F4;
loc_8226C2DC:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c2f4
	goto loc_8226C2F4;
loc_8226C2F0:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C2F4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c308
	if (ctx.cr6.eq) goto loc_8226C308;
	// lfs f0,72(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	ctx.f0.f64 = double(temp.f32);
	// stfs f0,68(r29)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + 68, temp.u32);
loc_8226C308:
	// lwz r11,26912(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 26912);
	// lfs f30,384(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 384);
	ctx.f30.f64 = double(temp.f32);
	// lfs f0,52(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 52);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// stw r8,80(r29)
	PPC_STORE_U32(ctx.r29.u32 + 80, ctx.r8.u32);
	// bne cr6,0x8226c348
	if (!ctx.cr6.eq) goto loc_8226C348;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226C344;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stfs f1,52(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r29.u32 + 52, temp.u32);
loc_8226C348:
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226C35C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lfs f0,48(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	ctx.f0.f64 = double(temp.f32);
	// stfs f1,8(r29)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8, temp.u32);
	// fcmpu cr6,f0,f30
	ctx.cr6.compare(ctx.f0.f64, ctx.f30.f64);
	// ble cr6,0x8226c578
	if (!ctx.cr6.gt) goto loc_8226C578;
	// lbz r11,64(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 64);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c3f4
	if (!ctx.cr6.eq) goto loc_8226C3F4;
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226C38C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmr f2,f1
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f1.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x821e3198
	ctx.lr = 0x8226C3A0;
	sub_821E3198(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x8226c3f4
	if (ctx.cr6.eq) goto loc_8226C3F4;
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// addi r9,r11,-28336
	ctx.r9.s64 = ctx.r11.s64 + -28336;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// lfs f13,-25888(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -25888);
	ctx.f13.f64 = double(temp.f32);
	// vmsum3fp128 v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx128 v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,80(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	ctx.f0.f64 = double(temp.f32);
	// fsqrts f0,f0
	ctx.f0.f64 = double(float(sqrt(ctx.f0.f64)));
	// fcmpu cr6,f0,f13
	ctx.cr6.compare(ctx.f0.f64, ctx.f13.f64);
	// ble cr6,0x8226c3f4
	if (!ctx.cr6.gt) goto loc_8226C3F4;
	// lfs f13,48(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 48);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 / ctx.f0.f64));
	// stfs f12,48(r29)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r29.u32 + 48, temp.u32);
loc_8226C3F4:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82275968
	ctx.lr = 0x8226C3FC;
	sub_82275968(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226c418
	if (ctx.cr6.eq) goto loc_8226C418;
	// lbz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x8226c41c
	if (!ctx.cr6.eq) goto loc_8226C41C;
loc_8226C418:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C41C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c578
	if (!ctx.cr6.eq) goto loc_8226C578;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r31,r28
	ctx.r31.u64 = ctx.r28.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226C434;
	sub_8226C6C0(ctx, base);
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// rlwinm r10,r11,4,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226c534
	if (ctx.cr6.eq) goto loc_8226C534;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c470
	if (ctx.cr6.eq) goto loc_8226C470;
	// lbz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 60);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c538
	goto loc_8226C538;
loc_8226C470:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226c4dc
	if (!ctx.cr0.gt) goto loc_8226C4DC;
loc_8226C48C:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,60
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 60, ctx.xer);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// blt cr6,0x8226c4ac
	if (ctx.cr6.lt) goto loc_8226C4AC;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_8226C4AC:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226c4c8
	if (ctx.cr6.eq) goto loc_8226C4C8;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226c4d0
	goto loc_8226C4D0;
loc_8226C4C8:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226C4D0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226c48c
	if (ctx.cr6.gt) goto loc_8226C48C;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226C4DC:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226c520
	if (ctx.cr6.eq) goto loc_8226C520;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,60
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 60, ctx.xer);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// bgt cr6,0x8226c4f8
	if (ctx.cr6.gt) goto loc_8226C4F8;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C4F8:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c520
	if (!ctx.cr6.eq) goto loc_8226C520;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c538
	goto loc_8226C538;
loc_8226C520:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c538
	goto loc_8226C538;
loc_8226C534:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C538:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c578
	if (ctx.cr6.eq) goto loc_8226C578;
	// lwz r11,168(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x8226c578
	if (!ctx.cr6.eq) goto loc_8226C578;
	// stb r28,66(r29)
	PPC_STORE_U8(ctx.r29.u32 + 66, ctx.r28.u8);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826757c0
	ctx.lr = 0x8226C560;
	sub_826757C0(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmr f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f30.f64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821da7a8
	ctx.lr = 0x8226C574;
	sub_821DA7A8(ctx, base);
	// stfs f1,44(r29)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r29.u32 + 44, temp.u32);
loc_8226C578:
	// stb r27,84(r29)
	PPC_STORE_U8(ctx.r29.u32 + 84, ctx.r27.u8);
	// li r11,12
	ctx.r11.s64 = 12;
	// lwz r6,4(r29)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r29.u32 + 4);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r28.u32);
	// lwz r9,24(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// lwz r5,28(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// subf r10,r9,r5
	ctx.r10.s64 = ctx.r5.s64 - ctx.r9.s64;
	// divw. r11,r10,r11
	ctx.r11.s32 = ctx.r10.s32 / ctx.r11.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226c5f8
	if (!ctx.cr0.gt) goto loc_8226C5F8;
loc_8226C5A0:
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r7,1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 1, ctx.xer);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// blt cr6,0x8226c5c8
	if (ctx.cr6.lt) goto loc_8226C5C8;
	// mr r7,r28
	ctx.r7.u64 = ctx.r28.u64;
loc_8226C5C8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226c5e4
	if (ctx.cr6.eq) goto loc_8226C5E4;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226c5ec
	goto loc_8226C5EC;
loc_8226C5E4:
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_8226C5EC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226c5a0
	if (ctx.cr6.gt) goto loc_8226C5A0;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
loc_8226C5F8:
	// cmplw cr6,r9,r5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x8226c630
	if (ctx.cr6.eq) goto loc_8226C630;
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// bgt cr6,0x8226c614
	if (ctx.cr6.gt) goto loc_8226C614;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C614:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c630
	if (!ctx.cr6.eq) goto loc_8226C630;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// b 0x8226c634
	goto loc_8226C634;
loc_8226C630:
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
loc_8226C634:
	// lwz r10,28(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x8226c64c
	if (ctx.cr6.eq) goto loc_8226C64C;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// b 0x8226c654
	goto loc_8226C654;
loc_8226C64C:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
loc_8226C654:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c6ac
	if (ctx.cr6.eq) goto loc_8226C6AC;
	// lfs f31,16(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 16);
	ctx.f31.f64 = double(temp.f32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x8269bf50
	ctx.lr = 0x8226C674;
	sub_8269BF50(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c6a4
	if (!ctx.cr6.eq) goto loc_8226C6A4;
	// lfs f0,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// fmuls f2,f31,f0
	ctx.f2.f64 = double(float(ctx.f31.f64 * ctx.f0.f64));
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// bl 0x8269bf50
	ctx.lr = 0x8226C694;
	sub_8269BF50(ctx, base);
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// beq cr6,0x8226c6a8
	if (ctx.cr6.eq) goto loc_8226C6A8;
loc_8226C6A4:
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_8226C6A8:
	// stb r11,84(r29)
	PPC_STORE_U8(ctx.r29.u32 + 84, ctx.r11.u8);
loc_8226C6AC:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// lfd f30,-72(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// lfd f31,-64(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -64);
	// b 0x82ca2c30
	// ERROR 82CA2C30
	return;
}

__attribute__((alias("__imp__sub_8226C6BC"))) PPC_WEAK_FUNC(sub_8226C6BC);
PPC_FUNC_IMPL(__imp__sub_8226C6BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_8226C6C0"))) PPC_WEAK_FUNC(sub_8226C6C0);
PPC_FUNC_IMPL(__imp__sub_8226C6C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// addi r3,r11,4
	ctx.r3.s64 = ctx.r11.s64 + 4;
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c704
	if (ctx.cr6.eq) goto loc_8226C704;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226c700
	if (ctx.cr6.eq) goto loc_8226C700;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_8226C700:
	// bl 0x821940c8
	ctx.lr = 0x8226C704;
	sub_821940C8(ctx, base);
loc_8226C704:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_8226C718"))) PPC_WEAK_FUNC(sub_8226C718);
PPC_FUNC_IMPL(__imp__sub_8226C718) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2be8
	ctx.lr = 0x8226C720;
	sub_82CA2BE8(ctx, base);
	// stfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r11,24(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 24);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x8226c754
	if (ctx.cr6.lt) goto loc_8226C754;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,24(r29)
	PPC_STORE_U32(ctx.r29.u32 + 24, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-56(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
loc_8226C754:
	// lfs f0,8(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// stfs f0,12(r29)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r29.u32 + 12, temp.u32);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r28,r11,-27468
	ctx.r28.s64 = ctx.r11.s64 + -27468;
	// li r31,0
	ctx.r31.s64 = 0;
	// lfs f30,12(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 12);
	ctx.f30.f64 = double(temp.f32);
	// bl 0x8226c6c0
	ctx.lr = 0x8226C774;
	sub_8226C6C0(ctx, base);
	// lbz r11,48(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 48);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226c868
	if (ctx.cr6.eq) goto loc_8226C868;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226c7ac
	if (ctx.cr6.eq) goto loc_8226C7AC;
	// lbz r10,120(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 120);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8226c86c
	goto loc_8226C86C;
loc_8226C7AC:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r31.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226c818
	if (!ctx.cr0.gt) goto loc_8226C818;
loc_8226C7C8:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,120
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 120, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226c7e8
	if (ctx.cr6.lt) goto loc_8226C7E8;
	// mr r7,r31
	ctx.r7.u64 = ctx.r31.u64;
loc_8226C7E8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226c804
	if (ctx.cr6.eq) goto loc_8226C804;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226c80c
	goto loc_8226C80C;
loc_8226C804:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226C80C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226c7c8
	if (ctx.cr6.gt) goto loc_8226C7C8;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226C818:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226c858
	if (ctx.cr6.eq) goto loc_8226C858;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,120
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 120, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226c834
	if (ctx.cr6.gt) goto loc_8226C834;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
loc_8226C834:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226c858
	if (!ctx.cr6.eq) goto loc_8226C858;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226c86c
	goto loc_8226C86C;
loc_8226C858:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226c86c
	goto loc_8226C86C;
loc_8226C868:
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
loc_8226C86C:
	// clrlwi r10,r11,24
	ctx.r10.u64 = ctx.r11.u32 & 0xFF;
	// lfs f31,0(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	ctx.f31.f64 = double(temp.f32);
	// lis r11,-32240
	ctx.r11.s64 = -2112880640;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// addi r30,r11,368
	ctx.r30.s64 = ctx.r11.s64 + 368;
	// beq cr6,0x8226c8e4
	if (ctx.cr6.eq) goto loc_8226C8E4;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lfs f13,44(r31)
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// lfs f0,12(r28)
	temp.u32 = PPC_LOAD_U32(ctx.r28.u32 + 12);
	ctx.f0.f64 = double(temp.f32);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// extsw r8,r10
	ctx.r8.s64 = ctx.r10.s32;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// lfd f11,80(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f8,f12
	ctx.f8.f64 = double(ctx.f12.s64);
	// fcfid f10,f11
	ctx.f10.f64 = double(ctx.f11.s64);
	// frsp f6,f8
	ctx.f6.f64 = double(float(ctx.f8.f64));
	// frsp f9,f10
	ctx.f9.f64 = double(float(ctx.f10.f64));
	// fmuls f7,f9,f13
	ctx.f7.f64 = double(float(ctx.f9.f64 * ctx.f13.f64));
	// fdivs f5,f7,f6
	ctx.f5.f64 = double(float(ctx.f7.f64 / ctx.f6.f64));
	// fsubs f4,f5,f0
	ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f0.f64));
	// fcmpu cr6,f4,f31
	ctx.cr6.compare(ctx.f4.f64, ctx.f31.f64);
	// mfcr r7
	ctx.r7.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r7.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r7.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r7.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r7.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r7.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r7.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r7.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r7.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r7.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r7.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r7.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r7.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r7.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r7.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r7.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r7.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r7.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r7.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r7.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r7.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r7.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r7.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r7.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r7.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r7.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r7.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r7.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r7.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r7.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r7.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r7.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r6,r7,27,29,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x4;
	// rlwinm r5,r7,30,29,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x4;
	// or r4,r6,r5
	ctx.r4.u64 = ctx.r6.u64 | ctx.r5.u64;
	// lfsx f3,r30,r4
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r4.u32);
	ctx.f3.f64 = double(temp.f32);
	// fsel f30,f3,f5,f0
	ctx.f30.f64 = ctx.f3.f64 >= 0.0 ? ctx.f5.f64 : ctx.f0.f64;
loc_8226C8E4:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226C8EC;
	sub_8226C6C0(ctx, base);
	// bl 0x82207928
	ctx.lr = 0x8226C8F0;
	sub_82207928(ctx, base);
	// lis r11,-31950
	ctx.r11.s64 = -2093875200;
	// lfs f0,32(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 32);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,20(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// lfs f11,8(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 8);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,16(r29)
	temp.u32 = PPC_LOAD_U32(ctx.r29.u32 + 16);
	ctx.f10.f64 = double(temp.f32);
	// lfd f0,-27376(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -27376);
	// fdiv f9,f1,f0
	ctx.f9.f64 = ctx.f1.f64 / ctx.f0.f64;
	// fmuls f8,f12,f30
	ctx.f8.f64 = double(float(ctx.f12.f64 * ctx.f30.f64));
	// frsp f7,f9
	ctx.f7.f64 = double(float(ctx.f9.f64));
	// fmadds f6,f8,f7,f11
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f7.f64 + ctx.f11.f64));
	// stfs f6,8(r29)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8, temp.u32);
	// fmr f5,f6
	ctx.f5.f64 = ctx.f6.f64;
	// fsubs f4,f5,f10
	ctx.f4.f64 = double(float(ctx.f5.f64 - ctx.f10.f64));
	// fcmpu cr6,f4,f31
	ctx.cr6.compare(ctx.f4.f64, ctx.f31.f64);
	// mfcr r10
	ctx.r10.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r10.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r10.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r10.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r10.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r10.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r10.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r10.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r10.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r10.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r10.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r10.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r10.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r10.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r10.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r10.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r10.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r10.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r10.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r10.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r10.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r10.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r10.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r10.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r10.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r10.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r10.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r10.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r10.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r10.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r10.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r10.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r9,r10,27,29,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x4;
	// rlwinm r8,r10,30,29,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x4;
	// or r7,r9,r8
	ctx.r7.u64 = ctx.r9.u64 | ctx.r8.u64;
	// lfsx f3,r30,r7
	temp.u32 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r7.u32);
	ctx.f3.f64 = double(temp.f32);
	// fsel f2,f3,f10,f5
	ctx.f2.f64 = ctx.f3.f64 >= 0.0 ? ctx.f10.f64 : ctx.f5.f64;
	// stfs f2,8(r29)
	temp.f32 = float(ctx.f2.f64);
	PPC_STORE_U32(ctx.r29.u32 + 8, temp.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-56(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x82ca2c38
	// ERROR 82CA2C38
	return;
}

__attribute__((alias("__imp__sub_8226C958"))) PPC_WEAK_FUNC(sub_8226C958);
PPC_FUNC_IMPL(__imp__sub_8226C958) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x82ca2bdc
	ctx.lr = 0x8226C960;
	sub_82CA2BDC(ctx, base);
	// stfd f29,-88(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f29.u64);
	// stfd f30,-80(r1)
	PPC_STORE_U64(ctx.r1.u32 + -80, ctx.f30.u64);
	// stfd f31,-72(r1)
	PPC_STORE_U64(ctx.r1.u32 + -72, ctx.f31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r28,-31927
	ctx.r28.s64 = -2092367872;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// lwz r11,26912(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r31,0(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// bl 0x8226c6c0
	ctx.lr = 0x8226C990;
	sub_8226C6C0(ctx, base);
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// li r25,0
	ctx.r25.s64 = 0;
	// addi r7,r8,5732
	ctx.r7.s64 = ctx.r8.s64 + 5732;
	// stw r25,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r25.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822149c8
	ctx.lr = 0x8226C9B0;
	sub_822149C8(ctx, base);
	// lwz r6,72(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 72);
	// addi r3,r31,68
	ctx.r3.s64 = ctx.r31.s64 + 68;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// stw r6,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r6.u32);
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x82704f68
	ctx.lr = 0x8226C9CC;
	sub_82704F68(ctx, base);
	// lis r5,-32241
	ctx.r5.s64 = -2112946176;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r5,5732
	ctx.r4.s64 = ctx.r5.s64 + 5732;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// bl 0x821940c8
	ctx.lr = 0x8226C9E0;
	sub_821940C8(ctx, base);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// addi r3,r26,64
	ctx.r3.s64 = ctx.r26.s64 + 64;
	// addi r4,r11,3392
	ctx.r4.s64 = ctx.r11.s64 + 3392;
	// bl 0x82275368
	ctx.lr = 0x8226C9F0;
	sub_82275368(ctx, base);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226C9F8;
	sub_8226C6C0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r4,124(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 124);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,68(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 68);
	// mtctr r8
	ctx.ctr.u64 = ctx.r8.u64;
	// bctrl 
	ctx.lr = 0x8226CA14;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// ld r3,0(r7)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
	// ld r4,8(r7)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r7.u32 + 8);
	// bl 0x821f5a28
	ctx.lr = 0x8226CA24;
	sub_821F5A28(ctx, base);
	// li r6,80
	ctx.r6.s64 = 80;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// stvx128 v1,r26,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r26.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8226c6c0
	ctx.lr = 0x8226CA38;
	sub_8226C6C0(ctx, base);
	// lwz r5,36(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r4,r5,13,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 13) & 0x1;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x8226cb18
	if (ctx.cr6.eq) goto loc_8226CB18;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226ca6c
	if (ctx.cr6.eq) goto loc_8226CA6C;
	// lbz r10,19(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 19);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8226cb18
	goto loc_8226CB18;
loc_8226CA6C:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226cad8
	if (!ctx.cr0.gt) goto loc_8226CAD8;
loc_8226CA88:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,19
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 19, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226caa8
	if (ctx.cr6.lt) goto loc_8226CAA8;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
loc_8226CAA8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226cac4
	if (ctx.cr6.eq) goto loc_8226CAC4;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226cacc
	goto loc_8226CACC;
loc_8226CAC4:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226CACC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226ca88
	if (ctx.cr6.gt) goto loc_8226CA88;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226CAD8:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226cb10
	if (ctx.cr6.eq) goto loc_8226CB10;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,19
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 19, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226caf4
	if (ctx.cr6.gt) goto loc_8226CAF4;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226CAF4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226cb10
	if (!ctx.cr6.eq) goto loc_8226CB10;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x8226cb14
	goto loc_8226CB14;
loc_8226CB10:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_8226CB14:
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
loc_8226CB18:
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,3400
	ctx.r4.s64 = ctx.r11.s64 + 3400;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CB2C;
	sub_8222CF18(ctx, base);
	// addi r30,r26,8
	ctx.r30.s64 = ctx.r26.s64 + 8;
	// addi r29,r26,28
	ctx.r29.s64 = ctx.r26.s64 + 28;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8223e410
	ctx.lr = 0x8226CB48;
	sub_8223E410(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8226CB50;
	sub_82214F08(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,4832
	ctx.r4.s64 = ctx.r10.s64 + 4832;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CB64;
	sub_8222CF18(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8223e410
	ctx.lr = 0x8226CB78;
	sub_8223E410(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8226CB80;
	sub_82214F08(ctx, base);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r9,3408
	ctx.r4.s64 = ctx.r9.s64 + 3408;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CB94;
	sub_8222CF18(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8223e410
	ctx.lr = 0x8226CBA8;
	sub_8223E410(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8226CBB0;
	sub_82214F08(ctx, base);
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r8,4860
	ctx.r4.s64 = ctx.r8.s64 + 4860;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CBC4;
	sub_8222CF18(ctx, base);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8223e410
	ctx.lr = 0x8226CBD8;
	sub_8223E410(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	ctx.lr = 0x8226CBE0;
	sub_82214F08(ctx, base);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226CBEC;
	sub_8226C6C0(ctx, base);
	// lwz r7,36(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// rlwinm r6,r7,26,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 26) & 0x1;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x8226cce0
	if (ctx.cr6.eq) goto loc_8226CCE0;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226cc24
	if (ctx.cr6.eq) goto loc_8226CC24;
	// lbz r10,6(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 6);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8226cce4
	goto loc_8226CCE4;
loc_8226CC24:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226cc90
	if (!ctx.cr0.gt) goto loc_8226CC90;
loc_8226CC40:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,6
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 6, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226cc60
	if (ctx.cr6.lt) goto loc_8226CC60;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
loc_8226CC60:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226cc7c
	if (ctx.cr6.eq) goto loc_8226CC7C;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226cc84
	goto loc_8226CC84;
loc_8226CC7C:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226CC84:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226cc40
	if (ctx.cr6.gt) goto loc_8226CC40;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226CC90:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226ccd0
	if (ctx.cr6.eq) goto loc_8226CCD0;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226ccac
	if (ctx.cr6.gt) goto loc_8226CCAC;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226CCAC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226ccd0
	if (!ctx.cr6.eq) goto loc_8226CCD0;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226cce4
	goto loc_8226CCE4;
loc_8226CCD0:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226cce4
	goto loc_8226CCE4;
loc_8226CCE0:
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226CCE4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226cd28
	if (ctx.cr6.eq) goto loc_8226CD28;
	// li r3,4
	ctx.r3.s64 = 4;
	// lwz r31,148(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// bl 0x8221f388
	ctx.lr = 0x8226CCFC;
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226cd10
	if (ctx.cr6.eq) goto loc_8226CD10;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r25,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r25.u32);
	// b 0x8226cd14
	goto loc_8226CD14;
loc_8226CD10:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
loc_8226CD14:
	// addi r3,r26,160
	ctx.r3.s64 = ctx.r26.s64 + 160;
	// bl 0x825505b8
	ctx.lr = 0x8226CD1C;
	sub_825505B8(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r3,160(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 160);
	// bl 0x821f5ae8
	ctx.lr = 0x8226CD28;
	sub_821F5AE8(ctx, base);
loc_8226CD28:
	// lwz r6,4(r26)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// li r10,12
	ctx.r10.s64 = 12;
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// addi r27,r11,-18104
	ctx.r27.s64 = ctx.r11.s64 + -18104;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// lwz r9,24(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 24);
	// lwz r5,28(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// lfs f30,-9364(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + -9364);
	ctx.f30.f64 = double(temp.f32);
	// subf r8,r9,r5
	ctx.r8.s64 = ctx.r5.s64 - ctx.r9.s64;
	// fmr f31,f30
	ctx.f31.f64 = ctx.f30.f64;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// divw. r11,r8,r10
	ctx.r11.s32 = ctx.r8.s32 / ctx.r10.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226cdb8
	if (!ctx.cr0.gt) goto loc_8226CDB8;
loc_8226CD60:
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r7,2
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 2, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226cd88
	if (ctx.cr6.lt) goto loc_8226CD88;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
loc_8226CD88:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226cda4
	if (ctx.cr6.eq) goto loc_8226CDA4;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226cdac
	goto loc_8226CDAC;
loc_8226CDA4:
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_8226CDAC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226cd60
	if (ctx.cr6.gt) goto loc_8226CD60;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
loc_8226CDB8:
	// cmplw cr6,r9,r5
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r5.u32, ctx.xer);
	// beq cr6,0x8226cdf0
	if (ctx.cr6.eq) goto loc_8226CDF0;
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226cdd4
	if (ctx.cr6.gt) goto loc_8226CDD4;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226CDD4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226cdf0
	if (!ctx.cr6.eq) goto loc_8226CDF0;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// b 0x8226cdf4
	goto loc_8226CDF4;
loc_8226CDF0:
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
loc_8226CDF4:
	// lwz r10,28(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 28);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x8226ce0c
	if (ctx.cr6.eq) goto loc_8226CE0C;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226ce10
	goto loc_8226CE10;
loc_8226CE0C:
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226CE10:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226cf18
	if (ctx.cr6.eq) goto loc_8226CF18;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226CE2C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x8226cf18
	if (ctx.cr6.eq) goto loc_8226CF18;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226CE4C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x821f03d8
	ctx.lr = 0x8226CE58;
	sub_821F03D8(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82285728
	ctx.lr = 0x8226CE64;
	sub_82285728(ctx, base);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r9,6208
	ctx.r4.s64 = ctx.r9.s64 + 6208;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CE78;
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8226d580
	ctx.lr = 0x8226CE84;
	sub_8226D580(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// bl 0x82214f08
	ctx.lr = 0x8226CE90;
	sub_82214F08(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lfs f0,-25888(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + -25888);
	ctx.f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	ctx.cr6.compare(ctx.f31.f64, ctx.f0.f64);
	// bge cr6,0x8226cf08
	if (!ctx.cr6.lt) goto loc_8226CF08;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,4992
	ctx.r4.s64 = ctx.r11.s64 + 4992;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CEB4;
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821da9a0
	ctx.lr = 0x8226CEC0;
	sub_821DA9A0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = ctx.f1.f64;
	// bl 0x82214f08
	ctx.lr = 0x8226CECC;
	sub_82214F08(ctx, base);
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,4340
	ctx.r4.s64 = ctx.r10.s64 + 4340;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	ctx.lr = 0x8226CEE0;
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x821da9a0
	ctx.lr = 0x8226CEEC;
	sub_821DA9A0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// fmr f29,f1
	ctx.fpscr.disableFlushMode();
	ctx.f29.f64 = ctx.f1.f64;
	// bl 0x82214f08
	ctx.lr = 0x8226CEF8;
	sub_82214F08(ctx, base);
	// lfs f0,192(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 192);
	ctx.f0.f64 = double(temp.f32);
	// fmadds f13,f31,f0,f29
	ctx.f13.f64 = double(float(ctx.f31.f64 * ctx.f0.f64 + ctx.f29.f64));
	// lfs f0,0(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
loc_8226CF08:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226CF10;
	sub_8226C6C0(ctx, base);
	// bl 0x82207928
	ctx.lr = 0x8226CF14;
	sub_82207928(ctx, base);
	// fdivs f31,f31,f1
	ctx.fpscr.disableFlushMode();
	ctx.f31.f64 = double(float(ctx.f31.f64 / ctx.f1.f64));
loc_8226CF18:
	// lwz r11,26912(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 26912);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r11,12(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r31,0(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// bl 0x82275968
	ctx.lr = 0x8226CF34;
	sub_82275968(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82232748
	ctx.lr = 0x8226CF40;
	sub_82232748(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// bl 0x82275968
	ctx.lr = 0x8226CF50;
	sub_82275968(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226d0e0
	if (ctx.cr6.eq) goto loc_8226D0E0;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82275968
	ctx.lr = 0x8226CF60;
	sub_82275968(ctx, base);
	// lwz r11,48(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 48);
	// rlwinm r10,r11,9,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 9) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226d060
	if (ctx.cr6.eq) goto loc_8226D060;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226cf9c
	if (ctx.cr6.eq) goto loc_8226CF9C;
	// lbz r10,119(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 119);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d064
	goto loc_8226D064;
loc_8226CF9C:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226d008
	if (!ctx.cr0.gt) goto loc_8226D008;
loc_8226CFB8:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,119
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 119, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226cfd8
	if (ctx.cr6.lt) goto loc_8226CFD8;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
loc_8226CFD8:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226cff4
	if (ctx.cr6.eq) goto loc_8226CFF4;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226cffc
	goto loc_8226CFFC;
loc_8226CFF4:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226CFFC:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226cfb8
	if (ctx.cr6.gt) goto loc_8226CFB8;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226D008:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226d04c
	if (ctx.cr6.eq) goto loc_8226D04C;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,119
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 119, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226d024
	if (ctx.cr6.gt) goto loc_8226D024;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226D024:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226d04c
	if (!ctx.cr6.eq) goto loc_8226D04C;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d064
	goto loc_8226D064;
loc_8226D04C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r30,r11
	ctx.r30.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d064
	goto loc_8226D064;
loc_8226D060:
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226D064:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d0e0
	if (ctx.cr6.eq) goto loc_8226D0E0;
	// clrlwi r11,r29,24
	ctx.r11.u64 = ctx.r29.u32 & 0xFF;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226d090
	if (!ctx.cr6.eq) goto loc_8226D090;
	// lbz r11,49(r26)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r26.u32 + 49);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d090
	if (ctx.cr6.eq) goto loc_8226D090;
	// li r31,1
	ctx.r31.s64 = 1;
loc_8226D090:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82275968
	ctx.lr = 0x8226D098;
	sub_82275968(ctx, base);
	// bl 0x821b1730
	ctx.lr = 0x8226D09C;
	sub_821B1730(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x8226d0b4
	if (ctx.cr6.eq) goto loc_8226D0B4;
	// lwz r11,4(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x8226d0b4
	if (!ctx.cr6.gt) goto loc_8226D0B4;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
loc_8226D0B4:
	// clrlwi r11,r31,24
	ctx.r11.u64 = ctx.r31.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226d0e0
	if (!ctx.cr6.eq) goto loc_8226D0E0;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r31,4(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// bl 0x8226c6c0
	ctx.lr = 0x8226D0CC;
	sub_8226C6C0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f31.f64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// bl 0x82294720
	ctx.lr = 0x8226D0E0;
	sub_82294720(ctx, base);
loc_8226D0E0:
	// clrlwi r11,r29,24
	ctx.r11.u64 = ctx.r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d0f4
	if (ctx.cr6.eq) goto loc_8226D0F4;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82275968
	ctx.lr = 0x8226D0F4;
	sub_82275968(ctx, base);
loc_8226D0F4:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226D100;
	sub_8226C6C0(ctx, base);
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226d200
	if (ctx.cr6.eq) goto loc_8226D200;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d13c
	if (ctx.cr6.eq) goto loc_8226D13C;
	// lbz r10,34(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 34);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d204
	goto loc_8226D204;
loc_8226D13C:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226d1a8
	if (!ctx.cr0.gt) goto loc_8226D1A8;
loc_8226D158:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,34
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 34, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226d178
	if (ctx.cr6.lt) goto loc_8226D178;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
loc_8226D178:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226d194
	if (ctx.cr6.eq) goto loc_8226D194;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226d19c
	goto loc_8226D19C;
loc_8226D194:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226D19C:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226d158
	if (ctx.cr6.gt) goto loc_8226D158;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226D1A8:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226d1ec
	if (ctx.cr6.eq) goto loc_8226D1EC;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,34
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 34, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226d1c4
	if (ctx.cr6.gt) goto loc_8226D1C4;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226D1C4:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226d1ec
	if (!ctx.cr6.eq) goto loc_8226D1EC;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d204
	goto loc_8226D204;
loc_8226D1EC:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r31,r11
	ctx.r31.u64 = ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d204
	goto loc_8226D204;
loc_8226D200:
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226D204:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d220
	if (ctx.cr6.eq) goto loc_8226D220;
	// lfs f0,56(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 56);
	ctx.f0.f64 = double(temp.f32);
	// lfs f13,60(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 60);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f12,f0,f13
	ctx.f12.f64 = double(float(ctx.f0.f64 * ctx.f13.f64));
	// stfs f12,60(r26)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r26.u32 + 60, temp.u32);
loc_8226D220:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// bl 0x8226c6c0
	ctx.lr = 0x8226D22C;
	sub_8226C6C0(ctx, base);
	// lbz r11,48(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 48);
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x8226d320
	if (ctx.cr6.eq) goto loc_8226D320;
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d264
	if (ctx.cr6.eq) goto loc_8226D264;
	// lbz r10,120(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 120);
	// lwz r11,72(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,4(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// b 0x8226d324
	goto loc_8226D324;
loc_8226D264:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r25.u32);
	// subf r11,r10,r6
	ctx.r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226d2d0
	if (!ctx.cr0.gt) goto loc_8226D2D0;
loc_8226D280:
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,120
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 120, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226d2a0
	if (ctx.cr6.lt) goto loc_8226D2A0;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
loc_8226D2A0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226d2bc
	if (ctx.cr6.eq) goto loc_8226D2BC;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226d2c4
	goto loc_8226D2C4;
loc_8226D2BC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_8226D2C4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226d280
	if (ctx.cr6.gt) goto loc_8226D280;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_8226D2D0:
	// cmplw cr6,r10,r6
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226d310
	if (ctx.cr6.eq) goto loc_8226D310;
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,120
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 120, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226d2ec
	if (ctx.cr6.gt) goto loc_8226D2EC;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226D2EC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226d310
	if (!ctx.cr6.eq) goto loc_8226D310;
	// ld r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r11.u64);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d324
	goto loc_8226D324;
loc_8226D310:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r31,4(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d324
	goto loc_8226D324;
loc_8226D320:
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_8226D324:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d3a4
	if (ctx.cr6.eq) goto loc_8226D3A4;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// lfs f13,40(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	ctx.f13.f64 = double(temp.f32);
	// lwz r10,28(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// lfs f0,-9352(r27)
	temp.u32 = PPC_LOAD_U32(ctx.r27.u32 + -9352);
	ctx.f0.f64 = double(temp.f32);
	// extsw r9,r11
	ctx.r9.s64 = ctx.r11.s32;
	// lfs f12,60(r26)
	temp.u32 = PPC_LOAD_U32(ctx.r26.u32 + 60);
	ctx.f12.f64 = double(temp.f32);
	// extsw r8,r10
	ctx.r8.s64 = ctx.r10.s32;
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lis r7,-32240
	ctx.r7.s64 = -2112880640;
	// addi r6,r7,368
	ctx.r6.s64 = ctx.r7.s64 + 368;
	// lfd f11,96(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r8,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r8.u64);
	// lfd f10,96(r1)
	ctx.f10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f9,f10
	ctx.f9.f64 = double(ctx.f10.s64);
	// frsp f8,f9
	ctx.f8.f64 = double(float(ctx.f9.f64));
	// fcfid f7,f11
	ctx.f7.f64 = double(ctx.f11.s64);
	// fmuls f6,f8,f13
	ctx.f6.f64 = double(float(ctx.f8.f64 * ctx.f13.f64));
	// frsp f5,f7
	ctx.f5.f64 = double(float(ctx.f7.f64));
	// fdivs f4,f6,f5
	ctx.f4.f64 = double(float(ctx.f6.f64 / ctx.f5.f64));
	// fsubs f3,f4,f0
	ctx.f3.f64 = double(float(ctx.f4.f64 - ctx.f0.f64));
	// fcmpu cr6,f3,f30
	ctx.cr6.compare(ctx.f3.f64, ctx.f30.f64);
	// mfcr r5
	ctx.r5.u64 = ctx.cr0.lt ? 0x80000000 : 0;
	ctx.r5.u64 |= ctx.cr0.gt ? 0x40000000 : 0;
	ctx.r5.u64 |= ctx.cr0.eq ? 0x20000000 : 0;
	ctx.r5.u64 |= ctx.cr0.so ? 0x10000000 : 0;
	ctx.r5.u64 |= ctx.cr1.lt ? 0x8000000 : 0;
	ctx.r5.u64 |= ctx.cr1.gt ? 0x4000000 : 0;
	ctx.r5.u64 |= ctx.cr1.eq ? 0x2000000 : 0;
	ctx.r5.u64 |= ctx.cr1.so ? 0x1000000 : 0;
	ctx.r5.u64 |= ctx.cr2.lt ? 0x800000 : 0;
	ctx.r5.u64 |= ctx.cr2.gt ? 0x400000 : 0;
	ctx.r5.u64 |= ctx.cr2.eq ? 0x200000 : 0;
	ctx.r5.u64 |= ctx.cr2.so ? 0x100000 : 0;
	ctx.r5.u64 |= ctx.cr3.lt ? 0x80000 : 0;
	ctx.r5.u64 |= ctx.cr3.gt ? 0x40000 : 0;
	ctx.r5.u64 |= ctx.cr3.eq ? 0x20000 : 0;
	ctx.r5.u64 |= ctx.cr3.so ? 0x10000 : 0;
	ctx.r5.u64 |= ctx.cr4.lt ? 0x8000 : 0;
	ctx.r5.u64 |= ctx.cr4.gt ? 0x4000 : 0;
	ctx.r5.u64 |= ctx.cr4.eq ? 0x2000 : 0;
	ctx.r5.u64 |= ctx.cr4.so ? 0x1000 : 0;
	ctx.r5.u64 |= ctx.cr5.lt ? 0x800 : 0;
	ctx.r5.u64 |= ctx.cr5.gt ? 0x400 : 0;
	ctx.r5.u64 |= ctx.cr5.eq ? 0x200 : 0;
	ctx.r5.u64 |= ctx.cr5.so ? 0x100 : 0;
	ctx.r5.u64 |= ctx.cr6.lt ? 0x80 : 0;
	ctx.r5.u64 |= ctx.cr6.gt ? 0x40 : 0;
	ctx.r5.u64 |= ctx.cr6.eq ? 0x20 : 0;
	ctx.r5.u64 |= ctx.cr6.so ? 0x10 : 0;
	ctx.r5.u64 |= ctx.cr7.lt ? 0x8 : 0;
	ctx.r5.u64 |= ctx.cr7.gt ? 0x4 : 0;
	ctx.r5.u64 |= ctx.cr7.eq ? 0x2 : 0;
	ctx.r5.u64 |= ctx.cr7.so ? 0x1 : 0;
	// rlwinm r4,r5,27,29,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x4;
	// rlwinm r3,r5,30,29,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 30) & 0x4;
	// or r11,r4,r3
	ctx.r11.u64 = ctx.r4.u64 | ctx.r3.u64;
	// lfsx f2,r6,r11
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r11.u32);
	ctx.f2.f64 = double(temp.f32);
	// fsel f1,f2,f4,f0
	ctx.f1.f64 = ctx.f2.f64 >= 0.0 ? ctx.f4.f64 : ctx.f0.f64;
	// fmuls f0,f1,f12
	ctx.f0.f64 = double(float(ctx.f1.f64 * ctx.f12.f64));
	// stfs f0,60(r26)
	temp.f32 = float(ctx.f0.f64);
	PPC_STORE_U32(ctx.r26.u32 + 60, temp.u32);
loc_8226D3A4:
	// lwz r3,4(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 4);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,28(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x8226D3B8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stfs f1,96(r26)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(ctx.r26.u32 + 96, temp.u32);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x821b7e58
	ctx.lr = 0x8226D3C4;
	sub_821B7E58(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lfd f29,-88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f29.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// lfd f30,-80(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -80);
	// lfd f31,-72(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -72);
	// b 0x82ca2c2c
	// ERROR 82CA2C2C
	return;
}

__attribute__((alias("__imp__sub_8226D3D8"))) PPC_WEAK_FUNC(sub_8226D3D8);
PPC_FUNC_IMPL(__imp__sub_8226D3D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r9,24(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// li r11,12
	ctx.r11.s64 = 12;
	// lwz r6,28(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// li r5,0
	ctx.r5.s64 = 0;
	// subf r10,r9,r6
	ctx.r10.s64 = ctx.r6.s64 - ctx.r9.s64;
	// stw r5,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r5.u32);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// divw. r11,r10,r11
	ctx.r11.s32 = ctx.r10.s32 / ctx.r11.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x8226d454
	if (!ctx.cr0.gt) goto loc_8226D454;
loc_8226D3FC:
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r8
	ctx.r8.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r7,1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 1, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x8226d424
	if (ctx.cr6.lt) goto loc_8226D424;
	// mr r7,r5
	ctx.r7.u64 = ctx.r5.u64;
loc_8226D424:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x8226d440
	if (ctx.cr6.eq) goto loc_8226D440;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r9,r9,12
	ctx.r9.s64 = ctx.r9.s64 + 12;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// b 0x8226d448
	goto loc_8226D448;
loc_8226D440:
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
loc_8226D448:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x8226d3fc
	if (ctx.cr6.gt) goto loc_8226D3FC;
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
loc_8226D454:
	// cmplw cr6,r9,r6
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r6.u32, ctx.xer);
	// beq cr6,0x8226d48c
	if (ctx.cr6.eq) goto loc_8226D48C;
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x8226d470
	if (ctx.cr6.gt) goto loc_8226D470;
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
loc_8226D470:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x8226d48c
	if (!ctx.cr6.eq) goto loc_8226D48C;
	// ld r11,-16(r1)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// std r11,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r11.u64);
	// lwz r11,-12(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -12);
	// b 0x8226d490
	goto loc_8226D490;
loc_8226D48C:
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
loc_8226D490:
	// lwz r10,28(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x8226d4a8
	if (ctx.cr6.eq) goto loc_8226D4A8;
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r11,1
	ctx.r11.s64 = 1;
	// b 0x8226d4ac
	goto loc_8226D4AC;
loc_8226D4A8:
	// mr r11,r5
	ctx.r11.u64 = ctx.r5.u64;
loc_8226D4AC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x8226d4c0
	if (ctx.cr6.eq) goto loc_8226D4C0;
	// lfs f1,8(r5)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 8);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
loc_8226D4C0:
	// lis r11,-32246
	ctx.r11.s64 = -2113273856;
	// lfs f1,-27468(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -27468);
	ctx.f1.f64 = double(temp.f32);
	// blr 
	return;
}


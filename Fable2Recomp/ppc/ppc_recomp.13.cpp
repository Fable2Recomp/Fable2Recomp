#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_822E2E50"))) PPC_WEAK_FUNC(sub_822E2E50);
PPC_FUNC_IMPL(__imp__sub_822E2E50) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r25{};
	PPCRegister r28{};
	PPCRegister r31{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v88{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCRegister temp{};
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r17,11876(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 11876);
	// lwz r17,11900(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 11900);
	// lwz r17,11924(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 11924);
	// lwz r17,12100(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12100);
	// addi r11,r1,1424
	r11.s64 = ctx.r1.s64 + 1424;
	// li r28,1
	r28.s64 = 1;
	// mr r25,r28
	r25.u64 = r28.u64;
	// stvx128 v94,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,1440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1440, r28.u32);
	// b 0x822e2f70
	goto loc_822E2F70;
	// addi r11,r1,1424
	r11.s64 = ctx.r1.s64 + 1424;
	// li r28,1
	r28.s64 = 1;
	// mr r25,r28
	r25.u64 = r28.u64;
	// stvx128 v95,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,1440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1440, r28.u32);
	// b 0x822e2f70
	goto loc_822E2F70;
	// rlwinm r11,r9,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r10,r9,29,30,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 29) & 0x3;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vand128 v13,v0,v90
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v90.u8)));
	// lvrx v11,r14,r11
	temp.u32 = r14.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v11,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// blt cr6,0x822e2ef8
	if (cr6.lt) goto loc_822E2EF8;
	// vor128 v12,v94,v94
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v94.u8));
	// beq cr6,0x822e2ee8
	if (cr6.eq) goto loc_822E2EE8;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// blt cr6,0x822e2edc
	if (cr6.lt) goto loc_822E2EDC;
	// vor128 v7,v82,v82
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v82.u8));
	// vor128 v11,v93,v93
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v93.u8));
	// b 0x822e2ef0
	goto loc_822E2EF0;
loc_822E2EDC:
	// vor128 v7,v83,v83
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v83.u8));
	// vor128 v11,v92,v92
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v92.u8));
	// b 0x822e2ef0
	goto loc_822E2EF0;
loc_822E2EE8:
	// vor128 v7,v84,v84
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v84.u8));
	// vor128 v11,v81,v81
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v81.u8));
loc_822E2EF0:
	// vperm v0,v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubsws v13,v13,v11
	temp.s64 = int64_t(ctx.v13.s32[0]) - int64_t(ctx.v11.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[1]) - int64_t(ctx.v11.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[2]) - int64_t(ctx.v11.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[3]) - int64_t(ctx.v11.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
loc_822E2EF8:
	// vor128 v12,v93,v93
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v93.u8));
	// addi r11,r1,1424
	r11.s64 = ctx.r1.s64 + 1424;
	// vor128 v11,v91,v91
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v91.u8));
	// li r28,1
	r28.s64 = 1;
	// vor128 v10,v77,v77
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v77.u8));
	// addi r9,r9,24
	ctx.r9.s64 = ctx.r9.s64 + 24;
	// vor128 v9,v78,v78
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v78.u8));
	// stw r28,1440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1440, r28.u32);
	// vaddsws v13,v13,v12
	// mr r25,r28
	r25.u64 = r28.u64;
	// vsubsws v13,v11,v13
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v13.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v13.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v13.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v13.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vslw v12,v0,v13
	ctx.v12.u32[0] = ctx.v0.u32[0] << (ctx.v13.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v0.u32[1] << (ctx.v13.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v0.u32[2] << (ctx.v13.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v0.u32[3] << (ctx.v13.u8[12] & 0x1F);
	// vsrw128 v11,v12,v81
	ctx.v11.u32[0] = ctx.v12.u32[0] >> (v81.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v12.u32[1] >> (v81.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v12.u32[2] >> (v81.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v12.u32[3] >> (v81.u8[12] & 0x1F);
	// vspltw v8,v11,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vcfux v0,v8,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmaddfp v7,v0,v9,v10
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v10.f32)));
	// vor128 v88,v7,v7
	_mm_store_si128((__m128i*)v88.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvx128 v88,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e2f70
	goto loc_822E2F70;
	// addi r9,r1,1424
	ctx.r9.s64 = ctx.r1.s64 + 1424;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82234108
	sub_82234108(ctx, base);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// li r28,1
	r28.s64 = 1;
loc_822E2F70:
	// clrlwi r11,r9,27
	r11.u64 = ctx.r9.u32 & 0x1F;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r10,r9,2
	ctx.r10.s64 = ctx.r9.s64 + 2;
	// addi r5,r11,2
	ctx.r5.s64 = r11.s64 + 2;
	// rlwinm r8,r9,27,5,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x7FFFFFF;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// cmplwi cr6,r5,32
	cr6.compare<uint32_t>(ctx.r5.u32, 32, xer);
	// rlwinm r9,r8,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// bgt cr6,0x822e2fa4
	if (cr6.gt) {
		// ERROR 822E2FA4
		return;
	}
	// lwzx r8,r9,r6
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r6.u32);
	// srw r5,r8,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (ctx.r8.u32 >> (r11.u8 & 0x3F));
	// clrlwi r11,r5,30
	r11.u64 = ctx.r5.u32 & 0x3;
	// b 0x822e2fc4
	// ERROR 822E2FC4
	return;
}

__attribute__((alias("__imp__sub_822E2FD8"))) PPC_WEAK_FUNC(sub_822E2FD8);
PPC_FUNC_IMPL(__imp__sub_822E2FD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r31{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v88{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCRegister temp{};
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r17,12268(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12268);
	// lwz r17,12288(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12288);
	// lwz r17,12308(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12308);
	// lwz r17,12480(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12480);
	// addi r11,r1,1600
	r11.s64 = ctx.r1.s64 + 1600;
	// mr r27,r28
	r27.u64 = r28.u64;
	// stvx128 v94,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,1616(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1616, r28.u32);
	// b 0x822e30e8
	goto loc_822E30E8;
	// addi r11,r1,1600
	r11.s64 = ctx.r1.s64 + 1600;
	// mr r27,r28
	r27.u64 = r28.u64;
	// stvx128 v95,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,1616(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1616, r28.u32);
	// b 0x822e30e8
	goto loc_822E30E8;
	// rlwinm r11,r10,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r8,r1,84
	ctx.r8.s64 = ctx.r1.s64 + 84;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r9,r10,29,30,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 29) & 0x3;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vand128 v13,v0,v90
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v90.u8)));
	// lvrx v11,r14,r11
	temp.u32 = r14.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v11,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// blt cr6,0x822e3078
	if (cr6.lt) goto loc_822E3078;
	// vor128 v12,v94,v94
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v94.u8));
	// beq cr6,0x822e3068
	if (cr6.eq) goto loc_822E3068;
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// blt cr6,0x822e305c
	if (cr6.lt) goto loc_822E305C;
	// vor128 v7,v82,v82
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v82.u8));
	// vor128 v11,v93,v93
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v93.u8));
	// b 0x822e3070
	goto loc_822E3070;
loc_822E305C:
	// vor128 v7,v83,v83
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v83.u8));
	// vor128 v11,v92,v92
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v92.u8));
	// b 0x822e3070
	goto loc_822E3070;
loc_822E3068:
	// vor128 v7,v84,v84
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v84.u8));
	// vor128 v11,v81,v81
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v81.u8));
loc_822E3070:
	// vperm v0,v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubsws v13,v13,v11
	temp.s64 = int64_t(ctx.v13.s32[0]) - int64_t(ctx.v11.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[1]) - int64_t(ctx.v11.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[2]) - int64_t(ctx.v11.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[3]) - int64_t(ctx.v11.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
loc_822E3078:
	// vor128 v12,v93,v93
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v93.u8));
	// addi r11,r1,1600
	r11.s64 = ctx.r1.s64 + 1600;
	// vor128 v11,v91,v91
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v91.u8));
	// stw r28,1616(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1616, r28.u32);
	// vor128 v10,v77,v77
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v77.u8));
	// addi r10,r10,24
	ctx.r10.s64 = ctx.r10.s64 + 24;
	// vor128 v9,v78,v78
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v78.u8));
	// mr r27,r28
	r27.u64 = r28.u64;
	// vaddsws v13,v13,v12
	// vsubsws v13,v11,v13
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v13.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v13.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v13.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v13.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vslw v12,v0,v13
	ctx.v12.u32[0] = ctx.v0.u32[0] << (ctx.v13.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v0.u32[1] << (ctx.v13.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v0.u32[2] << (ctx.v13.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v0.u32[3] << (ctx.v13.u8[12] & 0x1F);
	// vsrw128 v11,v12,v81
	ctx.v11.u32[0] = ctx.v12.u32[0] >> (v81.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v12.u32[1] >> (v81.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v12.u32[2] >> (v81.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v12.u32[3] >> (v81.u8[12] & 0x1F);
	// vspltw v8,v11,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vcfux v0,v8,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmaddfp v7,v0,v9,v10
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v10.f32)));
	// vor128 v88,v7,v7
	_mm_store_si128((__m128i*)v88.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvx128 v88,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e30e8
	goto loc_822E30E8;
	// addi r9,r1,1600
	ctx.r9.s64 = ctx.r1.s64 + 1600;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82234108
	sub_82234108(ctx, base);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
loc_822E30E8:
	// clrlwi r11,r10,27
	r11.u64 = ctx.r10.u32 & 0x1F;
	// lwz r7,0(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r8,r10,2
	ctx.r8.s64 = ctx.r10.s64 + 2;
	// addi r5,r11,2
	ctx.r5.s64 = r11.s64 + 2;
	// rlwinm r9,r10,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x7FFFFFF;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// cmplwi cr6,r5,32
	cr6.compare<uint32_t>(ctx.r5.u32, 32, xer);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bgt cr6,0x822e311c
	if (cr6.gt) goto loc_822E311C;
	// lwzx r9,r10,r6
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// srw r5,r9,r11
	ctx.r5.u64 = r11.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (r11.u8 & 0x3F));
	// clrlwi r11,r5,30
	r11.u64 = ctx.r5.u32 & 0x3;
	// b 0x822e313c
	// ERROR 822E313C
	return;
loc_822E311C:
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// subfic r9,r11,32
	xer.ca = r11.u32 <= 32;
	ctx.r9.s64 = 32 - r11.s64;
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
}

__attribute__((alias("__imp__sub_822E3150"))) PPC_WEAK_FUNC(sub_822E3150);
PPC_FUNC_IMPL(__imp__sub_822E3150) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r28{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v88{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCRegister temp{};
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r17,12644(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12644);
	// lwz r17,12660(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12660);
	// lwz r17,12676(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12676);
	// lwz r17,12848(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 12848);
	// addi r11,r1,1776
	r11.s64 = ctx.r1.s64 + 1776;
	// stvx128 v94,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,1792(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1792, r28.u32);
	// b 0x822e33d0
	// ERROR 822E33D0
	return;
	// addi r11,r1,1776
	r11.s64 = ctx.r1.s64 + 1776;
	// stvx128 v95,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r28,1792(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1792, r28.u32);
	// b 0x822e33d0
	// ERROR 822E33D0
	return;
	// rlwinm r11,r8,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r10,r8,29,30,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x3;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vand128 v13,v0,v90
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v90.u8)));
	// lvrx v11,r14,r11
	temp.u32 = r14.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v11,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// blt cr6,0x822e31e8
	if (cr6.lt) goto loc_822E31E8;
	// vor128 v12,v94,v94
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v94.u8));
	// beq cr6,0x822e31d8
	if (cr6.eq) goto loc_822E31D8;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// blt cr6,0x822e31cc
	if (cr6.lt) goto loc_822E31CC;
	// vor128 v7,v82,v82
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v82.u8));
	// vor128 v11,v93,v93
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v93.u8));
	// b 0x822e31e0
	goto loc_822E31E0;
loc_822E31CC:
	// vor128 v7,v83,v83
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v83.u8));
	// vor128 v11,v92,v92
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v92.u8));
	// b 0x822e31e0
	goto loc_822E31E0;
loc_822E31D8:
	// vor128 v7,v84,v84
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v84.u8));
	// vor128 v11,v81,v81
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v81.u8));
loc_822E31E0:
	// vperm v0,v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubsws v13,v13,v11
	temp.s64 = int64_t(ctx.v13.s32[0]) - int64_t(ctx.v11.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[1]) - int64_t(ctx.v11.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[2]) - int64_t(ctx.v11.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[3]) - int64_t(ctx.v11.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
loc_822E31E8:
	// vor128 v12,v93,v93
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v93.u8));
	// addi r11,r1,1776
	r11.s64 = ctx.r1.s64 + 1776;
	// vor128 v11,v91,v91
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v91.u8));
	// addi r10,r8,24
	ctx.r10.s64 = ctx.r8.s64 + 24;
	// vor128 v10,v77,v77
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v77.u8));
	// stw r28,1792(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1792, r28.u32);
	// vor128 v9,v78,v78
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v78.u8));
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// vaddsws v13,v13,v12
	// vsubsws v13,v11,v13
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v13.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v13.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v13.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v13.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vslw v12,v0,v13
	ctx.v12.u32[0] = ctx.v0.u32[0] << (ctx.v13.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v0.u32[1] << (ctx.v13.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v0.u32[2] << (ctx.v13.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v0.u32[3] << (ctx.v13.u8[12] & 0x1F);
	// vsrw128 v11,v12,v81
	ctx.v11.u32[0] = ctx.v12.u32[0] >> (v81.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v12.u32[1] >> (v81.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v12.u32[2] >> (v81.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v12.u32[3] >> (v81.u8[12] & 0x1F);
	// vspltw v8,v11,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vcfux v0,v8,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmaddfp v7,v0,v9,v10
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v10.f32)));
	// vor128 v88,v7,v7
	_mm_store_si128((__m128i*)v88.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvx128 v88,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e33d0
	// ERROR 822E33D0
	return;
	// addi r9,r1,1776
	ctx.r9.s64 = ctx.r1.s64 + 1776;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82234108
	sub_82234108(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// b 0x822e33d0
	// ERROR 822E33D0
	return;
	// lwz r9,2132(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 2132);
	// cmplwi cr6,r7,32
	cr6.compare<uint32_t>(ctx.r7.u32, 32, xer);
	// addi r8,r11,2
	ctx.r8.s64 = r11.s64 + 2;
	// addi r9,r9,20
	ctx.r9.s64 = ctx.r9.s64 + 20;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r11,27,5,31
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x7FFFFFF;
	// rlwinm r11,r9,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// bgt cr6,0x822e3288
	if (cr6.gt) goto loc_822E3288;
	// lwzx r9,r11,r6
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// srw r5,r9,r10
	ctx.r5.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 >> (ctx.r10.u8 & 0x3F));
	// clrlwi r11,r5,30
	r11.u64 = ctx.r5.u32 & 0x3;
	// b 0x822e32a8
	// ERROR 822E32A8
	return;
loc_822E3288:
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// subfic r9,r10,32
	xer.ca = ctx.r10.u32 <= 32;
	ctx.r9.s64 = 32 - ctx.r10.s64;
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// slw r3,r5,r9
	ctx.r3.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r5.u32 << (ctx.r9.u8 & 0x3F));
	// srw r11,r4,r10
	r11.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r4.u32 >> (ctx.r10.u8 & 0x3F));
	// or r10,r3,r11
	ctx.r10.u64 = ctx.r3.u64 | r11.u64;
}

__attribute__((alias("__imp__sub_822E32BC"))) PPC_WEAK_FUNC(sub_822E32BC);
PPC_FUNC_IMPL(__imp__sub_822E32BC) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r26{};
	PPCRegister r29{};
	PPCRegister r31{};
	PPCVRegister v77{};
	PPCVRegister v78{};
	PPCVRegister v81{};
	PPCVRegister v82{};
	PPCVRegister v83{};
	PPCVRegister v84{};
	PPCVRegister v88{};
	PPCVRegister v90{};
	PPCVRegister v91{};
	PPCVRegister v92{};
	PPCVRegister v93{};
	PPCVRegister v94{};
	PPCVRegister v95{};
	PPCRegister temp{};
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r17,13008(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 13008);
	// lwz r17,13028(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 13028);
	// lwz r17,13048(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 13048);
	// lwz r17,13224(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + 13224);
	// addi r11,r1,1424
	r11.s64 = ctx.r1.s64 + 1424;
	// li r26,1
	r26.s64 = 1;
	// stvx128 v94,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v94.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r26,1440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1440, r26.u32);
	// b 0x822e33d0
	goto loc_822E33D0;
	// addi r11,r1,1424
	r11.s64 = ctx.r1.s64 + 1424;
	// li r26,1
	r26.s64 = 1;
	// stvx128 v95,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v95.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r26,1440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1440, r26.u32);
	// b 0x822e33d0
	goto loc_822E33D0;
	// rlwinm r11,r8,29,3,29
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x1FFFFFFC;
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// add r11,r11,r6
	r11.u64 = r11.u64 + ctx.r6.u64;
	// rlwinm r10,r8,29,30,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 29) & 0x3;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vand128 v13,v0,v90
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)v90.u8)));
	// lvrx v11,r14,r11
	temp.u32 = r14.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, temp.u32 & 0xF ? _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskR[(temp.u32 & 0xF) * 16])) : _mm_setzero_si128());
	// vor v0,v11,v12
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_or_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v12.u8)));
	// blt cr6,0x822e335c
	if (cr6.lt) goto loc_822E335C;
	// vor128 v12,v94,v94
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v94.u8));
	// beq cr6,0x822e334c
	if (cr6.eq) goto loc_822E334C;
	// cmplwi cr6,r10,3
	cr6.compare<uint32_t>(ctx.r10.u32, 3, xer);
	// blt cr6,0x822e3340
	if (cr6.lt) goto loc_822E3340;
	// vor128 v7,v82,v82
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v82.u8));
	// vor128 v11,v93,v93
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v93.u8));
	// b 0x822e3354
	goto loc_822E3354;
loc_822E3340:
	// vor128 v7,v83,v83
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v83.u8));
	// vor128 v11,v92,v92
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v92.u8));
	// b 0x822e3354
	goto loc_822E3354;
loc_822E334C:
	// vor128 v7,v84,v84
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_load_si128((__m128i*)v84.u8));
	// vor128 v11,v81,v81
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v81.u8));
loc_822E3354:
	// vperm v0,v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// vsubsws v13,v13,v11
	temp.s64 = int64_t(ctx.v13.s32[0]) - int64_t(ctx.v11.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[1]) - int64_t(ctx.v11.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[2]) - int64_t(ctx.v11.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v13.s32[3]) - int64_t(ctx.v11.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
loc_822E335C:
	// vor128 v12,v93,v93
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_load_si128((__m128i*)v93.u8));
	// addi r11,r1,1424
	r11.s64 = ctx.r1.s64 + 1424;
	// vor128 v11,v91,v91
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_load_si128((__m128i*)v91.u8));
	// li r26,1
	r26.s64 = 1;
	// vor128 v10,v77,v77
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_load_si128((__m128i*)v77.u8));
	// addi r10,r8,24
	ctx.r10.s64 = ctx.r8.s64 + 24;
	// vor128 v9,v78,v78
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v78.u8));
	// stw r26,1440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1440, r26.u32);
	// vaddsws v13,v13,v12
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// vsubsws v13,v11,v13
	temp.s64 = int64_t(ctx.v11.s32[0]) - int64_t(ctx.v13.s32[0]);
	ctx.v13.s32[0] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[1]) - int64_t(ctx.v13.s32[1]);
	ctx.v13.s32[1] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[2]) - int64_t(ctx.v13.s32[2]);
	ctx.v13.s32[2] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	temp.s64 = int64_t(ctx.v11.s32[3]) - int64_t(ctx.v13.s32[3]);
	ctx.v13.s32[3] = temp.s64 > INT_MAX ? INT_MAX : temp.s64 < INT_MIN ? INT_MIN : temp.s64;
	// vslw v12,v0,v13
	ctx.v12.u32[0] = ctx.v0.u32[0] << (ctx.v13.u8[0] & 0x1F);
	ctx.v12.u32[1] = ctx.v0.u32[1] << (ctx.v13.u8[4] & 0x1F);
	ctx.v12.u32[2] = ctx.v0.u32[2] << (ctx.v13.u8[8] & 0x1F);
	ctx.v12.u32[3] = ctx.v0.u32[3] << (ctx.v13.u8[12] & 0x1F);
	// vsrw128 v11,v12,v81
	ctx.v11.u32[0] = ctx.v12.u32[0] >> (v81.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v12.u32[1] >> (v81.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v12.u32[2] >> (v81.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v12.u32[3] >> (v81.u8[12] & 0x1F);
	// vspltw v8,v11,0
	_mm_store_si128((__m128i*)ctx.v8.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vcfux v0,v8,0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_cvtepu32_ps_(_mm_load_si128((__m128i*)ctx.v8.u32)));
	// vmaddfp v7,v0,v9,v10
	_mm_store_ps(ctx.v7.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v10.f32)));
	// vor128 v88,v7,v7
	_mm_store_si128((__m128i*)v88.u8, _mm_load_si128((__m128i*)ctx.v7.u8));
	// stvx128 v88,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v88.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e33d0
	goto loc_822E33D0;
	// addi r9,r1,1424
	ctx.r9.s64 = ctx.r1.s64 + 1424;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r6,r17
	ctx.r6.u64 = r17.u64;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// mr r4,r18
	ctx.r4.u64 = r18.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82234108
	sub_82234108(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// b 0x822e33d0
	goto loc_822E33D0;
	// li r26,1
	r26.s64 = 1;
loc_822E33D0:
	// addi r31,r16,8
	r31.s64 = r16.s64 + 8;
	// cmpw cr6,r20,r29
	cr6.compare<int32_t>(r20.s32, r29.s32, xer);
	// ble cr6,0x822e33e0
	if (!cr6.gt) goto loc_822E33E0;
	// mr r29,r20
	r29.u64 = r20.u64;
loc_822E33E0:
	// cmpw cr6,r21,r29
	cr6.compare<int32_t>(r21.s32, r29.s32, xer);
	// ble cr6,0x822e33ec
	if (!cr6.gt) goto loc_822E33EC;
	// mr r29,r21
	r29.u64 = r21.u64;
loc_822E33EC:
	// cmpw cr6,r22,r29
	cr6.compare<int32_t>(r22.s32, r29.s32, xer);
	// ble cr6,0x822e33f8
	if (!cr6.gt) goto loc_822E33F8;
	// mr r29,r22
	r29.u64 = r22.u64;
loc_822E33F8:
	// lwz r11,208(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x822e343c
	if (!cr6.eq) {
		// ERROR 822E343C
		return;
	}
	// addi r11,r1,192
	r11.s64 = ctx.r1.s64 + 192;
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
}

__attribute__((alias("__imp__sub_822E38E0"))) PPC_WEAK_FUNC(sub_822E38E0);
PPC_FUNC_IMPL(__imp__sub_822E38E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// addi r5,r11,-26584
	ctx.r5.s64 = r11.s64 + -26584;
	// addi r4,r10,10316
	ctx.r4.s64 = ctx.r10.s64 + 10316;
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x82a1f740
	sub_82A1F740(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lwz r7,8(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// li r31,3
	r31.s64 = 3;
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r5,r8,10324
	ctx.r5.s64 = ctx.r8.s64 + 10324;
	// lfs f0,-28512(r9)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + -28512);
	f0.f64 = double(temp.f32);
	// li r4,-2
	ctx.r4.s64 = -2;
	// stfs f0,0(r7)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r7.u32 + 0, temp.u32);
	// stw r31,4(r7)
	PPC_STORE_U32(ctx.r7.u32 + 4, r31.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r6,r11,8
	ctx.r6.s64 = r11.s64 + 8;
	// stw r6,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r6.u32);
	// bl 0x82a1ec08
	sub_82A1EC08(ctx, base);
	// lis r3,-32255
	ctx.r3.s64 = -2113863680;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r4,-2
	ctx.r4.s64 = -2;
	// addi r5,r11,10328
	ctx.r5.s64 = r11.s64 + 10328;
	// lfd f0,4048(r3)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r3.u32 + 4048);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// frsp f0,f0
	f0.f64 = double(float(f0.f64));
	// stfs f0,0(r10)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r10.u32 + 0, temp.u32);
	// stw r31,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r31.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r9,r11,8
	ctx.r9.s64 = r11.s64 + 8;
	// stw r9,8(r30)
	PPC_STORE_U32(r30.u32 + 8, ctx.r9.u32);
	// bl 0x82a1ec08
	sub_82A1EC08(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822E39A0"))) PPC_WEAK_FUNC(sub_822E39A0);
PPC_FUNC_IMPL(__imp__sub_822E39A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f14{};
	PPCRegister f15{};
	PPCRegister f16{};
	PPCRegister f17{};
	PPCRegister f18{};
	PPCRegister f19{};
	PPCRegister f20{};
	PPCRegister f21{};
	PPCRegister f22{};
	PPCRegister f23{};
	PPCRegister f24{};
	PPCRegister f25{};
	PPCRegister f26{};
	PPCRegister f27{};
	PPCRegister f28{};
	PPCRegister f29{};
	PPCRegister f30{};
	PPCRegister f31{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCVRegister v121{};
	PPCVRegister v122{};
	PPCVRegister v123{};
	PPCVRegister v124{};
	PPCVRegister v125{};
	PPCVRegister v126{};
	PPCVRegister v127{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x82ca74d0
	// addi r12,r1,-304
	r12.s64 = ctx.r1.s64 + -304;
	// bl 0x82ffff2c
	// stwu r1,-3024(r1)
	ea = -3024 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// vor128 v125,v1,v1
	_mm_store_si128((__m128i*)v125.u8, _mm_load_si128((__m128i*)ctx.v1.u8));
	// li r29,0
	r29.s64 = 0;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// stw r29,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r29.u32);
	// stvx128 v125,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e0f98
	sub_821E0F98(ctx, base);
	// lwz r10,8(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// vor128 v1,v125,v125
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v125.u8));
	// bl 0x821ff5b8
	sub_821FF5B8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// mr r15,r29
	r15.u64 = r29.u64;
	// stw r29,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r29.u32);
	// stw r29,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r29.u32);
	// stw r15,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r15.u32);
	// stw r29,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r29.u32);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// rlwinm r9,r10,13,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 13) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e3b2c
	if (cr6.eq) goto loc_822E3B2C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e3a5c
	if (cr6.eq) goto loc_822E3A5C;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,19(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 19);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r11,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e3b30
	goto loc_822E3B30;
loc_822E3A5C:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e3ad0
	if (!cr0.gt) goto loc_822E3AD0;
loc_822E3A80:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,19
	cr6.compare<int32_t>(ctx.r7.s32, 19, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e3aa0
	if (cr6.lt) goto loc_822E3AA0;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E3AA0:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e3abc
	if (cr6.eq) goto loc_822E3ABC;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e3ac4
	goto loc_822E3AC4;
loc_822E3ABC:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E3AC4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e3a80
	if (cr6.gt) goto loc_822E3A80;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E3AD0:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e3b18
	if (cr6.eq) goto loc_822E3B18;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,19
	cr6.compare<int32_t>(ctx.r10.s32, 19, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e3af0
	if (cr6.gt) goto loc_822E3AF0;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E3AF0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e3b18
	if (!cr6.eq) goto loc_822E3B18;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1088(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1088, r11.u64);
	// lwz r11,1092(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1092);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e3b30
	goto loc_822E3B30;
loc_822E3B18:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r29,1088(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1088, r29.u32);
	// stw r11,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e3b30
	goto loc_822E3B30;
loc_822E3B2C:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E3B30:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// rlwinm r9,r10,11,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 11) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e3c50
	if (cr6.eq) goto loc_822E3C50;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e3b80
	if (cr6.eq) goto loc_822E3B80;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,21(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e3c54
	goto loc_822E3C54;
loc_822E3B80:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e3bf4
	if (!cr0.gt) goto loc_822E3BF4;
loc_822E3BA4:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,21
	cr6.compare<int32_t>(ctx.r7.s32, 21, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e3bc4
	if (cr6.lt) goto loc_822E3BC4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E3BC4:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e3be0
	if (cr6.eq) goto loc_822E3BE0;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e3be8
	goto loc_822E3BE8;
loc_822E3BE0:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E3BE8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e3ba4
	if (cr6.gt) goto loc_822E3BA4;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E3BF4:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e3c3c
	if (cr6.eq) goto loc_822E3C3C;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,21
	cr6.compare<int32_t>(ctx.r10.s32, 21, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e3c14
	if (cr6.gt) goto loc_822E3C14;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E3C14:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e3c3c
	if (!cr6.eq) goto loc_822E3C3C;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1048(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1048, r11.u64);
	// lwz r11,1052(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e3c54
	goto loc_822E3C54;
loc_822E3C3C:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r29,1048(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1048, r29.u32);
	// stw r11,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r11.u32);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e3c54
	goto loc_822E3C54;
loc_822E3C50:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E3C54:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// rlwinm r9,r10,26,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 26) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e3d80
	if (cr6.eq) goto loc_822E3D80;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e3ca8
	if (cr6.eq) goto loc_822E3CA8;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,6(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r15,r11
	r15.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// stw r15,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r15.u32);
	// b 0x822e3d84
	goto loc_822E3D84;
loc_822E3CA8:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e3d1c
	if (!cr0.gt) goto loc_822E3D1C;
loc_822E3CCC:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,6
	cr6.compare<int32_t>(ctx.r7.s32, 6, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e3cec
	if (cr6.lt) goto loc_822E3CEC;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E3CEC:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e3d08
	if (cr6.eq) goto loc_822E3D08;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e3d10
	goto loc_822E3D10;
loc_822E3D08:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E3D10:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e3ccc
	if (cr6.gt) goto loc_822E3CCC;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E3D1C:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e3d68
	if (cr6.eq) goto loc_822E3D68;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,6
	cr6.compare<int32_t>(ctx.r10.s32, 6, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e3d3c
	if (cr6.gt) goto loc_822E3D3C;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E3D3C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e3d68
	if (!cr6.eq) goto loc_822E3D68;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1064(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1064, r11.u64);
	// lwz r11,1068(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1068);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r15,r11
	r15.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// stw r15,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r15.u32);
	// b 0x822e3d84
	goto loc_822E3D84;
loc_822E3D68:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r29,1064(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1064, r29.u32);
	// mr r15,r11
	r15.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// stw r15,420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 420, r15.u32);
	// b 0x822e3d84
	goto loc_822E3D84;
loc_822E3D80:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E3D84:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r30,r10,-27596
	r30.s64 = ctx.r10.s64 + -27596;
	// addi r16,r9,-28176
	r16.s64 = ctx.r9.s64 + -28176;
	// stw r30,308(r1)
	PPC_STORE_U32(ctx.r1.u32 + 308, r30.u32);
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// stw r16,576(r1)
	PPC_STORE_U32(ctx.r1.u32 + 576, r16.u32);
	// rlwinm r9,r10,4,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0x1;
	// lfs f28,128(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 128);
	f28.f64 = double(temp.f32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e3f88
	if (cr6.eq) goto loc_822E3F88;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e3de8
	if (cr6.eq) goto loc_822E3DE8;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,60(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 60);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r31,4(r8)
	r31.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e3ea0
	goto loc_822E3EA0;
loc_822E3DE8:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e3e5c
	if (!cr0.gt) goto loc_822E3E5C;
loc_822E3E0C:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,60
	cr6.compare<int32_t>(ctx.r7.s32, 60, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e3e2c
	if (cr6.lt) goto loc_822E3E2C;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E3E2C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e3e48
	if (cr6.eq) goto loc_822E3E48;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e3e50
	goto loc_822E3E50;
loc_822E3E48:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E3E50:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e3e0c
	if (cr6.gt) goto loc_822E3E0C;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E3E5C:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e3e98
	if (cr6.eq) goto loc_822E3E98;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,60
	cr6.compare<int32_t>(ctx.r10.s32, 60, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e3e7c
	if (cr6.gt) goto loc_822E3E7C;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E3E7C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e3e98
	if (!cr6.eq) goto loc_822E3E98;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1080(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1080, r11.u64);
	// lwz r11,1084(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1084);
	// b 0x822e3e9c
	goto loc_822E3E9C;
loc_822E3E98:
	// stw r29,1080(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1080, r29.u32);
loc_822E3E9C:
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
loc_822E3EA0:
	// stw r31,372(r1)
	PPC_STORE_U32(ctx.r1.u32 + 372, r31.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822e3f88
	if (cr6.eq) goto loc_822E3F88;
	// lbz r11,187(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 187);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stb r11,2385(r22)
	PPC_STORE_U8(r22.u32 + 2385, r11.u8);
	// beq cr6,0x822e3f88
	if (cr6.eq) goto loc_822E3F88;
	// lbz r11,186(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e3f08
	if (cr6.eq) goto loc_822E3F08;
	// addi r9,r1,368
	ctx.r9.s64 = ctx.r1.s64 + 368;
	// stfs f28,368(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 368, temp.u32);
	// addi r8,r1,320
	ctx.r8.s64 = ctx.r1.s64 + 320;
	// stfs f28,320(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 320, temp.u32);
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// stfs f28,400(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// addi r10,r1,304
	ctx.r10.s64 = ctx.r1.s64 + 304;
	// stfs f28,304(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx128 v125,r0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)v125.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// vrlimi128 v125,v12,4,3
	_mm_store_ps(v125.f32, _mm_blend_ps(_mm_load_ps(v125.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v125,v13,3,2
	_mm_store_ps(v125.f32, _mm_blend_ps(_mm_load_ps(v125.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 78), 3));
	// b 0x822e3f88
	goto loc_822E3F88;
loc_822E3F08:
	// stw r29,704(r1)
	PPC_STORE_U32(ctx.r1.u32 + 704, r29.u32);
	// addi r4,r1,704
	ctx.r4.s64 = ctx.r1.s64 + 704;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e3f88
	if (cr6.eq) goto loc_822E3F88;
	// addi r3,r1,1568
	ctx.r3.s64 = ctx.r1.s64 + 1568;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// stfs f28,400(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r10,r1,720
	ctx.r10.s64 = ctx.r1.s64 + 720;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lvx128 v7,r0,r16
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r16.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,400
	ctx.r8.s64 = ctx.r1.s64 + 400;
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v0,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lfs f1,-25888(r11)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// vperm v11,v0,v12,v7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx128 v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821ee7c8
	sub_821EE7C8(ctx, base);
	// li r6,232
	ctx.r6.s64 = 232;
	// addi r5,r1,288
	ctx.r5.s64 = ctx.r1.s64 + 288;
	// lvlx v10,r31,r6
	temp.u32 = r31.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// lvx128 v8,r0,r5
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v125,v8,v9
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(v125.f32, _mm_mul_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32)));
loc_822E3F88:
	// lis r8,-32240
	ctx.r8.s64 = -2112880640;
	// lwz r7,828(r22)
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// lfs f15,-264(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + -264);
	f15.f64 = double(temp.f32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lfs f17,8344(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 8344);
	f17.f64 = double(temp.f32);
	// lis r9,-32245
	ctx.r9.s64 = -2113208320;
	// addi r6,r11,63
	ctx.r6.s64 = r11.s64 + 63;
	// addi r5,r10,-28336
	ctx.r5.s64 = ctx.r10.s64 + -28336;
	// lfs f14,1204(r8)
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 1204);
	f14.f64 = double(temp.f32);
	// li r17,2080
	r17.s64 = 2080;
	// stw r6,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, ctx.r6.u32);
	// cmpwi cr6,r7,1
	cr6.compare<int32_t>(ctx.r7.s32, 1, xer);
	// stw r5,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, ctx.r5.u32);
	// addi r14,r9,-6572
	r14.s64 = ctx.r9.s64 + -6572;
	// beq cr6,0x822e5544
	if (cr6.eq) goto loc_822E5544;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lbz r10,27572(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 27572);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5544
	if (cr6.eq) goto loc_822E5544;
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lwz r11,-5932(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -5932);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e3ff0
	if (cr6.eq) goto loc_822E3FF0;
	// cmplw cr6,r11,r22
	cr6.compare<uint32_t>(r11.u32, r22.u32, xer);
	// bne cr6,0x822e5544
	if (!cr6.eq) goto loc_822E5544;
loc_822E3FF0:
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r29,0
	r29.s64 = 0;
	// mr r31,r29
	r31.u64 = r29.u64;
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// rlwinm r9,r10,5,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e410c
	if (cr6.eq) goto loc_822E410C;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e403c
	if (cr6.eq) goto loc_822E403C;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,59(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 59);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e4110
	goto loc_822E4110;
loc_822E403C:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e40b0
	if (!cr0.gt) goto loc_822E40B0;
loc_822E4060:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,59
	cr6.compare<int32_t>(ctx.r7.s32, 59, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e4080
	if (cr6.lt) goto loc_822E4080;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E4080:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e409c
	if (cr6.eq) goto loc_822E409C;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e40a4
	goto loc_822E40A4;
loc_822E409C:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E40A4:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e4060
	if (cr6.gt) goto loc_822E4060;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E40B0:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e40f8
	if (cr6.eq) goto loc_822E40F8;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,59
	cr6.compare<int32_t>(ctx.r10.s32, 59, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e40d0
	if (cr6.gt) goto loc_822E40D0;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E40D0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e40f8
	if (!cr6.eq) goto loc_822E40F8;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1112, r11.u64);
	// lwz r11,1116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1116);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e4110
	goto loc_822E4110;
loc_822E40F8:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r29,1112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1112, r29.u32);
	// mr r31,r11
	r31.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e4110
	goto loc_822E4110;
loc_822E410C:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E4110:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// li r18,200
	r18.s64 = 200;
	// li r19,255
	r19.s64 = 255;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4230
	if (cr6.eq) goto loc_822E4230;
	// addi r4,r1,272
	ctx.r4.s64 = ctx.r1.s64 + 272;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82267268
	sub_82267268(ctx, base);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lvx128 v127,r0,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x822e416c
	if (cr6.eq) goto loc_822E416C;
	// stb r29,513(r1)
	PPC_STORE_U8(ctx.r1.u32 + 513, r29.u8);
	// vor128 v2,v127,v127
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v127.u8));
	// stb r19,515(r1)
	PPC_STORE_U8(ctx.r1.u32 + 515, r19.u8);
	// addi r4,r1,512
	ctx.r4.s64 = ctx.r1.s64 + 512;
	// stb r19,514(r1)
	PPC_STORE_U8(ctx.r1.u32 + 514, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// stb r19,512(r1)
	PPC_STORE_U8(ctx.r1.u32 + 512, r19.u8);
	// fmr f1,f15
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f15.f64;
	// lvx128 v1,r22,r17
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32 + r17.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290c7d8
	sub_8290C7D8(ctx, base);
loc_822E416C:
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82683fe8
	sub_82683FE8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4230
	if (cr6.eq) goto loc_822E4230;
	// stfs f28,400(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// addi r11,r1,400
	r11.s64 = ctx.r1.s64 + 400;
	// stfs f28,304(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// addi r9,r1,304
	ctx.r9.s64 = ctx.r1.s64 + 304;
	// stfs f28,384(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// addi r6,r1,272
	ctx.r6.s64 = ctx.r1.s64 + 272;
	// lvx128 v9,r22,r17
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32 + r17.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,384
	ctx.r8.s64 = ctx.r1.s64 + 384;
	// stb r19,530(r1)
	PPC_STORE_U8(ctx.r1.u32 + 530, r19.u8);
	// lfs f0,-252(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -252);
	f0.f64 = double(temp.f32);
	// addi r5,r1,400
	ctx.r5.s64 = ctx.r1.s64 + 400;
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stb r18,529(r1)
	PPC_STORE_U8(ctx.r1.u32 + 529, r18.u8);
	// lvlx v10,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stb r18,528(r1)
	PPC_STORE_U8(ctx.r1.u32 + 528, r18.u8);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// stb r19,531(r1)
	PPC_STORE_U8(ctx.r1.u32 + 531, r19.u8);
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r11,r1,304
	r11.s64 = ctx.r1.s64 + 304;
	// stfs f28,400(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// stfs f28,304(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r4,r1,528
	ctx.r4.s64 = ctx.r1.s64 + 528;
	// lvlx v8,0,r5
	temp.u32 = ctx.r5.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvlx v7,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// fmr f1,f15
	ctx.f1.f64 = f15.f64;
	// lvlx v6,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stfs f28,288(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v11,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 57), 4));
	// vrlimi128 v7,v8,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// vrlimi128 v6,v12,4,3
	_mm_store_ps(ctx.v6.f32, _mm_blend_ps(_mm_load_ps(ctx.v6.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v7,v10,3,2
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 78), 3));
	// vrlimi128 v13,v6,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v6.f32), 78), 3));
	// vaddfp v1,v9,v7
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v7.f32)));
	// vaddfp128 v2,v127,v13
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v13.f32)));
	// bl 0x8290c7d8
	sub_8290C7D8(ctx, base);
loc_822E4230:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lbz r10,-5928(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -5928);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e4304
	if (cr6.eq) goto loc_822E4304;
	// lwz r11,1408(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1408);
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// beq cr6,0x822e4254
	if (cr6.eq) goto loc_822E4254;
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
loc_822E4254:
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// beq cr6,0x822e4264
	if (cr6.eq) goto loc_822E4264;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E4264:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// mr r11,r19
	r11.u64 = r19.u64;
	// beq cr6,0x822e4274
	if (cr6.eq) goto loc_822E4274;
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E4274:
	// addi r3,r1,1872
	ctx.r3.s64 = ctx.r1.s64 + 1872;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// stfs f28,384(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 384, temp.u32);
	// mr r31,r11
	r31.u64 = r11.u64;
	// stfs f17,400(r1)
	temp.f32 = float(f17.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// mr r30,r10
	r30.u64 = ctx.r10.u64;
	// stfs f28,272(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// mr r28,r9
	r28.u64 = ctx.r9.u64;
	// stfs f28,288(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r3,r1,1904
	ctx.r3.s64 = ctx.r1.s64 + 1904;
	// lvx128 v127,r0,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r31,538(r1)
	PPC_STORE_U8(ctx.r1.u32 + 538, r31.u8);
	// stb r30,537(r1)
	PPC_STORE_U8(ctx.r1.u32 + 537, r30.u8);
	// stb r28,536(r1)
	PPC_STORE_U8(ctx.r1.u32 + 536, r28.u8);
	// stb r19,539(r1)
	PPC_STORE_U8(ctx.r1.u32 + 539, r19.u8);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// lvlx v0,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,384
	ctx.r8.s64 = ctx.r1.s64 + 384;
	// addi r7,r1,400
	ctx.r7.s64 = ctx.r1.s64 + 400;
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// addi r4,r1,536
	ctx.r4.s64 = ctx.r1.s64 + 536;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v13,v0,4,3
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v12,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vrlimi128 v13,v11,3,2
	_mm_store_ps(ctx.v13.f32, _mm_blend_ps(_mm_load_ps(ctx.v13.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// vaddfp128 v2,v127,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v13.f32)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
loc_822E4304:
	// lwz r11,372(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mr r20,r29
	r20.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4324
	if (cr6.eq) goto loc_822E4324;
	// lwz r11,168(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 168);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822e4324
	if (!cr6.eq) goto loc_822E4324;
	// li r20,1
	r20.s64 = 1;
loc_822E4324:
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// mr r21,r29
	r21.u64 = r29.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r9,48(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 48);
	// rlwinm r8,r9,19,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 19) & 0x1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822e4440
	if (cr6.eq) goto loc_822E4440;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e4370
	if (cr6.eq) goto loc_822E4370;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,109(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 109);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e4444
	goto loc_822E4444;
loc_822E4370:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e43e4
	if (!cr0.gt) goto loc_822E43E4;
loc_822E4394:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,109
	cr6.compare<int32_t>(ctx.r7.s32, 109, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e43b4
	if (cr6.lt) goto loc_822E43B4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E43B4:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e43d0
	if (cr6.eq) goto loc_822E43D0;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e43d8
	goto loc_822E43D8;
loc_822E43D0:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E43D8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e4394
	if (cr6.gt) goto loc_822E4394;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E43E4:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e442c
	if (cr6.eq) goto loc_822E442C;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,109
	cr6.compare<int32_t>(ctx.r10.s32, 109, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e4404
	if (cr6.gt) goto loc_822E4404;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E4404:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e442c
	if (!cr6.eq) goto loc_822E442C;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1128, r11.u64);
	// lwz r11,1132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1132);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e4444
	goto loc_822E4444;
loc_822E442C:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r29,1128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1128, r29.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e4444
	goto loc_822E4444;
loc_822E4440:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E4444:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4460
	if (cr6.eq) goto loc_822E4460;
	// lwz r11,80(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822e4460
	if (cr6.eq) goto loc_822E4460;
	// li r21,1
	r21.s64 = 1;
loc_822E4460:
	// lis r11,-32242
	r11.s64 = -2113011712;
	// lwz r10,2004(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 2004);
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// stw r14,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r14.u32);
	// lis r8,-32243
	ctx.r8.s64 = -2113077248;
	// lis r7,-32241
	ctx.r7.s64 = -2112946176;
	// lis r6,-32241
	ctx.r6.s64 = -2112946176;
	// lis r5,-32241
	ctx.r5.s64 = -2112946176;
	// addi r4,r11,19264
	ctx.r4.s64 = r11.s64 + 19264;
	// addi r3,r9,-25276
	ctx.r3.s64 = ctx.r9.s64 + -25276;
	// addi r11,r8,31000
	r11.s64 = ctx.r8.s64 + 31000;
	// stw r4,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, ctx.r4.u32);
	// addi r9,r7,-25260
	ctx.r9.s64 = ctx.r7.s64 + -25260;
	// stw r3,340(r1)
	PPC_STORE_U32(ctx.r1.u32 + 340, ctx.r3.u32);
	// addi r8,r6,-25248
	ctx.r8.s64 = ctx.r6.s64 + -25248;
	// stw r11,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r11.u32);
	// addi r7,r5,-25240
	ctx.r7.s64 = ctx.r5.s64 + -25240;
	// stw r9,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, ctx.r9.u32);
	// stw r8,356(r1)
	PPC_STORE_U32(ctx.r1.u32 + 356, ctx.r8.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r7,360(r1)
	PPC_STORE_U32(ctx.r1.u32 + 360, ctx.r7.u32);
	// beq cr6,0x822e44d8
	if (cr6.eq) goto loc_822E44D8;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e44d8
	if (cr6.eq) goto loc_822E44D8;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// li r23,1
	r23.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e44dc
	if (!cr6.eq) goto loc_822E44DC;
loc_822E44D8:
	// mr r23,r29
	r23.u64 = r29.u64;
loc_822E44DC:
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// mr r31,r29
	r31.u64 = r29.u64;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
	// lwz r9,36(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// rlwinm r8,r9,30,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822e45f8
	if (cr6.eq) goto loc_822E45F8;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e4528
	if (cr6.eq) goto loc_822E4528;
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e45fc
	goto loc_822E45FC;
loc_822E4528:
	// lwz r9,72(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// addi r6,r11,68
	ctx.r6.s64 = r11.s64 + 68;
	// lwz r8,76(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r29.u32);
	// subf r7,r9,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r9.s64;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
	// srawi. r11,r7,3
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7) != 0);
	r11.s64 = ctx.r7.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822e459c
	if (!cr0.gt) goto loc_822E459C;
loc_822E454C:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,2
	cr6.compare<int32_t>(ctx.r7.s32, 2, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822e456c
	if (cr6.lt) goto loc_822E456C;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
loc_822E456C:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e4588
	if (cr6.eq) goto loc_822E4588;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822e4590
	goto loc_822E4590;
loc_822E4588:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822E4590:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822e454c
	if (cr6.gt) goto loc_822E454C;
	// stw r10,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r10.u32);
loc_822E459C:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e45e4
	if (cr6.eq) goto loc_822E45E4;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e45bc
	if (cr6.gt) goto loc_822E45BC;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E45BC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e45e4
	if (!cr6.eq) goto loc_822E45E4;
	// ld r11,272(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// std r11,1072(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1072, r11.u64);
	// lwz r11,1076(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1076);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e45fc
	goto loc_822E45FC;
loc_822E45E4:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r29,1072(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1072, r29.u32);
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e45fc
	goto loc_822E45FC;
loc_822E45F8:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E45FC:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4618
	if (cr6.eq) goto loc_822E4618;
	// lbz r11,170(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 170);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4618
	if (cr6.eq) goto loc_822E4618;
	// li r31,1
	r31.s64 = 1;
loc_822E4618:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// lis r7,-32241
	ctx.r7.s64 = -2112946176;
	// lis r6,-32241
	ctx.r6.s64 = -2112946176;
	// lbz r11,-5927(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + -5927);
	// lis r5,-32241
	ctx.r5.s64 = -2112946176;
	// lis r4,-32241
	ctx.r4.s64 = -2112946176;
	// lis r3,-32241
	ctx.r3.s64 = -2112946176;
	// addi r10,r10,-25236
	ctx.r10.s64 = ctx.r10.s64 + -25236;
	// addi r9,r9,-25220
	ctx.r9.s64 = ctx.r9.s64 + -25220;
	// addi r8,r8,-25208
	ctx.r8.s64 = ctx.r8.s64 + -25208;
	// stw r10,400(r1)
	PPC_STORE_U32(ctx.r1.u32 + 400, ctx.r10.u32);
	// addi r7,r7,-25184
	ctx.r7.s64 = ctx.r7.s64 + -25184;
	// stw r9,404(r1)
	PPC_STORE_U32(ctx.r1.u32 + 404, ctx.r9.u32);
	// addi r6,r6,-25168
	ctx.r6.s64 = ctx.r6.s64 + -25168;
	// stw r8,408(r1)
	PPC_STORE_U32(ctx.r1.u32 + 408, ctx.r8.u32);
	// addi r5,r5,-25152
	ctx.r5.s64 = ctx.r5.s64 + -25152;
	// stw r7,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, ctx.r7.u32);
	// addi r4,r4,-25132
	ctx.r4.s64 = ctx.r4.s64 + -25132;
	// stw r6,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, ctx.r6.u32);
	// addi r3,r3,-25112
	ctx.r3.s64 = ctx.r3.s64 + -25112;
	// stw r5,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, ctx.r5.u32);
	// stw r4,392(r1)
	PPC_STORE_U32(ctx.r1.u32 + 392, ctx.r4.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r3,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, ctx.r3.u32);
	// beq cr6,0x822e497c
	if (cr6.eq) goto loc_822E497C;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r3,r1,1936
	ctx.r3.s64 = ctx.r1.s64 + 1936;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,76(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,320(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r7,r1,272
	ctx.r7.s64 = ctx.r1.s64 + 272;
	// lis r6,-32241
	ctx.r6.s64 = -2112946176;
	// lwz r4,312(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// lis r5,-32244
	ctx.r5.s64 = -2113142784;
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// lis r3,-32241
	ctx.r3.s64 = -2112946176;
	// lvx128 v0,r0,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32241
	r11.s64 = -2112946176;
	// vand v12,v13,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r10,r6,-25088
	ctx.r10.s64 = ctx.r6.s64 + -25088;
	// addi r8,r3,-25080
	ctx.r8.s64 = ctx.r3.s64 + -25080;
	// addi r9,r5,-15592
	ctx.r9.s64 = ctx.r5.s64 + -15592;
	// stw r10,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, ctx.r10.u32);
	// addi r6,r11,-25072
	ctx.r6.s64 = r11.s64 + -25072;
	// stw r8,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, ctx.r8.u32);
	// vmsum3fp128 v11,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32), 0xEF));
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// stw r9,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r9.u32);
	// stw r6,300(r1)
	PPC_STORE_U32(ctx.r1.u32 + 300, ctx.r6.u32);
	// stvx128 v11,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,272(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	f0.f64 = double(temp.f32);
	// fsqrts f31,f0
	f31.f64 = double(float(sqrt(f0.f64)));
	// bl 0x82211798
	sub_82211798(ctx, base);
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// lwz r25,272(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// bl 0x82b73880
	sub_82B73880(ctx, base);
	// lwz r10,1424(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// lwz r5,848(r22)
	ctx.r5.u64 = PPC_LOAD_U32(r22.u32 + 848);
	// lwz r4,8(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r3,12(r5)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r5.u32 + 12);
	// cmpw cr6,r3,r4
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r4.s32, xer);
	// beq cr6,0x822e473c
	if (cr6.eq) goto loc_822E473C;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r24,r11,-25068
	r24.s64 = r11.s64 + -25068;
	// b 0x822e4744
	goto loc_822E4744;
loc_822E473C:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r24,r11,-25064
	r24.s64 = r11.s64 + -25064;
loc_822E4744:
	// clrlwi r9,r31,24
	ctx.r9.u64 = r31.u32 & 0xFF;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// addi r31,r11,31244
	r31.s64 = r11.s64 + 31244;
	// beq cr6,0x822e4764
	if (cr6.eq) goto loc_822E4764;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r26,r11,-25060
	r26.s64 = r11.s64 + -25060;
	// b 0x822e4768
	goto loc_822E4768;
loc_822E4764:
	// mr r26,r31
	r26.u64 = r31.u64;
loc_822E4768:
	// clrlwi r11,r23,24
	r11.u64 = r23.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4780
	if (cr6.eq) goto loc_822E4780;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r27,r11,-25052
	r27.s64 = r11.s64 + -25052;
	// b 0x822e4784
	goto loc_822E4784;
loc_822E4780:
	// mr r27,r31
	r27.u64 = r31.u64;
loc_822E4784:
	// clrlwi r11,r21,24
	r11.u64 = r21.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e479c
	if (cr6.eq) goto loc_822E479C;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r28,r11,-25048
	r28.s64 = r11.s64 + -25048;
	// b 0x822e47a0
	goto loc_822E47A0;
loc_822E479C:
	// mr r28,r31
	r28.u64 = r31.u64;
loc_822E47A0:
	// clrlwi r11,r20,24
	r11.u64 = r20.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e47b8
	if (cr6.eq) goto loc_822E47B8;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r29,r11,-25044
	r29.s64 = r11.s64 + -25044;
	// b 0x822e47bc
	goto loc_822E47BC;
loc_822E47B8:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_822E47BC:
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x822e47d4
	if (!cr6.eq) goto loc_822E47D4;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r30,r11,-25040
	r30.s64 = r11.s64 + -25040;
	// b 0x822e47dc
	goto loc_822E47DC;
loc_822E47D4:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r30,r11,-25032
	r30.s64 = r11.s64 + -25032;
loc_822E47DC:
	// lwz r23,372(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// lbz r11,186(r23)
	r11.u64 = PPC_LOAD_U8(r23.u32 + 186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e47f4
	if (cr6.eq) goto loc_822E47F4;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r31,r11,-25024
	r31.s64 = r11.s64 + -25024;
loc_822E47F4:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// addi r3,r1,1968
	ctx.r3.s64 = ctx.r1.s64 + 1968;
	// lwz r21,4(r22)
	r21.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// fmr f2,f28
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// lfs f3,8364(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8364);
	ctx.f3.f64 = double(temp.f32);
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// addi r3,r1,2000
	ctx.r3.s64 = ctx.r1.s64 + 2000;
	// lvx128 v127,r0,r10
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// vaddfp128 v13,v0,v127
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v127.f32)));
	// lwz r11,400(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stvx128 v13,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x822e4848
	if (!cr6.eq) goto loc_822E4848;
	// lwz r11,368(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// b 0x822e484c
	goto loc_822E484C;
loc_822E4848:
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E484C:
	// lwz r9,320(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// addi r8,r1,432
	ctx.r8.s64 = ctx.r1.s64 + 432;
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// lwz r7,312(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// addi r6,r1,288
	ctx.r6.s64 = ctx.r1.s64 + 288;
	// lfs f13,2456(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2456);
	ctx.f13.f64 = double(temp.f32);
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// lwz r5,1424(r22)
	ctx.r5.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// stfd f2,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.f2.u64);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r4,16(r23)
	ctx.r4.u64 = PPC_LOAD_U32(r23.u32 + 16);
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lfs f0,2836(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2836);
	f0.f64 = double(temp.f32);
	// lwz r10,148(r7)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + 148);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * f0.f64));
	// lwz r3,12(r23)
	ctx.r3.u64 = PPC_LOAD_U32(r23.u32 + 12);
	// addi r23,r1,336
	r23.s64 = ctx.r1.s64 + 336;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r20,0(r5)
	r20.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// lwz r9,4(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r21,148(r15)
	r21.u64 = PPC_LOAD_U32(r15.u32 + 148);
	// lis r5,-32241
	ctx.r5.s64 = -2112946176;
	// stw r4,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r4.u32);
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
	// stw r3,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r3.u32);
	// addi r5,r5,-25016
	ctx.r5.s64 = ctx.r5.s64 + -25016;
	// lwzx r11,r10,r6
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r6.u32);
	// addi r4,r1,384
	ctx.r4.s64 = ctx.r1.s64 + 384;
	// stw r9,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r9.u32);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// stw r21,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r21.u32);
	// fctiwz f11,f12
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f11.s64 = (ctx.f12.f64 > double(INT_MAX)) ? INT_MAX : _mm_cvttsd_si32(_mm_load_sd(&ctx.f12.f64));
	// stfd f11,272(r1)
	PPC_STORE_U64(ctx.r1.u32 + 272, ctx.f11.u64);
	// stw r22,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r22.u32);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, r11.u32);
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// stw r20,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r20.u32);
	// addi r6,r11,4
	ctx.r6.s64 = r11.s64 + 4;
	// lwz r11,276(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// stw r24,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r24.u32);
	// stvx128 v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r26.u32);
	// lfs f10,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f10.f64 = double(temp.f32);
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r27.u32);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r11.u32);
	// lwz r10,828(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// fsqrts f1,f10
	ctx.f1.f64 = double(float(sqrt(ctx.f10.f64)));
	// stfd f1,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.f1.u64);
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r8,r23
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + r23.u32);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// bl 0x8290d7d0
	sub_8290D7D0(ctx, base);
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x822e4d7c
	if (cr6.eq) goto loc_822E4D7C;
	// addi r11,r25,4
	r11.s64 = r25.s64 + 4;
loc_822E4948:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e4948
	if (!cr0.eq) goto loc_822E4948;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e4d7c
	if (!cr6.eq) goto loc_822E4D7C;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// b 0x822e4d70
	goto loc_822E4D70;
loc_822E497C:
	// addi r3,r1,2096
	ctx.r3.s64 = ctx.r1.s64 + 2096;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// bl 0x8263e5c0
	sub_8263E5C0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,1376
	ctx.r3.s64 = ctx.r1.s64 + 1376;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// bl 0x8263e5c0
	sub_8263E5C0(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// lwz r3,4(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82933fd8
	sub_82933FD8(ctx, base);
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// lwz r3,4(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82933fd8
	sub_82933FD8(ctx, base);
	// mr r11,r29
	r11.u64 = r29.u64;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r4,36(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// rlwinm r3,r4,30,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e4a70
	if (cr6.eq) goto loc_822E4A70;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4a08
	if (cr6.eq) goto loc_822E4A08;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e4a74
	goto loc_822E4A74;
loc_822E4A08:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e4a60
	if (cr6.eq) goto loc_822E4A60;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e4a3c
	if (cr6.gt) goto loc_822E4A3C;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E4A3C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e4a60
	if (!cr6.eq) goto loc_822E4A60;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,1096(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1096, r11.u64);
	// lwz r11,1100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1100);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x822e4a74
	goto loc_822E4A74;
loc_822E4A60:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r29,1096(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1096, r29.u32);
	// b 0x822e4a74
	goto loc_822E4A74;
loc_822E4A70:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E4A74:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e4a8c
	if (cr6.eq) goto loc_822E4A8C;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x824c6410
	sub_824C6410(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_822E4A8C:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r25,0
	r25.s64 = 0;
	// mr r30,r25
	r30.u64 = r25.u64;
	// mr r11,r25
	r11.u64 = r25.u64;
	// lwz r10,36(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// rlwinm r9,r10,30,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 30) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e4b48
	if (cr6.eq) goto loc_822E4B48;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4ae0
	if (cr6.eq) goto loc_822E4AE0;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e4b4c
	goto loc_822E4B4C;
loc_822E4AE0:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e4b38
	if (cr6.eq) goto loc_822E4B38;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e4b14
	if (cr6.gt) goto loc_822E4B14;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_822E4B14:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e4b38
	if (!cr6.eq) goto loc_822E4B38;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,960(r1)
	PPC_STORE_U64(ctx.r1.u32 + 960, r11.u64);
	// lwz r11,964(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 964);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x822e4b4c
	goto loc_822E4B4C;
loc_822E4B38:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r25,960(r1)
	PPC_STORE_U32(ctx.r1.u32 + 960, r25.u32);
	// b 0x822e4b4c
	goto loc_822E4B4C;
loc_822E4B48:
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
loc_822E4B4C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e4b5c
	if (cr6.eq) goto loc_822E4B5C;
	// lbz r30,170(r11)
	r30.u64 = PPC_LOAD_U8(r11.u32 + 170);
loc_822E4B5C:
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// lwz r4,312(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// bl 0x82211798
	sub_82211798(ctx, base);
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lbz r10,-5926(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -5926);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lwz r24,272(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// beq cr6,0x822e4b8c
	if (cr6.eq) goto loc_822E4B8C;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x822d8c58
	sub_822D8C58(ctx, base);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
loc_822E4B8C:
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4ba4
	if (cr6.eq) goto loc_822E4BA4;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r26,r11,-24944
	r26.s64 = r11.s64 + -24944;
	// b 0x822e4bac
	goto loc_822E4BAC;
loc_822E4BA4:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r26,r11,-24940
	r26.s64 = r11.s64 + -24940;
loc_822E4BAC:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822e4bd4
	if (cr6.eq) goto loc_822E4BD4;
	// lwz r11,128(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 128);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e4bc8
	if (!cr6.eq) goto loc_822E4BC8;
	// lwz r27,368(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// b 0x822e4bdc
	goto loc_822E4BDC;
loc_822E4BC8:
	// lwz r11,128(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 128);
	// lwz r27,0(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e4bdc
	goto loc_822E4BDC;
loc_822E4BD4:
	// lis r11,-32244
	r11.s64 = -2113142784;
	// addi r27,r11,30216
	r27.s64 = r11.s64 + 30216;
loc_822E4BDC:
	// lwz r15,372(r1)
	r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r31,r11,31244
	r31.s64 = r11.s64 + 31244;
	// lbz r11,186(r15)
	r11.u64 = PPC_LOAD_U8(r15.u32 + 186);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4c00
	if (cr6.eq) goto loc_822E4C00;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r28,r11,-25024
	r28.s64 = r11.s64 + -25024;
	// b 0x822e4c04
	goto loc_822E4C04;
loc_822E4C00:
	// mr r28,r31
	r28.u64 = r31.u64;
loc_822E4C04:
	// clrlwi r11,r23,24
	r11.u64 = r23.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4c1c
	if (cr6.eq) goto loc_822E4C1C;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r29,r11,-25052
	r29.s64 = r11.s64 + -25052;
	// b 0x822e4c20
	goto loc_822E4C20;
loc_822E4C1C:
	// mr r29,r31
	r29.u64 = r31.u64;
loc_822E4C20:
	// clrlwi r11,r21,24
	r11.u64 = r21.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4c38
	if (cr6.eq) goto loc_822E4C38;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r30,r11,-25048
	r30.s64 = r11.s64 + -25048;
	// b 0x822e4c3c
	goto loc_822E4C3C;
loc_822E4C38:
	// mr r30,r31
	r30.u64 = r31.u64;
loc_822E4C3C:
	// clrlwi r11,r20,24
	r11.u64 = r20.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e4c50
	if (cr6.eq) goto loc_822E4C50;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r31,r11,-25044
	r31.s64 = r11.s64 + -25044;
loc_822E4C50:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// addi r3,r1,1824
	ctx.r3.s64 = ctx.r1.s64 + 1824;
	// lwz r23,4(r22)
	r23.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// fmr f2,f28
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// lfs f3,8364(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8364);
	ctx.f3.f64 = double(temp.f32);
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// addi r3,r1,1408
	ctx.r3.s64 = ctx.r1.s64 + 1408;
	// lvx128 v127,r0,r10
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lwz r9,320(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp128 v13,v0,v127
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v127.f32)));
	// lwz r7,168(r15)
	ctx.r7.u64 = PPC_LOAD_U32(r15.u32 + 168);
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// lwz r6,608(r15)
	ctx.r6.u64 = PPC_LOAD_U32(r15.u32 + 608);
	// rlwinm r3,r7,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r4,16(r15)
	ctx.r4.u64 = PPC_LOAD_U32(r15.u32 + 16);
	// addi r7,r1,432
	ctx.r7.s64 = ctx.r1.s64 + 432;
	// lwz r11,12(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 12);
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// vand128 v12,v125,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r1,400
	ctx.r9.s64 = ctx.r1.s64 + 400;
	// stw r26,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r26.u32);
	// stw r4,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r4.u32);
	// addi r26,r1,336
	r26.s64 = ctx.r1.s64 + 336;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// vmsum3fp128 v11,v12,v125
	_mm_store_ps(ctx.v11.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v125.f32), 0xEF));
	// lwzx r6,r3,r5
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + ctx.r5.u32);
	// stw r25,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r25.u32);
	// lis r5,-32241
	ctx.r5.s64 = -2112946176;
	// lwzx r3,r10,r9
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// stvx128 v13,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r27.u32);
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// addi r5,r5,-24936
	ctx.r5.s64 = ctx.r5.s64 + -24936;
	// stw r6,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r6.u32);
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// lwz r6,4(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stvx128 v11,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,272(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	f0.f64 = double(temp.f32);
	// fsqrts f1,f0
	ctx.f1.f64 = double(float(sqrt(f0.f64)));
	// stfd f1,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.f1.u64);
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// lwzx r8,r11,r26
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + r26.u32);
	// bl 0x8290d7d0
	sub_8290D7D0(ctx, base);
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x822e4d7c
	if (cr6.eq) goto loc_822E4D7C;
	// addi r11,r24,4
	r11.s64 = r24.s64 + 4;
loc_822E4D40:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e4d40
	if (!cr0.eq) goto loc_822E4D40;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e4d7c
	if (!cr6.eq) goto loc_822E4D7C;
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
loc_822E4D70:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E4D7C:
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// lwz r27,308(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lis r11,-31950
	r11.s64 = -2093875200;
	// addi r31,r11,-20468
	r31.s64 = r11.s64 + -20468;
	// lbz r9,-5925(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -5925);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e4e9c
	if (cr6.eq) goto loc_822E4E9C;
	// lis r28,-31950
	r28.s64 = -2093875200;
	// lwz r29,372(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// addi r5,r1,544
	ctx.r5.s64 = ctx.r1.s64 + 544;
	// lfs f1,9492(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// addi r26,r28,-20456
	r26.s64 = r28.s64 + -20456;
	// addi r30,r29,96
	r30.s64 = r29.s64 + 96;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lbz r9,-20456(r28)
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + -20456);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lbz r11,2(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 2);
	// lbz r10,1(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 1);
	// lbz r8,3(r26)
	ctx.r8.u64 = PPC_LOAD_U8(r26.u32 + 3);
	// stb r9,544(r1)
	PPC_STORE_U8(ctx.r1.u32 + 544, ctx.r9.u8);
	// stb r11,546(r1)
	PPC_STORE_U8(ctx.r1.u32 + 546, r11.u8);
	// stb r10,545(r1)
	PPC_STORE_U8(ctx.r1.u32 + 545, ctx.r10.u8);
	// stb r8,547(r1)
	PPC_STORE_U8(ctx.r1.u32 + 547, ctx.r8.u8);
	// bl 0x8290d588
	sub_8290D588(ctx, base);
	// lbz r11,2(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 2);
	// lbz r10,1(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// addi r29,r29,112
	r29.s64 = r29.s64 + 112;
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// addi r5,r1,552
	ctx.r5.s64 = ctx.r1.s64 + 552;
	// lbz r8,3(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfs f1,9492(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// stb r11,554(r1)
	PPC_STORE_U8(ctx.r1.u32 + 554, r11.u8);
	// stb r10,553(r1)
	PPC_STORE_U8(ctx.r1.u32 + 553, ctx.r10.u8);
	// stb r9,552(r1)
	PPC_STORE_U8(ctx.r1.u32 + 552, ctx.r9.u8);
	// stb r8,555(r1)
	PPC_STORE_U8(ctx.r1.u32 + 555, ctx.r8.u8);
	// bl 0x8290d588
	sub_8290D588(ctx, base);
	// lbz r10,1(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 1);
	// lbz r11,2(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 2);
	// addi r3,r1,2144
	ctx.r3.s64 = ctx.r1.s64 + 2144;
	// lbz r9,-20456(r28)
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + -20456);
	// lbz r8,3(r26)
	ctx.r8.u64 = PPC_LOAD_U8(r26.u32 + 3);
	// stb r10,561(r1)
	PPC_STORE_U8(ctx.r1.u32 + 561, ctx.r10.u8);
	// stb r11,562(r1)
	PPC_STORE_U8(ctx.r1.u32 + 562, r11.u8);
	// stb r9,560(r1)
	PPC_STORE_U8(ctx.r1.u32 + 560, ctx.r9.u8);
	// stb r8,563(r1)
	PPC_STORE_U8(ctx.r1.u32 + 563, ctx.r8.u8);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r4,r1,560
	ctx.r4.s64 = ctx.r1.s64 + 560;
	// lvx128 v2,r0,r30
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
	// lbz r11,2(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 2);
	// lbz r10,1(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 1);
	// addi r3,r1,1440
	ctx.r3.s64 = ctx.r1.s64 + 1440;
	// lbz r9,-20456(r28)
	ctx.r9.u64 = PPC_LOAD_U8(r28.u32 + -20456);
	// lbz r8,3(r26)
	ctx.r8.u64 = PPC_LOAD_U8(r26.u32 + 3);
	// stb r11,570(r1)
	PPC_STORE_U8(ctx.r1.u32 + 570, r11.u8);
	// stb r10,569(r1)
	PPC_STORE_U8(ctx.r1.u32 + 569, ctx.r10.u8);
	// stb r9,568(r1)
	PPC_STORE_U8(ctx.r1.u32 + 568, ctx.r9.u8);
	// stb r8,571(r1)
	PPC_STORE_U8(ctx.r1.u32 + 571, ctx.r8.u8);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r4,r1,568
	ctx.r4.s64 = ctx.r1.s64 + 568;
	// lvx128 v2,r0,r29
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
loc_822E4E9C:
	// lis r11,-31924
	r11.s64 = -2092171264;
	// lwz r26,320(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// lbz r10,-5924(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -5924);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e4f70
	if (cr6.eq) goto loc_822E4F70;
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r10,4(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,1856
	ctx.r3.s64 = ctx.r1.s64 + 1856;
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// lwz r4,124(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 124);
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r8,76(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 76);
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r6,r1,432
	ctx.r6.s64 = ctx.r1.s64 + 432;
	// addi r3,r1,1472
	ctx.r3.s64 = ctx.r1.s64 + 1472;
	// lfs f3,8364(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 8364);
	ctx.f3.f64 = double(temp.f32);
	// lwz r30,4(r22)
	r30.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// lvx128 v12,r0,r7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v10,v11,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v12.f32), 0xEF));
	// stvx128 v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,2064
	ctx.r3.s64 = ctx.r1.s64 + 2064;
	// lvx128 v127,r0,r5
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lvx128 v9,r0,r3
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,432
	r11.s64 = ctx.r1.s64 + 432;
	// vaddfp128 v8,v9,v127
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v8.f32, _mm_add_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(v127.f32)));
	// lfs f0,432(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	f0.f64 = double(temp.f32);
	// lfs f13,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f13.f64 = double(temp.f32);
	// fsqrts f2,f0
	ctx.f2.f64 = double(float(sqrt(f0.f64)));
	// stfd f2,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.f2.u64);
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// mr r6,r22
	ctx.r6.u64 = r22.u64;
	// addi r5,r10,-24880
	ctx.r5.s64 = ctx.r10.s64 + -24880;
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// stvx128 v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// fsqrts f1,f13
	ctx.f1.f64 = double(float(sqrt(ctx.f13.f64)));
	// stfd f1,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.f1.u64);
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// bl 0x8290d7d0
	sub_8290D7D0(ctx, base);
loc_822E4F70:
	// addi r28,r22,2080
	r28.s64 = r22.s64 + 2080;
	// lfs f0,-252(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + -252);
	f0.f64 = double(temp.f32);
	// addi r11,r1,432
	r11.s64 = ctx.r1.s64 + 432;
	// lvx128 v7,r0,r16
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r16.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,432
	ctx.r8.s64 = ctx.r1.s64 + 432;
	// lbz r9,2385(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 2385);
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvx128 v0,r0,r28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 + f0.f64));
	// stfs f12,272(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm128 v127,v0,v13,v7
	_mm_store_si128((__m128i*)v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx128 v127,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x822e4fbc
	if (cr6.eq) goto loc_822E4FBC;
	// li r29,1
	r29.s64 = 1;
	// b 0x822e4fc8
	goto loc_822E4FC8;
loc_822E4FBC:
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x8290c360
	sub_8290C360(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_822E4FC8:
	// li r30,100
	r30.s64 = 100;
	// stb r19,503(r1)
	PPC_STORE_U8(ctx.r1.u32 + 503, r19.u8);
	// addi r3,r1,1504
	ctx.r3.s64 = ctx.r1.s64 + 1504;
	// stb r30,500(r1)
	PPC_STORE_U8(ctx.r1.u32 + 500, r30.u8);
	// stb r30,501(r1)
	PPC_STORE_U8(ctx.r1.u32 + 501, r30.u8);
	// stb r30,502(r1)
	PPC_STORE_U8(ctx.r1.u32 + 502, r30.u8);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// vor128 v2,v127,v127
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v127.u8));
	// addi r4,r1,500
	ctx.r4.s64 = ctx.r1.s64 + 500;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
	// stb r30,534(r1)
	PPC_STORE_U8(ctx.r1.u32 + 534, r30.u8);
	// stb r30,533(r1)
	PPC_STORE_U8(ctx.r1.u32 + 533, r30.u8);
	// addi r5,r1,532
	ctx.r5.s64 = ctx.r1.s64 + 532;
	// stb r30,532(r1)
	PPC_STORE_U8(ctx.r1.u32 + 532, r30.u8);
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// stb r19,535(r1)
	PPC_STORE_U8(ctx.r1.u32 + 535, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfs f1,8732(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 8732);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8290d588
	sub_8290D588(ctx, base);
	// clrlwi r10,r29,24
	ctx.r10.u64 = r29.u32 & 0xFF;
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e504c
	if (cr6.eq) goto loc_822E504C;
	// stb r19,521(r1)
	PPC_STORE_U8(ctx.r1.u32 + 521, r19.u8);
	// addi r11,r1,520
	r11.s64 = ctx.r1.s64 + 520;
	// stb r19,523(r1)
	PPC_STORE_U8(ctx.r1.u32 + 523, r19.u8);
	// stb r29,522(r1)
	PPC_STORE_U8(ctx.r1.u32 + 522, r29.u8);
	// stb r29,520(r1)
	PPC_STORE_U8(ctx.r1.u32 + 520, r29.u8);
	// b 0x822e5060
	goto loc_822E5060;
loc_822E504C:
	// stb r19,494(r1)
	PPC_STORE_U8(ctx.r1.u32 + 494, r19.u8);
	// addi r11,r1,492
	r11.s64 = ctx.r1.s64 + 492;
	// stb r19,495(r1)
	PPC_STORE_U8(ctx.r1.u32 + 495, r19.u8);
	// stb r29,493(r1)
	PPC_STORE_U8(ctx.r1.u32 + 493, r29.u8);
	// stb r29,492(r1)
	PPC_STORE_U8(ctx.r1.u32 + 492, r29.u8);
loc_822E5060:
	// lbz r10,3(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 3);
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r4,r1,496
	ctx.r4.s64 = ctx.r1.s64 + 496;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + 2);
	// vaddfp128 v2,v127,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_load_ps(v127.f32), _mm_load_ps(v125.f32)));
	// fmr f1,f15
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f1.f64 = f15.f64;
	// stb r10,499(r1)
	PPC_STORE_U8(ctx.r1.u32 + 499, ctx.r10.u8);
	// stb r9,496(r1)
	PPC_STORE_U8(ctx.r1.u32 + 496, ctx.r9.u8);
	// stb r8,497(r1)
	PPC_STORE_U8(ctx.r1.u32 + 497, ctx.r8.u8);
	// stb r7,498(r1)
	PPC_STORE_U8(ctx.r1.u32 + 498, ctx.r7.u8);
	// bl 0x8290c7d8
	sub_8290C7D8(ctx, base);
	// lis r6,-31924
	ctx.r6.s64 = -2092171264;
	// lbz r5,-5923(r6)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r6.u32 + -5923);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822e51c4
	if (cr6.eq) goto loc_822E51C4;
	// addi r3,r1,1888
	ctx.r3.s64 = ctx.r1.s64 + 1888;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x821e70c8
	sub_821E70C8(ctx, base);
	// lfs f0,-256(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + -256);
	f0.f64 = double(temp.f32);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r29,506(r1)
	PPC_STORE_U8(ctx.r1.u32 + 506, r29.u8);
	// stb r19,505(r1)
	PPC_STORE_U8(ctx.r1.u32 + 505, r19.u8);
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// stb r29,504(r1)
	PPC_STORE_U8(ctx.r1.u32 + 504, r29.u8);
	// addi r4,r1,504
	ctx.r4.s64 = ctx.r1.s64 + 504;
	// stb r19,507(r1)
	PPC_STORE_U8(ctx.r1.u32 + 507, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v2,v13,0
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vmaddcfp128 v2,v0,v2,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(v127.f32)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
	// addi r4,r22,1408
	ctx.r4.s64 = r22.s64 + 1408;
	// addi r3,r1,2160
	ctx.r3.s64 = ctx.r1.s64 + 2160;
	// bl 0x82191990
	sub_82191990(ctx, base);
	// li r9,16
	ctx.r9.s64 = 16;
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// lfs f0,152(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 152);
	f0.f64 = double(temp.f32);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// addi r4,r1,564
	ctx.r4.s64 = ctx.r1.s64 + 564;
	// lvx128 v0,r3,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r29,566(r1)
	PPC_STORE_U8(ctx.r1.u32 + 566, r29.u8);
	// stb r29,565(r1)
	PPC_STORE_U8(ctx.r1.u32 + 565, r29.u8);
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// stb r19,564(r1)
	PPC_STORE_U8(ctx.r1.u32 + 564, r19.u8);
	// vspltw v2,v12,0
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// stb r19,567(r1)
	PPC_STORE_U8(ctx.r1.u32 + 567, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// vmaddcfp128 v2,v0,v2,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(v127.f32)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
	// lvx128 v0,r0,r26
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r26.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand128 v11,v125,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r8,r1,432
	ctx.r8.s64 = ctx.r1.s64 + 432;
	// vmsum3fp128 v10,v11,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,432(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f14
	cr6.compare(f0.f64, f14.f64);
	// bge cr6,0x822e51c4
	if (!cr6.lt) goto loc_822E51C4;
	// lfs f13,816(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,8356(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + 8356);
	f0.f64 = double(temp.f32);
	// fmuls f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 * f0.f64));
	// bl 0x82239f68
	sub_82239F68(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// lfs f0,-256(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + -256);
	f0.f64 = double(temp.f32);
	// lfs f13,140(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// addi r3,r1,1536
	ctx.r3.s64 = ctx.r1.s64 + 1536;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// fabs f11,f12
	ctx.f11.u64 = ctx.f12.u64 & ~0x8000000000000000;
	// fmadds f3,f11,f0,f13
	ctx.f3.f64 = double(float(ctx.f11.f64 * f0.f64 + ctx.f13.f64));
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,432
	r11.s64 = ctx.r1.s64 + 432;
	// vaddfp128 v13,v127,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v0.f32)));
	// stb r30,510(r1)
	PPC_STORE_U8(ctx.r1.u32 + 510, r30.u8);
	// stb r19,509(r1)
	PPC_STORE_U8(ctx.r1.u32 + 509, r19.u8);
	// addi r5,r1,508
	ctx.r5.s64 = ctx.r1.s64 + 508;
	// stb r30,508(r1)
	PPC_STORE_U8(ctx.r1.u32 + 508, r30.u8);
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// stb r19,511(r1)
	PPC_STORE_U8(ctx.r1.u32 + 511, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfs f1,9492(r27)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290d588
	sub_8290D588(ctx, base);
loc_822E51C4:
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r3,r1,1168
	ctx.r3.s64 = ctx.r1.s64 + 1168;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// stw r29,640(r1)
	PPC_STORE_U32(ctx.r1.u32 + 640, r29.u32);
	// addi r4,r1,640
	ctx.r4.s64 = ctx.r1.s64 + 640;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822e5250
	if (!cr6.eq) goto loc_822E5250;
	// addi r3,r1,2128
	ctx.r3.s64 = ctx.r1.s64 + 2128;
	// fmr f3,f28
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = f28.f64;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// addi r11,r1,1168
	r11.s64 = ctx.r1.s64 + 1168;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// stb r29,542(r1)
	PPC_STORE_U8(ctx.r1.u32 + 542, r29.u8);
	// stb r29,541(r1)
	PPC_STORE_U8(ctx.r1.u32 + 541, r29.u8);
	// addi r5,r1,540
	ctx.r5.s64 = ctx.r1.s64 + 540;
	// stb r19,540(r1)
	PPC_STORE_U8(ctx.r1.u32 + 540, r19.u8);
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// stb r19,543(r1)
	PPC_STORE_U8(ctx.r1.u32 + 543, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v12,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// lfs f1,9492(r27)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290d588
	sub_8290D588(ctx, base);
	// b 0x822e5354
	goto loc_822E5354;
loc_822E5250:
	// stb r19,518(r1)
	PPC_STORE_U8(ctx.r1.u32 + 518, r19.u8);
	// addi r11,r1,656
	r11.s64 = ctx.r1.s64 + 656;
	// stb r19,517(r1)
	PPC_STORE_U8(ctx.r1.u32 + 517, r19.u8);
	// addi r10,r1,1168
	ctx.r10.s64 = ctx.r1.s64 + 1168;
	// stb r19,516(r1)
	PPC_STORE_U8(ctx.r1.u32 + 516, r19.u8);
	// addi r4,r1,516
	ctx.r4.s64 = ctx.r1.s64 + 516;
	// stb r19,519(r1)
	PPC_STORE_U8(ctx.r1.u32 + 519, r19.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfs f1,8732(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 8732);
	ctx.f1.f64 = double(temp.f32);
	// lvx128 v127,r0,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor128 v2,v127,v127
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_load_si128((__m128i*)v127.u8));
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290cb38
	sub_8290CB38(ctx, base);
	// lbz r30,644(r1)
	r30.u64 = PPC_LOAD_U8(ctx.r1.u32 + 644);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822e52d4
	if (cr6.eq) goto loc_822E52D4;
	// lfs f0,9492(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9492);
	f0.f64 = double(temp.f32);
	// stb r19,558(r1)
	PPC_STORE_U8(ctx.r1.u32 + 558, r19.u8);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// stb r29,557(r1)
	PPC_STORE_U8(ctx.r1.u32 + 557, r29.u8);
	// stb r29,556(r1)
	PPC_STORE_U8(ctx.r1.u32 + 556, r29.u8);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stb r19,559(r1)
	PPC_STORE_U8(ctx.r1.u32 + 559, r19.u8);
	// addi r10,r1,672
	ctx.r10.s64 = ctx.r1.s64 + 672;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// addi r4,r1,556
	ctx.r4.s64 = ctx.r1.s64 + 556;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfs f1,8732(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + 8732);
	ctx.f1.f64 = double(temp.f32);
	// lvx128 v0,r0,r10
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v2,v13,0
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vmaddcfp128 v2,v0,v2,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(v127.f32)));
	// bl 0x8290cb38
	sub_8290CB38(ctx, base);
loc_822E52D4:
	// lbz r11,2(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 2);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lbz r10,1(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 1);
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// lbz r8,3(r31)
	ctx.r8.u64 = PPC_LOAD_U8(r31.u32 + 3);
	// beq cr6,0x822e5330
	if (cr6.eq) goto loc_822E5330;
	// lis r6,-31921
	ctx.r6.s64 = -2091974656;
	// lis r5,1
	ctx.r5.s64 = 65536;
	// ori r4,r5,3533
	ctx.r4.u64 = ctx.r5.u64 | 3533;
	// lwz r7,27320(r6)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r6.u32 + 27320);
	// mullw r7,r7,r4
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r4.s32);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// rlwinm r3,r7,0,21,21
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0x400;
	// stw r7,27320(r6)
	PPC_STORE_U32(ctx.r6.u32 + 27320, ctx.r7.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822e5330
	if (cr6.eq) goto loc_822E5330;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lwz r11,-20472(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + -20472);
	// stw r11,304(r1)
	PPC_STORE_U32(ctx.r1.u32 + 304, r11.u32);
	// lbz r8,307(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 307);
	// lbz r11,306(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 306);
	// lbz r10,305(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 305);
	// lbz r9,304(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 304);
loc_822E5330:
	// stb r11,526(r1)
	PPC_STORE_U8(ctx.r1.u32 + 526, r11.u8);
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// stb r10,525(r1)
	PPC_STORE_U8(ctx.r1.u32 + 525, ctx.r10.u8);
	// addi r5,r1,524
	ctx.r5.s64 = ctx.r1.s64 + 524;
	// stb r9,524(r1)
	PPC_STORE_U8(ctx.r1.u32 + 524, ctx.r9.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// stb r8,527(r1)
	PPC_STORE_U8(ctx.r1.u32 + 527, ctx.r8.u8);
	// lfs f1,688(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8290cd10
	sub_8290CD10(ctx, base);
loc_822E5354:
	// lfs f0,820(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 820);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// ble cr6,0x822e53f0
	if (!cr6.gt) goto loc_822E53F0;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r3,r1,1152
	ctx.r3.s64 = ctx.r1.s64 + 1152;
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r8,r1,1152
	ctx.r8.s64 = ctx.r1.s64 + 1152;
	// addi r7,r1,432
	ctx.r7.s64 = ctx.r1.s64 + 432;
	// lfs f13,820(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 820);
	ctx.f13.f64 = double(temp.f32);
	// addi r6,r1,272
	ctx.r6.s64 = ctx.r1.s64 + 272;
	// lfs f0,9844(r27)
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9844);
	f0.f64 = double(temp.f32);
	// lvx128 v7,r0,r16
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r16.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,1152
	ctx.r5.s64 = ctx.r1.s64 + 1152;
	// addi r3,r1,1344
	ctx.r3.s64 = ctx.r1.s64 + 1344;
	// lvx128 v0,r0,r8
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f11,f13,f0,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 * f0.f64 + ctx.f12.f64));
	// stfs f11,272(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lvlx v13,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v12,v0,v13,v7
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx128 v12,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x821e70c8
	sub_821E70C8(ctx, base);
	// lvx128 v11,r0,r3
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r19,550(r1)
	PPC_STORE_U8(ctx.r1.u32 + 550, r19.u8);
	// addi r11,r1,1152
	r11.s64 = ctx.r1.s64 + 1152;
	// stb r18,549(r1)
	PPC_STORE_U8(ctx.r1.u32 + 549, r18.u8);
	// addi r4,r1,548
	ctx.r4.s64 = ctx.r1.s64 + 548;
	// stb r18,548(r1)
	PPC_STORE_U8(ctx.r1.u32 + 548, r18.u8);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// stb r19,551(r1)
	PPC_STORE_U8(ctx.r1.u32 + 551, r19.u8);
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v2,v1,v11
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v2.f32, _mm_add_ps(_mm_load_ps(ctx.v1.f32), _mm_load_ps(ctx.v11.f32)));
	// bl 0x8290c9e8
	sub_8290C9E8(ctx, base);
loc_822E53F0:
	// lis r31,-31921
	r31.s64 = -2091974656;
	// addi r3,r1,1136
	ctx.r3.s64 = ctx.r1.s64 + 1136;
	// lwz r11,27316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 27316);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,27316(r31)
	PPC_STORE_U32(r31.u32 + 27316, r11.u32);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r4,124(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r9,64(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 64);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r8,2385(r22)
	ctx.r8.u64 = PPC_LOAD_U8(r22.u32 + 2385);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822e5430
	if (cr6.eq) goto loc_822E5430;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x822e543c
	goto loc_822E543C;
loc_822E5430:
	// addi r11,r1,1136
	r11.s64 = ctx.r1.s64 + 1136;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x8290c360
	sub_8290C360(ctx, base);
loc_822E543C:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e549c
	if (!cr6.eq) goto loc_822E549C;
	// addi r3,r1,1920
	ctx.r3.s64 = ctx.r1.s64 + 1920;
	// lfs f3,-256(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + -256);
	ctx.f3.f64 = double(temp.f32);
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lwz r11,27316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 27316);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r19,482(r1)
	PPC_STORE_U8(ctx.r1.u32 + 482, r19.u8);
	// addi r10,r1,1136
	ctx.r10.s64 = ctx.r1.s64 + 1136;
	// rlwinm r9,r11,0,24,24
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x80;
	// stb r29,480(r1)
	PPC_STORE_U8(ctx.r1.u32 + 480, r29.u8);
	// stb r19,483(r1)
	PPC_STORE_U8(ctx.r1.u32 + 483, r19.u8);
	// addi r5,r1,480
	ctx.r5.s64 = ctx.r1.s64 + 480;
	// subfic r8,r9,0
	xer.ca = ctx.r9.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r9.s64;
	// lfs f1,9492(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// subfe r7,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vaddfp v1,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// stb r7,481(r1)
	PPC_STORE_U8(ctx.r1.u32 + 481, ctx.r7.u8);
	// bl 0x8290cd10
	sub_8290CD10(ctx, base);
loc_822E549C:
	// lbz r11,2385(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2385);
	// lvx128 v1,r0,r28
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e54b4
	if (cr6.eq) goto loc_822E54B4;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x822e54b8
	goto loc_822E54B8;
loc_822E54B4:
	// bl 0x8290c360
	sub_8290C360(ctx, base);
loc_822E54B8:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// lfs f3,140(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bne cr6,0x822e550c
	if (!cr6.eq) goto loc_822E550C;
	// addi r3,r1,1600
	ctx.r3.s64 = ctx.r1.s64 + 1600;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lwz r11,27316(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 27316);
	// addi r10,r1,1136
	ctx.r10.s64 = ctx.r1.s64 + 1136;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// rlwinm r9,r11,0,25,25
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// stb r19,486(r1)
	PPC_STORE_U8(ctx.r1.u32 + 486, r19.u8);
	// stb r19,485(r1)
	PPC_STORE_U8(ctx.r1.u32 + 485, r19.u8);
	// addi r5,r1,484
	ctx.r5.s64 = ctx.r1.s64 + 484;
	// subfic r8,r9,0
	xer.ca = ctx.r9.u32 <= 0;
	ctx.r8.s64 = 0 - ctx.r9.s64;
	// stb r19,487(r1)
	PPC_STORE_U8(ctx.r1.u32 + 487, r19.u8);
	// subfe r7,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r7,484(r1)
	PPC_STORE_U8(ctx.r1.u32 + 484, ctx.r7.u8);
	// b 0x822e5534
	goto loc_822E5534;
loc_822E550C:
	// addi r3,r1,2032
	ctx.r3.s64 = ctx.r1.s64 + 2032;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// addi r11,r1,1136
	r11.s64 = ctx.r1.s64 + 1136;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r29,490(r1)
	PPC_STORE_U8(ctx.r1.u32 + 490, r29.u8);
	// stb r19,489(r1)
	PPC_STORE_U8(ctx.r1.u32 + 489, r19.u8);
	// addi r5,r1,488
	ctx.r5.s64 = ctx.r1.s64 + 488;
	// stb r29,488(r1)
	PPC_STORE_U8(ctx.r1.u32 + 488, r29.u8);
	// stb r19,491(r1)
	PPC_STORE_U8(ctx.r1.u32 + 491, r19.u8);
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E5534:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lfs f1,-256(r27)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r27.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// vaddfp v1,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v1.f32, _mm_add_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// bl 0x8290cd10
	sub_8290CD10(ctx, base);
loc_822E5544:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x822e5c2c
	if (!cr6.eq) goto loc_822E5C2C;
	// lwz r25,316(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r30,312(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// mr r5,r25
	ctx.r5.u64 = r25.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// li r3,28
	ctx.r3.s64 = 28;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e55c4
	if (cr6.eq) goto loc_822E55C4;
	// addi r3,r22,1408
	ctx.r3.s64 = r22.s64 + 1408;
	// fmr f2,f28
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = f28.f64;
	// bl 0x821ae7d0
	sub_821AE7D0(ctx, base);
	// lwz r26,308(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// addi r3,r22,832
	ctx.r3.s64 = r22.s64 + 832;
	// lfs f1,140(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 140);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821ae7d0
	sub_821AE7D0(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// bl 0x8272d9f8
	sub_8272D9F8(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// b 0x822e55d0
	goto loc_822E55D0;
loc_822E55C4:
	// li r27,0
	r27.s64 = 0;
	// lwz r26,308(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
loc_822E55D0:
	// addi r31,r22,1984
	r31.s64 = r22.s64 + 1984;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// lwz r11,1992(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1992);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e55f8
	if (cr6.eq) goto loc_822E55F8;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ade58
	sub_822ADE58(ctx, base);
loc_822E55F8:
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,596
	ctx.r3.s64 = ctx.r1.s64 + 596;
	// bl 0x821cdd88
	sub_821CDD88(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,1992(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 1992);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e56a4
	if (cr6.eq) goto loc_822E56A4;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5670
	if (cr6.eq) goto loc_822E5670;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E5638:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e5638
	if (!cr0.eq) goto loc_822E5638;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e566c
	if (!cr6.eq) goto loc_822E566C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E566C:
	// stw r27,1992(r22)
	PPC_STORE_U32(r22.u32 + 1992, r27.u32);
loc_822E5670:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,1992(r22)
	PPC_STORE_U32(r22.u32 + 1992, r11.u32);
	// beq cr6,0x822e56a4
	if (cr6.eq) goto loc_822E56A4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E5688:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e5688
	if (!cr0.eq) goto loc_822E5688;
loc_822E56A4:
	// lwz r11,596(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 596);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e56f4
	if (cr6.eq) goto loc_822E56F4;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E56B8:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e56b8
	if (!cr0.eq) goto loc_822E56B8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e56f0
	if (!cr6.eq) goto loc_822E56F0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E56F0:
	// stw r27,596(r1)
	PPC_STORE_U32(ctx.r1.u32 + 596, r27.u32);
loc_822E56F4:
	// stfs f28,2448(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2448, temp.u32);
	// addi r3,r1,1632
	ctx.r3.s64 = ctx.r1.s64 + 1632;
	// stfs f28,2456(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2456, temp.u32);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// stfs f28,2460(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2460, temp.u32);
	// stfs f28,2452(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2452, temp.u32);
	// stfs f28,2468(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2468, temp.u32);
	// stfs f28,2464(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2464, temp.u32);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r22,r17
	_mm_store_si128((__m128i*)(base + ((r22.u32 + r17.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32241
	r11.s64 = -2112946176;
	// stfs f28,2096(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2096, temp.u32);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,-24860
	ctx.r4.s64 = r11.s64 + -24860;
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r28,368(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5754
	if (!cr6.eq) goto loc_822E5754;
	// mr r31,r28
	r31.u64 = r28.u64;
	// b 0x822e575c
	goto loc_822E575C;
loc_822E5754:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E575C:
	// lwz r11,400(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	// mr r30,r28
	r30.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5770
	if (cr6.eq) goto loc_822E5770;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E5770:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822E5774:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5774
	if (!cr6.eq) goto loc_822E5774;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x822e57a0
	if (!cr6.eq) goto loc_822E57A0;
loc_822E5798:
	// li r11,1
	r11.s64 = 1;
	// b 0x822e57fc
	goto loc_822E57FC;
loc_822E57A0:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e57f8
	if (cr6.eq) goto loc_822E57F8;
loc_822E57AC:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// extsb r3,r11
	ctx.r3.s64 = r11.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// extsb r3,r10
	ctx.r3.s64 = ctx.r10.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// cmpw cr6,r24,r3
	cr6.compare<int32_t>(r24.s32, ctx.r3.s32, xer);
	// bne cr6,0x822e57e8
	if (!cr6.eq) goto loc_822E57E8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cab438
	sub_82CAB438(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822e5798
	if (cr6.eq) goto loc_822E5798;
loc_822E57E8:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e57ac
	if (!cr6.eq) goto loc_822E57AC;
loc_822E57F8:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_822E57FC:
	// stb r11,2380(r22)
	PPC_STORE_U8(r22.u32 + 2380, r11.u8);
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,-24852
	ctx.r4.s64 = r11.s64 + -24852;
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5834
	if (!cr6.eq) goto loc_822E5834;
	// mr r31,r28
	r31.u64 = r28.u64;
	// b 0x822e583c
	goto loc_822E583C;
loc_822E5834:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E583C:
	// lwz r11,400(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	// mr r30,r28
	r30.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5850
	if (cr6.eq) goto loc_822E5850;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E5850:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822E5854:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5854
	if (!cr6.eq) goto loc_822E5854;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x822e5880
	if (!cr6.eq) goto loc_822E5880;
loc_822E5878:
	// li r11,1
	r11.s64 = 1;
	// b 0x822e58dc
	goto loc_822E58DC;
loc_822E5880:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e58d8
	if (cr6.eq) goto loc_822E58D8;
loc_822E588C:
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// extsb r3,r11
	ctx.r3.s64 = r11.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// lbz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// extsb r3,r10
	ctx.r3.s64 = ctx.r10.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// cmpw cr6,r3,r24
	cr6.compare<int32_t>(ctx.r3.s32, r24.s32, xer);
	// bne cr6,0x822e58c8
	if (!cr6.eq) goto loc_822E58C8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cab438
	sub_82CAB438(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822e5878
	if (cr6.eq) goto loc_822E5878;
loc_822E58C8:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e588c
	if (!cr6.eq) goto loc_822E588C;
loc_822E58D8:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_822E58DC:
	// stb r11,2382(r22)
	PPC_STORE_U8(r22.u32 + 2382, r11.u8);
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,-24844
	ctx.r4.s64 = r11.s64 + -24844;
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5914
	if (!cr6.eq) goto loc_822E5914;
	// mr r31,r28
	r31.u64 = r28.u64;
	// b 0x822e591c
	goto loc_822E591C;
loc_822E5914:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E591C:
	// lwz r11,400(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 400);
	// mr r30,r28
	r30.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5930
	if (cr6.eq) goto loc_822E5930;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E5930:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822E5934:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5934
	if (!cr6.eq) goto loc_822E5934;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x822e5960
	if (!cr6.eq) goto loc_822E5960;
loc_822E5958:
	// li r11,1
	r11.s64 = 1;
	// b 0x822e59bc
	goto loc_822E59BC;
loc_822E5960:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e59b8
	if (cr6.eq) goto loc_822E59B8;
loc_822E596C:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// extsb r3,r11
	ctx.r3.s64 = r11.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// lbz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// extsb r3,r10
	ctx.r3.s64 = ctx.r10.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// cmpw cr6,r28,r3
	cr6.compare<int32_t>(r28.s32, ctx.r3.s32, xer);
	// bne cr6,0x822e59a8
	if (!cr6.eq) goto loc_822E59A8;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cab438
	sub_82CAB438(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822e5958
	if (cr6.eq) goto loc_822E5958;
loc_822E59A8:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e596c
	if (!cr6.eq) goto loc_822E596C;
loc_822E59B8:
	// mr r11,r27
	r11.u64 = r27.u64;
loc_822E59BC:
	// stb r11,2381(r22)
	PPC_STORE_U8(r22.u32 + 2381, r11.u8);
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lwz r30,372(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// stb r27,2384(r22)
	PPC_STORE_U8(r22.u32 + 2384, r27.u8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stb r27,2385(r22)
	PPC_STORE_U8(r22.u32 + 2385, r27.u8);
	// beq cr6,0x822e5c38
	if (cr6.eq) goto loc_822E5C38;
	// lis r11,-32245
	r11.s64 = -2113208320;
	// stw r27,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, r27.u32);
	// li r5,-1
	ctx.r5.s64 = -1;
	// stw r27,388(r1)
	PPC_STORE_U32(ctx.r1.u32 + 388, r27.u32);
	// addi r4,r11,-6576
	ctx.r4.s64 = r11.s64 + -6576;
	// stw r27,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r27.u32);
	// addi r3,r1,572
	ctx.r3.s64 = ctx.r1.s64 + 572;
	// stw r27,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r27.u32);
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r11,r27
	r11.u64 = r27.u64;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// rlwinm r9,r10,6,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e5ab4
	if (cr6.eq) goto loc_822E5AB4;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,58
	ctx.r10.s64 = 58;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5a4c
	if (cr6.eq) goto loc_822E5A4C;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,58(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 58);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e5ab8
	goto loc_822E5AB8;
loc_822E5A4C:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,404(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 404);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e5aa4
	if (cr6.eq) goto loc_822E5AA4;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,58
	cr6.compare<int32_t>(ctx.r10.s32, 58, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e5a80
	if (cr6.gt) goto loc_822E5A80;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_822E5A80:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5aa4
	if (!cr6.eq) goto loc_822E5AA4;
	// ld r11,400(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 400);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,1056(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1056, r11.u64);
	// lwz r11,1060(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1060);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x822e5ab8
	goto loc_822E5AB8;
loc_822E5AA4:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r27,1056(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1056, r27.u32);
	// b 0x822e5ab8
	goto loc_822E5AB8;
loc_822E5AB4:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_822E5AB8:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5adc
	if (cr6.eq) goto loc_822E5ADC;
	// lbz r10,113(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 113);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5adc
	if (cr6.eq) goto loc_822E5ADC;
	// addi r4,r11,52
	ctx.r4.s64 = r11.s64 + 52;
	// addi r3,r1,572
	ctx.r3.s64 = ctx.r1.s64 + 572;
	// bl 0x82265160
	sub_82265160(ctx, base);
loc_822E5ADC:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,-24836
	ctx.r4.s64 = r11.s64 + -24836;
	// addi r3,r1,624
	ctx.r3.s64 = ctx.r1.s64 + 624;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,624
	ctx.r5.s64 = ctx.r1.s64 + 624;
	// addi r4,r1,288
	ctx.r4.s64 = ctx.r1.s64 + 288;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// addi r3,r1,624
	ctx.r3.s64 = ctx.r1.s64 + 624;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,572
	ctx.r5.s64 = ctx.r1.s64 + 572;
	// addi r4,r1,384
	ctx.r4.s64 = ctx.r1.s64 + 384;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// lwz r11,288(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 288);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5b9c
	if (cr6.eq) goto loc_822E5B9C;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lfs f1,40(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// bl 0x82298480
	sub_82298480(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// lfs f0,-25888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// stvx128 v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x822e5b84
	if (!cr6.gt) goto loc_822E5B84;
	// lfs f0,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	f0.f64 = double(temp.f32);
	// fabs f12,f0
	ctx.f12.u64 = f0.u64 & ~0x8000000000000000;
	// lfs f13,-256(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + -256);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f12,f31
	f0.f64 = double(float(ctx.f12.f64 / f31.f64));
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x822e5b88
	if (!cr6.lt) goto loc_822E5B88;
loc_822E5B84:
	// lfs f0,8928(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 8928);
	f0.f64 = double(temp.f32);
loc_822E5B88:
	// stfs f0,232(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 232, temp.u32);
	// lfs f13,244(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 244);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x822e5b9c
	if (!cr6.gt) goto loc_822E5B9C;
	// stfs f0,244(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 244, temp.u32);
loc_822E5B9C:
	// lwz r11,384(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5c10
	if (cr6.eq) goto loc_822E5C10;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// lfs f1,40(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// bl 0x82298480
	sub_82298480(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// lfs f0,-25888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// stvx128 v1,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x822e5bf8
	if (!cr6.gt) goto loc_822E5BF8;
	// lfs f0,436(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 436);
	f0.f64 = double(temp.f32);
	// fabs f12,f0
	ctx.f12.u64 = f0.u64 & ~0x8000000000000000;
	// lfs f13,8364(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 8364);
	ctx.f13.f64 = double(temp.f32);
	// fdivs f0,f12,f31
	f0.f64 = double(float(ctx.f12.f64 / f31.f64));
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x822e5bfc
	if (!cr6.lt) goto loc_822E5BFC;
loc_822E5BF8:
	// lfs f0,12052(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 12052);
	f0.f64 = double(temp.f32);
loc_822E5BFC:
	// stfs f0,236(r30)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 236, temp.u32);
	// lfs f13,244(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 244);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x822e5c10
	if (!cr6.gt) goto loc_822E5C10;
	// stfs f0,244(r30)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r30.u32 + 244, temp.u32);
loc_822E5C10:
	// addi r3,r1,572
	ctx.r3.s64 = ctx.r1.s64 + 572;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// b 0x822e5c38
	goto loc_822E5C38;
loc_822E5C2C:
	// lwz r26,308(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// li r27,0
	r27.s64 = 0;
	// lwz r30,372(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
loc_822E5C38:
	// lfs f23,8928(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 8928);
	f23.f64 = double(temp.f32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lfs f18,12052(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 12052);
	f18.f64 = double(temp.f32);
	// beq cr6,0x822e5c50
	if (cr6.eq) goto loc_822E5C50;
	// lfs f23,236(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 236);
	f23.f64 = double(temp.f32);
	// lfs f18,232(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 232);
	f18.f64 = double(temp.f32);
loc_822E5C50:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// fmr f27,f28
	ctx.fpscr.disableFlushMode();
	f27.f64 = f28.f64;
	// li r18,181
	r18.s64 = 181;
	// fmr f24,f28
	f24.f64 = f28.f64;
	// mr r11,r27
	r11.u64 = r27.u64;
	// fmr f22,f28
	f22.f64 = f28.f64;
	// fmr f21,f28
	f21.f64 = f28.f64;
	// lwz r10,56(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 56);
	// rlwinm r9,r10,11,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 11) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e5d14
	if (cr6.eq) goto loc_822E5D14;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r18,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r18.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5cac
	if (cr6.eq) goto loc_822E5CAC;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,181(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 181);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e5d18
	goto loc_822E5D18;
loc_822E5CAC:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e5d04
	if (cr6.eq) goto loc_822E5D04;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,181
	cr6.compare<int32_t>(ctx.r10.s32, 181, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e5ce0
	if (cr6.gt) goto loc_822E5CE0;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_822E5CE0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5d04
	if (!cr6.eq) goto loc_822E5D04;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,968(r1)
	PPC_STORE_U64(ctx.r1.u32 + 968, r11.u64);
	// lwz r11,972(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 972);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x822e5d18
	goto loc_822E5D18;
loc_822E5D04:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r27,968(r1)
	PPC_STORE_U32(ctx.r1.u32 + 968, r27.u32);
	// b 0x822e5d18
	goto loc_822E5D18;
loc_822E5D14:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_822E5D18:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5e1c
	if (cr6.eq) goto loc_822E5E1C;
	// lbz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 128);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5d40
	if (cr6.eq) goto loc_822E5D40;
	// lwz r10,124(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bne cr6,0x822e5d44
	if (!cr6.eq) goto loc_822E5D44;
loc_822E5D40:
	// lbz r10,129(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 129);
loc_822E5D44:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5e1c
	if (cr6.eq) goto loc_822E5E1C;
	// lbz r10,129(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 129);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5d64
	if (cr6.eq) goto loc_822E5D64;
	// lfs f30,36(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 36);
	f30.f64 = double(temp.f32);
	// b 0x822e5d68
	goto loc_822E5D68;
loc_822E5D64:
	// fmr f30,f28
	ctx.fpscr.disableFlushMode();
	f30.f64 = f28.f64;
loc_822E5D68:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5d78
	if (cr6.eq) goto loc_822E5D78;
	// lfs f29,40(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 40);
	f29.f64 = double(temp.f32);
	// b 0x822e5d7c
	goto loc_822E5D7C;
loc_822E5D78:
	// fmr f29,f28
	ctx.fpscr.disableFlushMode();
	f29.f64 = f28.f64;
loc_822E5D7C:
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// lis r9,-30584
	ctx.r9.s64 = -2004353024;
	// ori r8,r9,34953
	ctx.r8.u64 = ctx.r9.u64 | 34953;
	// lwz r7,24(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r6,20(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// lwz r10,-27380(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -27380);
	// or r5,r7,r6
	ctx.r5.u64 = ctx.r7.u64 | ctx.r6.u64;
	// clrlwi r11,r5,28
	r11.u64 = ctx.r5.u32 & 0xF;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// mulhwu r3,r4,r8
	ctx.r3.u64 = (uint64_t(ctx.r4.u32) * uint64_t(ctx.r8.u32)) >> 32;
	// rlwinm r11,r3,29,3,31
	r11.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 29) & 0x1FFFFFFF;
	// rlwinm r10,r11,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// subf. r8,r9,r4
	ctx.r8.s64 = ctx.r4.s64 - ctx.r9.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x822e5dec
	if (!cr0.eq) goto loc_822E5DEC;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// fabs f1,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.u64 = f30.u64 & ~0x8000000000000000;
	// lfd f31,3368(r11)
	f31.u64 = PPC_LOAD_U64(r11.u32 + 3368);
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// stfs f0,2472(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2472, temp.u32);
	// fmr f2,f31
	ctx.f2.f64 = f31.f64;
	// fabs f1,f29
	ctx.f1.u64 = f29.u64 & ~0x8000000000000000;
	// bl 0x821fe378
	sub_821FE378(ctx, base);
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// stfs f13,2476(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2476, temp.u32);
loc_822E5DEC:
	// lfs f0,2472(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2472);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f30,f28
	cr6.compare(f30.f64, f28.f64);
	// lfs f13,2476(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2476);
	ctx.f13.f64 = double(temp.f32);
	// blt cr6,0x822e5e04
	if (cr6.lt) goto loc_822E5E04;
	// fmr f27,f0
	f27.f64 = f0.f64;
	// b 0x822e5e08
	goto loc_822E5E08;
loc_822E5E04:
	// fmr f24,f0
	ctx.fpscr.disableFlushMode();
	f24.f64 = f0.f64;
loc_822E5E08:
	// fcmpu cr6,f29,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f29.f64, f28.f64);
	// blt cr6,0x822e5e18
	if (cr6.lt) goto loc_822E5E18;
	// fmr f21,f13
	f21.f64 = ctx.f13.f64;
	// b 0x822e5e1c
	goto loc_822E5E1C;
loc_822E5E18:
	// fmr f22,f13
	ctx.fpscr.disableFlushMode();
	f22.f64 = ctx.f13.f64;
loc_822E5E1C:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r26,0
	r26.s64 = 0;
	// li r15,48
	r15.s64 = 48;
	// mr r11,r26
	r11.u64 = r26.u64;
	// lhz r10,40(r31)
	ctx.r10.u64 = PPC_LOAD_U16(r31.u32 + 40);
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e5ed4
	if (cr6.eq) goto loc_822E5ED4;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r15,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r15.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5e6c
	if (cr6.eq) goto loc_822E5E6C;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 48);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e5ed8
	goto loc_822E5ED8;
loc_822E5E6C:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e5ec4
	if (cr6.eq) goto loc_822E5EC4;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,48
	cr6.compare<int32_t>(ctx.r10.s32, 48, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e5ea0
	if (cr6.gt) goto loc_822E5EA0;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822E5EA0:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5ec4
	if (!cr6.eq) goto loc_822E5EC4;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,1120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1120, r11.u64);
	// lwz r11,1124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1124);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x822e5ed8
	goto loc_822E5ED8;
loc_822E5EC4:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r26,1120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1120, r26.u32);
	// b 0x822e5ed8
	goto loc_822E5ED8;
loc_822E5ED4:
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822E5ED8:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5f0c
	if (cr6.eq) goto loc_822E5F0C;
	// lfs f0,12(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 12);
	f0.f64 = double(temp.f32);
	// lfs f13,16(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 16);
	ctx.f13.f64 = double(temp.f32);
	// li r11,1
	r11.s64 = 1;
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x822e5efc
	if (!cr6.lt) goto loc_822E5EFC;
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822E5EFC:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5f0c
	if (cr6.eq) goto loc_822E5F0C;
	// li r27,1
	r27.s64 = 1;
loc_822E5F0C:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r11,36(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// rlwinm r10,r11,7,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e5f94
	if (cr6.eq) goto loc_822E5F94;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,25
	ctx.r10.s64 = 25;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e5f8c
	if (!cr6.eq) goto loc_822E5F8C;
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e5f84
	if (cr6.eq) goto loc_822E5F84;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,25
	cr6.compare<int32_t>(ctx.r10.s32, 25, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e5f68
	if (cr6.gt) goto loc_822E5F68;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822E5F68:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e5f84
	if (!cr6.eq) goto loc_822E5F84;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,984(r1)
	PPC_STORE_U64(ctx.r1.u32 + 984, r11.u64);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e5f98
	goto loc_822E5F98;
loc_822E5F84:
	// stw r11,988(r1)
	PPC_STORE_U32(ctx.r1.u32 + 988, r11.u32);
	// stw r26,984(r1)
	PPC_STORE_U32(ctx.r1.u32 + 984, r26.u32);
loc_822E5F8C:
	// li r11,1
	r11.s64 = 1;
	// b 0x822e5f98
	goto loc_822E5F98;
loc_822E5F94:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822E5F98:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e5fb8
	if (cr6.eq) goto loc_822E5FB8;
	// fmr f22,f28
	ctx.fpscr.disableFlushMode();
	f22.f64 = f28.f64;
	// mr r27,r26
	r27.u64 = r26.u64;
	// fmr f21,f28
	f21.f64 = f28.f64;
	// fmr f27,f28
	f27.f64 = f28.f64;
	// fmr f24,f28
	f24.f64 = f28.f64;
loc_822E5FB8:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,21
	ctx.r4.s64 = 21;
	// mr r30,r26
	r30.u64 = r26.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// li r23,21
	r23.s64 = 21;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6080
	if (cr6.eq) goto loc_822E6080;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r23,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r23.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6010
	if (cr6.eq) goto loc_822E6010;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,21(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e6084
	goto loc_822E6084;
loc_822E6010:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e606c
	if (cr6.eq) goto loc_822E606C;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,21
	cr6.compare<int32_t>(ctx.r10.s32, 21, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e6044
	if (cr6.gt) goto loc_822E6044;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822E6044:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e606c
	if (!cr6.eq) goto loc_822E606C;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,1008(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1008, r11.u64);
	// lwz r11,1012(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1012);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e6084
	goto loc_822E6084;
loc_822E606C:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r26,1008(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1008, r26.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e6084
	goto loc_822E6084;
loc_822E6080:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822E6084:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e60bc
	if (cr6.eq) goto loc_822E60BC;
	// li r4,51
	ctx.r4.s64 = 51;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821ff4e0
	sub_821FF4E0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e60bc
	if (cr6.eq) goto loc_822E60BC;
	// fmr f22,f28
	ctx.fpscr.disableFlushMode();
	f22.f64 = f28.f64;
	// mr r27,r26
	r27.u64 = r26.u64;
	// fmr f21,f28
	f21.f64 = f28.f64;
	// fmr f27,f28
	f27.f64 = f28.f64;
	// fmr f24,f28
	f24.f64 = f28.f64;
loc_822E60BC:
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lwz r3,4(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lfd f11,-27376(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f11.u64 = PPC_LOAD_U64(r11.u32 + -27376);
	// bl 0x82207928
	sub_82207928(ctx, base);
	// fdiv f13,f1,f11
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64 / ctx.f11.f64;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// lfs f0,2684(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 2684);
	f0.f64 = double(temp.f32);
	// frsp f19,f13
	f19.f64 = double(float(ctx.f13.f64));
	// stfs f19,400(r1)
	temp.f32 = float(f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
	// fcmpu cr6,f19,f0
	cr6.compare(f19.f64, f0.f64);
	// blt cr6,0x822e60f8
	if (cr6.lt) goto loc_822E60F8;
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lfs f0,8732(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8732);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f19,f0
	cr6.compare(f19.f64, f0.f64);
	// ble cr6,0x822e6100
	if (!cr6.gt) goto loc_822E6100;
loc_822E60F8:
	// fmr f19,f0
	ctx.fpscr.disableFlushMode();
	f19.f64 = f0.f64;
	// stfs f19,400(r1)
	temp.f32 = float(f19.f64);
	PPC_STORE_U32(ctx.r1.u32 + 400, temp.u32);
loc_822E6100:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,6
	ctx.r4.s64 = 6;
	// li r29,1
	r29.s64 = 1;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// mr r30,r26
	r30.u64 = r26.u64;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// li r19,6
	r19.s64 = 6;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e61c0
	if (cr6.eq) goto loc_822E61C0;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r19,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r19.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6158
	if (cr6.eq) goto loc_822E6158;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,6(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 6);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r11,1
	r11.s64 = 1;
	// lwz r30,4(r8)
	r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e61c4
	goto loc_822E61C4;
loc_822E6158:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e61b0
	if (cr6.eq) goto loc_822E61B0;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,6
	cr6.compare<int32_t>(ctx.r10.s32, 6, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e618c
	if (cr6.gt) goto loc_822E618C;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822E618C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e61b0
	if (!cr6.eq) goto loc_822E61B0;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,1000(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1000, r11.u64);
	// lwz r11,1004(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1004);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e61c4
	goto loc_822E61C4;
loc_822E61B0:
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// stw r26,1000(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1000, r26.u32);
	// b 0x822e61c4
	goto loc_822E61C4;
loc_822E61C0:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822E61C4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e61dc
	if (cr6.eq) goto loc_822E61DC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822011a0
	sub_822011A0(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_822E61DC:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e62e0
	if (!cr6.eq) goto loc_822E62E0;
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822e62e0
	if (!cr6.eq) goto loc_822E62E0;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,21
	ctx.r4.s64 = 21;
	// mr r30,r26
	r30.u64 = r26.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e62b8
	if (cr6.eq) goto loc_822E62B8;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r23,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r23.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6248
	if (cr6.eq) goto loc_822E6248;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,21(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e62bc
	goto loc_822E62BC;
loc_822E6248:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e62a4
	if (cr6.eq) goto loc_822E62A4;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,21
	cr6.compare<int32_t>(ctx.r10.s32, 21, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e627c
	if (cr6.gt) goto loc_822E627C;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822E627C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e62a4
	if (!cr6.eq) goto loc_822E62A4;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,1016(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1016, r11.u64);
	// lwz r11,1020(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e62bc
	goto loc_822E62BC;
loc_822E62A4:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r26,1016(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1016, r26.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e62bc
	goto loc_822E62BC;
loc_822E62B8:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822E62BC:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e62e0
	if (cr6.eq) goto loc_822E62E0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8226aeb8
	sub_8226AEB8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e62e0
	if (cr6.eq) goto loc_822E62E0;
	// li r29,1
	r29.s64 = 1;
loc_822E62E0:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea348
	if (cr6.eq) goto loc_822EA348;
	// lwz r11,1996(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1996);
	// addi r21,r22,1996
	r21.s64 = r22.s64 + 1996;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6328
	if (!cr6.eq) goto loc_822E6328;
	// li r3,160
	ctx.r3.s64 = 160;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e631c
	if (cr6.eq) goto loc_822E631C;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x828db1c0
	sub_828DB1C0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822e6320
	goto loc_822E6320;
loc_822E631C:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
loc_822E6320:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E6328:
	// lwz r11,2004(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2004);
	// addi r25,r22,2004
	r25.s64 = r22.s64 + 2004;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6364
	if (!cr6.eq) goto loc_822E6364;
	// li r3,80
	ctx.r3.s64 = 80;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6358
	if (cr6.eq) goto loc_822E6358;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x821b6be0
	sub_821B6BE0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822e635c
	goto loc_822E635C;
loc_822E6358:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
loc_822E635C:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E6364:
	// addi r16,r22,832
	r16.s64 = r22.s64 + 832;
	// lfs f4,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f4.f64 = double(temp.f32);
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x822692d8
	sub_822692D8(ctx, base);
	// addi r17,r22,1408
	r17.s64 = r22.s64 + 1408;
	// fmr f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = ctx.f1.f64;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x822692d8
	sub_822692D8(ctx, base);
	// fadds f0,f1,f5
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// lwz r24,308(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lfs f20,24664(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 24664);
	f20.f64 = double(temp.f32);
	// lfs f3,140(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// fcmpu cr6,f0,f20
	cr6.compare(f0.f64, f20.f64);
	// bge cr6,0x822e63b4
	if (!cr6.lt) goto loc_822E63B4;
	// lfs f0,-256(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + -256);
	f0.f64 = double(temp.f32);
	// fmr f31,f0
	f31.f64 = f0.f64;
	// fmr f30,f0
	f30.f64 = f0.f64;
	// b 0x822e63c0
	goto loc_822E63C0;
loc_822E63B4:
	// fdivs f0,f3,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f3.f64 / f0.f64));
	// fmuls f31,f0,f5
	f31.f64 = double(float(f0.f64 * ctx.f5.f64));
	// fmuls f30,f0,f1
	f30.f64 = double(float(f0.f64 * ctx.f1.f64));
loc_822E63C0:
	// addi r20,r22,848
	r20.s64 = r22.s64 + 848;
	// stfs f30,276(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f30.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// stfs f31,272(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// fmr f16,f28
	f16.f64 = f28.f64;
	// fmr f26,f28
	f26.f64 = f28.f64;
	// addi r30,r1,272
	r30.s64 = ctx.r1.s64 + 272;
	// fmr f29,f28
	f29.f64 = f28.f64;
	// mr r31,r20
	r31.u64 = r20.u64;
	// li r29,2
	r29.s64 = 2;
loc_822E63E4:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// li r26,0
	r26.s64 = 0;
	// mr r11,r26
	r11.u64 = r26.u64;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822e6400
	if (!cr6.eq) goto loc_822E6400;
	// li r11,1
	r11.s64 = 1;
loc_822E6400:
	// lwz r10,12(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822e6410
	if (!cr6.eq) goto loc_822E6410;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
loc_822E6410:
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x822e6420
	if (!cr6.eq) goto loc_822E6420;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// fadds f16,f16,f0
	f16.f64 = double(float(f16.f64 + f0.f64));
loc_822E6420:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lbz r10,5(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 5);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e6438
	if (cr6.eq) goto loc_822E6438;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// fadds f26,f26,f0
	f26.f64 = double(float(f26.f64 + f0.f64));
loc_822E6438:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x822e645c
	if (!cr6.eq) goto loc_822E645C;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x822e645c
	if (!cr6.eq) goto loc_822E645C;
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// fadds f29,f29,f0
	f29.f64 = double(float(f29.f64 + f0.f64));
	// b 0x822e64b4
	goto loc_822E64B4;
loc_822E645C:
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822e6488
	if (!cr6.eq) goto loc_822E6488;
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r9,1
	cr6.compare<int32_t>(ctx.r9.s32, 1, xer);
	// bne cr6,0x822e6488
	if (!cr6.eq) goto loc_822E6488;
	// addi r3,r31,-16
	ctx.r3.s64 = r31.s64 + -16;
	// fmr f1,f4
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x8218b738
	sub_8218B738(ctx, base);
	// lfs f0,0(r30)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	f0.f64 = double(temp.f32);
	// fmadds f29,f1,f0,f29
	f29.f64 = double(float(ctx.f1.f64 * f0.f64 + f29.f64));
	// b 0x822e64b4
	goto loc_822E64B4;
loc_822E6488:
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x822e64b4
	if (!cr6.eq) goto loc_822E64B4;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e64b4
	if (!cr6.eq) goto loc_822E64B4;
	// addi r3,r31,-16
	ctx.r3.s64 = r31.s64 + -16;
	// fmr f1,f4
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x8218b738
	sub_8218B738(ctx, base);
	// fsubs f0,f3,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f3.f64 - ctx.f1.f64));
	// lfs f13,0(r30)
	temp.u32 = PPC_LOAD_U32(r30.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f29,f0,f13,f29
	f29.f64 = double(float(f0.f64 * ctx.f13.f64 + f29.f64));
loc_822E64B4:
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// addi r31,r31,576
	r31.s64 = r31.s64 + 576;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// bne 0x822e63e4
	if (!cr0.eq) goto loc_822E63E4;
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// lwz r4,1984(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 1984);
	// bl 0x8225fb20
	sub_8225FB20(ctx, base);
	// lfs f0,-252(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + -252);
	f0.f64 = double(temp.f32);
	// lfs f25,17944(r24)
	temp.u32 = PPC_LOAD_U32(r24.u32 + 17944);
	f25.f64 = double(temp.f32);
	// fcmpu cr6,f27,f0
	cr6.compare(f27.f64, f0.f64);
	// bge cr6,0x822e6500
	if (!cr6.lt) goto loc_822E6500;
	// fcmpu cr6,f24,f0
	cr6.compare(f24.f64, f0.f64);
	// bge cr6,0x822e6500
	if (!cr6.lt) goto loc_822E6500;
	// lwz r3,304(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// stw r26,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r26.u32);
	// stw r26,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r26.u32);
	// stw r26,2112(r22)
	PPC_STORE_U32(r22.u32 + 2112, r26.u32);
	// stw r26,2208(r22)
	PPC_STORE_U32(r22.u32 + 2208, r26.u32);
	// b 0x822e6840
	goto loc_822E6840;
loc_822E6500:
	// fcmpu cr6,f27,f24
	ctx.fpscr.disableFlushMode();
	cr6.compare(f27.f64, f24.f64);
	// ble cr6,0x822e6788
	if (!cr6.gt) goto loc_822E6788;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// lbz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e6538
	if (!cr6.eq) goto loc_822E6538;
	// lwz r11,1424(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// lbz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e6538
	if (!cr6.eq) goto loc_822E6538;
	// lwz r3,304(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// stw r26,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r26.u32);
	// stw r26,2208(r22)
	PPC_STORE_U32(r22.u32 + 2208, r26.u32);
	// b 0x822e671c
	goto loc_822E671C;
loc_822E6538:
	// li r11,2
	r11.s64 = 2;
	// addi r30,r22,2224
	r30.s64 = r22.s64 + 2224;
	// stw r11,2208(r22)
	PPC_STORE_U32(r22.u32 + 2208, r11.u32);
	// stfs f27,2212(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(r22.u32 + 2212, temp.u32);
	// addi r28,r22,2208
	r28.s64 = r22.s64 + 2208;
	// mr r29,r19
	r29.u64 = r19.u64;
	// mr r31,r30
	r31.u64 = r30.u64;
loc_822E6554:
	// addi r3,r1,1952
	ctx.r3.s64 = ctx.r1.s64 + 1952;
	// fmr f4,f28
	ctx.fpscr.disableFlushMode();
	ctx.f4.f64 = f28.f64;
	// fmr f3,f28
	ctx.f3.f64 = f28.f64;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x823313e8
	sub_823313E8(ctx, base);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// addic. r29,r29,-1
	xer.ca = r29.u32 > 0;
	r29.s64 = r29.s64 + -1;
	cr0.compare<int32_t>(r29.s32, 0, xer);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// ld r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 8);
	// std r10,8(r31)
	PPC_STORE_U64(r31.u32 + 8, ctx.r10.u64);
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// bne 0x822e6554
	if (!cr0.eq) goto loc_822E6554;
	// fcmpu cr6,f31,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, f28.f64);
	// bne cr6,0x822e65d0
	if (!cr6.eq) goto loc_822E65D0;
	// lfs f0,2212(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2212);
	f0.f64 = double(temp.f32);
	// li r10,304
	ctx.r10.s64 = 304;
	// fmuls f13,f30,f0
	ctx.f13.f64 = double(float(f30.f64 * f0.f64));
	// stfs f13,2212(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2212, temp.u32);
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822E65A4:
	// lwz r9,1424(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r10,400
	cr6.compare<int32_t>(ctx.r10.s32, 400, xer);
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// ld r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r7,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r7.u64);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// blt cr6,0x822e65a4
	if (cr6.lt) goto loc_822E65A4;
	// b 0x822e66f0
	goto loc_822E66F0;
loc_822E65D0:
	// fcmpu cr6,f30,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f28.f64);
	// bne cr6,0x822e6618
	if (!cr6.eq) goto loc_822E6618;
	// lfs f0,2212(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2212);
	f0.f64 = double(temp.f32);
	// li r10,304
	ctx.r10.s64 = 304;
	// fmuls f13,f30,f0
	ctx.f13.f64 = double(float(f30.f64 * f0.f64));
	// stfs f13,2212(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2212, temp.u32);
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822E65EC:
	// lwz r9,0(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r10,400
	cr6.compare<int32_t>(ctx.r10.s32, 400, xer);
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// ld r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r7,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r7.u64);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// blt cr6,0x822e65ec
	if (cr6.lt) goto loc_822E65EC;
	// b 0x822e66f0
	goto loc_822E66F0;
loc_822E6618:
	// li r11,304
	r11.s64 = 304;
loc_822E661C:
	// lwz r9,1424(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// lwz r10,0(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// ldx r9,r11,r9
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + ctx.r9.u32);
	// addi r31,r1,384
	r31.s64 = ctx.r1.s64 + 384;
	// ldx r3,r11,r10
	ctx.r3.u64 = PPC_LOAD_U64(r11.u32 + ctx.r10.u32);
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// ld r5,8(r5)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r5.u32 + 8);
	// ld r6,8(r6)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r6.u32 + 8);
	// std r9,0(r4)
	PPC_STORE_U64(ctx.r4.u32 + 0, ctx.r9.u64);
	// std r3,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r3.u64);
	// std r5,8(r4)
	PPC_STORE_U64(ctx.r4.u32 + 8, ctx.r5.u64);
	// std r6,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r6.u64);
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum4fp128 v12,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32), 0xFF));
	// stvx128 v12,r0,r31
	_mm_store_si128((__m128i*)(base + ((r31.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,384(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x822e66b0
	if (!cr6.lt) goto loc_822E66B0;
	// vspltisw v12,-1
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_set1_epi32(int(0xFFFFFFFF)));
	// addi r10,r1,464
	ctx.r10.s64 = ctx.r1.s64 + 464;
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// vslw v11,v12,v12
	ctx.v11.u32[0] = ctx.v12.u32[0] << (ctx.v12.u8[0] & 0x1F);
	ctx.v11.u32[1] = ctx.v12.u32[1] << (ctx.v12.u8[4] & 0x1F);
	ctx.v11.u32[2] = ctx.v12.u32[2] << (ctx.v12.u8[8] & 0x1F);
	ctx.v11.u32[3] = ctx.v12.u32[3] << (ctx.v12.u8[12] & 0x1F);
	// vxor v10,v0,v11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_xor_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v11.u8)));
	// stvx128 v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r5,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, ctx.r5.u64);
	// ld r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r6,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r6.u64);
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E66B0:
	// addi r10,r1,276
	ctx.r10.s64 = ctx.r1.s64 + 276;
	// vsubfp v13,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// addi r9,r1,336
	ctx.r9.s64 = ctx.r1.s64 + 336;
	// addi r8,r1,336
	ctx.r8.s64 = ctx.r1.s64 + 336;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// cmpwi cr6,r11,400
	cr6.compare<int32_t>(r11.s32, 400, xer);
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// vmaddfp v10,v13,v11,v0
	_mm_store_ps(ctx.v10.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v10,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + 0);
	// ld r6,8(r8)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r8.u32 + 8);
	// std r7,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r7.u64);
	// std r6,8(r30)
	PPC_STORE_U64(r30.u32 + 8, ctx.r6.u64);
	// addi r30,r30,16
	r30.s64 = r30.s64 + 16;
	// blt cr6,0x822e661c
	if (cr6.lt) goto loc_822E661C;
loc_822E66F0:
	// lfs f0,2212(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2212);
	f0.f64 = double(temp.f32);
	// lwz r3,304(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bne cr6,0x822e670c
	if (!cr6.eq) goto loc_822E670C;
	// stw r26,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r26.u32);
	// stw r26,0(r28)
	PPC_STORE_U32(r28.u32 + 0, r26.u32);
	// b 0x822e671c
	goto loc_822E671C;
loc_822E670C:
	// lfs f13,140(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f0,f25,f13
	f0.f64 = double(float(f0.f64 * f25.f64 + ctx.f13.f64));
	// stfs f0,2212(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2212, temp.u32);
	// stw r28,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r28.u32);
loc_822E671C:
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822e6738
	if (!cr6.eq) goto loc_822E6738;
	// stw r26,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r26.u32);
	// stw r26,2112(r22)
	PPC_STORE_U32(r22.u32 + 2112, r26.u32);
	// b 0x822e6840
	goto loc_822E6840;
loc_822E6738:
	// li r11,1
	r11.s64 = 1;
	// lfs f0,9356(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 9356);
	f0.f64 = double(temp.f32);
	// fmuls f0,f27,f0
	f0.f64 = double(float(f27.f64 * f0.f64));
	// addi r8,r22,2112
	ctx.r8.s64 = r22.s64 + 2112;
	// stw r11,2112(r22)
	PPC_STORE_U32(r22.u32 + 2112, r11.u32);
	// stfs f0,2116(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2116, temp.u32);
	// li r10,400
	ctx.r10.s64 = 400;
	// addi r11,r22,2128
	r11.s64 = r22.s64 + 2128;
loc_822E6758:
	// lwz r9,8(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r10,480
	cr6.compare<int32_t>(ctx.r10.s32, 480, xer);
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// ld r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r6,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r6.u64);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// blt cr6,0x822e6758
	if (cr6.lt) goto loc_822E6758;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// b 0x822e6840
	goto loc_822E6840;
loc_822E6788:
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// lwz r10,60(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 60);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822e67b0
	if (!cr6.eq) goto loc_822E67B0;
	// lwz r3,304(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// stw r26,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r26.u32);
	// stw r26,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, r26.u32);
	// stw r26,2112(r22)
	PPC_STORE_U32(r22.u32 + 2112, r26.u32);
	// stw r26,2208(r22)
	PPC_STORE_U32(r22.u32 + 2208, r26.u32);
	// b 0x822e6840
	goto loc_822E6840;
loc_822E67B0:
	// li r6,3
	ctx.r6.s64 = 3;
	// lfs f0,152(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + 152);
	f0.f64 = double(temp.f32);
	// fmuls f0,f24,f0
	f0.f64 = double(float(f24.f64 * f0.f64));
	// addi r8,r22,2112
	ctx.r8.s64 = r22.s64 + 2112;
	// stw r6,2112(r22)
	PPC_STORE_U32(r22.u32 + 2112, ctx.r6.u32);
	// stfs f0,2116(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2116, temp.u32);
	// li r10,480
	ctx.r10.s64 = 480;
	// addi r11,r22,2128
	r11.s64 = r22.s64 + 2128;
loc_822E67D0:
	// lwz r9,8(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r10,560
	cr6.compare<int32_t>(ctx.r10.s32, 560, xer);
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r7,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r7.u64);
	// ld r5,8(r9)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r5,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r5.u64);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// blt cr6,0x822e67d0
	if (cr6.lt) goto loc_822E67D0;
	// lwz r3,304(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	// addi r7,r22,2208
	ctx.r7.s64 = r22.s64 + 2208;
	// li r10,560
	ctx.r10.s64 = 560;
	// addi r11,r22,2224
	r11.s64 = r22.s64 + 2224;
	// stw r8,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r8.u32);
	// stw r6,2208(r22)
	PPC_STORE_U32(r22.u32 + 2208, ctx.r6.u32);
	// stfs f0,2212(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2212, temp.u32);
loc_822E6814:
	// lwz r9,8(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmpwi cr6,r10,656
	cr6.compare<int32_t>(ctx.r10.s32, 656, xer);
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r8,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r8.u64);
	// ld r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// std r6,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r6.u64);
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// blt cr6,0x822e6814
	if (cr6.lt) goto loc_822E6814;
	// stw r7,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r7.u32);
loc_822E6840:
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
loc_822E6844:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e6844
	if (!cr0.eq) goto loc_822E6844;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e687c
	if (!cr6.eq) goto loc_822E687C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E687C:
	// lfs f0,-252(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + -252);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f22,f0
	cr6.compare(f22.f64, f0.f64);
	// bge cr6,0x822e6920
	if (!cr6.lt) goto loc_822E6920;
	// lwz r11,2336(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2336);
	// lwz r26,312(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e68a8
	if (cr6.eq) goto loc_822E68A8;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825154d8
	sub_825154D8(ctx, base);
loc_822E68A8:
	// lwz r11,2336(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2336);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e68f8
	if (cr6.eq) goto loc_822E68F8;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E68BC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e68bc
	if (!cr0.eq) goto loc_822E68BC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e68f0
	if (!cr6.eq) goto loc_822E68F0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E68F0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,2336(r22)
	PPC_STORE_U32(r22.u32 + 2336, r11.u32);
loc_822E68F8:
	// li r11,0
	r11.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2336(r22)
	PPC_STORE_U32(r22.u32 + 2336, r11.u32);
	// addi r3,r22,2328
	ctx.r3.s64 = r22.s64 + 2328;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r22,2320
	ctx.r3.s64 = r22.s64 + 2320;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// lwz r29,420(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// b 0x822e6c10
	goto loc_822E6C10;
loc_822E6920:
	// lwz r11,2320(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2320);
	// addi r31,r22,2320
	r31.s64 = r22.s64 + 2320;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e69b0
	if (!cr6.eq) goto loc_822E69B0;
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// lwz r10,4(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r7,r9,-24800
	ctx.r7.s64 = ctx.r9.s64 + -24800;
	// addi r8,r11,-24820
	ctx.r8.s64 = r11.s64 + -24820;
	// stw r7,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r7.u32);
	// addi r6,r1,272
	ctx.r6.s64 = ctx.r1.s64 + 272;
	// lwz r4,24(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,584
	ctx.r3.s64 = ctx.r1.s64 + 584;
	// stw r8,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r8.u32);
	// rlwinm r11,r4,2,29,29
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0x4;
	// lwzx r4,r11,r6
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + ctx.r6.u32);
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,316(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,584
	ctx.r5.s64 = ctx.r1.s64 + 584;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// addi r3,r1,584
	ctx.r3.s64 = ctx.r1.s64 + 584;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e69e8
	if (cr6.eq) goto loc_822E69E8;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x822e69e8
	goto loc_822E69E8;
loc_822E69B0:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fadds f7,f0,f19
	ctx.f7.f64 = double(float(f0.f64 + f19.f64));
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f7.f64, ctx.f1.f64);
	// ble cr6,0x822e69d0
	if (!cr6.gt) goto loc_822E69D0;
	// fsubs f7,f7,f1
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f1.f64));
loc_822E69D0:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// fmr f1,f7
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f7.f64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E69E8:
	// lwz r11,2328(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2328);
	// addi r30,r22,2328
	r30.s64 = r22.s64 + 2328;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6a80
	if (!cr6.eq) goto loc_822E6A80;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6a80
	if (cr6.eq) goto loc_822E6A80;
	// lwz r29,420(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// lwz r4,148(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 148);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822e6a84
	if (cr6.eq) goto loc_822E6A84;
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8272dae0
	sub_8272DAE0(ctx, base);
	// addi r5,r29,204
	ctx.r5.s64 = r29.s64 + 204;
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8272db98
	sub_8272DB98(ctx, base);
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6a64
	if (cr6.eq) goto loc_822E6A64;
	// lwz r26,312(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// addi r6,r1,864
	ctx.r6.s64 = ctx.r1.s64 + 864;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8272dd30
	sub_8272DD30(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
	// b 0x822e6a88
	goto loc_822E6A88;
loc_822E6A64:
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r26,312(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
	// b 0x822e6a88
	goto loc_822E6A88;
loc_822E6A80:
	// lwz r29,420(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
loc_822E6A84:
	// lwz r26,312(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
loc_822E6A88:
	// lwz r11,2336(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2336);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6bf0
	if (!cr6.eq) goto loc_822E6BF0;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6bf0
	if (cr6.eq) goto loc_822E6BF0;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6ac0
	if (cr6.eq) goto loc_822E6AC0;
	// lfs f1,-256(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8228b6b0
	sub_8228B6B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822e6ac4
	goto loc_822E6AC4;
loc_822E6AC0:
	// li r31,0
	r31.s64 = 0;
loc_822E6AC4:
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6ae4
	if (cr6.eq) goto loc_822E6AE4;
	// lfs f1,-256(r24)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r24.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82229838
	sub_82229838(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// b 0x822e6ae8
	goto loc_822E6AE8;
loc_822E6AE4:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822E6AE8:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r8,9
	ctx.r8.s64 = 9;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,580
	ctx.r3.s64 = ctx.r1.s64 + 580;
	// bl 0x82515248
	sub_82515248(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,2336(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2336);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e6b9c
	if (cr6.eq) goto loc_822E6B9C;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6b68
	if (cr6.eq) goto loc_822E6B68;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E6B2C:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e6b2c
	if (!cr0.eq) goto loc_822E6B2C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e6b60
	if (!cr6.eq) goto loc_822E6B60;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E6B60:
	// li r11,0
	r11.s64 = 0;
	// stw r11,2336(r22)
	PPC_STORE_U32(r22.u32 + 2336, r11.u32);
loc_822E6B68:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,2336(r22)
	PPC_STORE_U32(r22.u32 + 2336, r11.u32);
	// beq cr6,0x822e6b9c
	if (cr6.eq) goto loc_822E6B9C;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E6B80:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e6b80
	if (!cr0.eq) goto loc_822E6B80;
loc_822E6B9C:
	// lwz r11,580(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 580);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6bf0
	if (cr6.eq) goto loc_822E6BF0;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E6BB0:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e6bb0
	if (!cr0.eq) goto loc_822E6BB0;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e6be8
	if (!cr6.eq) goto loc_822E6BE8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E6BE8:
	// li r11,0
	r11.s64 = 0;
	// stw r11,580(r1)
	PPC_STORE_U32(ctx.r1.u32 + 580, r11.u32);
loc_822E6BF0:
	// lwz r11,2336(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2336);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6c10
	if (cr6.eq) goto loc_822E6C10;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// fmuls f0,f29,f22
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f29.f64 * f22.f64));
	// lfs f13,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,24(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
loc_822E6C10:
	// lwz r28,308(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lis r11,-21846
	r11.s64 = -1431699456;
	// ori r24,r11,43691
	r24.u64 = r11.u64 | 43691;
	// lfs f0,-252(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r28.u32 + -252);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f21,f0
	cr6.compare(f21.f64, f0.f64);
	// bge cr6,0x822e6cb8
	if (!cr6.lt) goto loc_822E6CB8;
	// lwz r11,2356(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2356);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6c44
	if (cr6.eq) goto loc_822E6C44;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825154d8
	sub_825154D8(ctx, base);
loc_822E6C44:
	// lwz r11,2356(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2356);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6c94
	if (cr6.eq) goto loc_822E6C94;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E6C58:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e6c58
	if (!cr0.eq) goto loc_822E6C58;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e6c8c
	if (!cr6.eq) goto loc_822E6C8C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E6C8C:
	// li r11,0
	r11.s64 = 0;
	// stw r11,2356(r22)
	PPC_STORE_U32(r22.u32 + 2356, r11.u32);
loc_822E6C94:
	// li r11,0
	r11.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2356(r22)
	PPC_STORE_U32(r22.u32 + 2356, r11.u32);
	// addi r3,r22,2348
	ctx.r3.s64 = r22.s64 + 2348;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r22,2340
	ctx.r3.s64 = r22.s64 + 2340;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// b 0x822e6fa0
	goto loc_822E6FA0;
loc_822E6CB8:
	// lwz r11,2340(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2340);
	// addi r31,r22,2340
	r31.s64 = r22.s64 + 2340;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6d68
	if (!cr6.eq) goto loc_822E6D68;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lis r10,-32241
	ctx.r10.s64 = -2112946176;
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// lis r8,-32241
	ctx.r8.s64 = -2112946176;
	// addi r7,r10,-24780
	ctx.r7.s64 = ctx.r10.s64 + -24780;
	// addi r6,r9,-24760
	ctx.r6.s64 = ctx.r9.s64 + -24760;
	// lwz r4,24(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// addi r3,r8,-24740
	ctx.r3.s64 = ctx.r8.s64 + -24740;
	// stw r7,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r7.u32);
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// mulhwu r8,r4,r24
	ctx.r8.u64 = (uint64_t(ctx.r4.u32) * uint64_t(r24.u32)) >> 32;
	// stw r6,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r6.u32);
	// stw r3,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, ctx.r3.u32);
	// rlwinm r11,r8,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// li r5,-1
	ctx.r5.s64 = -1;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r1,620
	ctx.r3.s64 = ctx.r1.s64 + 620;
	// add r7,r11,r10
	ctx.r7.u64 = r11.u64 + ctx.r10.u64;
	// subf r6,r7,r4
	ctx.r6.s64 = ctx.r4.s64 - ctx.r7.s64;
	// rlwinm r4,r6,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r4,r4,r9
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r9.u32);
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,316(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,620
	ctx.r5.s64 = ctx.r1.s64 + 620;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// addi r3,r1,620
	ctx.r3.s64 = ctx.r1.s64 + 620;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6da0
	if (cr6.eq) goto loc_822E6DA0;
	// rotlwi r3,r3,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x822e6da0
	goto loc_822E6DA0;
loc_822E6D68:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fadds f7,f0,f19
	ctx.f7.f64 = double(float(f0.f64 + f19.f64));
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f7.f64, ctx.f1.f64);
	// ble cr6,0x822e6d88
	if (!cr6.gt) goto loc_822E6D88;
	// fsubs f7,f7,f1
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f1.f64));
loc_822E6D88:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// fmr f1,f7
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f7.f64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E6DA0:
	// lwz r11,2348(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2348);
	// addi r30,r22,2348
	r30.s64 = r22.s64 + 2348;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6e18
	if (!cr6.eq) goto loc_822E6E18;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6e18
	if (cr6.eq) goto loc_822E6E18;
	// lwz r4,148(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 148);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822e6e18
	if (cr6.eq) goto loc_822E6E18;
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8272dae0
	sub_8272DAE0(ctx, base);
	// addi r5,r29,204
	ctx.r5.s64 = r29.s64 + 204;
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8272db98
	sub_8272DB98(ctx, base);
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6e04
	if (cr6.eq) goto loc_822E6E04;
	// addi r6,r1,864
	ctx.r6.s64 = ctx.r1.s64 + 864;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8272dd30
	sub_8272DD30(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822e6e08
	goto loc_822E6E08;
loc_822E6E04:
	// li r4,0
	ctx.r4.s64 = 0;
loc_822E6E08:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
loc_822E6E18:
	// lwz r11,2356(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2356);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e6f80
	if (!cr6.eq) goto loc_822E6F80;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6f80
	if (cr6.eq) goto loc_822E6F80;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6e50
	if (cr6.eq) goto loc_822E6E50;
	// lfs f1,-256(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r28.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8228b6b0
	sub_8228B6B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822e6e54
	goto loc_822E6E54;
loc_822E6E50:
	// li r31,0
	r31.s64 = 0;
loc_822E6E54:
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e6e74
	if (cr6.eq) goto loc_822E6E74;
	// lfs f1,-256(r28)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r28.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82229838
	sub_82229838(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// b 0x822e6e78
	goto loc_822E6E78;
loc_822E6E74:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822E6E78:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r8,9
	ctx.r8.s64 = 9;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,628
	ctx.r3.s64 = ctx.r1.s64 + 628;
	// bl 0x82515248
	sub_82515248(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,2356(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2356);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e6f2c
	if (cr6.eq) goto loc_822E6F2C;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6ef8
	if (cr6.eq) goto loc_822E6EF8;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E6EBC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e6ebc
	if (!cr0.eq) goto loc_822E6EBC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e6ef0
	if (!cr6.eq) goto loc_822E6EF0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E6EF0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,2356(r22)
	PPC_STORE_U32(r22.u32 + 2356, r11.u32);
loc_822E6EF8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,2356(r22)
	PPC_STORE_U32(r22.u32 + 2356, r11.u32);
	// beq cr6,0x822e6f2c
	if (cr6.eq) goto loc_822E6F2C;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E6F10:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e6f10
	if (!cr0.eq) goto loc_822E6F10;
loc_822E6F2C:
	// lwz r11,628(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 628);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6f80
	if (cr6.eq) goto loc_822E6F80;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E6F40:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e6f40
	if (!cr0.eq) goto loc_822E6F40;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e6f78
	if (!cr6.eq) goto loc_822E6F78;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E6F78:
	// li r11,0
	r11.s64 = 0;
	// stw r11,628(r1)
	PPC_STORE_U32(ctx.r1.u32 + 628, r11.u32);
loc_822E6F80:
	// lwz r11,2356(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2356);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6fa0
	if (cr6.eq) goto loc_822E6FA0;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// fmuls f0,f29,f21
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f29.f64 * f21.f64));
	// lfs f13,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,24(r11)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// stfs f0,20(r11)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
loc_822E6FA0:
	// clrlwi r28,r27,24
	r28.u64 = r27.u32 & 0xFF;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822e7044
	if (!cr6.eq) goto loc_822E7044;
	// lwz r11,2376(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2376);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e6fc8
	if (cr6.eq) goto loc_822E6FC8;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825154d8
	sub_825154D8(ctx, base);
loc_822E6FC8:
	// lwz r11,2376(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2376);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7018
	if (cr6.eq) goto loc_822E7018;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E6FDC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e6fdc
	if (!cr0.eq) goto loc_822E6FDC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7010
	if (!cr6.eq) goto loc_822E7010;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7010:
	// li r11,0
	r11.s64 = 0;
	// stw r11,2376(r22)
	PPC_STORE_U32(r22.u32 + 2376, r11.u32);
loc_822E7018:
	// li r11,0
	r11.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,2376(r22)
	PPC_STORE_U32(r22.u32 + 2376, r11.u32);
	// addi r3,r22,2368
	ctx.r3.s64 = r22.s64 + 2368;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r22,2360
	ctx.r3.s64 = r22.s64 + 2360;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// lwz r29,308(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// li r27,0
	r27.s64 = 0;
	// b 0x822e7354
	goto loc_822E7354;
loc_822E7044:
	// lwz r11,2360(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2360);
	// addi r31,r22,2360
	r31.s64 = r22.s64 + 2360;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e70b0
	if (!cr6.eq) goto loc_822E70B0;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,-24720
	ctx.r4.s64 = r11.s64 + -24720;
	// addi r3,r1,608
	ctx.r3.s64 = ctx.r1.s64 + 608;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,316(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// addi r5,r1,608
	ctx.r5.s64 = ctx.r1.s64 + 608;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// addi r3,r1,608
	ctx.r3.s64 = ctx.r1.s64 + 608;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e70e8
	if (cr6.eq) goto loc_822E70E8;
	// rotlwi r3,r10,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x822e70e8
	goto loc_822E70E8;
loc_822E70B0:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fadds f7,f0,f19
	ctx.f7.f64 = double(float(f0.f64 + f19.f64));
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f7.f64, ctx.f1.f64);
	// ble cr6,0x822e70d0
	if (!cr6.gt) goto loc_822E70D0;
	// fsubs f7,f7,f1
	ctx.f7.f64 = double(float(ctx.f7.f64 - ctx.f1.f64));
loc_822E70D0:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// fmr f1,f7
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f7.f64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E70E8:
	// lwz r11,2368(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2368);
	// addi r30,r22,2368
	r30.s64 = r22.s64 + 2368;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e71bc
	if (!cr6.eq) goto loc_822E71BC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e71bc
	if (cr6.eq) goto loc_822E71BC;
	// lwz r4,148(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 148);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822e71bc
	if (cr6.eq) goto loc_822E71BC;
	// addi r3,r1,1184
	ctx.r3.s64 = ctx.r1.s64 + 1184;
	// bl 0x8272dae0
	sub_8272DAE0(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r3,r1,1204
	ctx.r3.s64 = ctx.r1.s64 + 1204;
	// bl 0x82b77f08
	sub_82B77F08(ctx, base);
	// lwz r29,308(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lis r11,-31926
	r11.s64 = -2092302336;
	// addi r3,r1,1204
	ctx.r3.s64 = ctx.r1.s64 + 1204;
	// addi r4,r11,-19576
	ctx.r4.s64 = r11.s64 + -19576;
	// lfs f1,140(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 140);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82b78208
	sub_82B78208(ctx, base);
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r10,-19568
	ctx.r4.s64 = ctx.r10.s64 + -19568;
	// addi r3,r1,1204
	ctx.r3.s64 = ctx.r1.s64 + 1204;
	// bl 0x82b78120
	sub_82B78120(ctx, base);
	// lwz r9,1204(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1204);
	// addi r4,r1,1244
	ctx.r4.s64 = ctx.r1.s64 + 1244;
	// addi r3,r1,1188
	ctx.r3.s64 = ctx.r1.s64 + 1188;
	// stw r9,1184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1184, ctx.r9.u32);
	// bl 0x8272e610
	sub_8272E610(ctx, base);
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e71a0
	if (cr6.eq) goto loc_822E71A0;
	// addi r6,r1,1184
	ctx.r6.s64 = ctx.r1.s64 + 1184;
	// lwz r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// bl 0x8272dd30
	sub_8272DD30(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// li r27,0
	r27.s64 = 0;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,1184
	ctx.r3.s64 = ctx.r1.s64 + 1184;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
	// b 0x822e71c4
	goto loc_822E71C4;
loc_822E71A0:
	// li r27,0
	r27.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,1184
	ctx.r3.s64 = ctx.r1.s64 + 1184;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
	// b 0x822e71c4
	goto loc_822E71C4;
loc_822E71BC:
	// lwz r29,308(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// li r27,0
	r27.s64 = 0;
loc_822E71C4:
	// lwz r11,2376(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2376);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e7324
	if (!cr6.eq) goto loc_822E7324;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7324
	if (cr6.eq) goto loc_822E7324;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e71fc
	if (cr6.eq) goto loc_822E71FC;
	// lfs f1,-256(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8228b6b0
	sub_8228B6B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822e7200
	goto loc_822E7200;
loc_822E71FC:
	// mr r31,r27
	r31.u64 = r27.u64;
loc_822E7200:
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e7220
	if (cr6.eq) goto loc_822E7220;
	// lfs f1,-256(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + -256);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82229838
	sub_82229838(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// b 0x822e7224
	goto loc_822E7224;
loc_822E7220:
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
loc_822E7224:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r8,9
	ctx.r8.s64 = 9;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,592
	ctx.r3.s64 = ctx.r1.s64 + 592;
	// bl 0x82515248
	sub_82515248(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,2376(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2376);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e72d4
	if (cr6.eq) goto loc_822E72D4;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e72a0
	if (cr6.eq) goto loc_822E72A0;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E7268:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e7268
	if (!cr0.eq) goto loc_822E7268;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e729c
	if (!cr6.eq) goto loc_822E729C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E729C:
	// stw r27,2376(r22)
	PPC_STORE_U32(r22.u32 + 2376, r27.u32);
loc_822E72A0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,2376(r22)
	PPC_STORE_U32(r22.u32 + 2376, r11.u32);
	// beq cr6,0x822e72d4
	if (cr6.eq) goto loc_822E72D4;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E72B8:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e72b8
	if (!cr0.eq) goto loc_822E72B8;
loc_822E72D4:
	// lwz r11,592(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 592);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7324
	if (cr6.eq) goto loc_822E7324;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E72E8:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e72e8
	if (!cr0.eq) goto loc_822E72E8;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7320
	if (!cr6.eq) goto loc_822E7320;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7320:
	// stw r27,592(r1)
	PPC_STORE_U32(ctx.r1.u32 + 592, r27.u32);
loc_822E7324:
	// lwz r11,2376(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2376);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7354
	if (cr6.eq) goto loc_822E7354;
	// lfs f13,2456(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2456);
	ctx.f13.f64 = double(temp.f32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lfs f0,140(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 140);
	f0.f64 = double(temp.f32);
	// fsubs f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 - ctx.f13.f64));
	// fsubs f11,f0,f26
	ctx.f11.f64 = double(float(f0.f64 - f26.f64));
	// lfs f10,20(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 20);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,24(r11)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r11.u32 + 24, temp.u32);
	// fmuls f9,f12,f11
	ctx.f9.f64 = double(float(ctx.f12.f64 * ctx.f11.f64));
	// stfs f9,20(r11)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(r11.u32 + 20, temp.u32);
loc_822E7354:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822e80bc
	if (!cr6.eq) goto loc_822E80BC;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821d6c08
	sub_821D6C08(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822e7398
	if (!cr6.eq) goto loc_822E7398;
	// lfs f0,2100(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2100);
	f0.f64 = double(temp.f32);
	// fadds f13,f19,f0
	ctx.f13.f64 = double(float(f19.f64 + f0.f64));
	// lfs f0,8844(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 8844);
	f0.f64 = double(temp.f32);
	// stfs f13,2100(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2100, temp.u32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bge cr6,0x822e739c
	if (!cr6.lt) goto loc_822E739C;
	// li r11,2
	r11.s64 = 2;
	// stw r11,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r11.u32);
	// b 0x822e739c
	goto loc_822E739C;
loc_822E7398:
	// stfs f28,2100(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2100, temp.u32);
loc_822E739C:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822e80a0
	if (!cr6.eq) goto loc_822E80A0;
	// lfs f0,820(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 820);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bne cr6,0x822e80a0
	if (!cr6.eq) goto loc_822E80A0;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lfs f1,816(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82260988
	sub_82260988(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lfs f1,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82260988
	sub_82260988(ctx, base);
	// lwz r11,1992(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1992);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7438
	if (cr6.eq) goto loc_822E7438;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822ade58
	sub_822ADE58(ctx, base);
	// lwz r11,1992(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1992);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7434
	if (cr6.eq) goto loc_822E7434;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E73FC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e73fc
	if (!cr0.eq) goto loc_822E73FC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7430
	if (!cr6.eq) goto loc_822E7430;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7430:
	// stw r27,1992(r22)
	PPC_STORE_U32(r22.u32 + 1992, r27.u32);
loc_822E7434:
	// stw r27,1992(r22)
	PPC_STORE_U32(r22.u32 + 1992, r27.u32);
loc_822E7438:
	// lwz r9,2516(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 2516);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r28,r22,2516
	r28.s64 = r22.s64 + 2516;
	// lis r26,-31927
	r26.s64 = -2092367872;
	// addi r25,r11,3608
	r25.s64 = r11.s64 + 3608;
	// addi r27,r10,3224
	r27.s64 = ctx.r10.s64 + 3224;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e7478
	if (cr6.eq) goto loc_822E7478;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fadds f7,f19,f0
	ctx.f7.f64 = double(float(f19.f64 + f0.f64));
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f7.f64, ctx.f1.f64);
	// blt cr6,0x822e7770
	if (cr6.lt) goto loc_822E7770;
loc_822E7478:
	// li r19,0
	r19.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stb r19,2545(r22)
	PPC_STORE_U8(r22.u32 + 2545, r19.u8);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lwz r11,26912(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 26912);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r3,48(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// bl 0x825d6d98
	sub_825D6D98(ctx, base);
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r4,21
	ctx.r4.s64 = 21;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// mr r30,r19
	r30.u64 = r19.u64;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822e7568
	if (cr6.eq) goto loc_822E7568;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r23,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r23.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7500
	if (cr6.eq) goto loc_822E7500;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,21(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 21);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r11,1
	r11.s64 = 1;
	// lwz r30,4(r8)
	r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e756c
	goto loc_822E756C;
loc_822E7500:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e7558
	if (cr6.eq) goto loc_822E7558;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,21
	cr6.compare<int32_t>(ctx.r10.s32, 21, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e7534
	if (cr6.gt) goto loc_822E7534;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
loc_822E7534:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e7558
	if (!cr6.eq) goto loc_822E7558;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,1032(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1032, r11.u64);
	// lwz r11,1036(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1036);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e756c
	goto loc_822E756C;
loc_822E7558:
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// stw r19,1032(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1032, r19.u32);
	// b 0x822e756c
	goto loc_822E756C;
loc_822E7568:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822E756C:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e765c
	if (cr6.eq) goto loc_822E765C;
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e765c
	if (!cr6.eq) goto loc_822E765C;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8226aeb8
	sub_8226AEB8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// lis r11,-32245
	r11.s64 = -2113208320;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// addi r29,r11,12012
	r29.s64 = r11.s64 + 12012;
	// beq cr6,0x822e7624
	if (cr6.eq) goto loc_822E7624;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8226aeb8
	sub_8226AEB8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e761c
	if (cr6.eq) goto loc_822E761C;
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e75cc
	if (cr6.eq) goto loc_822E75CC;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r3,r11,8
	ctx.r3.s64 = r11.s64 + 8;
	// b 0x822e75d4
	goto loc_822E75D4;
loc_822E75CC:
	// lis r11,-31926
	r11.s64 = -2092302336;
	// addi r3,r11,23404
	ctx.r3.s64 = r11.s64 + 23404;
loc_822E75D4:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r4,r11,-24700
	ctx.r4.s64 = r11.s64 + -24700;
	// bl 0x8229ad78
	sub_8229AD78(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e761c
	if (cr6.eq) goto loc_822E761C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,588
	ctx.r3.s64 = ctx.r1.s64 + 588;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// addi r4,r1,588
	ctx.r4.s64 = ctx.r1.s64 + 588;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r11.u32);
	// bl 0x823f3758
	sub_823F3758(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e7624
	if (!cr6.eq) goto loc_822E7624;
loc_822E761C:
	// mr r11,r19
	r11.u64 = r19.u64;
	// b 0x822e7628
	goto loc_822E7628;
loc_822E7624:
	// li r11,1
	r11.s64 = 1;
loc_822E7628:
	// lwz r10,364(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x822e7644
	if (cr6.eq) goto loc_822E7644;
	// addi r3,r1,588
	ctx.r3.s64 = ctx.r1.s64 + 588;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
loc_822E7644:
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e765c
	if (cr6.eq) goto loc_822E765C;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// bl 0x82275368
	sub_82275368(ctx, base);
loc_822E765C:
	// lbz r11,2381(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2381);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e769c
	if (cr6.eq) goto loc_822E769C;
	// lbz r11,2384(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2384);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e769c
	if (cr6.eq) goto loc_822E769C;
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// lbz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e769c
	if (cr6.eq) goto loc_822E769C;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// addi r4,r11,-24692
	ctx.r4.s64 = r11.s64 + -24692;
	// bl 0x82275368
	sub_82275368(ctx, base);
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r10,2545(r22)
	PPC_STORE_U8(r22.u32 + 2545, ctx.r10.u8);
loc_822E769C:
	// stb r19,2384(r22)
	PPC_STORE_U8(r22.u32 + 2384, r19.u8);
	// lwz r11,448(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 448);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e76b8
	if (cr6.eq) goto loc_822E76B8;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e76c4
	if (!cr6.eq) goto loc_822E76C4;
loc_822E76B8:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// bl 0x82275368
	sub_82275368(ctx, base);
loc_822E76C4:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e76f0
	if (cr6.eq) goto loc_822E76F0;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E76F0:
	// lwz r23,316(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,448
	ctx.r5.s64 = ctx.r1.s64 + 448;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r7,2512(r22)
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + 2512);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e7764
	if (cr6.eq) goto loc_822E7764;
	// addi r3,r1,636
	ctx.r3.s64 = ctx.r1.s64 + 636;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// rotlwi r31,r7,0
	r31.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// bl 0x8225fb20
	sub_8225FB20(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82514d68
	sub_82514D68(ctx, base);
loc_822E7764:
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// b 0x822e7778
	goto loc_822E7778;
loc_822E7770:
	// lwz r23,316(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// li r19,0
	r19.s64 = 0;
loc_822E7778:
	// lwz r11,2512(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2512);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e78ac
	if (!cr6.eq) goto loc_822E78AC;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// lwz r20,308(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e77a8
	if (cr6.eq) goto loc_822E77A8;
	// lfs f1,9492(r20)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r20.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8228b6b0
	sub_8228B6B0(ctx, base);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// b 0x822e77ac
	goto loc_822E77AC;
loc_822E77A8:
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
loc_822E77AC:
	// lwz r21,312(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// li r8,0
	ctx.r8.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// addi r3,r1,616
	ctx.r3.s64 = ctx.r1.s64 + 616;
	// bl 0x821cdd88
	sub_821CDD88(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,2512(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2512);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e7858
	if (cr6.eq) goto loc_822E7858;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7824
	if (cr6.eq) goto loc_822E7824;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E77EC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e77ec
	if (!cr0.eq) goto loc_822E77EC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7820
	if (!cr6.eq) goto loc_822E7820;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7820:
	// stw r19,2512(r22)
	PPC_STORE_U32(r22.u32 + 2512, r19.u32);
loc_822E7824:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,2512(r22)
	PPC_STORE_U32(r22.u32 + 2512, r11.u32);
	// beq cr6,0x822e7858
	if (cr6.eq) goto loc_822E7858;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E783C:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e783c
	if (!cr0.eq) goto loc_822E783C;
loc_822E7858:
	// lwz r11,616(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e78b4
	if (cr6.eq) goto loc_822E78B4;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E786C:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e786c
	if (!cr0.eq) goto loc_822E786C;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e78a4
	if (!cr6.eq) goto loc_822E78A4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E78A4:
	// stw r19,616(r1)
	PPC_STORE_U32(ctx.r1.u32 + 616, r19.u32);
	// b 0x822e78b4
	goto loc_822E78B4;
loc_822E78AC:
	// lwz r20,308(r1)
	r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r21,312(r1)
	r21.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
loc_822E78B4:
	// lbz r11,2545(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2545);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e79a4
	if (cr6.eq) goto loc_822E79A4;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e79a4
	if (cr6.eq) goto loc_822E79A4;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x8226b928
	sub_8226B928(ctx, base);
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// lfs f1,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	ctx.f1.f64 = double(temp.f32);
	// fadds f2,f19,f1
	ctx.f2.f64 = double(float(f19.f64 + ctx.f1.f64));
	// bl 0x821f4c68
	sub_821F4C68(ctx, base);
	// addi r5,r1,704
	ctx.r5.s64 = ctx.r1.s64 + 704;
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// addi r3,r1,640
	ctx.r3.s64 = ctx.r1.s64 + 640;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// lfs f0,140(r20)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r20.u32 + 140);
	f0.f64 = double(temp.f32);
	// fdivs f0,f0,f19
	f0.f64 = double(float(f0.f64 / f19.f64));
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r11,r1,752
	r11.s64 = ctx.r1.s64 + 752;
	// lwz r9,576(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	// addi r10,r1,688
	ctx.r10.s64 = ctx.r1.s64 + 688;
	// stfs f28,288(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v12,r0,r10
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v12,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// vmulfp128 v7,v11,v9
	_mm_store_ps(ctx.v7.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v9.f32)));
	// vperm128 v127,v7,v8,v0
	_mm_store_si128((__m128i*)v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x821ff5b8
	sub_821FF5B8(ctx, base);
	// clrlwi r6,r3,24
	ctx.r6.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x822e7978
	if (cr6.eq) goto loc_822E7978;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,128(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 128);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7978:
	// addi r11,r1,656
	r11.s64 = ctx.r1.s64 + 656;
	// lvx128 v127,r0,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x82282a98
	sub_82282A98(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e79a4
	if (cr6.eq) goto loc_822E79A4;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// bl 0x822664d8
	sub_822664D8(ctx, base);
loc_822E79A4:
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// fadds f1,f0,f19
	ctx.f1.f64 = double(float(f0.f64 + f19.f64));
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x821f51f0
	sub_821F51F0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e7aa4
	if (cr6.eq) goto loc_822E7AA4;
	// lwz r11,2532(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2532);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7a40
	if (cr6.eq) goto loc_822E7A40;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x825154d8
	sub_825154D8(ctx, base);
	// lwz r11,2532(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2532);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7a3c
	if (cr6.eq) goto loc_822E7A3C;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E7A04:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e7a04
	if (!cr0.eq) goto loc_822E7A04;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7a38
	if (!cr6.eq) goto loc_822E7A38;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7A38:
	// stw r19,2532(r22)
	PPC_STORE_U32(r22.u32 + 2532, r19.u32);
loc_822E7A3C:
	// stw r19,2532(r22)
	PPC_STORE_U32(r22.u32 + 2532, r19.u32);
loc_822E7A40:
	// lwz r11,2536(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2536);
	// addi r31,r22,2536
	r31.s64 = r22.s64 + 2536;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7a70
	if (cr6.eq) goto loc_822E7A70;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E7A70:
	// lwz r11,2524(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2524);
	// addi r31,r22,2524
	r31.s64 = r22.s64 + 2524;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8080
	if (cr6.eq) goto loc_822E8080;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// b 0x822e8080
	goto loc_822E8080;
loc_822E7AA4:
	// lwz r11,2524(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2524);
	// addi r29,r22,2524
	r29.s64 = r22.s64 + 2524;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7ad0
	if (cr6.eq) goto loc_822E7AD0;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// lfs f1,40(r3)
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// fadds f7,f0,f19
	ctx.f7.f64 = double(float(f0.f64 + f19.f64));
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fcmpu cr6,f7,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f7.f64, ctx.f1.f64);
	// ble cr6,0x822e7e20
	if (!cr6.gt) goto loc_822E7E20;
loc_822E7AD0:
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// stb r19,2544(r22)
	PPC_STORE_U8(r22.u32 + 2544, r19.u8);
	// li r4,181
	ctx.r4.s64 = 181;
	// mr r30,r19
	r30.u64 = r19.u64;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7ba8
	if (cr6.eq) goto loc_822E7BA8;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r18,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r18.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7b38
	if (cr6.eq) goto loc_822E7B38;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,181(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 181);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e7bac
	goto loc_822E7BAC;
loc_822E7B38:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e7b94
	if (cr6.eq) goto loc_822E7B94;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,181
	cr6.compare<int32_t>(ctx.r10.s32, 181, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e7b6c
	if (cr6.gt) goto loc_822E7B6C;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
loc_822E7B6C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e7b94
	if (!cr6.eq) goto loc_822E7B94;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,1024(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1024, r11.u64);
	// lwz r11,1028(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1028);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e7bac
	goto loc_822E7BAC;
loc_822E7B94:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r19,1024(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1024, r19.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e7bac
	goto loc_822E7BAC;
loc_822E7BA8:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822E7BAC:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7c40
	if (cr6.eq) goto loc_822E7C40;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,632
	ctx.r3.s64 = ctx.r1.s64 + 632;
	// bl 0x8291d8d0
	sub_8291D8D0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// addi r3,r1,632
	ctx.r3.s64 = ctx.r1.s64 + 632;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// bl 0x8229ad78
	sub_8229AD78(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7c40
	if (cr6.eq) goto loc_822E7C40;
	// lwz r11,26912(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 26912);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r9,120(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 120);
	// mulli r10,r9,9377
	ctx.r10.s64 = ctx.r9.s64 * 9377;
	// addi r8,r10,9439
	ctx.r8.s64 = ctx.r10.s64 + 9439;
	// rotlwi r7,r8,19
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r8.u32, 19);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// stw r7,120(r11)
	PPC_STORE_U32(r11.u32 + 120, ctx.r7.u32);
	// mulhwu r5,r6,r24
	ctx.r5.u64 = (uint64_t(ctx.r6.u32) * uint64_t(r24.u32)) >> 32;
	// rlwinm r11,r5,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// subf r3,r4,r6
	ctx.r3.s64 = ctx.r6.s64 - ctx.r4.s64;
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// beq cr6,0x822e7c40
	if (cr6.eq) goto loc_822E7C40;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// bl 0x82275368
	sub_82275368(ctx, base);
loc_822E7C40:
	// lwz r11,416(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 416);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e7c80
	if (!cr6.eq) goto loc_822E7C80;
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// mr r11,r14
	r11.u64 = r14.u64;
loc_822E7C54:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// beq cr6,0x822e7c78
	if (cr6.eq) goto loc_822E7C78;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x822e7c54
	if (cr6.eq) goto loc_822E7C54;
loc_822E7C78:
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// b 0x822e7c90
	goto loc_822E7C90;
loc_822E7C80:
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x8226d7a8
	sub_8226D7A8(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
loc_822E7C90:
	// rlwinm r11,r11,27,31,31
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7d34
	if (cr6.eq) goto loc_822E7D34;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r31,r11,-24668
	r31.s64 = r11.s64 + -24668;
	// addi r3,r1,600
	ctx.r3.s64 = ctx.r1.s64 + 600;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// addi r4,r1,600
	ctx.r4.s64 = ctx.r1.s64 + 600;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x823f3758
	sub_823F3758(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r1,600
	ctx.r3.s64 = ctx.r1.s64 + 600;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7d34
	if (cr6.eq) goto loc_822E7D34;
	// lwz r11,26912(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 26912);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r9,120(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 120);
	// mulli r10,r9,9377
	ctx.r10.s64 = ctx.r9.s64 * 9377;
	// addi r8,r10,9439
	ctx.r8.s64 = ctx.r10.s64 + 9439;
	// rotlwi r7,r8,19
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r8.u32, 19);
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// stw r7,120(r11)
	PPC_STORE_U32(r11.u32 + 120, ctx.r7.u32);
	// mulhwu r5,r6,r24
	ctx.r5.u64 = (uint64_t(ctx.r6.u32) * uint64_t(r24.u32)) >> 32;
	// rlwinm r11,r5,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r11,r10
	ctx.r4.u64 = r11.u64 + ctx.r10.u64;
	// subf r3,r4,r6
	ctx.r3.s64 = ctx.r6.s64 - ctx.r4.s64;
	// cmplwi cr6,r3,1
	cr6.compare<uint32_t>(ctx.r3.u32, 1, xer);
	// bne cr6,0x822e7d34
	if (!cr6.eq) goto loc_822E7D34;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// bl 0x82275368
	sub_82275368(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// stb r11,2544(r22)
	PPC_STORE_U8(r22.u32 + 2544, r11.u8);
loc_822E7D34:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7d60
	if (cr6.eq) goto loc_822E7D60;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E7D60:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,416
	ctx.r5.s64 = ctx.r1.s64 + 416;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x821c21b0
	sub_821C21B0(ctx, base);
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// fmr f1,f28
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f28.f64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,2536(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 2536);
	// addi r31,r22,2536
	r31.s64 = r22.s64 + 2536;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822e7e0c
	if (!cr6.eq) goto loc_822E7E0C;
	// lwz r9,420(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// lwz r4,148(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 148);
	// bl 0x8272dae0
	sub_8272DAE0(ctx, base);
	// addi r5,r9,204
	ctx.r5.s64 = ctx.r9.s64 + 204;
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8272db98
	sub_8272DB98(ctx, base);
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e7df4
	if (cr6.eq) goto loc_822E7DF4;
	// addi r6,r1,864
	ctx.r6.s64 = ctx.r1.s64 + 864;
	// lwz r5,0(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// bl 0x8272dd30
	sub_8272DD30(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
	// b 0x822e7e18
	goto loc_822E7E18;
loc_822E7DF4:
	// mr r4,r19
	ctx.r4.u64 = r19.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8277d718
	sub_8277D718(ctx, base);
	// b 0x822e7e18
	goto loc_822E7E18;
loc_822E7E0C:
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// bl 0x8272e1d8
	sub_8272E1D8(ctx, base);
loc_822E7E18:
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
loc_822E7E20:
	// lwz r11,2532(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2532);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e7f74
	if (!cr6.eq) goto loc_822E7F74;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e7e4c
	if (cr6.eq) goto loc_822E7E4C;
	// lfs f1,9492(r20)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r20.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8228b6b0
	sub_8228B6B0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822e7e50
	goto loc_822E7E50;
loc_822E7E4C:
	// mr r31,r19
	r31.u64 = r19.u64;
loc_822E7E50:
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e7e70
	if (cr6.eq) goto loc_822E7E70;
	// lfs f1,9492(r20)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r20.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82229838
	sub_82229838(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// b 0x822e7e74
	goto loc_822E7E74;
loc_822E7E70:
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
loc_822E7E74:
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// lwz r5,2536(r22)
	ctx.r5.u64 = PPC_LOAD_U32(r22.u32 + 2536);
	// li r8,3
	ctx.r8.s64 = 3;
	// li r7,3
	ctx.r7.s64 = 3;
	// li r6,1
	ctx.r6.s64 = 1;
	// mr r4,r21
	ctx.r4.u64 = r21.u64;
	// addi r3,r1,604
	ctx.r3.s64 = ctx.r1.s64 + 604;
	// bl 0x82515248
	sub_82515248(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,2532(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2532);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e7f24
	if (cr6.eq) goto loc_822E7F24;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7ef0
	if (cr6.eq) goto loc_822E7EF0;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E7EB8:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e7eb8
	if (!cr0.eq) goto loc_822E7EB8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7eec
	if (!cr6.eq) goto loc_822E7EEC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7EEC:
	// stw r19,2532(r22)
	PPC_STORE_U32(r22.u32 + 2532, r19.u32);
loc_822E7EF0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,2532(r22)
	PPC_STORE_U32(r22.u32 + 2532, r11.u32);
	// beq cr6,0x822e7f24
	if (cr6.eq) goto loc_822E7F24;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E7F08:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e7f08
	if (!cr0.eq) goto loc_822E7F08;
loc_822E7F24:
	// lwz r11,604(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7f74
	if (cr6.eq) goto loc_822E7F74;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E7F38:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e7f38
	if (!cr0.eq) goto loc_822E7F38;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e7f70
	if (!cr6.eq) goto loc_822E7F70;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E7F70:
	// stw r19,604(r1)
	PPC_STORE_U32(ctx.r1.u32 + 604, r19.u32);
loc_822E7F74:
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lfs f0,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	f0.f64 = double(temp.f32);
	// fadds f1,f0,f19
	ctx.f1.f64 = double(float(f0.f64 + f19.f64));
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r9,2544(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 2544);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822e8080
	if (cr6.eq) goto loc_822E8080;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,44
	ctx.r4.s64 = 44;
	// mr r30,r19
	r30.u64 = r19.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8064
	if (cr6.eq) goto loc_822E8064;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,44
	ctx.r10.s64 = 44;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e7ff4
	if (cr6.eq) goto loc_822E7FF4;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e8068
	goto loc_822E8068;
loc_822E7FF4:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e8050
	if (cr6.eq) goto loc_822E8050;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,44
	cr6.compare<int32_t>(ctx.r10.s32, 44, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e8028
	if (cr6.gt) goto loc_822E8028;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
loc_822E8028:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e8050
	if (!cr6.eq) goto loc_822E8050;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,1104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1104, r11.u64);
	// lwz r11,1108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1108);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e8068
	goto loc_822E8068;
loc_822E8050:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r19,1104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1104, r19.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822e8068
	goto loc_822E8068;
loc_822E8064:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822E8068:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8080
	if (cr6.eq) goto loc_822E8080;
	// lwz r11,100(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 100);
	// ori r10,r11,15
	ctx.r10.u64 = r11.u64 | 15;
	// stw r10,100(r30)
	PPC_STORE_U32(r30.u32 + 100, ctx.r10.u32);
loc_822E8080:
	// addi r3,r1,1664
	ctx.r3.s64 = ctx.r1.s64 + 1664;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e17d0
	sub_821E17D0(ctx, base);
	// lwz r26,312(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
loc_822E80A0:
	// lwz r11,2480(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2480);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8290f160
	sub_8290F160(ctx, base);
	// b 0x822ea8a8
	goto loc_822EA8A8;
loc_822E80BC:
	// lwz r11,2512(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2512);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8128
	if (cr6.eq) goto loc_822E8128;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822ade58
	sub_822ADE58(ctx, base);
	// lwz r11,2512(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2512);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8124
	if (cr6.eq) goto loc_822E8124;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E80EC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e80ec
	if (!cr0.eq) goto loc_822E80EC;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e8120
	if (!cr6.eq) goto loc_822E8120;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E8120:
	// stw r27,2512(r22)
	PPC_STORE_U32(r22.u32 + 2512, r27.u32);
loc_822E8124:
	// stw r27,2512(r22)
	PPC_STORE_U32(r22.u32 + 2512, r27.u32);
loc_822E8128:
	// lwz r11,2516(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2516);
	// addi r31,r22,2516
	r31.s64 = r22.s64 + 2516;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8158
	if (cr6.eq) goto loc_822E8158;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E8158:
	// lwz r11,2532(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2532);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e81c4
	if (cr6.eq) goto loc_822E81C4;
	// li r5,0
	ctx.r5.s64 = 0;
	// rotlwi r4,r11,0
	ctx.r4.u64 = __builtin_rotateleft32(r11.u32, 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825154d8
	sub_825154D8(ctx, base);
	// lwz r11,2532(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2532);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e81c0
	if (cr6.eq) goto loc_822E81C0;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E8188:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e8188
	if (!cr0.eq) goto loc_822E8188;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e81bc
	if (!cr6.eq) goto loc_822E81BC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E81BC:
	// stw r27,2532(r22)
	PPC_STORE_U32(r22.u32 + 2532, r27.u32);
loc_822E81C0:
	// stw r27,2532(r22)
	PPC_STORE_U32(r22.u32 + 2532, r27.u32);
loc_822E81C4:
	// lwz r11,2536(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2536);
	// addi r31,r22,2536
	r31.s64 = r22.s64 + 2536;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e81f4
	if (cr6.eq) goto loc_822E81F4;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E81F4:
	// lwz r11,2524(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2524);
	// addi r31,r22,2524
	r31.s64 = r22.s64 + 2524;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8224
	if (cr6.eq) goto loc_822E8224;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822E8224:
	// lwz r11,1992(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1992);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e837c
	if (!cr6.eq) goto loc_822E837C;
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e8250
	if (cr6.eq) goto loc_822E8250;
	// lfs f1,9492(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 9492);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8228b6b0
	sub_8228B6B0(ctx, base);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// b 0x822e8254
	goto loc_822E8254;
loc_822E8250:
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
loc_822E8254:
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r5,1984(r22)
	ctx.r5.u64 = PPC_LOAD_U32(r22.u32 + 1984);
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,612
	ctx.r3.s64 = ctx.r1.s64 + 612;
	// bl 0x821cdd88
	sub_821CDD88(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,1992(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 1992);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpw cr6,r3,r11
	cr6.compare<int32_t>(ctx.r3.s32, r11.s32, xer);
	// beq cr6,0x822e82fc
	if (cr6.eq) goto loc_822E82FC;
	// rotlwi r11,r3,0
	r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e82c8
	if (cr6.eq) goto loc_822E82C8;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822E8290:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822e8290
	if (!cr0.eq) goto loc_822E8290;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e82c4
	if (!cr6.eq) goto loc_822E82C4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E82C4:
	// stw r27,1992(r22)
	PPC_STORE_U32(r22.u32 + 1992, r27.u32);
loc_822E82C8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r11,1992(r22)
	PPC_STORE_U32(r22.u32 + 1992, r11.u32);
	// beq cr6,0x822e82fc
	if (cr6.eq) goto loc_822E82FC;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822E82E0:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e82e0
	if (!cr0.eq) goto loc_822E82E0;
loc_822E82FC:
	// lwz r11,612(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 612);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e834c
	if (cr6.eq) goto loc_822E834C;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822E8310:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822e8310
	if (!cr0.eq) goto loc_822E8310;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822e8348
	if (!cr6.eq) goto loc_822E8348;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E8348:
	// stw r27,612(r1)
	PPC_STORE_U32(ctx.r1.u32 + 612, r27.u32);
loc_822E834C:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x822e837c
	if (cr6.eq) goto loc_822E837C;
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x822e837c
	if (cr6.eq) goto loc_822E837C;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r5,316(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
loc_822E837C:
	// lfs f0,820(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 820);
	f0.f64 = double(temp.f32);
	// fsubs f13,f0,f19
	ctx.f13.f64 = double(float(f0.f64 - f19.f64));
	// stfs f13,820(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 820, temp.u32);
	// fcmpu cr6,f13,f28
	cr6.compare(ctx.f13.f64, f28.f64);
	// bge cr6,0x822e8394
	if (!cr6.lt) goto loc_822E8394;
	// stfs f28,820(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 820, temp.u32);
loc_822E8394:
	// lwz r30,320(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// fsubs f13,f23,f18
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(f23.f64 - f18.f64));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// lfs f0,10792(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 10792);
	f0.f64 = double(temp.f32);
	// mr r31,r27
	r31.u64 = r27.u64;
	// lvx128 v0,r0,r30
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// fmadds f12,f13,f0,f18
	ctx.fpscr.disableFlushModeUnconditional();
	ctx.f12.f64 = double(float(ctx.f13.f64 * f0.f64 + f18.f64));
	// fmuls f11,f12,f12
	ctx.f11.f64 = double(float(ctx.f12.f64 * ctx.f12.f64));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f10,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f10.f64 = double(temp.f32);
	// fcmpu cr6,f10,f11
	cr6.compare(ctx.f10.f64, ctx.f11.f64);
	// blt cr6,0x822e83d0
	if (cr6.lt) goto loc_822E83D0;
	// li r31,1
	r31.s64 = 1;
loc_822E83D0:
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8464
	if (cr6.eq) goto loc_822E8464;
	// stw r27,640(r1)
	PPC_STORE_U32(ctx.r1.u32 + 640, r27.u32);
	// addi r4,r1,640
	ctx.r4.s64 = ctx.r1.s64 + 640;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8464
	if (cr6.eq) goto loc_822E8464;
	// lwz r11,640(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 640);
	// lfs f31,8940(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + 8940);
	f31.f64 = double(temp.f32);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822e8420
	if (!cr6.eq) goto loc_822E8420;
	// lfs f0,8364(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 8364);
	f0.f64 = double(temp.f32);
	// lfs f13,688(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 688);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// blt cr6,0x822e8420
	if (cr6.lt) goto loc_822E8420;
	// lfs f0,9492(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 9492);
	f0.f64 = double(temp.f32);
	// fadds f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 + f0.f64));
loc_822E8420:
	// addi r3,r1,2048
	ctx.r3.s64 = ctx.r1.s64 + 2048;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r11,r1,656
	r11.s64 = ctx.r1.s64 + 656;
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// lvx128 v0,r0,r30
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r30.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmuls f0,f31,f31
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f31.f64 * f31.f64));
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v11,v13,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// vand v10,v11,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v9,v10,v11
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v11.f32), 0xEF));
	// stvx128 v9,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x822e8464
	if (cr6.gt) goto loc_822E8464;
	// mr r31,r27
	r31.u64 = r27.u64;
loc_822E8464:
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// lfs f13,2448(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2448);
	ctx.f13.f64 = double(temp.f32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f0,2832(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2832);
	f0.f64 = double(temp.f32);
	// beq cr6,0x822e849c
	if (cr6.eq) goto loc_822E849C;
	// fmadds f11,f19,f0,f13
	ctx.f11.f64 = double(float(f19.f64 * f0.f64 + ctx.f13.f64));
	// lfs f12,140(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + 140);
	ctx.f12.f64 = double(temp.f32);
	// stfs f11,2448(r22)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(r22.u32 + 2448, temp.u32);
	// fcmpu cr6,f11,f12
	cr6.compare(ctx.f11.f64, ctx.f12.f64);
	// ble cr6,0x822e84b0
	if (!cr6.gt) goto loc_822E84B0;
	// fmr f0,f12
	f0.f64 = ctx.f12.f64;
	// stfs f0,2448(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2448, temp.u32);
	// b 0x822e84b0
	goto loc_822E84B0;
loc_822E849C:
	// fnmsubs f12,f19,f0,f13
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(-(f19.f64 * f0.f64 - ctx.f13.f64)));
	// stfs f12,2448(r22)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r22.u32 + 2448, temp.u32);
	// fcmpu cr6,f12,f28
	cr6.compare(ctx.f12.f64, f28.f64);
	// bge cr6,0x822e84b0
	if (!cr6.lt) goto loc_822E84B0;
	// stfs f28,2448(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2448, temp.u32);
loc_822E84B0:
	// lfs f0,2452(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2452);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// ble cr6,0x822e84d0
	if (!cr6.gt) goto loc_822E84D0;
	// fsubs f0,f0,f19
	f0.f64 = double(float(f0.f64 - f19.f64));
	// stfs f0,2452(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2452, temp.u32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x822e84d0
	if (!cr6.lt) goto loc_822E84D0;
	// stfs f28,2452(r22)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2452, temp.u32);
loc_822E84D0:
	// lfs f13,2448(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2448);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,-256(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -256);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x822e8538
	if (!cr6.gt) goto loc_822E8538;
	// lfs f0,2452(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2452);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bne cr6,0x822e8538
	if (!cr6.eq) goto loc_822E8538;
	// lwz r11,2480(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2480);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e850c
	if (!cr6.eq) goto loc_822E850C;
	// vor128 v1,v125,v125
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v125.u8));
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r4,316(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// bl 0x8290ee00
	sub_8290EE00(ctx, base);
loc_822E850C:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lfs f0,2456(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2456);
	f0.f64 = double(temp.f32);
	// addi r26,r22,2456
	r26.s64 = r22.s64 + 2456;
	// lfs f13,8364(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 8364);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f19,f13,f0
	ctx.f13.f64 = double(float(f19.f64 * ctx.f13.f64 + f0.f64));
	// lfs f0,140(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 140);
	f0.f64 = double(temp.f32);
	// stfs f13,2456(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2456, temp.u32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x822e85c0
	if (!cr6.gt) goto loc_822E85C0;
	// stfs f0,0(r26)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r26.u32 + 0, temp.u32);
	// b 0x822e85c0
	goto loc_822E85C0;
loc_822E8538:
	// lwz r11,308(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lfs f0,2456(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2456);
	f0.f64 = double(temp.f32);
	// addi r26,r22,2456
	r26.s64 = r22.s64 + 2456;
	// lfs f13,140(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bne cr6,0x822e85a0
	if (!cr6.eq) goto loc_822E85A0;
	// lwz r3,372(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e8584
	if (cr6.eq) goto loc_822E8584;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x821e7b18
	sub_821E7B18(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8584
	if (cr6.eq) goto loc_822E8584;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r5,316(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r4,312(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// b 0x822e8594
	goto loc_822E8594;
loc_822E8584:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lwz r5,316(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r4,312(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// bl 0x8290ea98
	sub_8290EA98(ctx, base);
loc_822E8594:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
loc_822E85A0:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,2828(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2828);
	f0.f64 = double(temp.f32);
	// fnmsubs f12,f19,f0,f13
	ctx.f12.f64 = double(float(-(f19.f64 * f0.f64 - ctx.f13.f64)));
	// stfs f12,0(r26)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(r26.u32 + 0, temp.u32);
	// fcmpu cr6,f12,f28
	cr6.compare(ctx.f12.f64, f28.f64);
	// bgt cr6,0x822e85c0
	if (cr6.gt) goto loc_822E85C0;
	// stfs f28,0(r26)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r26.u32 + 0, temp.u32);
loc_822E85C0:
	// lfs f0,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bne cr6,0x822e85ec
	if (!cr6.eq) goto loc_822E85EC;
	// lwz r11,2480(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2480);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e85ec
	if (cr6.eq) goto loc_822E85EC;
	// lwz r14,312(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// bl 0x8290f160
	sub_8290F160(ctx, base);
	// b 0x822e85f0
	goto loc_822E85F0;
loc_822E85EC:
	// lwz r14,312(r1)
	r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 312);
loc_822E85F0:
	// lwz r23,308(r1)
	r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lfs f12,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// addi r3,r1,1696
	ctx.r3.s64 = ctx.r1.s64 + 1696;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lfs f0,9492(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 9492);
	f0.f64 = double(temp.f32);
	// lfs f13,-256(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f31,f12,f0,f13
	f31.f64 = double(float(ctx.f12.f64 * f0.f64 + ctx.f13.f64));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r29,r22,2080
	r29.s64 = r22.s64 + 2080;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,464
	ctx.r10.s64 = ctx.r1.s64 + 464;
	// lfs f0,8364(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 8364);
	f0.f64 = double(temp.f32);
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// fmuls f10,f31,f0
	ctx.f10.f64 = double(float(f31.f64 * f0.f64));
	// lfs f13,18868(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 18868);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,10792(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 10792);
	f0.f64 = double(temp.f32);
	// fmuls f11,f31,f13
	ctx.f11.f64 = double(float(f31.f64 * ctx.f13.f64));
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// fmuls f0,f31,f0
	f0.f64 = double(float(f31.f64 * f0.f64));
	// vsubfp v12,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f30,468(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	f30.f64 = double(temp.f32);
	// stvx128 v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f31,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f31.f64 = double(temp.f32);
	// fmuls f9,f31,f31
	ctx.f9.f64 = double(float(f31.f64 * f31.f64));
	// fmadds f13,f30,f30,f9
	ctx.f13.f64 = double(float(f30.f64 * f30.f64 + ctx.f9.f64));
	// fcmpu cr6,f13,f10
	cr6.compare(ctx.f13.f64, ctx.f10.f64);
	// blt cr6,0x822e867c
	if (cr6.lt) goto loc_822E867C;
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822e867c
	if (!cr6.eq) goto loc_822E867C;
	// lfs f12,2096(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2096);
	ctx.f12.f64 = double(temp.f32);
	// fadds f10,f19,f12
	ctx.f10.f64 = double(float(f19.f64 + ctx.f12.f64));
	// stfs f10,2096(r22)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(r22.u32 + 2096, temp.u32);
	// b 0x822e8680
	goto loc_822E8680;
loc_822E867C:
	// stfs f28,2096(r22)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(r22.u32 + 2096, temp.u32);
loc_822E8680:
	// lfs f10,2096(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2096);
	ctx.f10.f64 = double(temp.f32);
	// lfs f12,8364(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 8364);
	ctx.f12.f64 = double(temp.f32);
	// fcmpu cr6,f10,f12
	cr6.compare(ctx.f10.f64, ctx.f12.f64);
	// ble cr6,0x822e86b8
	if (!cr6.gt) goto loc_822E86B8;
	// addi r3,r1,1984
	ctx.r3.s64 = ctx.r1.s64 + 1984;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e17d0
	sub_821E17D0(ctx, base);
	// lwz r18,576(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	// lwz r24,320(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// b 0x822e89f8
	goto loc_822E89F8;
loc_822E86B8:
	// fsubs f10,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(ctx.f13.f64 - f0.f64));
	// lfs f13,18916(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 18916);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f9,f11,f0
	ctx.f9.f64 = double(float(ctx.f11.f64 - f0.f64));
	// stfs f20,288(r1)
	temp.f32 = float(f20.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// stfs f12,272(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r4,r1,288
	ctx.r4.s64 = ctx.r1.s64 + 288;
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// fdivs f8,f10,f9
	ctx.f8.f64 = double(float(ctx.f10.f64 / ctx.f9.f64));
	// fmadds f7,f8,f13,f12
	ctx.f7.f64 = double(float(ctx.f8.f64 * ctx.f13.f64 + ctx.f12.f64));
	// stfs f7,304(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 304, temp.u32);
	// bl 0x821714f8
	sub_821714F8(ctx, base);
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// addi r8,r1,400
	ctx.r8.s64 = ctx.r1.s64 + 400;
	// addi r7,r1,304
	ctx.r7.s64 = ctx.r1.s64 + 304;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stvx128 v125,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// stvx128 v125,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v0,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v11,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// lfs f1,-25888(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// vmulfp128 v12,v125,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(v125.f32), _mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v126,v12,v10
	_mm_store_ps(v126.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)));
	// lfs f6,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f6.f64 = double(temp.f32);
	// lfs f5,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f5.f64 = double(temp.f32);
	// stfs f5,276(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 276, temp.u32);
	// stfs f6,272(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// bl 0x822a8ef8
	sub_822A8EF8(ctx, base);
	// lfs f4,272(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f4.f64 = double(temp.f32);
	// fmuls f3,f4,f31
	ctx.f3.f64 = double(float(ctx.f4.f64 * f31.f64));
	// lfs f2,276(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	ctx.f2.f64 = double(temp.f32);
	// lfs f13,9492(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 9492);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f2,f30,f3
	f0.f64 = double(float(ctx.f2.f64 * f30.f64 + ctx.f3.f64));
	// ld r30,272(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 272);
	// fcmpu cr6,f0,f13
	cr6.compare(f0.f64, ctx.f13.f64);
	// bge cr6,0x822e878c
	if (!cr6.lt) goto loc_822E878C;
	// fsubs f0,f13,f0
	f0.f64 = double(float(ctx.f13.f64 - f0.f64));
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// ble cr6,0x822e878c
	if (!cr6.gt) goto loc_822E878C;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// addi r3,r1,1728
	ctx.r3.s64 = ctx.r1.s64 + 1728;
	// bl 0x82193850
	sub_82193850(ctx, base);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v13,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v13,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vmaddfp128 v126,v0,v12,v126
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(v126.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32)), _mm_load_ps(v126.f32)));
loc_822E878C:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,59
	ctx.r4.s64 = 59;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8848
	if (cr6.eq) goto loc_822E8848;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,59
	ctx.r10.s64 = 59;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e87dc
	if (cr6.eq) goto loc_822E87DC;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,59(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 59);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e8850
	goto loc_822E8850;
loc_822E87DC:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e8834
	if (cr6.eq) goto loc_822E8834;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,59
	cr6.compare<int32_t>(ctx.r10.s32, 59, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e8810
	if (cr6.gt) goto loc_822E8810;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822E8810:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e8834
	if (!cr6.eq) goto loc_822E8834;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// li r10,1
	ctx.r10.s64 = 1;
	// std r11,1040(r1)
	PPC_STORE_U64(ctx.r1.u32 + 1040, r11.u64);
	// lwz r11,1044(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1044);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// b 0x822e8850
	goto loc_822E8850;
loc_822E8834:
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,1040(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1040, ctx.r10.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// b 0x822e8850
	goto loc_822E8850;
loc_822E8848:
	// lwz r11,636(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	// li r10,0
	ctx.r10.s64 = 0;
loc_822E8850:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e8924
	if (cr6.eq) goto loc_822E8924;
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// bl 0x82267268
	sub_82267268(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8924
	if (cr6.eq) goto loc_822E8924;
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r24,320(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// lvx128 v127,r0,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v12,v127,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v11,v12,v0
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v10,v11,v125
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v10,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// blt cr6,0x822e8928
	if (cr6.lt) goto loc_822E8928;
	// vand128 v12,v125,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r11,r1,464
	r11.s64 = ctx.r1.s64 + 464;
	// vsubfp128 v11,v13,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v127.f32)));
	// lfs f0,304(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 304);
	f0.f64 = double(temp.f32);
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// vmsum3fp128 v10,v12,v125
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v125.f32), 0xEF));
	// vand v9,v11,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v8,v9,v11
	_mm_store_ps(ctx.v8.f32, _mm_dp_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v11.f32), 0xEF));
	// stvx128 v10,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v8,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,464(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f12.f64 = double(temp.f32);
	// fsqrts f11,f12
	ctx.f11.f64 = double(float(sqrt(ctx.f12.f64)));
	// lfs f13,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f10,f11,f0
	ctx.f10.f64 = double(float(ctx.f11.f64 * f0.f64));
	// fmuls f9,f10,f19
	ctx.f9.f64 = double(float(ctx.f10.f64 * f19.f64));
	// fmuls f8,f9,f9
	ctx.f8.f64 = double(float(ctx.f9.f64 * ctx.f9.f64));
	// fcmpu cr6,f13,f8
	cr6.compare(ctx.f13.f64, ctx.f8.f64);
	// bge cr6,0x822e8928
	if (!cr6.lt) goto loc_822E8928;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// addi r3,r1,2080
	ctx.r3.s64 = ctx.r1.s64 + 2080;
	// bl 0x82193850
	sub_82193850(ctx, base);
	// lfs f0,360(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 360);
	f0.f64 = double(temp.f32);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// lvx128 v0,r0,r29
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v13,v127,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_sub_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v0.f32)));
	// lvlx v12,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// vmulfp128 v126,v13,v11
	_mm_store_ps(v126.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32)));
	// b 0x822e8928
	goto loc_822E8928;
loc_822E8924:
	// lwz r24,320(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
loc_822E8928:
	// addi r3,r1,1760
	ctx.r3.s64 = ctx.r1.s64 + 1760;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lvx128 v127,r0,r29
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// vaddfp128 v13,v127,v126
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_add_ps(_mm_load_ps(v127.f32), _mm_load_ps(v126.f32)));
	// lwz r18,576(r1)
	r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 576);
	// addi r9,r1,344
	ctx.r9.s64 = ctx.r1.s64 + 344;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// lvx128 v12,r0,r11
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r18
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v1,v13,v11,v0
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// bl 0x82295d30
	sub_82295D30(ctx, base);
	// clrlwi r8,r3,24
	ctx.r8.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822e8980
	if (cr6.eq) goto loc_822E8980;
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e89f0
	goto loc_822E89F0;
loc_822E8980:
	// addi r3,r1,2016
	ctx.r3.s64 = ctx.r1.s64 + 2016;
	// lfs f0,-256(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	f0.f64 = double(temp.f32);
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lvx128 v127,r0,r29
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// lvx128 v0,r0,r18
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// addi r8,r1,344
	ctx.r8.s64 = ctx.r1.s64 + 344;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v13,r0,r11
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v10,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmaddfp128 v127,v126,v11,v127
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(v127.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(v127.f32)));
	// vperm128 v127,v127,v10,v0
	_mm_store_si128((__m128i*)v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x82295d30
	sub_82295D30(ctx, base);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822e89ec
	if (cr6.eq) goto loc_822E89EC;
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e89f0
	goto loc_822E89F0;
loc_822E89EC:
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
loc_822E89F0:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821e17d0
	sub_821E17D0(ctx, base);
loc_822E89F8:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lwz r10,2480(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 2480);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lfs f29,2824(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2824);
	f29.f64 = double(temp.f32);
	// beq cr6,0x822e8b2c
	if (cr6.eq) goto loc_822E8B2C;
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fsqrts f13,f0
	ctx.f13.f64 = double(float(sqrt(f0.f64)));
	// fdivs f0,f13,f23
	f0.f64 = double(float(ctx.f13.f64 / f23.f64));
	// fcmpu cr6,f0,f29
	cr6.compare(f0.f64, f29.f64);
	// bge cr6,0x822e8a3c
	if (!cr6.lt) goto loc_822E8A3C;
	// fmr f0,f29
	f0.f64 = f29.f64;
	// b 0x822e8a48
	goto loc_822E8A48;
loc_822E8A3C:
	// fcmpu cr6,f0,f25
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f25.f64);
	// ble cr6,0x822e8a48
	if (!cr6.gt) goto loc_822E8A48;
	// fmr f0,f25
	f0.f64 = f25.f64;
loc_822E8A48:
	// lfs f13,2460(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2460);
	ctx.f13.f64 = double(temp.f32);
	// stfs f13,2464(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2464, temp.u32);
	// lfs f12,2460(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2460);
	ctx.f12.f64 = double(temp.f32);
	// fmadds f1,f0,f19,f12
	ctx.f1.f64 = double(float(f0.f64 * f19.f64 + ctx.f12.f64));
	// stfs f1,2460(r22)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r22.u32 + 2460, temp.u32);
	// lwz r11,2496(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2496);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8afc
	if (cr6.eq) goto loc_822E8AFC;
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,2496(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2496);
	// lfs f1,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// lfs f0,2460(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2460);
	f0.f64 = double(temp.f32);
	// lwz r3,2480(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2480);
	// fsubs f31,f0,f1
	f31.f64 = double(float(f0.f64 - ctx.f1.f64));
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r7,-31950
	ctx.r7.s64 = -2093875200;
	// lfs f13,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// lfs f0,2252(r7)
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + 2252);
	f0.f64 = double(temp.f32);
	// fsubs f12,f31,f0
	ctx.f12.f64 = double(float(f31.f64 - f0.f64));
	// fmuls f0,f12,f17
	f0.f64 = double(float(ctx.f12.f64 * f17.f64));
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bge cr6,0x822e8adc
	if (!cr6.lt) goto loc_822E8ADC;
	// fmr f0,f28
	f0.f64 = f28.f64;
	// lwz r3,2504(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2504);
	// fsubs f1,f13,f0
	ctx.f1.f64 = double(float(ctx.f13.f64 - f0.f64));
	// stfs f1,2468(r22)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r22.u32 + 2468, temp.u32);
	// bl 0x8228a618
	sub_8228A618(ctx, base);
	// b 0x822e8b10
	goto loc_822E8B10;
loc_822E8ADC:
	// fcmpu cr6,f0,f13
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, ctx.f13.f64);
	// ble cr6,0x822e8ae8
	if (!cr6.gt) goto loc_822E8AE8;
	// fmr f0,f13
	f0.f64 = ctx.f13.f64;
loc_822E8AE8:
	// fsubs f1,f13,f0
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(ctx.f13.f64 - f0.f64));
	// stfs f1,2468(r22)
	temp.f32 = float(ctx.f1.f64);
	PPC_STORE_U32(r22.u32 + 2468, temp.u32);
	// lwz r3,2504(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2504);
	// bl 0x8228a618
	sub_8228A618(ctx, base);
	// b 0x822e8b10
	goto loc_822E8B10;
loc_822E8AFC:
	// lwz r3,2480(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2480);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E8B10:
	// lwz r3,2488(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2488);
	// lfs f31,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	f31.f64 = double(temp.f32);
	// stfs f31,24(r3)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 24, temp.u32);
	// bl 0x8272cb38
	sub_8272CB38(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822e8b2c
	if (cr6.eq) goto loc_822E8B2C;
	// stfs f31,32(r3)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r3.u32 + 32, temp.u32);
loc_822E8B2C:
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8b58
	if (cr6.eq) goto loc_822E8B58;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e8b58
	if (cr6.eq) goto loc_822E8B58;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x822e8b5c
	if (!cr6.eq) goto loc_822E8B5C;
loc_822E8B58:
	// li r11,0
	r11.s64 = 0;
loc_822E8B5C:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8b74
	if (cr6.eq) goto loc_822E8B74;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x821d5e90
	sub_821D5E90(ctx, base);
	// b 0x822ea8a8
	goto loc_822EA8A8;
loc_822E8B74:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bgt cr6,0x822ea8a8
	if (cr6.gt) goto loc_822EA8A8;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-29796
	r12.s64 = r12.s64 + -29796;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822E8E14;
	case 1:
		goto loc_822E8E14;
	case 2:
		goto loc_822E8BB0;
	case 3:
		goto loc_822E8C34;
	case 4:
		goto loc_822E8DC8;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-29164(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -29164);
	// lwz r17,-29164(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -29164);
	// lwz r17,-29776(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -29776);
	// lwz r17,-29644(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -29644);
	// lwz r17,-29240(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -29240);
loc_822E8BB0:
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x828de210
	sub_828DE210(ctx, base);
	// addi r3,r1,1792
	ctx.r3.s64 = ctx.r1.s64 + 1792;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e17d0
	sub_821E17D0(ctx, base);
	// lwz r10,0(r21)
	ctx.r10.u64 = PPC_LOAD_U32(r21.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e8bf4
	if (cr6.eq) goto loc_822E8BF4;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r10,84(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 84);
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e8bf8
	if (!cr6.eq) goto loc_822E8BF8;
loc_822E8BF4:
	// li r11,0
	r11.s64 = 0;
loc_822E8BF8:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ea8a8
	if (!cr6.eq) goto loc_822EA8A8;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821d6c08
	sub_821D6C08(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// lwz r5,316(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// b 0x822ea8a8
	goto loc_822EA8A8;
loc_822E8C34:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x821d5e90
	sub_821D5E90(ctx, base);
	// addi r3,r1,2112
	ctx.r3.s64 = ctx.r1.s64 + 2112;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e17d0
	sub_821E17D0(ctx, base);
	// lwz r10,0(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e8c84
	if (cr6.eq) goto loc_822E8C84;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822e8c84
	if (cr6.eq) goto loc_822E8C84;
	// lwz r11,28(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 28);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x822e8c88
	if (!cr6.eq) goto loc_822E8C88;
loc_822E8C84:
	// li r11,0
	r11.s64 = 0;
loc_822E8C88:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ea8a8
	if (!cr6.eq) goto loc_822EA8A8;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r29,0
	r29.s64 = 0;
	// li r4,109
	ctx.r4.s64 = 109;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// mr r30,r29
	r30.u64 = r29.u64;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8d54
	if (cr6.eq) goto loc_822E8D54;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,109
	ctx.r10.s64 = 109;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8cec
	if (cr6.eq) goto loc_822E8CEC;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,109(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 109);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r11,1
	r11.s64 = 1;
	// lwz r30,4(r8)
	r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822e8d58
	goto loc_822E8D58;
loc_822E8CEC:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822e8d44
	if (cr6.eq) goto loc_822E8D44;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,109
	cr6.compare<int32_t>(ctx.r10.s32, 109, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822e8d20
	if (cr6.gt) goto loc_822E8D20;
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822E8D20:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e8d44
	if (!cr6.eq) goto loc_822E8D44;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,832(r1)
	PPC_STORE_U64(ctx.r1.u32 + 832, r11.u64);
	// lwz r11,836(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 836);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// b 0x822e8d58
	goto loc_822E8D58;
loc_822E8D44:
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// stw r29,832(r1)
	PPC_STORE_U32(ctx.r1.u32 + 832, r29.u32);
	// b 0x822e8d58
	goto loc_822E8D58;
loc_822E8D54:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822E8D58:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8d98
	if (cr6.eq) goto loc_822E8D98;
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822e8d98
	if (cr6.eq) goto loc_822E8D98;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// bl 0x821af050
	sub_821AF050(ctx, base);
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// lwz r10,4(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r3,124(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 124);
	// bl 0x822664d8
	sub_822664D8(ctx, base);
	// stw r29,80(r30)
	PPC_STORE_U32(r30.u32 + 80, r29.u32);
	// stw r29,92(r30)
	PPC_STORE_U32(r30.u32 + 92, r29.u32);
loc_822E8D98:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821d6c08
	sub_821D6C08(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// lwz r5,316(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// b 0x822ea8a8
	goto loc_822EA8A8;
loc_822E8DC8:
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lwz r27,316(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f14
	cr6.compare(f0.f64, f14.f64);
	// bgt cr6,0x822e8e00
	if (cr6.gt) goto loc_822E8E00;
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// b 0x822e8e04
	goto loc_822E8E04;
loc_822E8E00:
	// bl 0x8290ea98
	sub_8290EA98(ctx, base);
loc_822E8E04:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// b 0x822e8e18
	goto loc_822E8E18;
loc_822E8E14:
	// lwz r27,316(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
loc_822E8E18:
	// lfs f0,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f27,f28
	cr6.compare(f27.f64, f28.f64);
	// fmr f31,f0
	f31.f64 = f0.f64;
	// ble cr6,0x822e8e34
	if (!cr6.gt) goto loc_822E8E34;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,2820(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2820);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f31,f27,f13,f0
	f31.f64 = double(float(f27.f64 * ctx.f13.f64 + f0.f64));
loc_822E8E34:
	// fcmpu cr6,f24,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f24.f64, f28.f64);
	// ble cr6,0x822e8e4c
	if (!cr6.gt) goto loc_822E8E4C;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,2816(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2816);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f24,f13,f0
	ctx.f13.f64 = double(float(f24.f64 * ctx.f13.f64 + f0.f64));
	// fmuls f31,f13,f31
	f31.f64 = double(float(ctx.f13.f64 * f31.f64));
loc_822E8E4C:
	// fcmpu cr6,f22,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f22.f64, f28.f64);
	// ble cr6,0x822e8e64
	if (!cr6.gt) goto loc_822E8E64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,2812(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2812);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f22,f13,f0
	ctx.f13.f64 = double(float(f22.f64 * ctx.f13.f64 + f0.f64));
	// fmuls f31,f13,f31
	f31.f64 = double(float(ctx.f13.f64 * f31.f64));
loc_822E8E64:
	// fcmpu cr6,f21,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f21.f64, f28.f64);
	// ble cr6,0x822e8e7c
	if (!cr6.gt) goto loc_822E8E7C;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f13,2808(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2808);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f13,f21,f13,f0
	ctx.f13.f64 = double(float(f21.f64 * ctx.f13.f64 + f0.f64));
	// fmuls f31,f13,f31
	f31.f64 = double(float(ctx.f13.f64 * f31.f64));
loc_822E8E7C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822e8e8c
	if (cr6.eq) goto loc_822E8E8C;
	// lfs f13,15360(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 15360);
	ctx.f13.f64 = double(temp.f32);
	// fmuls f31,f31,f13
	f31.f64 = double(float(f31.f64 * ctx.f13.f64));
loc_822E8E8C:
	// lbz r11,2380(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2380);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e8ea4
	if (cr6.eq) goto loc_822E8EA4;
	// lfs f13,-256(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	ctx.f13.f64 = double(temp.f32);
	// fnmsubs f13,f16,f13,f0
	ctx.f13.f64 = double(float(-(f16.f64 * ctx.f13.f64 - f0.f64)));
	// fmuls f31,f13,f31
	f31.f64 = double(float(ctx.f13.f64 * f31.f64));
loc_822E8EA4:
	// fcmpu cr6,f26,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f26.f64, f28.f64);
	// ble cr6,0x822e8eb8
	if (!cr6.gt) goto loc_822E8EB8;
	// lfs f13,8912(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 8912);
	ctx.f13.f64 = double(temp.f32);
	// fmadds f0,f26,f13,f0
	f0.f64 = double(float(f26.f64 * ctx.f13.f64 + f0.f64));
	// fmuls f31,f0,f31
	f31.f64 = double(float(f0.f64 * f31.f64));
loc_822E8EB8:
	// lfs f0,820(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 820);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// bgt cr6,0x822e9188
	if (cr6.gt) goto loc_822E9188;
	// addi r31,r22,2016
	r31.s64 = r22.s64 + 2016;
	// addi r8,r1,688
	ctx.r8.s64 = ctx.r1.s64 + 688;
	// addi r10,r1,640
	ctx.r10.s64 = ctx.r1.s64 + 640;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r9,r19
	ctx.r9.u64 = r19.u64;
	// lvx128 v0,r31,r15
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r31.u32 + r15.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822E8EE4:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822e8ee4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822E8EE4;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x8226b928
	sub_8226B928(ctx, base);
	// addi r11,r1,752
	r11.s64 = ctx.r1.s64 + 752;
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// lbz r7,2382(r22)
	ctx.r7.u64 = PPC_LOAD_U8(r22.u32 + 2382);
	// addi r9,r1,688
	ctx.r9.s64 = ctx.r1.s64 + 688;
	// lvx128 v7,r0,r18
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,344
	ctx.r8.s64 = ctx.r1.s64 + 344;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v12,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v13,v13,v12,v7
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// beq cr6,0x822e8f44
	if (cr6.eq) goto loc_822E8F44;
	// fcmpu cr6,f16,f28
	ctx.fpscr.disableFlushMode();
	cr6.compare(f16.f64, f28.f64);
	// li r11,1
	r11.s64 = 1;
	// bne cr6,0x822e8f48
	if (!cr6.eq) goto loc_822E8F48;
loc_822E8F44:
	// li r11,0
	r11.s64 = 0;
loc_822E8F48:
	// vsubfp128 v127,v0,v13
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(v127.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v13.f32)));
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,656
	ctx.r10.s64 = ctx.r1.s64 + 656;
	// lbz r9,2383(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 2383);
	// addi r8,r1,336
	ctx.r8.s64 = ctx.r1.s64 + 336;
	// addi r7,r1,720
	ctx.r7.s64 = ctx.r1.s64 + 720;
	// addi r6,r1,464
	ctx.r6.s64 = ctx.r1.s64 + 464;
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// vand v12,v13,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lvx128 v11,r0,r7
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v10,v12,v11
	_mm_store_ps(ctx.v10.f32, _mm_dp_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v11.f32), 0xEF));
	// vor128 v9,v127,v127
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_load_si128((__m128i*)v127.u8));
	// vand v8,v9,v0
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v7,v8,v9
	_mm_store_ps(ctx.v7.f32, _mm_dp_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(ctx.v9.f32), 0xEF));
	// stvx128 v10,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f30,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f30.f64 = double(temp.f32);
	// beq cr6,0x822e8fd0
	if (cr6.eq) goto loc_822E8FD0;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// fsqrts f1,f30
	ctx.f1.f64 = double(float(sqrt(f30.f64)));
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e8fb4
	if (!cr6.eq) goto loc_822E8FB4;
	// lwz r4,368(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// b 0x822e8fbc
	goto loc_822E8FBC;
loc_822E8FB4:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E8FBC:
	// stfd f1,32(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.f1.u64);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r3,r11,-24656
	ctx.r3.s64 = r11.s64 + -24656;
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// bl 0x8290c698
	sub_8290C698(ctx, base);
loc_822E8FD0:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f0,2804(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 2804);
	f0.f64 = double(temp.f32);
	// li r11,1
	r11.s64 = 1;
	// fcmpu cr6,f30,f0
	cr6.compare(f30.f64, f0.f64);
	// bge cr6,0x822e8fe8
	if (!cr6.lt) goto loc_822E8FE8;
	// li r11,0
	r11.s64 = 0;
loc_822E8FE8:
	// lis r9,-31950
	ctx.r9.s64 = -2093875200;
	// lfs f13,464(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f13.f64 = double(temp.f32);
	// clrlwi r10,r11,24
	ctx.r10.u64 = r11.u32 & 0xFF;
	// li r11,1
	r11.s64 = 1;
	// lfs f0,2320(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 2320);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// ble cr6,0x822e9008
	if (!cr6.gt) goto loc_822E9008;
	// li r11,0
	r11.s64 = 0;
loc_822E9008:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e90ac
	if (!cr6.eq) goto loc_822E90AC;
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e90ac
	if (!cr6.eq) goto loc_822E90AC;
	// clrlwi r11,r10,24
	r11.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9188
	if (cr6.eq) goto loc_822E9188;
	// addi r3,r1,640
	ctx.r3.s64 = ctx.r1.s64 + 640;
	// bl 0x8223bd98
	sub_8223BD98(ctx, base);
	// lfs f0,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f0.f64 = double(temp.f32);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// addi r9,r11,-28160
	ctx.r9.s64 = r11.s64 + -28160;
	// addi r8,r1,688
	ctx.r8.s64 = ctx.r1.s64 + 688;
	// addi r4,r1,640
	ctx.r4.s64 = ctx.r1.s64 + 640;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm128 v12,v127,v13,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822b7f50
	sub_822B7F50(ctx, base);
	// addi r4,r1,640
	ctx.r4.s64 = ctx.r1.s64 + 640;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x822b7f50
	sub_822B7F50(ctx, base);
	// addi r5,r1,640
	ctx.r5.s64 = ctx.r1.s64 + 640;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r11,r1,704
	r11.s64 = ctx.r1.s64 + 704;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822E9094:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822e9094
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822E9094;
	// b 0x822e9188
	goto loc_822E9188;
loc_822E90AC:
	// addi r4,r1,640
	ctx.r4.s64 = ctx.r1.s64 + 640;
	// addi r3,r1,2544
	ctx.r3.s64 = ctx.r1.s64 + 2544;
	// bl 0x82260fa0
	sub_82260FA0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r5,r1,704
	ctx.r5.s64 = ctx.r1.s64 + 704;
	// addi r3,r1,768
	ctx.r3.s64 = ctx.r1.s64 + 768;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r3,r1,1360
	ctx.r3.s64 = ctx.r1.s64 + 1360;
	// lfs f3,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r9,r11,-28400
	ctx.r9.s64 = r11.s64 + -28400;
	// addi r8,r1,800
	ctx.r8.s64 = ctx.r1.s64 + 800;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// addi r3,r1,768
	ctx.r3.s64 = ctx.r1.s64 + 768;
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand v12,v13,v0
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lfs f1,-28492(r7)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r7.u32 + -28492);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v12,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82275890
	sub_82275890(ctx, base);
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// addi r5,r1,816
	ctx.r5.s64 = ctx.r1.s64 + 816;
	// lfs f0,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f0.f64 = double(temp.f32);
	// addi r11,r6,-28160
	r11.s64 = ctx.r6.s64 + -28160;
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// addi r9,r1,816
	ctx.r9.s64 = ctx.r1.s64 + 816;
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// lvx128 v10,r0,r5
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v9,v10,v11,v0
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v9,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822b7f50
	sub_822B7F50(ctx, base);
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x822b7f50
	sub_822B7F50(ctx, base);
	// addi r5,r1,768
	ctx.r5.s64 = ctx.r1.s64 + 768;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r11,r1,704
	r11.s64 = ctx.r1.s64 + 704;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822E9174:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822e9174
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822E9174;
loc_822E9188:
	// addi r3,r1,1392
	ctx.r3.s64 = ctx.r1.s64 + 1392;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f12,344(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f12.f64 = double(temp.f32);
	// fmr f1,f12
	ctx.f1.f64 = ctx.f12.f64;
	// bl 0x821b00b0
	sub_821B00B0(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x821b00b0
	sub_821B00B0(ctx, base);
	// lfs f3,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// lfs f0,0(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f3
	cr6.compare(f0.f64, ctx.f3.f64);
	// beq cr6,0x822e9228
	if (cr6.eq) goto loc_822E9228;
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822e9218
	if (!cr6.eq) goto loc_822E9218;
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fsqrts f13,f0
	ctx.f13.f64 = double(float(sqrt(f0.f64)));
	// fdivs f0,f13,f18
	f0.f64 = double(float(ctx.f13.f64 / f18.f64));
	// fcmpu cr6,f0,f29
	cr6.compare(f0.f64, f29.f64);
	// bge cr6,0x822e9208
	if (!cr6.lt) goto loc_822E9208;
	// fmr f0,f29
	f0.f64 = f29.f64;
	// b 0x822e9214
	goto loc_822E9214;
loc_822E9208:
	// fcmpu cr6,f0,f25
	ctx.fpscr.disableFlushMode();
	cr6.compare(f0.f64, f25.f64);
	// ble cr6,0x822e9214
	if (!cr6.gt) goto loc_822E9214;
	// fmr f0,f25
	f0.f64 = f25.f64;
loc_822E9214:
	// fmuls f31,f31,f0
	ctx.fpscr.disableFlushMode();
	f31.f64 = double(float(f31.f64 * f0.f64));
loc_822E9218:
	// lfs f0,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	f0.f64 = double(temp.f32);
	// fmadds f13,f31,f19,f0
	ctx.f13.f64 = double(float(f31.f64 * f19.f64 + f0.f64));
	// lfs f3,140(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// stfs f13,816(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 816, temp.u32);
loc_822E9228:
	// lfs f5,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f5.f64 = double(temp.f32);
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// fmr f1,f5
	ctx.f1.f64 = ctx.f5.f64;
	// bl 0x821af298
	sub_821AF298(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e972c
	if (cr6.eq) goto loc_822E972C;
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// fmr f1,f5
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f5.f64;
	// bl 0x822692d8
	sub_822692D8(ctx, base);
	// fcmpu cr6,f1,f3
	ctx.fpscr.disableFlushMode();
	cr6.compare(ctx.f1.f64, ctx.f3.f64);
	// bge cr6,0x822e972c
	if (!cr6.lt) goto loc_822E972C;
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r25,2
	r25.s64 = 2;
	// stw r10,768(r1)
	PPC_STORE_U32(ctx.r1.u32 + 768, ctx.r10.u32);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822e92a8
	if (!cr6.eq) goto loc_822E92A8;
	// lwz r11,1424(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822e928c
	if (!cr6.eq) goto loc_822E928C;
	// li r11,1
	r11.s64 = 1;
	// stw r25,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r25.u32);
	// stb r11,2384(r22)
	PPC_STORE_U8(r22.u32 + 2384, r11.u8);
loc_822E928C:
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e92a8
	if (!cr6.eq) goto loc_822E92A8;
	// stw r25,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r25.u32);
loc_822E92A8:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822e92e4
	if (!cr6.eq) goto loc_822E92E4;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821d6c08
	sub_821D6C08(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822e92e4
	if (!cr6.eq) goto loc_822E92E4;
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e92e4
	if (!cr6.eq) goto loc_822E92E4;
	// stw r25,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r25.u32);
loc_822E92E4:
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x822b8658
	sub_822B8658(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// addi r11,r11,-2
	r11.s64 = r11.s64 + -2;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bgt cr6,0x822e968c
	if (cr6.gt) goto loc_822E968C;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-27884
	r12.s64 = r12.s64 + -27884;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822E9324;
	case 1:
		goto loc_822E93C0;
	case 2:
		goto loc_822E9324;
	case 3:
		goto loc_822E9324;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-27868(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -27868);
	// lwz r17,-27712(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -27712);
	// lwz r17,-27868(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -27868);
	// lwz r17,-27868(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -27868);
loc_822E9324:
	// lwz r11,1424(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmpwi cr6,r10,1
	cr6.compare<int32_t>(ctx.r10.s32, 1, xer);
	// bne cr6,0x822e9364
	if (!cr6.eq) goto loc_822E9364;
	// lwz r11,0(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 0);
	// lwz r10,8(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// lwz r11,20(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r7,r8,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// xori r6,r7,1
	ctx.r6.u64 = ctx.r7.u64 ^ 1;
	// mulli r11,r6,168
	r11.s64 = ctx.r6.s64 * 168;
	// add r5,r11,r10
	ctx.r5.u64 = r11.u64 + ctx.r10.u64;
	// lwz r4,176(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 176);
	// stw r4,864(r1)
	PPC_STORE_U32(ctx.r1.u32 + 864, ctx.r4.u32);
	// b 0x822e968c
	goto loc_822E968C;
loc_822E9364:
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// lwz r10,52(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 52);
	// lwz r9,56(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stw r10,864(r1)
	PPC_STORE_U32(ctx.r1.u32 + 864, ctx.r10.u32);
	// beq cr6,0x822e968c
	if (cr6.eq) goto loc_822E968C;
	// lwz r11,1424(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// addi r10,r22,16
	ctx.r10.s64 = r22.s64 + 16;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822e968c
	if (!cr6.eq) goto loc_822E968C;
	// lis r11,1
	r11.s64 = 65536;
	// lwz r10,824(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 824);
	// ori r9,r11,3533
	ctx.r9.u64 = r11.u64 | 3533;
	// mullw r11,r10,r9
	r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// rlwinm r8,r11,0,17,24
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x7F80;
	// stw r11,824(r22)
	PPC_STORE_U32(r22.u32 + 824, r11.u32);
	// cmplwi cr6,r8,30976
	cr6.compare<uint32_t>(ctx.r8.u32, 30976, xer);
	// bge cr6,0x822e968c
	if (!cr6.lt) goto loc_822E968C;
	// lwz r11,8(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 8);
	// lwz r10,56(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 56);
	// stw r10,864(r1)
	PPC_STORE_U32(ctx.r1.u32 + 864, ctx.r10.u32);
	// b 0x822e968c
	goto loc_822E968C;
loc_822E93C0:
	// lbz r11,2382(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2382);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e942c
	if (cr6.eq) goto loc_822E942C;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// addi r3,r1,2480
	ctx.r3.s64 = ctx.r1.s64 + 2480;
	// bl 0x82191990
	sub_82191990(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// lfs f0,8364(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 8364);
	f0.f64 = double(temp.f32);
	// addi r3,r1,2352
	ctx.r3.s64 = ctx.r1.s64 + 2352;
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lvx128 v127,r11,r15
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32 + r15.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82191990
	sub_82191990(ctx, base);
	// addi r10,r1,784
	ctx.r10.s64 = ctx.r1.s64 + 784;
	// lvx128 v0,r3,r15
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32 + r15.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,272
	ctx.r9.s64 = ctx.r1.s64 + 272;
	// lfs f13,816(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 816);
	ctx.f13.f64 = double(temp.f32);
	// addi r8,r1,784
	ctx.r8.s64 = ctx.r1.s64 + 784;
	// lfs f0,8364(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 8364);
	f0.f64 = double(temp.f32);
	// fmuls f12,f13,f0
	ctx.f12.f64 = double(float(ctx.f13.f64 * f0.f64));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v12,v13,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(v127.f32)));
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// stfs f12,816(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 816, temp.u32);
	// vmaddfp v9,v12,v10,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v9.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E942C:
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f14
	cr6.compare(f0.f64, f14.f64);
	// bgt cr6,0x822e9468
	if (cr6.gt) goto loc_822E9468;
	// addi r3,r1,1424
	ctx.r3.s64 = ctx.r1.s64 + 1424;
	// fmr f3,f28
	ctx.f3.f64 = f28.f64;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e94cc
	goto loc_822E94CC;
loc_822E9468:
	// addi r3,r1,1456
	ctx.r3.s64 = ctx.r1.s64 + 1456;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lfs f0,-256(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	f0.f64 = double(temp.f32);
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,288
	ctx.r9.s64 = ctx.r1.s64 + 288;
	// lvx128 v13,r0,r29
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,384
	ctx.r8.s64 = ctx.r1.s64 + 384;
	// vsubfp v11,v13,v12
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v12.f32)));
	// stfs f28,288(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// lvx128 v0,r0,r18
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// lfs f1,-25888(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// lvlx v10,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v10,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v10.u32), 0xFF));
	// lvlx v8,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmaddfp128 v11,v125,v9,v11
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v125.f32), _mm_load_ps(ctx.v9.f32)), _mm_load_ps(ctx.v11.f32)));
	// vperm v7,v11,v8,v0
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// stvx128 v7,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821ee7c8
	sub_821EE7C8(ctx, base);
	// addi r7,r1,384
	ctx.r7.s64 = ctx.r1.s64 + 384;
	// lvx128 v0,r0,r7
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E94CC:
	// lwz r11,16(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 16);
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// lfs f0,28(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 28);
	f0.f64 = double(temp.f32);
	// lfs f13,24(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + 24);
	ctx.f13.f64 = double(temp.f32);
	// fsubs f0,f0,f13
	f0.f64 = double(float(f0.f64 - ctx.f13.f64));
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fcmpu cr6,f0,f15
	cr6.compare(f0.f64, f15.f64);
	// bge cr6,0x822e94f0
	if (!cr6.lt) goto loc_822E94F0;
	// fmr f0,f15
	f0.f64 = f15.f64;
loc_822E94F0:
	// lwz r31,1424(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 1424);
	// lfs f13,1964(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 1964);
	ctx.f13.f64 = double(temp.f32);
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// fadds f31,f13,f0
	f31.f64 = double(float(ctx.f13.f64 + f0.f64));
	// addi r3,r1,2224
	ctx.r3.s64 = ctx.r1.s64 + 2224;
	// addi r30,r1,864
	r30.s64 = ctx.r1.s64 + 864;
	// addi r28,r1,768
	r28.s64 = ctx.r1.s64 + 768;
	// lwz r21,12(r31)
	r21.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// addi r20,r1,336
	r20.s64 = ctx.r1.s64 + 336;
	// lwz r19,20(r31)
	r19.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// bl 0x821917d8
	sub_821917D8(ctx, base);
	// mr r15,r3
	r15.u64 = ctx.r3.u64;
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// addi r3,r1,2288
	ctx.r3.s64 = ctx.r1.s64 + 2288;
	// bl 0x82191990
	sub_82191990(ctx, base);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// mr r5,r15
	ctx.r5.u64 = r15.u64;
	// mr r6,r19
	ctx.r6.u64 = r19.u64;
	// mr r7,r21
	ctx.r7.u64 = r21.u64;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// mr r10,r20
	ctx.r10.u64 = r20.u64;
	// bl 0x822721c0
	sub_822721C0(ctx, base);
	// lbz r11,2383(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2383);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e968c
	if (cr6.eq) goto loc_822E968C;
	// lis r11,-31926
	r11.s64 = -2092302336;
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// addi r9,r11,-9968
	ctx.r9.s64 = r11.s64 + -9968;
	// addi r10,r10,-10064
	ctx.r10.s64 = ctx.r10.s64 + -10064;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
	// li r8,12
	ctx.r8.s64 = 12;
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
loc_822E9580:
	// ld r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r8,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r8.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822e9580
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822E9580;
	// mr r10,r9
	ctx.r10.u64 = ctx.r9.u64;
	// addi r11,r1,864
	r11.s64 = ctx.r1.s64 + 864;
	// li r9,12
	ctx.r9.s64 = 12;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822E95A4:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822e95a4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822E95A4;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822e95d0
	if (!cr6.eq) goto loc_822E95D0;
	// lwz r4,368(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// b 0x822e95d8
	goto loc_822E95D8;
loc_822E95D0:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E95D8:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r3,r11,-24616
	ctx.r3.s64 = r11.s64 + -24616;
	// bl 0x8290c698
	sub_8290C698(ctx, base);
	// lwz r11,872(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e95f8
	if (cr6.eq) goto loc_822E95F8;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e95fc
	goto loc_822E95FC;
loc_822E95F8:
	// li r6,0
	ctx.r6.s64 = 0;
loc_822E95FC:
	// lwz r11,868(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 868);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9610
	if (cr6.eq) goto loc_822E9610;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e9614
	goto loc_822E9614;
loc_822E9610:
	// li r5,0
	ctx.r5.s64 = 0;
loc_822E9614:
	// lwz r11,864(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 864);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9628
	if (cr6.eq) goto loc_822E9628;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e962c
	goto loc_822E962C;
loc_822E9628:
	// li r4,0
	ctx.r4.s64 = 0;
loc_822E962C:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r3,r11,-24580
	ctx.r3.s64 = r11.s64 + -24580;
	// bl 0x8290c698
	sub_8290C698(ctx, base);
	// lwz r11,884(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 884);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e964c
	if (cr6.eq) goto loc_822E964C;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e9650
	goto loc_822E9650;
loc_822E964C:
	// li r6,0
	ctx.r6.s64 = 0;
loc_822E9650:
	// lwz r11,880(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 880);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9664
	if (cr6.eq) goto loc_822E9664;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e9668
	goto loc_822E9668;
loc_822E9664:
	// li r5,0
	ctx.r5.s64 = 0;
loc_822E9668:
	// lwz r11,876(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 876);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e967c
	if (cr6.eq) goto loc_822E967C;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822e9680
	goto loc_822E9680;
loc_822E967C:
	// li r4,0
	ctx.r4.s64 = 0;
loc_822E9680:
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r3,r11,-24556
	ctx.r3.s64 = r11.s64 + -24556;
	// bl 0x8290c698
	sub_8290C698(ctx, base);
loc_822E968C:
	// mr r4,r17
	ctx.r4.u64 = r17.u64;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x822c0340
	sub_822C0340(ctx, base);
	// lbz r11,905(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 905);
	// lbz r3,904(r1)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r1.u32 + 904);
	// addi r31,r1,928
	r31.s64 = ctx.r1.s64 + 928;
	// addi r10,r1,912
	ctx.r10.s64 = ctx.r1.s64 + 912;
	// lfs f4,948(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 948);
	ctx.f4.f64 = double(temp.f32);
	// stw r31,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r31.u32);
	// addi r9,r1,876
	ctx.r9.s64 = ctx.r1.s64 + 876;
	// addi r8,r1,864
	ctx.r8.s64 = ctx.r1.s64 + 864;
	// lfs f3,896(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 896);
	ctx.f3.f64 = double(temp.f32);
	// stb r11,119(r1)
	PPC_STORE_U8(ctx.r1.u32 + 119, r11.u8);
	// mr r7,r16
	ctx.r7.u64 = r16.u64;
	// stb r3,111(r1)
	PPC_STORE_U8(ctx.r1.u32 + 111, ctx.r3.u8);
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// mr r5,r14
	ctx.r5.u64 = r14.u64;
	// lfs f2,892(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	ctx.f2.f64 = double(temp.f32);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lfs f1,888(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 888);
	ctx.f1.f64 = double(temp.f32);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// bl 0x821b5848
	sub_821B5848(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e9710
	if (!cr6.eq) goto loc_822E9710;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r14
	ctx.r4.u64 = r14.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8290e740
	sub_8290E740(ctx, base);
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
loc_822E9710:
	// lwz r11,828(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 828);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lfs f3,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// b 0x822e9730
	goto loc_822E9730;
loc_822E972C:
	// li r25,2
	r25.s64 = 2;
loc_822E9730:
	// lfs f4,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f4.f64 = double(temp.f32);
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x822692d8
	sub_822692D8(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// fmr f5,f1
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = ctx.f1.f64;
	// fmr f1,f4
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x822692d8
	sub_822692D8(ctx, base);
	// fadds f0,f1,f5
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64 + ctx.f5.f64));
	// fcmpu cr6,f0,f20
	cr6.compare(f0.f64, f20.f64);
	// bge cr6,0x822e9764
	if (!cr6.lt) goto loc_822E9764;
	// lfs f31,-256(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	f31.f64 = double(temp.f32);
	// b 0x822e976c
	goto loc_822E976C;
loc_822E9764:
	// fdivs f0,f3,f0
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f3.f64 / f0.f64));
	// fmuls f31,f0,f5
	f31.f64 = double(float(f0.f64 * ctx.f5.f64));
loc_822E976C:
	// lwz r11,0(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea33c
	if (cr6.eq) goto loc_822EA33C;
	// lwz r11,0(r17)
	r11.u64 = PPC_LOAD_U32(r17.u32 + 0);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea33c
	if (cr6.eq) goto loc_822EA33C;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// fmr f1,f4
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = ctx.f4.f64;
	// bl 0x82260988
	sub_82260988(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// lfs f1,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x82260988
	sub_82260988(ctx, base);
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x821ae7d0
	sub_821AE7D0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r3,1984(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 1984);
	// bl 0x8228bc28
	sub_8228BC28(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x821ae7d0
	sub_821AE7D0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r3,1984(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 1984);
	// bl 0x821bc738
	sub_821BC738(ctx, base);
	// lwz r3,1984(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 1984);
	// fmr f1,f31
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = f31.f64;
	// bl 0x8228a618
	sub_8228A618(ctx, base);
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lfs f1,816(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 816);
	ctx.f1.f64 = double(temp.f32);
	// addi r3,r1,640
	ctx.r3.s64 = ctx.r1.s64 + 640;
	// bl 0x8228c280
	sub_8228C280(ctx, base);
	// lbz r11,2383(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2383);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e98a0
	if (cr6.eq) goto loc_822E98A0;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r10,r1,688
	ctx.r10.s64 = ctx.r1.s64 + 688;
	// li r9,2064
	ctx.r9.s64 = 2064;
	// lfs f3,696(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 696);
	ctx.f3.f64 = double(temp.f32);
	// addi r8,r1,272
	ctx.r8.s64 = ctx.r1.s64 + 272;
	// lfs f2,692(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 692);
	ctx.f2.f64 = double(temp.f32);
	// addi r7,r1,464
	ctx.r7.s64 = ctx.r1.s64 + 464;
	// addi r6,r1,336
	ctx.r6.s64 = ctx.r1.s64 + 336;
	// addi r5,r1,432
	ctx.r5.s64 = ctx.r1.s64 + 432;
	// lwz r4,128(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lvx128 v0,r22,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r22.u32 + ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stvx128 v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x822e983c
	if (!cr6.eq) goto loc_822E983C;
	// lwz r4,368(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// b 0x822e9844
	goto loc_822E9844;
loc_822E983C:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E9844:
	// lfs f5,340(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f5.f64 = double(temp.f32);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// lfs f6,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f6.f64 = double(temp.f32);
	// lfs f4,432(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 432);
	ctx.f4.f64 = double(temp.f32);
	// addi r3,r11,-24528
	ctx.r3.s64 = r11.s64 + -24528;
	// lfs f1,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f1.f64 = double(temp.f32);
	// stfd f3,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.f3.u64);
	// stfd f5,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.f5.u64);
	// stfd f2,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.f2.u64);
	// stfd f6,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.f6.u64);
	// stfd f4,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.f4.u64);
	// stfd f1,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.f1.u64);
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r6,40(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// bl 0x8290c698
	sub_8290C698(ctx, base);
	// addi r4,r1,640
	ctx.r4.s64 = ctx.r1.s64 + 640;
	// lfs f1,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f1.f64 = double(temp.f32);
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x8290d3f8
	sub_8290D3F8(ctx, base);
loc_822E98A0:
	// addi r30,r22,2016
	r30.s64 = r22.s64 + 2016;
	// addi r3,r1,2416
	ctx.r3.s64 = ctx.r1.s64 + 2416;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82260fa0
	sub_82260FA0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r5,r1,640
	ctx.r5.s64 = ctx.r1.s64 + 640;
	// addi r3,r1,1264
	ctx.r3.s64 = ctx.r1.s64 + 1264;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r3,r1,768
	ctx.r3.s64 = ctx.r1.s64 + 768;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x8226b928
	sub_8226B928(ctx, base);
	// addi r5,r1,1264
	ctx.r5.s64 = ctx.r1.s64 + 1264;
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r11,r1,752
	r11.s64 = ctx.r1.s64 + 752;
	// addi r10,r1,720
	ctx.r10.s64 = ctx.r1.s64 + 720;
	// lfs f0,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r9,r1,816
	ctx.r9.s64 = ctx.r1.s64 + 816;
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// lvx128 v121,r0,r9
	_mm_store_si128((__m128i*)v121.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v127,r0,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v126,r0,r10
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// ble cr6,0x822e9d88
	if (!cr6.gt) goto loc_822E9D88;
	// addi r3,r1,1488
	ctx.r3.s64 = ctx.r1.s64 + 1488;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lvx128 v124,r0,r29
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r29.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v0,v124,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(v124.f32), _mm_load_ps(ctx.v0.f32)));
	// lvx128 v13,r0,r24
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// vand v13,v0,v13
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v13.u8)));
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v12,v13,v0
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32), 0xEF));
	// stvx128 v12,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f14
	cr6.compare(f0.f64, f14.f64);
	// bge cr6,0x822e994c
	if (!cr6.lt) goto loc_822E994C;
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// vor128 v0,v125,v125
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)v125.u8));
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E994C:
	// stfs f28,272(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// lvx128 v7,r0,r18
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// lis r29,-32246
	r29.s64 = -2113273856;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// lfs f1,-25888(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v5,v0,v6,v7
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx128 v5,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821ee7c8
	sub_821EE7C8(ctx, base);
	// addi r9,r1,784
	ctx.r9.s64 = ctx.r1.s64 + 784;
	// lvlx v4,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vsubfp128 v3,v121,v127
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v3.f32, _mm_sub_ps(_mm_load_ps(v121.f32), _mm_load_ps(v127.f32)));
	// vspltw v2,v4,0
	_mm_store_si128((__m128i*)ctx.v2.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v4.u32), 0xFF));
	// lvlx v1,0,r26
	temp.u32 = r26.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r8,r1,384
	ctx.r8.s64 = ctx.r1.s64 + 384;
	// vspltw v31,v1,0
	_mm_store_si128((__m128i*)v31.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v1.u32), 0xFF));
	// addi r7,r1,288
	ctx.r7.s64 = ctx.r1.s64 + 288;
	// lwz r6,2496(r22)
	ctx.r6.u64 = PPC_LOAD_U32(r22.u32 + 2496);
	// lvx128 v122,r0,r9
	_mm_store_si128((__m128i*)v122.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r5,-32240
	ctx.r5.s64 = -2112880640;
	// vsubfp128 v30,v122,v126
	_mm_store_ps(v30.f32, _mm_sub_ps(_mm_load_ps(v122.f32), _mm_load_ps(v126.f32)));
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// lfs f30,476(r5)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r5.u32 + 476);
	f30.f64 = double(temp.f32);
	// vmaddfp128 v127,v3,v31,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(v127.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v3.f32), _mm_load_ps(v31.f32)), _mm_load_ps(v127.f32)));
	// vmaddfp128 v126,v30,v2,v126
	_mm_store_ps(v126.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v30.f32), _mm_load_ps(ctx.v2.f32)), _mm_load_ps(v126.f32)));
	// lvx128 v124,r0,r7
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// beq cr6,0x822e9b20
	if (cr6.eq) goto loc_822E9B20;
	// lfs f0,2468(r22)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2468);
	f0.f64 = double(temp.f32);
	// addi r31,r22,2468
	r31.s64 = r22.s64 + 2468;
	// fcmpu cr6,f0,f28
	cr6.compare(f0.f64, f28.f64);
	// ble cr6,0x822e9b20
	if (!cr6.gt) goto loc_822E9B20;
	// addi r4,r1,1264
	ctx.r4.s64 = ctx.r1.s64 + 1264;
	// lfs f2,2460(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2460);
	ctx.f2.f64 = double(temp.f32);
	// rotlwi r3,r6,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// lfs f1,2464(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2464);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x821f4c68
	sub_821F4C68(ctx, base);
	// addi r5,r1,768
	ctx.r5.s64 = ctx.r1.s64 + 768;
	// addi r4,r1,1264
	ctx.r4.s64 = ctx.r1.s64 + 1264;
	// addi r3,r1,864
	ctx.r3.s64 = ctx.r1.s64 + 864;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// lwz r3,2496(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2496);
	// lfs f1,40(r3)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822d3d80
	sub_822D3D80(ctx, base);
	// fmr f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = ctx.f1.f64;
	// lfs f0,-25888(r29)
	temp.u32 = PPC_LOAD_U32(r29.u32 + -25888);
	f0.f64 = double(temp.f32);
	// addi r4,r1,1264
	ctx.r4.s64 = ctx.r1.s64 + 1264;
	// lfs f1,2464(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2464);
	ctx.f1.f64 = double(temp.f32);
	// lwz r3,2496(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 2496);
	// fsubs f2,f13,f0
	ctx.f2.f64 = double(float(ctx.f13.f64 - f0.f64));
	// bl 0x821f4c68
	sub_821F4C68(ctx, base);
	// addi r5,r1,768
	ctx.r5.s64 = ctx.r1.s64 + 768;
	// addi r4,r1,1264
	ctx.r4.s64 = ctx.r1.s64 + 1264;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r11,r1,720
	r11.s64 = ctx.r1.s64 + 720;
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v124,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lfs f12,0(r26)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f11,f19,f12
	ctx.f11.f64 = double(float(f19.f64 * ctx.f12.f64));
	// lfs f0,17904(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 17904);
	f0.f64 = double(temp.f32);
	// lfs f13,140(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	ctx.f13.f64 = double(temp.f32);
	// fmr f12,f28
	ctx.f12.f64 = f28.f64;
	// fmuls f31,f11,f0
	f31.f64 = double(float(ctx.f11.f64 * f0.f64));
	// lvx128 v123,r0,r11
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmsum3fp128 v13,v0,v123
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v123.f32), 0xEF));
	// stvx128 v13,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f10,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f10.f64 = double(temp.f32);
	// fcmpu cr6,f10,f13
	cr6.compare(ctx.f10.f64, ctx.f13.f64);
	// bge cr6,0x822e9a80
	if (!cr6.lt) goto loc_822E9A80;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// lfd f1,3376(r11)
	ctx.f1.u64 = PPC_LOAD_U64(r11.u32 + 3376);
	// bl 0x82260900
	sub_82260900(ctx, base);
	// frsp f12,f1
	ctx.fpscr.disableFlushMode();
	ctx.f12.f64 = double(float(ctx.f1.f64));
loc_822E9A80:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// lfs f0,512(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + 512);
	f0.f64 = double(temp.f32);
	// fmuls f0,f12,f0
	f0.f64 = double(float(ctx.f12.f64 * f0.f64));
	// fcmpu cr6,f0,f31
	cr6.compare(f0.f64, f31.f64);
	// ble cr6,0x822e9a98
	if (!cr6.gt) goto loc_822E9A98;
	// fmr f0,f31
	f0.f64 = f31.f64;
loc_822E9A98:
	// vpermwi128 v0,v123,99
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v123.u32), 0x9C));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vpermwi128 v13,v124,135
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v124.u32), 0x78));
	// vpermwi128 v12,v123,135
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v123.u32), 0x78));
	// vpermwi128 v11,v124,99
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v124.u32), 0x9C));
	// vmulfp128 v10,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// vmulfp128 v9,v11,v12
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v8,v9,v10
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v10.f32)));
	// stvx128 v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,344(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f28
	cr6.compare(ctx.f13.f64, f28.f64);
	// ble cr6,0x822e9acc
	if (!cr6.gt) goto loc_822E9ACC;
	// fneg f0,f0
	f0.u64 = f0.u64 ^ 0x8000000000000000;
loc_822E9ACC:
	// addi r3,r1,1264
	ctx.r3.s64 = ctx.r1.s64 + 1264;
	// fmuls f1,f0,f30
	ctx.fpscr.disableFlushMode();
	ctx.f1.f64 = double(float(f0.f64 * f30.f64));
	// bl 0x821b4760
	sub_821B4760(ctx, base);
	// addi r5,r1,864
	ctx.r5.s64 = ctx.r1.s64 + 864;
	// addi r4,r1,1264
	ctx.r4.s64 = ctx.r1.s64 + 1264;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x8221acc0
	sub_8221ACC0(ctx, base);
	// addi r11,r1,720
	r11.s64 = ctx.r1.s64 + 720;
	// addi r10,r1,752
	ctx.r10.s64 = ctx.r1.s64 + 752;
	// lvlx v0,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v12,0,r31
	temp.u32 = r31.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// lvx128 v10,r0,r11
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v9,v10,v126
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v9.f32, _mm_sub_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(v126.f32)));
	// lvx128 v8,r0,r10
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v7,v8,v127
	_mm_store_ps(ctx.v7.f32, _mm_sub_ps(_mm_load_ps(ctx.v8.f32), _mm_load_ps(v127.f32)));
	// vmaddfp128 v126,v9,v13,v126
	_mm_store_ps(v126.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v13.f32)), _mm_load_ps(v126.f32)));
	// vmaddfp128 v127,v7,v11,v127
	_mm_store_ps(v127.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v7.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(v127.f32)));
	// stvx128 v126,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E9B20:
	// lfs f0,2468(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2468);
	f0.f64 = double(temp.f32);
	// lfs f31,140(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f31.f64 = double(temp.f32);
	// fsubs f13,f31,f0
	ctx.f13.f64 = double(float(f31.f64 - f0.f64));
	// lfs f12,0(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f12.f64 = double(temp.f32);
	// fmuls f27,f13,f12
	f27.f64 = double(float(ctx.f13.f64 * ctx.f12.f64));
	// stfs f27,272(r1)
	temp.f32 = float(f27.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// fcmpu cr6,f27,f28
	cr6.compare(f27.f64, f28.f64);
	// beq cr6,0x822e9d8c
	if (cr6.eq) goto loc_822E9D8C;
	// lfs f0,17904(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 17904);
	f0.f64 = double(temp.f32);
	// fmuls f0,f19,f0
	f0.f64 = double(float(f19.f64 * f0.f64));
	// fmuls f31,f0,f30
	f31.f64 = double(float(f0.f64 * f30.f64));
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82239e88
	sub_82239E88(ctx, base);
	// lvx128 v123,r0,r24
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v124,v123
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v124.u8), _mm_load_si128((__m128i*)v123.u8)));
	// frsp f13,f1
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(float(ctx.f1.f64));
	// vmsum3fp128 v13,v0,v122
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v122.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f30,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f30.f64 = double(temp.f32);
	// fcmpu cr6,f30,f13
	cr6.compare(f30.f64, ctx.f13.f64);
	// bge cr6,0x822e9c4c
	if (!cr6.lt) goto loc_822E9C4C;
	// fmr f1,f31
	ctx.f1.f64 = f31.f64;
	// bl 0x82293cd0
	sub_82293CD0(ctx, base);
	// vpermwi128 v0,v122,99
	_mm_store_si128((__m128i*)ctx.v0.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v122.u32), 0x9C));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vpermwi128 v13,v124,135
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v124.u32), 0x78));
	// addi r10,r1,464
	ctx.r10.s64 = ctx.r1.s64 + 464;
	// vpermwi128 v12,v122,135
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v122.u32), 0x78));
	// frsp f0,f1
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(ctx.f1.f64));
	// vpermwi128 v11,v124,99
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v124.u32), 0x9C));
	// stfs f0,364(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// fmr f3,f28
	ctx.f3.f64 = f28.f64;
	// vmulfp128 v10,v13,v0
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v10.f32, _mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v126,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vmulfp128 v9,v11,v12
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v11.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp v8,v9,v10
	_mm_store_ps(ctx.v8.f32, _mm_sub_ps(_mm_load_ps(ctx.v9.f32), _mm_load_ps(ctx.v10.f32)));
	// lfs f1,468(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v8,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// lfs f13,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f13.f64 = double(temp.f32);
	// stvx128 v126,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// fcmpu cr6,f13,f28
	cr6.compare(ctx.f13.f64, f28.f64);
	// lfs f0,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fneg f2,f0
	ctx.f2.u64 = f0.u64 ^ 0x8000000000000000;
	// bge cr6,0x822e9c04
	if (!cr6.lt) goto loc_822E9C04;
	// addi r3,r1,1520
	ctx.r3.s64 = ctx.r1.s64 + 1520;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// addi r9,r1,364
	ctx.r9.s64 = ctx.r1.s64 + 364;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// lvlx v13,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v12,v13,0
	_mm_store_si128((__m128i*)ctx.v12.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v13.u32), 0xFF));
	// vmulfp128 v11,v0,v12
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v11.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v12.f32)));
	// vsubfp128 v10,v126,v11
	_mm_store_ps(ctx.v10.f32, _mm_sub_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v11.f32)));
	// stvx128 v10,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e9c2c
	goto loc_822E9C2C;
loc_822E9C04:
	// addi r3,r1,1552
	ctx.r3.s64 = ctx.r1.s64 + 1552;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// addi r9,r1,364
	ctx.r9.s64 = ctx.r1.s64 + 364;
	// vor128 v0,v126,v126
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_load_si128((__m128i*)v126.u8));
	// lvx128 v13,r0,r3
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,288
	ctx.r8.s64 = ctx.r1.s64 + 288;
	// lvlx v12,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v11,v12,0
	_mm_store_si128((__m128i*)ctx.v11.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// vmaddfp128 v0,v13,v11,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v11.f32)), _mm_load_ps(ctx.v0.f32)));
	// stvx128 v0,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822E9C2C:
	// lfs f29,-25888(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + -25888);
	f29.f64 = double(temp.f32);
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// fmr f1,f29
	ctx.f1.f64 = f29.f64;
	// bl 0x821ee7c8
	sub_821EE7C8(ctx, base);
	// addi r11,r1,288
	r11.s64 = ctx.r1.s64 + 288;
	// lvx128 v123,r0,r24
	_mm_store_si128((__m128i*)v123.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v124,r0,r11
	_mm_store_si128((__m128i*)v124.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// b 0x822e9c50
	goto loc_822E9C50;
loc_822E9C4C:
	// lfs f29,-25888(r29)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r29.u32 + -25888);
	f29.f64 = double(temp.f32);
loc_822E9C50:
	// fmr f0,f30
	ctx.fpscr.disableFlushMode();
	f0.f64 = f30.f64;
	// fcmpu cr6,f30,f28
	cr6.compare(f30.f64, f28.f64);
	// bge cr6,0x822e9c60
	if (!cr6.lt) goto loc_822E9C60;
	// fmr f0,f28
	f0.f64 = f28.f64;
loc_822E9C60:
	// lfs f31,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f31.f64 = double(temp.f32);
	// fsubs f13,f31,f0
	ctx.f13.f64 = double(float(f31.f64 - f0.f64));
	// lfs f0,152(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + 152);
	f0.f64 = double(temp.f32);
	// fnmsubs f30,f13,f0,f31
	f30.f64 = double(float(-(ctx.f13.f64 * f0.f64 - f31.f64)));
	// fcmpu cr6,f30,f28
	cr6.compare(f30.f64, f28.f64);
	// bge cr6,0x822e9c7c
	if (!cr6.lt) goto loc_822E9C7C;
	// fmr f30,f28
	f30.f64 = f28.f64;
loc_822E9C7C:
	// vsubfp128 v0,v124,v126
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(v124.f32), _mm_load_ps(v126.f32)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stfs f28,288(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// lvx128 v7,r0,r18
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r9,r1,384
	ctx.r9.s64 = ctx.r1.s64 + 384;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// fmr f1,f29
	ctx.f1.f64 = f29.f64;
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v5,v6,0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), 0xFF));
	// vmaddfp128 v126,v0,v5,v126
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(v126.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v5.f32)), _mm_load_ps(v126.f32)));
	// lvlx v4,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm128 v3,v126,v4,v7
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// stvx128 v3,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821ee7c8
	sub_821EE7C8(ctx, base);
	// vand128 v2,v125,v123
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)v123.u8)));
	// addi r8,r1,336
	ctx.r8.s64 = ctx.r1.s64 + 336;
	// lbz r7,2385(r22)
	ctx.r7.u64 = PPC_LOAD_U8(r22.u32 + 2385);
	// addi r6,r1,364
	ctx.r6.s64 = ctx.r1.s64 + 364;
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// vmsum3fp128 v1,v2,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v1.f32, _mm_dp_ps(_mm_load_ps(ctx.v2.f32), _mm_load_ps(v125.f32), 0xEF));
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stvx128 v1,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v126,r0,r5
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fsqrts f13,f0
	ctx.f13.f64 = double(float(sqrt(f0.f64)));
	// fmuls f12,f13,f19
	ctx.f12.f64 = double(float(ctx.f13.f64 * f19.f64));
	// fmuls f11,f12,f30
	ctx.f11.f64 = double(float(ctx.f12.f64 * f30.f64));
	// fmuls f10,f11,f27
	ctx.f10.f64 = double(float(ctx.f11.f64 * f27.f64));
	// stfs f10,364(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// lvlx v31,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v30,v31,0
	_mm_store_si128((__m128i*)v30.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)v31.u32), 0xFF));
	// vmaddfp128 v127,v126,v30,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(v127.f32, _mm_add_ps(_mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(v30.f32)), _mm_load_ps(v127.f32)));
	// beq cr6,0x822e9d0c
	if (cr6.eq) goto loc_822E9D0C;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x822e9d18
	goto loc_822E9D18;
loc_822E9D0C:
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x8290c360
	sub_8290C360(ctx, base);
	// lfs f31,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f31.f64 = double(temp.f32);
loc_822E9D18:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e9d8c
	if (!cr6.eq) goto loc_822E9D8C;
	// addi r11,r1,364
	r11.s64 = ctx.r1.s64 + 364;
	// lfs f0,-256(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	f0.f64 = double(temp.f32);
	// addi r10,r1,272
	ctx.r10.s64 = ctx.r1.s64 + 272;
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lbz r9,2385(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 2385);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lvlx v11,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// vmulfp128 v12,v126,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v12.f32, _mm_mul_ps(_mm_load_ps(v126.f32), _mm_load_ps(ctx.v13.f32)));
	// vmulfp128 v9,v12,v10
	_mm_store_ps(ctx.v9.f32, _mm_mul_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(ctx.v10.f32)));
	// vsubfp128 v127,v127,v9
	_mm_store_ps(v127.f32, _mm_sub_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v9.f32)));
	// beq cr6,0x822e9d64
	if (cr6.eq) goto loc_822E9D64;
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x822e9d70
	goto loc_822E9D70;
loc_822E9D64:
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x8290c360
	sub_8290C360(ctx, base);
	// lfs f31,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f31.f64 = double(temp.f32);
loc_822E9D70:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822e9d8c
	if (!cr6.eq) goto loc_822E9D8C;
	// lfs f0,12052(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 12052);
	f0.f64 = double(temp.f32);
	// vor128 v127,v121,v121
	_mm_store_si128((__m128i*)v127.u8, _mm_load_si128((__m128i*)v121.u8));
	// stfs f0,2452(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2452, temp.u32);
loc_822E9D88:
	// lfs f31,140(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 140);
	f31.f64 = double(temp.f32);
loc_822E9D8C:
	// fdivs f0,f31,f19
	ctx.fpscr.disableFlushMode();
	f0.f64 = double(float(f31.f64 / f19.f64));
	// stfs f0,272(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// vsubfp128 v0,v127,v121
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v0.f32, _mm_sub_ps(_mm_load_ps(v127.f32), _mm_load_ps(v121.f32)));
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// stfs f28,288(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// addi r10,r1,288
	ctx.r10.s64 = ctx.r1.s64 + 288;
	// lbz r9,2382(r22)
	ctx.r9.u64 = PPC_LOAD_U8(r22.u32 + 2382);
	// lvx128 v7,r0,r18
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r18.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lvlx v6,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v5,v6,0
	_mm_store_si128((__m128i*)ctx.v5.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v6.u32), 0xFF));
	// lvlx v4,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vmulfp128 v3,v0,v5
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v3.f32, _mm_mul_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(ctx.v5.f32)));
	// vperm128 v127,v3,v4,v7
	_mm_store_si128((__m128i*)v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v7.u8)));
	// beq cr6,0x822e9e0c
	if (cr6.eq) goto loc_822E9E0C;
	// lfs f0,0(r26)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	f0.f64 = double(temp.f32);
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// fsubs f13,f16,f0
	ctx.f13.f64 = double(float(f16.f64 - f0.f64));
	// stfs f13,364(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 364, temp.u32);
	// stfs f31,272(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// addi r4,r1,288
	ctx.r4.s64 = ctx.r1.s64 + 288;
	// stfs f28,288(r1)
	temp.f32 = float(f28.f64);
	PPC_STORE_U32(ctx.r1.u32 + 288, temp.u32);
	// addi r3,r1,364
	ctx.r3.s64 = ctx.r1.s64 + 364;
	// bl 0x821714f8
	sub_821714F8(ctx, base);
	// lfs f0,9668(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 9668);
	f0.f64 = double(temp.f32);
	// lfs f12,364(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	ctx.f12.f64 = double(temp.f32);
	// addi r11,r1,272
	r11.s64 = ctx.r1.s64 + 272;
	// fmadds f11,f12,f0,f31
	ctx.f11.f64 = double(float(ctx.f12.f64 * f0.f64 + f31.f64));
	// stfs f11,272(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 272, temp.u32);
	// lvlx v0,0,r11
	temp.u32 = r11.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v13,v0,0
	_mm_store_si128((__m128i*)ctx.v13.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// vmulfp128 v127,v127,v13
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(v127.f32, _mm_mul_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v13.f32)));
loc_822E9E0C:
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x821ff5b8
	sub_821FF5B8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9e40
	if (cr6.eq) goto loc_822E9E40;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,128(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 128);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822E9E40:
	// vor128 v1,v126,v126
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v126.u8));
	// bl 0x82282a98
	sub_82282A98(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9e64
	if (cr6.eq) goto loc_822E9E64;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// vor128 v1,v126,v126
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v126.u8));
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// bl 0x822664d8
	sub_822664D8(ctx, base);
loc_822E9E64:
	// lbz r11,2383(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2383);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822e9f4c
	if (cr6.eq) goto loc_822E9F4C;
	// addi r3,r1,1584
	ctx.r3.s64 = ctx.r1.s64 + 1584;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r10,r1,432
	ctx.r10.s64 = ctx.r1.s64 + 432;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r3,r1,1616
	ctx.r3.s64 = ctx.r1.s64 + 1616;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r8,r1,464
	ctx.r8.s64 = ctx.r1.s64 + 464;
	// addi r3,r1,1648
	ctx.r3.s64 = ctx.r1.s64 + 1648;
	// lvx128 v13,r0,r9
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v13,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r7,r1,336
	ctx.r7.s64 = ctx.r1.s64 + 336;
	// addi r6,r1,288
	ctx.r6.s64 = ctx.r1.s64 + 288;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// addi r4,r1,272
	ctx.r4.s64 = ctx.r1.s64 + 272;
	// lwz r3,128(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// stvx128 v127,r0,r7
	_mm_store_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v127,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v127,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stvx128 v12,r0,r4
	_mm_store_si128((__m128i*)(base + ((ctx.r4.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bne cr6,0x822e9ef0
	if (!cr6.eq) goto loc_822E9EF0;
	// lwz r4,368(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 368);
	// b 0x822e9ef8
	goto loc_822E9EF8;
loc_822E9EF0:
	// lwz r11,128(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822E9EF8:
	// lfs f3,344(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f3.f64 = double(temp.f32);
	// lis r11,-32241
	r11.s64 = -2112946176;
	// lfs f5,468(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 468);
	ctx.f5.f64 = double(temp.f32);
	// lfs f6,440(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 440);
	ctx.f6.f64 = double(temp.f32);
	// addi r3,r11,-24456
	ctx.r3.s64 = r11.s64 + -24456;
	// lfs f4,272(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,292(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,384(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 384);
	ctx.f1.f64 = double(temp.f32);
	// stfd f3,48(r1)
	PPC_STORE_U64(ctx.r1.u32 + 48, ctx.f3.u64);
	// stfd f5,64(r1)
	PPC_STORE_U64(ctx.r1.u32 + 64, ctx.f5.u64);
	// stfd f6,72(r1)
	PPC_STORE_U64(ctx.r1.u32 + 72, ctx.f6.u64);
	// stfd f4,56(r1)
	PPC_STORE_U64(ctx.r1.u32 + 56, ctx.f4.u64);
	// stfd f2,40(r1)
	PPC_STORE_U64(ctx.r1.u32 + 40, ctx.f2.u64);
	// stfd f1,32(r1)
	PPC_STORE_U64(ctx.r1.u32 + 32, ctx.f1.u64);
	// ld r7,48(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 48);
	// ld r9,64(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 64);
	// ld r10,72(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 72);
	// ld r8,56(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 56);
	// ld r6,40(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 40);
	// ld r5,32(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 32);
	// bl 0x8290c698
	sub_8290C698(ctx, base);
loc_822E9F4C:
	// addi r11,r1,640
	r11.s64 = ctx.r1.s64 + 640;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// li r9,8
	ctx.r9.s64 = 8;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822E9F5C:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822e9f5c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822E9F5C;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// bl 0x821ebba0
	sub_821EBBA0(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// bl 0x821ebba0
	sub_821EBBA0(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r1,1680
	ctx.r3.s64 = ctx.r1.s64 + 1680;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lwz r11,26912(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r31,88(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lvx128 v1,r0,r9
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x82290090
	sub_82290090(ctx, base);
	// lfs f0,0(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 0);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f1,f0
	cr6.compare(ctx.f1.f64, f0.f64);
	// bge cr6,0x822ea8a8
	if (!cr6.lt) goto loc_822EA8A8;
	// lbz r11,2386(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2386);
	// lwz r30,420(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 420);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ea014
	if (!cr6.eq) goto loc_822EA014;
	// li r11,1
	r11.s64 = 1;
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// stb r11,2386(r22)
	PPC_STORE_U8(r22.u32 + 2386, r11.u8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r10,-19564
	ctx.r4.s64 = ctx.r10.s64 + -19564;
	// bl 0x82550010
	sub_82550010(ctx, base);
	// lis r9,-31926
	ctx.r9.s64 = -2092302336;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r4,r9,-19560
	ctx.r4.s64 = ctx.r9.s64 + -19560;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82550010
	sub_82550010(ctx, base);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x822ea014
	if (!cr6.gt) goto loc_822EA014;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x822ea014
	if (!cr6.gt) goto loc_822EA014;
	// li r11,1
	r11.s64 = 1;
	// stb r11,2387(r22)
	PPC_STORE_U8(r22.u32 + 2387, r11.u8);
	// stw r31,2392(r22)
	PPC_STORE_U32(r22.u32 + 2392, r31.u32);
	// stw r3,2396(r22)
	PPC_STORE_U32(r22.u32 + 2396, ctx.r3.u32);
loc_822EA014:
	// lbz r11,2387(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2387);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// li r24,0
	r24.s64 = 0;
	// lbz r31,2388(r22)
	r31.u64 = PPC_LOAD_U8(r22.u32 + 2388);
	// lfs f31,2400(r22)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2400);
	f31.f64 = double(temp.f32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lfs f30,2404(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2404);
	f30.f64 = double(temp.f32);
	// stb r24,2388(r22)
	PPC_STORE_U8(r22.u32 + 2388, r24.u8);
	// bl 0x82237468
	sub_82237468(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r11,16(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea0cc
	if (cr6.eq) goto loc_822EA0CC;
	// lwz r10,2392(r22)
	ctx.r10.u64 = PPC_LOAD_U32(r22.u32 + 2392);
	// addi r6,r1,336
	ctx.r6.s64 = ctx.r1.s64 + 336;
	// lwz r9,2396(r22)
	ctx.r9.u64 = PPC_LOAD_U32(r22.u32 + 2396);
	// addi r5,r1,464
	ctx.r5.s64 = ctx.r1.s64 + 464;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r4,r10,r8
	ctx.r4.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 + ctx.r7.u64;
	// rlwinm r10,r4,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r9,r3,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + r11.u64;
	// li r11,16
	r11.s64 = 16;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r3,r1,768
	ctx.r3.s64 = ctx.r1.s64 + 768;
	// lvx128 v1,r10,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v127,r9,r11
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32 + r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stb r8,2388(r22)
	PPC_STORE_U8(r22.u32 + 2388, ctx.r8.u8);
	// stvx128 v1,r0,r6
	_mm_store_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v127,r0,r5
	_mm_store_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,344(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	f0.f64 = double(temp.f32);
	// lfs f13,472(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 472);
	ctx.f13.f64 = double(temp.f32);
	// stfs f0,2400(r22)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r22.u32 + 2400, temp.u32);
	// stfs f13,2404(r22)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r22.u32 + 2404, temp.u32);
	// bl 0x821927c8
	sub_821927C8(ctx, base);
	// li r7,2416
	ctx.r7.s64 = 2416;
	// addi r3,r1,768
	ctx.r3.s64 = ctx.r1.s64 + 768;
	// stvx128 v1,r22,r7
	_mm_store_si128((__m128i*)(base + ((r22.u32 + ctx.r7.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// vor128 v1,v127,v127
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v127.u8));
	// bl 0x821927c8
	sub_821927C8(ctx, base);
	// li r6,2432
	ctx.r6.s64 = 2432;
	// stvx128 v1,r22,r6
	_mm_store_si128((__m128i*)(base + ((r22.u32 + ctx.r6.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822EA0CC:
	// lbz r11,2388(r22)
	r11.u64 = PPC_LOAD_U8(r22.u32 + 2388);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea8a8
	if (cr6.eq) goto loc_822EA8A8;
	// lfs f0,11036(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 11036);
	f0.f64 = double(temp.f32);
	// li r27,-1
	r27.s64 = -1;
	// fcmpu cr6,f31,f0
	cr6.compare(f31.f64, f0.f64);
	// ble cr6,0x822ea214
	if (!cr6.gt) goto loc_822EA214;
	// lfs f13,2400(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2400);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x822ea214
	if (cr6.gt) goto loc_822EA214;
	// lfs f13,0(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r28,1
	r28.s64 = 1;
	// lfs f0,-256(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x822ea118
	if (cr6.gt) goto loc_822EA118;
	// mr r28,r24
	r28.u64 = r24.u64;
loc_822EA118:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r29,r24
	r29.u64 = r24.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// mr r30,r24
	r30.u64 = r24.u64;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea1e0
	if (cr6.eq) goto loc_822EA1E0;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r25,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r25.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea170
	if (cr6.eq) goto loc_822EA170;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea1e4
	goto loc_822EA1E4;
loc_822EA170:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822ea1cc
	if (cr6.eq) goto loc_822EA1CC;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822ea1a4
	if (cr6.gt) goto loc_822EA1A4;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_822EA1A4:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ea1cc
	if (!cr6.eq) goto loc_822EA1CC;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,848(r1)
	PPC_STORE_U64(ctx.r1.u32 + 848, r11.u64);
	// lwz r11,852(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 852);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea1e4
	goto loc_822EA1E4;
loc_822EA1CC:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r24,848(r1)
	PPC_STORE_U32(ctx.r1.u32 + 848, r24.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea1e4
	goto loc_822EA1E4;
loc_822EA1E0:
	// mr r11,r24
	r11.u64 = r24.u64;
loc_822EA1E4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea1fc
	if (cr6.eq) goto loc_822EA1FC;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824c6410
	sub_824C6410(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_822EA1FC:
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwz r3,4(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x826e8878
	sub_826E8878(ctx, base);
	// lfs f0,11036(r23)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r23.u32 + 11036);
	f0.f64 = double(temp.f32);
loc_822EA214:
	// fcmpu cr6,f30,f0
	ctx.fpscr.disableFlushMode();
	cr6.compare(f30.f64, f0.f64);
	// ble cr6,0x822ea8a8
	if (!cr6.gt) goto loc_822EA8A8;
	// lfs f13,2404(r22)
	temp.u32 = PPC_LOAD_U32(r22.u32 + 2404);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x822ea8a8
	if (cr6.gt) goto loc_822EA8A8;
	// lfs f13,0(r26)
	temp.u32 = PPC_LOAD_U32(r26.u32 + 0);
	ctx.f13.f64 = double(temp.f32);
	// li r28,1
	r28.s64 = 1;
	// lfs f0,-256(r23)
	temp.u32 = PPC_LOAD_U32(r23.u32 + -256);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x822ea240
	if (cr6.gt) goto loc_822EA240;
	// mr r28,r24
	r28.u64 = r24.u64;
loc_822EA240:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r29,r24
	r29.u64 = r24.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// mr r30,r24
	r30.u64 = r24.u64;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea308
	if (cr6.eq) goto loc_822EA308;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// stw r25,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, r25.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea298
	if (cr6.eq) goto loc_822EA298;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,2(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 2);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea30c
	goto loc_822EA30C;
loc_822EA298:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822ea2f4
	if (cr6.eq) goto loc_822EA2F4;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822ea2cc
	if (cr6.gt) goto loc_822EA2CC;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
loc_822EA2CC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ea2f4
	if (!cr6.eq) goto loc_822EA2F4;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,840(r1)
	PPC_STORE_U64(ctx.r1.u32 + 840, r11.u64);
	// lwz r11,844(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 844);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea30c
	goto loc_822EA30C;
loc_822EA2F4:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r24,840(r1)
	PPC_STORE_U32(ctx.r1.u32 + 840, r24.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea30c
	goto loc_822EA30C;
loc_822EA308:
	// mr r11,r24
	r11.u64 = r24.u64;
loc_822EA30C:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea324
	if (cr6.eq) goto loc_822EA324;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x824c6410
	sub_824C6410(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
loc_822EA324:
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// lwz r3,4(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x826e8878
	sub_826E8878(ctx, base);
	// b 0x822ea8a8
	goto loc_822EA8A8;
loc_822EA33C:
	// li r11,0
	r11.s64 = 0;
	// stw r11,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r11.u32);
	// b 0x822ea8a8
	goto loc_822EA8A8;
loc_822EA348:
	// lwz r11,2004(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 2004);
	// addi r31,r22,2004
	r31.s64 = r22.s64 + 2004;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea36c
	if (cr6.eq) goto loc_822EA36C;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// bl 0x82296ef8
	sub_82296EF8(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822EA36C:
	// lwz r11,1996(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 1996);
	// addi r31,r22,1996
	r31.s64 = r22.s64 + 1996;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea390
	if (cr6.eq) goto loc_822EA390;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// bl 0x828db758
	sub_828DB758(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238a848
	sub_8238A848(ctx, base);
loc_822EA390:
	// stw r19,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r19.u32);
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// lwz r28,320(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 320);
	// addi r10,r1,384
	ctx.r10.s64 = ctx.r1.s64 + 384;
	// vor128 v126,v125,v125
	_mm_store_si128((__m128i*)v126.u8, _mm_load_si128((__m128i*)v125.u8));
	// stvx128 v126,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f14
	cr6.compare(f0.f64, f14.f64);
	// ble cr6,0x822ea6bc
	if (!cr6.gt) goto loc_822EA6BC;
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,59
	ctx.r4.s64 = 59;
	// mr r29,r26
	r29.u64 = r26.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// mr r30,r26
	r30.u64 = r26.u64;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea490
	if (cr6.eq) goto loc_822EA490;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,59
	ctx.r10.s64 = 59;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea420
	if (cr6.eq) goto loc_822EA420;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,59(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 59);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea494
	goto loc_822EA494;
loc_822EA420:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822ea47c
	if (cr6.eq) goto loc_822EA47C;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,59
	cr6.compare<int32_t>(ctx.r10.s32, 59, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822ea454
	if (cr6.gt) goto loc_822EA454;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822EA454:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ea47c
	if (!cr6.eq) goto loc_822EA47C;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,976(r1)
	PPC_STORE_U64(ctx.r1.u32 + 976, r11.u64);
	// lwz r11,980(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 980);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea494
	goto loc_822EA494;
loc_822EA47C:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r26,976(r1)
	PPC_STORE_U32(ctx.r1.u32 + 976, r26.u32);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea494
	goto loc_822EA494;
loc_822EA490:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822EA494:
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// clrlwi r9,r11,24
	ctx.r9.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lfs f31,-12728(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + -12728);
	f31.f64 = double(temp.f32);
	// beq cr6,0x822ea564
	if (cr6.eq) goto loc_822EA564;
	// addi r4,r1,464
	ctx.r4.s64 = ctx.r1.s64 + 464;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82267268
	sub_82267268(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea564
	if (cr6.eq) goto loc_822EA564;
	// lvx128 v0,r0,r28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v125,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v125.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// addi r3,r1,1712
	ctx.r3.s64 = ctx.r1.s64 + 1712;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// vmsum3fp128 v13,v0,v125
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v125.f32), 0xEF));
	// lfs f0,17944(r10)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 17944);
	f0.f64 = double(temp.f32);
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,336(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f13.f64 = double(temp.f32);
	// fsqrts f12,f13
	ctx.f12.f64 = double(float(sqrt(ctx.f13.f64)));
	// fmuls f11,f12,f19
	ctx.f11.f64 = double(float(ctx.f12.f64 * f19.f64));
	// fmuls f30,f11,f0
	f30.f64 = double(float(ctx.f11.f64 * f0.f64));
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// lvx128 v12,r0,r3
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvx128 v0,r0,r28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r8,r1,336
	ctx.r8.s64 = ctx.r1.s64 + 336;
	// fmuls f10,f30,f30
	ctx.fpscr.disableFlushMode();
	ctx.f10.f64 = double(float(f30.f64 * f30.f64));
	// lvx128 v127,r0,r9
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v11,v12,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v11.f32, _mm_sub_ps(_mm_load_ps(ctx.v12.f32), _mm_load_ps(v127.f32)));
	// vand v10,v11,v0
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_and_si128(_mm_load_si128((__m128i*)ctx.v11.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v9,v10,v11
	_mm_store_ps(ctx.v9.f32, _mm_dp_ps(_mm_load_ps(ctx.v10.f32), _mm_load_ps(ctx.v11.f32), 0xEF));
	// stvx128 v9,r0,r8
	_mm_store_si128((__m128i*)(base + ((ctx.r8.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f9,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	ctx.f9.f64 = double(temp.f32);
	// fcmpu cr6,f9,f10
	cr6.compare(ctx.f9.f64, ctx.f10.f64);
	// bgt cr6,0x822ea564
	if (cr6.gt) goto loc_822EA564;
	// addi r3,r1,1744
	ctx.r3.s64 = ctx.r1.s64 + 1744;
	// fmr f3,f31
	ctx.f3.f64 = f31.f64;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v1,v127,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v1.f32, _mm_sub_ps(_mm_load_ps(v127.f32), _mm_load_ps(ctx.v0.f32)));
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,100(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 100);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r29,1
	r29.s64 = 1;
loc_822EA564:
	// addi r3,r1,1776
	ctx.r3.s64 = ctx.r1.s64 + 1776;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lis r10,-32240
	ctx.r10.s64 = -2112880640;
	// stvx128 v0,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,1332(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 1332);
	f0.f64 = double(temp.f32);
	// lfs f13,344(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 344);
	ctx.f13.f64 = double(temp.f32);
	// fcmpu cr6,f13,f0
	cr6.compare(ctx.f13.f64, f0.f64);
	// bgt cr6,0x822ea68c
	if (cr6.gt) goto loc_822EA68C;
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82267268
	sub_82267268(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea5ec
	if (cr6.eq) goto loc_822EA5EC;
	// addi r3,r1,1808
	ctx.r3.s64 = ctx.r1.s64 + 1808;
	// fmr f3,f31
	ctx.fpscr.disableFlushMode();
	ctx.f3.f64 = f31.f64;
	// fmr f2,f28
	ctx.f2.f64 = f28.f64;
	// fmr f1,f28
	ctx.f1.f64 = f28.f64;
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// lvx128 v0,r0,r3
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r3.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lvx128 v13,r0,r10
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp v1,v13,v0
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v1.f32, _mm_sub_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v0.f32)));
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,112(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 112);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r29,1
	r29.s64 = 1;
	// b 0x822ea68c
	goto loc_822EA68C;
loc_822EA5EC:
	// stw r26,704(r1)
	PPC_STORE_U32(ctx.r1.u32 + 704, r26.u32);
	// addi r4,r1,704
	ctx.r4.s64 = ctx.r1.s64 + 704;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea630
	if (cr6.eq) goto loc_822EA630;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r10,r1,720
	ctx.r10.s64 = ctx.r1.s64 + 720;
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lvx128 v1,r0,r10
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r8,112(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 112);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r29,1
	r29.s64 = 1;
	// b 0x822ea68c
	goto loc_822EA68C;
loc_822EA630:
	// addi r3,r1,1328
	ctx.r3.s64 = ctx.r1.s64 + 1328;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// addi r11,r1,1328
	r11.s64 = ctx.r1.s64 + 1328;
	// addi r10,r1,336
	ctx.r10.s64 = ctx.r1.s64 + 336;
	// addi r9,r1,464
	ctx.r9.s64 = ctx.r1.s64 + 464;
	// lis r8,-32256
	ctx.r8.s64 = -2113929216;
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v0,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f3,3096(r8)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r8.u32 + 3096);
	ctx.f3.f64 = double(temp.f32);
	// lfs f2,340(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,464(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 464);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x822d6af0
	sub_822D6AF0(ctx, base);
	// lwz r7,4(r22)
	ctx.r7.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r6,r1,336
	ctx.r6.s64 = ctx.r1.s64 + 336;
	// lwz r3,124(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 124);
	// lvx128 v1,r0,r6
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r4,112(r5)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r5.u32 + 112);
	// mtctr r4
	ctr.u64 = ctx.r4.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822EA68C:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ea6c4
	if (!cr6.eq) goto loc_822EA6C4;
	// lwz r11,4(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,0
	ctx.r4.s64 = 0;
	// vor128 v1,v125,v125
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_load_si128((__m128i*)v125.u8));
	// lwz r3,124(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 124);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,128(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 128);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x822ea6c4
	goto loc_822EA6C4;
loc_822EA6BC:
	// li r11,2
	r11.s64 = 2;
	// stw r11,828(r22)
	PPC_STORE_U32(r22.u32 + 828, r11.u32);
loc_822EA6C4:
	// lwz r31,372(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ea734
	if (cr6.eq) goto loc_822EA734;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821e7b18
	sub_821E7B18(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea734
	if (cr6.eq) goto loc_822EA734;
	// stw r26,704(r1)
	PPC_STORE_U32(ctx.r1.u32 + 704, r26.u32);
	// addi r4,r1,704
	ctx.r4.s64 = ctx.r1.s64 + 704;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x821cfac8
	sub_821CFAC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea720
	if (cr6.eq) goto loc_822EA720;
	// lbz r10,708(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 708);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ea720
	if (cr6.eq) goto loc_822EA720;
	// addi r11,r1,736
	r11.s64 = ctx.r1.s64 + 736;
	// addi r10,r1,384
	ctx.r10.s64 = ctx.r1.s64 + 384;
	// lvx128 v126,r0,r11
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822EA720:
	// lwz r11,168(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 168);
	// cmpwi cr6,r11,3
	cr6.compare<int32_t>(r11.s32, 3, xer);
	// bne cr6,0x822ea734
	if (!cr6.eq) goto loc_822EA734;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82675048
	sub_82675048(ctx, base);
loc_822EA734:
	// lwz r31,4(r22)
	r31.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// li r4,109
	ctx.r4.s64 = 109;
	// mr r30,r26
	r30.u64 = r26.u64;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f0598
	sub_822F0598(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea7f0
	if (cr6.eq) goto loc_822EA7F0;
	// lwz r11,140(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 140);
	// li r10,109
	ctx.r10.s64 = 109;
	// stw r10,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea788
	if (cr6.eq) goto loc_822EA788;
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,72(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// lbz r9,109(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 109);
	// rotlwi r10,r9,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 3);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r11,1
	r11.s64 = 1;
	// lwz r30,4(r8)
	r30.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// b 0x822ea7f4
	goto loc_822EA7F4;
loc_822EA788:
	// addi r4,r31,68
	ctx.r4.s64 = r31.s64 + 68;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// addi r3,r1,288
	ctx.r3.s64 = ctx.r1.s64 + 288;
	// bl 0x82303ec0
	sub_82303EC0(ctx, base);
	// lwz r11,76(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 76);
	// lwz r10,292(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822ea7e0
	if (cr6.eq) goto loc_822EA7E0;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r10,109
	cr6.compare<int32_t>(ctx.r10.s32, 109, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x822ea7bc
	if (cr6.gt) goto loc_822EA7BC;
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
loc_822EA7BC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ea7e0
	if (!cr6.eq) goto loc_822EA7E0;
	// ld r11,288(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// std r11,992(r1)
	PPC_STORE_U64(ctx.r1.u32 + 992, r11.u64);
	// lwz r11,996(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 996);
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// b 0x822ea7f4
	goto loc_822EA7F4;
loc_822EA7E0:
	// lwz r30,4(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r11,1
	r11.s64 = 1;
	// stw r26,992(r1)
	PPC_STORE_U32(ctx.r1.u32 + 992, r26.u32);
	// b 0x822ea7f4
	goto loc_822EA7F4;
loc_822EA7F0:
	// mr r11,r26
	r11.u64 = r26.u64;
loc_822EA7F4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ea830
	if (cr6.eq) goto loc_822EA830;
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ea830
	if (cr6.eq) goto loc_822EA830;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// bl 0x821af050
	sub_821AF050(ctx, base);
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// addi r10,r1,384
	ctx.r10.s64 = ctx.r1.s64 + 384;
	// stw r26,80(r30)
	PPC_STORE_U32(r30.u32 + 80, r26.u32);
	// stw r26,92(r30)
	PPC_STORE_U32(r30.u32 + 92, r26.u32);
	// lvx128 v126,r0,r11
	_mm_store_si128((__m128i*)v126.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// stvx128 v126,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)VectorMaskL)));
loc_822EA830:
	// lvx128 v0,r0,r28
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r28.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// addi r11,r1,336
	r11.s64 = ctx.r1.s64 + 336;
	// vand128 v0,v126,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vmsum3fp128 v13,v0,v126
	ctx.fpscr.enableFlushMode();
	_mm_store_ps(ctx.v13.f32, _mm_dp_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v126.f32), 0xEF));
	// stvx128 v13,r0,r11
	_mm_store_si128((__m128i*)(base + ((r11.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,336(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 336);
	f0.f64 = double(temp.f32);
	// fcmpu cr6,f0,f14
	cr6.compare(f0.f64, f14.f64);
	// ble cr6,0x822ea88c
	if (!cr6.gt) goto loc_822EA88C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r1,384
	ctx.r10.s64 = ctx.r1.s64 + 384;
	// addi r9,r11,-28384
	ctx.r9.s64 = r11.s64 + -28384;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vand128 v0,v126,v0
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_and_si128(_mm_load_si128((__m128i*)v126.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// lfs f1,-25888(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -25888);
	ctx.f1.f64 = double(temp.f32);
	// stvx128 v0,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v0.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821ee7c8
	sub_821EE7C8(ctx, base);
	// lwz r8,4(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// addi r7,r1,384
	ctx.r7.s64 = ctx.r1.s64 + 384;
	// lwz r3,124(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 124);
	// lvx128 v1,r0,r7
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r7.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x822664d8
	sub_822664D8(ctx, base);
loc_822EA88C:
	// addi r3,r1,1840
	ctx.r3.s64 = ctx.r1.s64 + 1840;
	// lwz r4,4(r22)
	ctx.r4.u64 = PPC_LOAD_U32(r22.u32 + 4);
	// bl 0x82213bd0
	sub_82213BD0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// lvx128 v1,r0,r11
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// bl 0x821e17d0
	sub_821E17D0(ctx, base);
loc_822EA8A8:
	// addi r1,r1,3024
	ctx.r1.s64 = ctx.r1.s64 + 3024;
	// addi r12,r1,-304
	r12.s64 = ctx.r1.s64 + -304;
	// bl 0x830001c4
	// addi r12,r1,-152
	r12.s64 = ctx.r1.s64 + -152;
	// bl 0x82ca751c
	// b 0x82ca2c00
	return;
}

__attribute__((alias("__imp__sub_822EA8C0"))) PPC_WEAK_FUNC(sub_822EA8C0);
PPC_FUNC_IMPL(__imp__sub_822EA8C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82cbb638
	sub_82CBB638(ctx, base);
	// bl 0x82cbb570
	sub_82CBB570(ctx, base);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r4,r11,-1264
	ctx.r4.s64 = r11.s64 + -1264;
	// bl 0x821e6388
	sub_821E6388(ctx, base);
	// lis r10,-32209
	ctx.r10.s64 = -2110849024;
	// li r5,0
	ctx.r5.s64 = 0;
	// lis r4,4
	ctx.r4.s64 = 262144;
	// addi r3,r10,-22232
	ctx.r3.s64 = ctx.r10.s64 + -22232;
	// bl 0x82ca34b0
	sub_82CA34B0(ctx, base);
	// li r4,-1
	ctx.r4.s64 = -1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82196c58
	sub_82196C58(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cbbf60
	sub_82CBBF60(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EA928"))) PPC_WEAK_FUNC(sub_822EA928);
PPC_FUNC_IMPL(__imp__sub_822EA928) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,-2
	ctx.r3.s64 = -2;
	// bl 0x82cbbe20
	sub_82CBBE20(ctx, base);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r3,r11,-1252
	ctx.r3.s64 = r11.s64 + -1252;
	// bl 0x82b38798
	sub_82B38798(ctx, base);
	// bl 0x82b38840
	sub_82B38840(ctx, base);
	// bl 0x82a1b1a0
	sub_82A1B1A0(ctx, base);
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ea9a4
	if (cr6.eq) goto loc_822EA9A4;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// addi r10,r11,7384
	ctx.r10.s64 = r11.s64 + 7384;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x832b258c
	__imp__RtlInitializeCriticalSection(ctx, base);
	// lis r9,-32256
	ctx.r9.s64 = -2113929216;
	// li r28,0
	r28.s64 = 0;
	// addi r8,r9,10296
	ctx.r8.s64 = ctx.r9.s64 + 10296;
	// stw r28,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r28.u32);
	// mr r29,r31
	r29.u64 = r31.u64;
	// stw r28,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r28.u32);
	// stw r8,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r8.u32);
	// stb r28,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r28.u8);
	// stb r28,45(r31)
	PPC_STORE_U8(r31.u32 + 45, r28.u8);
	// b 0x822ea9ac
	goto loc_822EA9AC;
loc_822EA9A4:
	// li r28,0
	r28.s64 = 0;
	// mr r29,r28
	r29.u64 = r28.u64;
loc_822EA9AC:
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822ea9e8
	if (cr6.eq) goto loc_822EA9E8;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822ea9e8
	if (cr6.eq) goto loc_822EA9E8;
	// lis r11,-32105
	r11.s64 = -2104033280;
	// stw r29,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r29.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// addi r10,r11,-32320
	ctx.r10.s64 = r11.s64 + -32320;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r9,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r9.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// b 0x822ea9ec
	goto loc_822EA9EC;
loc_822EA9E8:
	// mr r31,r28
	r31.u64 = r28.u64;
loc_822EA9EC:
	// lis r11,-31926
	r11.s64 = -2092302336;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// addi r30,r11,-6420
	r30.s64 = r11.s64 + -6420;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822eaa28
	if (cr6.eq) goto loc_822EAA28;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r31.u32);
	// beq cr6,0x822eaa28
	if (cr6.eq) goto loc_822EAA28;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_822EAA28:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// bl 0x822eb0c8
	sub_822EB0C8(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f2518
	sub_822F2518(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f27c0
	sub_822F27C0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eaa80
	if (cr6.eq) goto loc_822EAA80;
	// bl 0x822f47f8
	sub_822F47F8(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f4690
	sub_822F4690(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r11,r28
	r11.u64 = r28.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r11,26776(r10)
	PPC_STORE_U32(ctx.r10.u32 + 26776, r11.u32);
	// bl 0x822f2608
	sub_822F2608(ctx, base);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c38
	return;
loc_822EAA80:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,1
	r11.s64 = 1;
	// stw r11,26776(r10)
	PPC_STORE_U32(ctx.r10.u32 + 26776, r11.u32);
	// bl 0x822f2608
	sub_822F2608(ctx, base);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822EAA98"))) PPC_WEAK_FUNC(sub_822EAA98);
PPC_FUNC_IMPL(__imp__sub_822EAA98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r17{};
	// lbz r11,2(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bgtlr cr6
	if (cr6.gt) return;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-21824
	r12.s64 = r12.s64 + -21824;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822EAAF4;
	case 1:
		goto loc_822EAAFC;
	case 2:
		goto loc_822EAB04;
	case 3:
		goto loc_822EAB0C;
	case 4:
		goto loc_822EAB28;
	case 5:
		goto loc_822EAB28;
	case 6:
		goto loc_822EAB28;
	case 7:
		goto loc_822EAB0C;
	case 8:
		goto loc_822EAB14;
	case 9:
		goto loc_822EAB28;
	case 10:
		goto loc_822EAB28;
	case 11:
		goto loc_822EAB1C;
	case 12:
		goto loc_822EAB24;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-21772(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21772);
	// lwz r17,-21764(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21764);
	// lwz r17,-21756(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21756);
	// lwz r17,-21748(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21748);
	// lwz r17,-21720(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21720);
	// lwz r17,-21720(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21720);
	// lwz r17,-21720(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21720);
	// lwz r17,-21748(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21748);
	// lwz r17,-21740(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21740);
	// lwz r17,-21720(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21720);
	// lwz r17,-21720(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21720);
	// lwz r17,-21732(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21732);
	// lwz r17,-21724(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -21724);
loc_822EAAF4:
	// li r3,134
	ctx.r3.s64 = 134;
	// blr 
	return;
loc_822EAAFC:
	// li r3,402
	ctx.r3.s64 = 402;
	// blr 
	return;
loc_822EAB04:
	// li r3,158
	ctx.r3.s64 = 158;
	// blr 
	return;
loc_822EAB0C:
	// li r3,17
	ctx.r3.s64 = 17;
	// blr 
	return;
loc_822EAB14:
	// li r3,21
	ctx.r3.s64 = 21;
	// blr 
	return;
loc_822EAB1C:
	// li r3,9
	ctx.r3.s64 = 9;
	// blr 
	return;
loc_822EAB24:
	// li r3,302
	ctx.r3.s64 = 302;
loc_822EAB28:
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EAB30"))) PPC_WEAK_FUNC(sub_822EAB30);
PPC_FUNC_IMPL(__imp__sub_822EAB30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x822eaa98
	sub_822EAA98(ctx, base);
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// lbz r10,2(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 2);
	// sth r3,0(r31)
	PPC_STORE_U16(r31.u32 + 0, ctx.r3.u16);
	// addi r9,r11,2
	ctx.r9.s64 = r11.s64 + 2;
	// cmplwi cr6,r10,11
	cr6.compare<uint32_t>(ctx.r10.u32, 11, xer);
	// clrlwi r3,r9,16
	ctx.r3.u64 = ctx.r9.u32 & 0xFFFF;
	// bne cr6,0x822eab74
	if (!cr6.eq) goto loc_822EAB74;
	// lhz r11,1275(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 1275);
	// clrlwi r10,r3,16
	ctx.r10.u64 = ctx.r3.u32 & 0xFFFF;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// clrlwi r3,r11,16
	ctx.r3.u64 = r11.u32 & 0xFFFF;
loc_822EAB74:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EAB88"))) PPC_WEAK_FUNC(sub_822EAB88);
PPC_FUNC_IMPL(__imp__sub_822EAB88) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32256
	ctx.r10.s64 = -2113929216;
	// addi r9,r11,20568
	ctx.r9.s64 = r11.s64 + 20568;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// addi r4,r10,3224
	ctx.r4.s64 = ctx.r10.s64 + 3224;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8236d480
	sub_8236D480(ctx, base);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r6,r7,5648
	ctx.r6.s64 = ctx.r7.s64 + 5648;
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// lwz r5,0(r8)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stw r6,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r6.u32);
	// stw r5,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r5.u32);
	// bl 0x82287710
	sub_82287710(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x821c6868
	sub_821C6868(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EAC08"))) PPC_WEAK_FUNC(sub_822EAC08);
PPC_FUNC_IMPL(__imp__sub_822EAC08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r4,r4,8
	ctx.r4.s64 = ctx.r4.s64 + 8;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82287710
	sub_82287710(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EAC40"))) PPC_WEAK_FUNC(sub_822EAC40);
PPC_FUNC_IMPL(__imp__sub_822EAC40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r5,1332(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1332);
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r5,1336(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 1336);
	// li r4,5
	ctx.r4.s64 = 5;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// lis r4,0
	ctx.r4.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EACA8"))) PPC_WEAK_FUNC(sub_822EACA8);
PPC_FUNC_IMPL(__imp__sub_822EACA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-1408(r1)
	ea = -1408 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,12
	r11.s64 = 12;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stb r11,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, r11.u8);
	// addi r4,r1,99
	ctx.r4.s64 = ctx.r1.s64 + 99;
	// addi r3,r29,8
	ctx.r3.s64 = r29.s64 + 8;
	// bl 0x82304570
	sub_82304570(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lwz r11,26912(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26912);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eace8
	if (cr6.eq) goto loc_822EACE8;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lbz r10,26821(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 26821);
	// b 0x822eacec
	goto loc_822EACEC;
loc_822EACE8:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822EACEC:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ead98
	if (cr6.eq) goto loc_822EAD98;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// lwz r10,184(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 184);
	// addi r31,r10,368
	r31.s64 = ctx.r10.s64 + 368;
	// mr r11,r31
	r11.u64 = r31.u64;
	// lwz r9,372(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 372);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822EAD1C:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ead30
	if (cr6.eq) goto loc_822EAD30;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822ead34
	if (cr6.eq) goto loc_822EAD34;
loc_822EAD30:
	// twi 31,r0,22
loc_822EAD34:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ead98
	if (cr6.eq) goto loc_822EAD98;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ead48
	if (!cr6.eq) goto loc_822EAD48;
	// twi 31,r0,22
loc_822EAD48:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822ead58
	if (!cr6.eq) goto loc_822EAD58;
	// twi 31,r0,22
loc_822EAD58:
	// lwz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// lbz r8,44(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 44);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822ead84
	if (cr6.eq) goto loc_822EAD84;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822ead74
	if (!cr6.eq) goto loc_822EAD74;
	// twi 31,r0,22
loc_822EAD74:
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lwz r5,100(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 100);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EAD84:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822ead1c
	goto loc_822EAD1C;
loc_822EAD98:
	// lwz r11,236(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822eadbc
	if (!cr6.eq) goto loc_822EADBC;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lwz r5,864(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 864);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EADB4:
	// addi r1,r1,1408
	ctx.r1.s64 = ctx.r1.s64 + 1408;
	// b 0x82ca2c3c
	return;
loc_822EADBC:
	// lwz r31,1156(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 1156);
	// addi r30,r29,1152
	r30.s64 = r29.s64 + 1152;
	// lwz r11,1160(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1160);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x822eadd4
	if (!cr6.gt) goto loc_822EADD4;
	// twi 31,r0,22
loc_822EADD4:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eade8
	if (!cr6.gt) goto loc_822EADE8;
	// twi 31,r0,22
loc_822EADE8:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eadf4
	if (cr6.eq) goto loc_822EADF4;
	// twi 31,r0,22
loc_822EADF4:
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x822eadb4
	if (cr6.eq) goto loc_822EADB4;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eae08
	if (cr6.lt) goto loc_822EAE08;
	// twi 31,r0,22
loc_822EAE08:
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eae28
	if (cr6.lt) goto loc_822EAE28;
	// twi 31,r0,22
loc_822EAE28:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eadd4
	goto loc_822EADD4;
}

__attribute__((alias("__imp__sub_822EAE30"))) PPC_WEAK_FUNC(sub_822EAE30);
PPC_FUNC_IMPL(__imp__sub_822EAE30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,1156
	ctx.r3.s64 = r11.s64 + 1156;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,780(r31)
	PPC_STORE_U32(r31.u32 + 780, r30.u32);
	// lbz r10,444(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 444);
	// addi r30,r31,232
	r30.s64 = r31.s64 + 232;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eae70
	if (cr6.eq) goto loc_822EAE70;
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// b 0x822eae74
	goto loc_822EAE74;
loc_822EAE70:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
loc_822EAE74:
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// beq cr6,0x822eaedc
	if (cr6.eq) goto loc_822EAEDC;
	// lbz r11,6(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 6);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822eaf34
	if (!cr6.eq) goto loc_822EAF34;
	// li r11,1
	r11.s64 = 1;
	// stb r11,6(r31)
	PPC_STORE_U8(r31.u32 + 6, r11.u8);
	// lbz r10,212(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 212);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eaeac
	if (cr6.eq) goto loc_822EAEAC;
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// bl 0x8233a0d0
	sub_8233A0D0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822EAEAC:
	// lbz r11,213(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 213);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eaecc
	if (cr6.eq) goto loc_822EAECC;
	// li r4,9
	ctx.r4.s64 = 9;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822EAECC:
	// li r4,11
	ctx.r4.s64 = 11;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822EAEDC:
	// li r29,0
	r29.s64 = 0;
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// stb r29,6(r31)
	PPC_STORE_U8(r31.u32 + 6, r29.u8);
	// addi r4,r30,72
	ctx.r4.s64 = r30.s64 + 72;
	// li r5,60
	ctx.r5.s64 = 60;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// stw r29,236(r31)
	PPC_STORE_U32(r31.u32 + 236, r29.u32);
	// lis r4,0
	ctx.r4.s64 = 0;
	// lwz r3,380(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// li r5,1
	ctx.r5.s64 = 1;
	// ori r4,r4,32778
	ctx.r4.u64 = ctx.r4.u64 | 32778;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// lis r4,0
	ctx.r4.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r3,380(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// ori r4,r4,32779
	ctx.r4.u64 = ctx.r4.u64 | 32779;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// li r11,46
	r11.s64 = 46;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r11,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r11.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
loc_822EAF34:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822EAF40"))) PPC_WEAK_FUNC(sub_822EAF40);
PPC_FUNC_IMPL(__imp__sub_822EAF40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r30,r31,1400
	r30.s64 = r31.s64 + 1400;
	// addi r3,r11,1192
	ctx.r3.s64 = r11.s64 + 1192;
	// lwz r6,1408(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 1408);
	// ld r5,1400(r31)
	ctx.r5.u64 = PPC_LOAD_U64(r31.u32 + 1400);
	// ld r4,1388(r31)
	ctx.r4.u64 = PPC_LOAD_U64(r31.u32 + 1388);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,1464(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1464);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r3,r31,304
	ctx.r3.s64 = r31.s64 + 304;
	// li r5,60
	ctx.r5.s64 = 60;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r11,26932(r10)
	PPC_STORE_U32(ctx.r10.u32 + 26932, r11.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r9,1464(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1464);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,1460(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 1460);
	// stw r9,380(r31)
	PPC_STORE_U32(r31.u32 + 380, ctx.r9.u32);
	// bl 0x822eae30
	sub_822EAE30(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EAFC0"))) PPC_WEAK_FUNC(sub_822EAFC0);
PPC_FUNC_IMPL(__imp__sub_822EAFC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x822eb9c8
	sub_822EB9C8(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,500(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 500);
	// subf r10,r11,r3
	ctx.r10.s64 = ctx.r3.s64 - r11.s64;
	// cmplwi cr6,r10,60000
	cr6.compare<uint32_t>(ctx.r10.u32, 60000, xer);
	// ble cr6,0x822eb0bc
	if (!cr6.gt) goto loc_822EB0BC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r29,r11,1288
	r29.s64 = r11.s64 + 1288;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// li r30,0
	r30.s64 = 0;
	// stw r29,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r29.u32);
	// lwz r3,26920(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 26920);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822eb024
	if (cr6.eq) goto loc_822EB024;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lbz r11,26917(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 26917);
	// b 0x822eb028
	goto loc_822EB028;
loc_822EB024:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822EB028:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eb054
	if (cr6.eq) goto loc_822EB054;
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lbz r10,196(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 196);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eb054
	if (cr6.eq) goto loc_822EB054;
	// lbz r11,92(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 92);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822eb054
	if (!cr6.eq) goto loc_822EB054;
	// bl 0x82356348
	sub_82356348(ctx, base);
loc_822EB054:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,1332
	ctx.r4.s64 = r11.s64 + 1332;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82303aa0
	sub_82303AA0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r4,4096
	ctx.r4.s64 = 4096;
	// addi r3,r10,1716
	ctx.r3.s64 = ctx.r10.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r9,456(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r9,4
	cr6.compare<int32_t>(ctx.r9.s32, 4, xer);
	// bne cr6,0x822eb0a8
	if (!cr6.eq) goto loc_822EB0A8;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r30,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r30.u32);
loc_822EB0A8:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,4096
	r11.s64 = 4096;
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r30,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r30.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
loc_822EB0BC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822EB0C8"))) PPC_WEAK_FUNC(sub_822EB0C8);
PPC_FUNC_IMPL(__imp__sub_822EB0C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r31,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r31.u32);
	// bl 0x82cf9c00
	sub_82CF9C00(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822eb0f4
	if (cr6.eq) goto loc_822EB0F4;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,1356
	ctx.r3.s64 = r11.s64 + 1356;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EB0F4:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82cf9b58
	sub_82CF9B58(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822eb0f4
	if (cr6.eq) goto loc_822EB0F4;
	// addi r29,r31,512
	r29.s64 = r31.s64 + 512;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82cf9af8
	sub_82CF9AF8(ctx, base);
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x82cbc5f0
	sub_82CBC5F0(ctx, base);
	// stw r3,448(r31)
	PPC_STORE_U32(r31.u32 + 448, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822eb130
	if (cr6.eq) goto loc_822EB130;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x822eb13c
	if (!cr6.eq) goto loc_822EB13C;
loc_822EB130:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,1384
	ctx.r3.s64 = r11.s64 + 1384;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EB13C:
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82cbc5f0
	sub_82CBC5F0(ctx, base);
	// stw r3,452(r31)
	PPC_STORE_U32(r31.u32 + 452, ctx.r3.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822eb158
	if (cr6.eq) goto loc_822EB158;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x822eb164
	if (!cr6.eq) goto loc_822EB164;
loc_822EB158:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,1436
	ctx.r3.s64 = r11.s64 + 1436;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EB164:
	// bl 0x82c81990
	sub_82C81990(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r11,r30
	r11.u64 = r30.u64;
	// lis r8,-31950
	ctx.r8.s64 = -2093875200;
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// lis r6,-31950
	ctx.r6.s64 = -2093875200;
	// stw r11,26824(r10)
	PPC_STORE_U32(ctx.r10.u32 + 26824, r11.u32);
	// li r10,2
	ctx.r10.s64 = 2;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// li r11,2
	r11.s64 = 2;
	// stw r10,-27360(r8)
	PPC_STORE_U32(ctx.r8.u32 + -27360, ctx.r10.u32);
	// stw r9,26828(r7)
	PPC_STORE_U32(ctx.r7.u32 + 26828, ctx.r9.u32);
	// li r3,3
	ctx.r3.s64 = 3;
	// stw r11,-27356(r6)
	PPC_STORE_U32(ctx.r6.u32 + -27356, r11.u32);
	// bl 0x82cbc5f0
	sub_82CBC5F0(ctx, base);
	// lis r5,-31927
	ctx.r5.s64 = -2092367872;
	// stw r3,26832(r5)
	PPC_STORE_U32(ctx.r5.u32 + 26832, ctx.r3.u32);
	// bl 0x82339b30
	sub_82339B30(ctx, base);
	// li r5,254
	ctx.r5.s64 = 254;
	// li r4,2
	ctx.r4.s64 = 2;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x82cfaa30
	sub_82CFAA30(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// stw r3,496(r31)
	PPC_STORE_U32(r31.u32 + 496, ctx.r3.u32);
	// bne cr6,0x822eb1d8
	if (!cr6.eq) goto loc_822EB1D8;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,1492
	ctx.r3.s64 = r11.s64 + 1492;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EB1D8:
	// li r11,1001
	r11.s64 = 1001;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// li r10,2
	ctx.r10.s64 = 2;
	// lwz r3,496(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 496);
	// sth r11,98(r1)
	PPC_STORE_U16(ctx.r1.u32 + 98, r11.u16);
	// li r5,16
	ctx.r5.s64 = 16;
	// sth r10,96(r1)
	PPC_STORE_U16(ctx.r1.u32 + 96, ctx.r10.u16);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82cfaa90
	sub_82CFAA90(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822eb218
	if (cr6.eq) goto loc_822EB218;
	// bl 0x82cfac30
	sub_82CFAC30(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r11,1524
	ctx.r3.s64 = r11.s64 + 1524;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EB218:
	// li r11,1
	r11.s64 = 1;
	// lwz r3,496(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 496);
	// lis r4,-32764
	ctx.r4.s64 = -2147221504;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// ori r4,r4,26238
	ctx.r4.u64 = ctx.r4.u64 | 26238;
	// bl 0x82cfaa58
	sub_82CFAA58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822eb248
	if (cr6.eq) goto loc_822EB248;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,1560
	ctx.r3.s64 = r11.s64 + 1560;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EB248:
	// li r11,8
	r11.s64 = 8;
	// stw r30,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r30.u32);
	// addi r10,r31,1172
	ctx.r10.s64 = r31.s64 + 1172;
	// stw r30,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r30.u32);
	// stw r11,460(r31)
	PPC_STORE_U32(r31.u32 + 460, r11.u32);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// stw r10,1204(r31)
	PPC_STORE_U32(r31.u32 + 1204, ctx.r10.u32);
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// bl 0x82304008
	sub_82304008(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822EB278"))) PPC_WEAK_FUNC(sub_822EB278);
PPC_FUNC_IMPL(__imp__sub_822EB278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-1392(r1)
	ea = -1392 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// stw r4,1420(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1420, ctx.r4.u32);
	// lwz r11,236(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822eb2c8
	if (!cr6.eq) goto loc_822EB2C8;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r31,r11,1604
	r31.s64 = r11.s64 + 1604;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// li r11,1
	r11.s64 = 1;
	// stw r31,384(r29)
	PPC_STORE_U32(r29.u32 + 384, r31.u32);
	// stb r11,26780(r9)
	PPC_STORE_U8(ctx.r9.u32 + 26780, r11.u8);
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x82ca2c3c
	return;
loc_822EB2C8:
	// lwz r11,1156(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1156);
	// addi r30,r29,1152
	r30.s64 = r29.s64 + 1152;
	// lwz r6,1160(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 1160);
	// li r4,0
	ctx.r4.s64 = 0;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// ble cr6,0x822eb2e4
	if (!cr6.gt) goto loc_822EB2E4;
	// twi 31,r0,22
loc_822EB2E4:
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 8);
loc_822EB2F0:
	// cmplw cr6,r3,r5
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r5.u32, xer);
	// ble cr6,0x822eb2fc
	if (!cr6.gt) goto loc_822EB2FC;
	// twi 31,r0,22
loc_822EB2FC:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eb308
	if (cr6.eq) goto loc_822EB308;
	// twi 31,r0,22
loc_822EB308:
	// cmplw cr6,r7,r5
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r5.u32, xer);
	// beq cr6,0x822eb374
	if (cr6.eq) goto loc_822EB374;
	// cmplw cr6,r7,r6
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r6.u32, xer);
	// blt cr6,0x822eb31c
	if (cr6.lt) goto loc_822EB31C;
	// twi 31,r0,22
loc_822EB31C:
	// addi r11,r7,8
	r11.s64 = ctx.r7.s64 + 8;
	// addi r10,r1,1420
	ctx.r10.s64 = ctx.r1.s64 + 1420;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822EB328:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r31,0(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822eb348
	if (!cr0.eq) goto loc_822EB348;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822eb328
	if (!cr6.eq) goto loc_822EB328;
loc_822EB348:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822eb360
	if (!cr6.eq) goto loc_822EB360;
	// cmplw cr6,r7,r6
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r6.u32, xer);
	// blt cr6,0x822eb35c
	if (cr6.lt) goto loc_822EB35C;
	// twi 31,r0,22
loc_822EB35C:
	// ld r4,0(r7)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r7.u32 + 0);
loc_822EB360:
	// cmplw cr6,r7,r6
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r6.u32, xer);
	// blt cr6,0x822eb36c
	if (cr6.lt) goto loc_822EB36C;
	// twi 31,r0,22
loc_822EB36C:
	// addi r7,r7,296
	ctx.r7.s64 = ctx.r7.s64 + 296;
	// b 0x822eb2f0
	goto loc_822EB2F0;
loc_822EB374:
	// cmpldi cr6,r4,0
	cr6.compare<uint64_t>(ctx.r4.u64, 0, xer);
	// beq cr6,0x822eb460
	if (cr6.eq) goto loc_822EB460;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r10,2
	ctx.r10.s64 = 2;
	// lwz r6,8(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r12,83
	r12.s64 = 83;
	// stdx r4,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r4.u64);
	// stb r10,82(r1)
	PPC_STORE_U8(ctx.r1.u32 + 82, ctx.r10.u8);
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// stb r9,127(r1)
	PPC_STORE_U8(ctx.r1.u32 + 127, ctx.r9.u8);
	// ble cr6,0x822eb3a8
	if (!cr6.gt) goto loc_822EB3A8;
	// twi 31,r0,22
loc_822EB3A8:
	// mr r31,r11
	r31.u64 = r11.u64;
loc_822EB3AC:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822eb3c0
	if (!cr6.gt) goto loc_822EB3C0;
	// twi 31,r0,22
loc_822EB3C0:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eb3cc
	if (cr6.eq) goto loc_822EB3CC;
	// twi 31,r0,22
loc_822EB3CC:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822eb44c
	if (cr6.eq) goto loc_822EB44C;
	// cmplw cr6,r31,r6
	cr6.compare<uint32_t>(r31.u32, ctx.r6.u32, xer);
	// blt cr6,0x822eb3e0
	if (cr6.lt) goto loc_822EB3E0;
	// twi 31,r0,22
loc_822EB3E0:
	// addi r7,r31,8
	ctx.r7.s64 = r31.s64 + 8;
	// addi r10,r1,1420
	ctx.r10.s64 = ctx.r1.s64 + 1420;
	// mr r11,r7
	r11.u64 = ctx.r7.u64;
	// addi r8,r7,4
	ctx.r8.s64 = ctx.r7.s64 + 4;
loc_822EB3F0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r5,0(r10)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r5,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r5.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822eb410
	if (!cr0.eq) goto loc_822EB410;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822eb3f0
	if (!cr6.eq) goto loc_822EB3F0;
loc_822EB410:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x822eb434
	if (cr6.eq) goto loc_822EB434;
	// cmplw cr6,r31,r6
	cr6.compare<uint32_t>(r31.u32, ctx.r6.u32, xer);
	// blt cr6,0x822eb424
	if (cr6.lt) goto loc_822EB424;
	// twi 31,r0,22
loc_822EB424:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EB434:
	// lwz r6,8(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r6
	cr6.compare<uint32_t>(r31.u32, ctx.r6.u32, xer);
	// blt cr6,0x822eb444
	if (cr6.lt) goto loc_822EB444;
	// twi 31,r0,22
loc_822EB444:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eb3ac
	goto loc_822EB3AC;
loc_822EB44C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,564(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 564);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EB460:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,1
	r11.s64 = 1;
	// stb r11,26780(r10)
	PPC_STORE_U8(ctx.r10.u32 + 26780, r11.u8);
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822EB478"))) PPC_WEAK_FUNC(sub_822EB478);
PPC_FUNC_IMPL(__imp__sub_822EB478) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,504(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 504);
	// subf r10,r11,r3
	ctx.r10.s64 = ctx.r3.s64 - r11.s64;
	// cmplwi cr6,r10,1000
	cr6.compare<uint32_t>(ctx.r10.u32, 1000, xer);
	// blt cr6,0x822eb530
	if (cr6.lt) goto loc_822EB530;
	// lwz r11,236(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 236);
	// stw r3,504(r29)
	PPC_STORE_U32(r29.u32 + 504, ctx.r3.u32);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822eb524
	if (cr6.eq) goto loc_822EB524;
	// lwz r31,1156(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 1156);
	// addi r30,r29,1152
	r30.s64 = r29.s64 + 1152;
	// lwz r11,1160(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1160);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x822eb4c4
	if (!cr6.gt) goto loc_822EB4C4;
	// twi 31,r0,22
loc_822EB4C4:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eb4d8
	if (!cr6.gt) goto loc_822EB4D8;
	// twi 31,r0,22
loc_822EB4D8:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eb4e4
	if (cr6.eq) goto loc_822EB4E4;
	// twi 31,r0,22
loc_822EB4E4:
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x822eb530
	if (cr6.eq) goto loc_822EB530;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eb4f8
	if (cr6.lt) goto loc_822EB4F8;
	// twi 31,r0,22
loc_822EB4F8:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb538
	sub_822EB538(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// blt cr6,0x822eb530
	if (cr6.lt) goto loc_822EB530;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eb51c
	if (cr6.lt) goto loc_822EB51C;
	// twi 31,r0,22
loc_822EB51C:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eb4c4
	goto loc_822EB4C4;
loc_822EB524:
	// addi r4,r29,856
	ctx.r4.s64 = r29.s64 + 856;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb538
	sub_822EB538(ctx, base);
loc_822EB530:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822EB538"))) PPC_WEAK_FUNC(sub_822EB538);
PPC_FUNC_IMPL(__imp__sub_822EB538) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-1376(r1)
	ea = -1376 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,9
	r11.s64 = 9;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// stb r11,82(r1)
	PPC_STORE_U8(ctx.r1.u32 + 82, r11.u8);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r10,248(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 248);
	// lis r9,0
	ctx.r9.s64 = 0;
	// subf r8,r10,r3
	ctx.r8.s64 = ctx.r3.s64 - ctx.r10.s64;
	// ori r7,r9,60000
	ctx.r7.u64 = ctx.r9.u64 | 60000;
	// subfc r6,r8,r7
	xer.ca = ctx.r7.u32 >= ctx.r8.u32;
	ctx.r6.s64 = ctx.r7.s64 - ctx.r8.s64;
	// subfe r5,r6,r6
	temp.u8 = (~ctx.r6.u32 + ctx.r6.u32 < ~ctx.r6.u32) | (~ctx.r6.u32 + ctx.r6.u32 + xer.ca < xer.ca);
	ctx.r5.u64 = ~ctx.r6.u64 + ctx.r6.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r4,r5,31
	ctx.r4.u64 = ctx.r5.u32 & 0x1;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beq cr6,0x822eb5b0
	if (cr6.eq) goto loc_822EB5B0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,1668
	ctx.r3.s64 = r11.s64 + 1668;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// addi r1,r1,1376
	ctx.r1.s64 = ctx.r1.s64 + 1376;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822EB5B0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,1376
	ctx.r1.s64 = ctx.r1.s64 + 1376;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EB5C8"))) PPC_WEAK_FUNC(sub_822EB5C8);
PPC_FUNC_IMPL(__imp__sub_822EB5C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,1700
	ctx.r3.s64 = r11.s64 + 1700;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lbz r11,560(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 560);
	// lwz r10,236(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// mr r28,r11
	r28.u64 = r11.u64;
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822eb608
	if (!cr6.eq) goto loc_822EB608;
	// lbz r11,904(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 904);
	// add r28,r11,r28
	r28.u64 = r11.u64 + r28.u64;
	// mr r30,r11
	r30.u64 = r11.u64;
	// b 0x822eb60c
	goto loc_822EB60C;
loc_822EB608:
	// addi r30,r11,1
	r30.s64 = r11.s64 + 1;
loc_822EB60C:
	// lwz r11,1156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1156);
	// addi r8,r31,1152
	ctx.r8.s64 = r31.s64 + 1152;
	// lwz r9,1160(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1160);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// ble cr6,0x822eb624
	if (!cr6.gt) goto loc_822EB624;
	// twi 31,r0,22
loc_822EB624:
	// lwz r6,4(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r7,8(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
loc_822EB62C:
	// cmplw cr6,r6,r7
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, xer);
	// ble cr6,0x822eb638
	if (!cr6.gt) goto loc_822EB638;
	// twi 31,r0,22
loc_822EB638:
	// cmplw cr6,r8,r8
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r8.u32, xer);
	// beq cr6,0x822eb644
	if (cr6.eq) goto loc_822EB644;
	// twi 31,r0,22
loc_822EB644:
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// beq cr6,0x822eb69c
	if (cr6.eq) goto loc_822EB69C;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822eb658
	if (cr6.lt) goto loc_822EB658;
	// twi 31,r0,22
loc_822EB658:
	// lbz r10,48(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 48);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// add r28,r10,r28
	r28.u64 = ctx.r10.u64 + r28.u64;
	// blt cr6,0x822eb66c
	if (cr6.lt) goto loc_822EB66C;
	// twi 31,r0,22
loc_822EB66C:
	// lwz r5,268(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 268);
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x822eb688
	if (cr6.eq) goto loc_822EB688;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822eb684
	if (cr6.lt) goto loc_822EB684;
	// twi 31,r0,22
loc_822EB684:
	// add r30,r10,r30
	r30.u64 = ctx.r10.u64 + r30.u64;
loc_822EB688:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822eb694
	if (cr6.lt) goto loc_822EB694;
	// twi 31,r0,22
loc_822EB694:
	// addi r11,r11,296
	r11.s64 = r11.s64 + 296;
	// b 0x822eb62c
	goto loc_822EB62C;
loc_822EB69C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r31,r31,232
	r31.s64 = r31.s64 + 232;
	// addi r29,r11,20948
	r29.s64 = r11.s64 + 20948;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r5,188(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,188(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 188);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822eb6d8
	if (cr6.lt) goto loc_822EB6D8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r30,188(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 188);
loc_822EB6D8:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lwz r6,196(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 196);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r29,r11,20984
	r29.s64 = r11.s64 + 20984;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,196(r31)
	PPC_STORE_U32(r31.u32 + 196, r30.u32);
	// subf r30,r30,r28
	r30.s64 = r28.s64 - r30.s64;
	// lwz r6,192(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 192);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,2
	ctx.r4.s64 = 2;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,192(r31)
	PPC_STORE_U32(r31.u32 + 192, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822EB720"))) PPC_WEAK_FUNC(sub_822EB720);
PPC_FUNC_IMPL(__imp__sub_822EB720) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r30,r29,1152
	r30.s64 = r29.s64 + 1152;
	// lwz r11,1156(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 1156);
	// lwz r10,1160(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 1160);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eb748
	if (!cr6.gt) goto loc_822EB748;
	// twi 31,r0,22
loc_822EB748:
	// li r8,0
	ctx.r8.s64 = 0;
loc_822EB74C:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r7,4(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r7,r9
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, xer);
	// ble cr6,0x822eb760
	if (!cr6.gt) goto loc_822EB760;
	// twi 31,r0,22
loc_822EB760:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eb76c
	if (cr6.eq) goto loc_822EB76C;
	// twi 31,r0,22
loc_822EB76C:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822eb79c
	if (cr6.eq) goto loc_822EB79C;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eb780
	if (cr6.lt) goto loc_822EB780;
	// twi 31,r0,22
loc_822EB780:
	// stw r8,272(r11)
	PPC_STORE_U32(r11.u32 + 272, ctx.r8.u32);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eb794
	if (cr6.lt) goto loc_822EB794;
	// twi 31,r0,22
loc_822EB794:
	// addi r11,r11,296
	r11.s64 = r11.s64 + 296;
	// b 0x822eb74c
	goto loc_822EB74C;
loc_822EB79C:
	// lwz r11,432(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 432);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// ble cr6,0x822eb854
	if (!cr6.gt) goto loc_822EB854;
	// li r6,1
	ctx.r6.s64 = 1;
loc_822EB7B4:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// ble cr6,0x822eb7c8
	if (!cr6.gt) goto loc_822EB7C8;
	// twi 31,r0,22
loc_822EB7C8:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r5,4(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r5,r10
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eb7dc
	if (!cr6.gt) goto loc_822EB7DC;
	// twi 31,r0,22
loc_822EB7DC:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eb7e8
	if (cr6.eq) goto loc_822EB7E8;
	// twi 31,r0,22
loc_822EB7E8:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x822eb83c
	if (cr6.eq) goto loc_822EB83C;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822eb7fc
	if (cr6.lt) goto loc_822EB7FC;
	// twi 31,r0,22
loc_822EB7FC:
	// lwz r10,432(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 432);
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// lwz r4,4(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// ldx r3,r4,r8
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r4.u32 + ctx.r8.u32);
	// cmpld cr6,r5,r3
	cr6.compare<uint64_t>(ctx.r5.u64, ctx.r3.u64, xer);
	// bne cr6,0x822eb824
	if (!cr6.eq) goto loc_822EB824;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822eb820
	if (cr6.lt) goto loc_822EB820;
	// twi 31,r0,22
loc_822EB820:
	// stw r6,272(r11)
	PPC_STORE_U32(r11.u32 + 272, ctx.r6.u32);
loc_822EB824:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822eb834
	if (cr6.lt) goto loc_822EB834;
	// twi 31,r0,22
loc_822EB834:
	// addi r11,r11,296
	r11.s64 = r11.s64 + 296;
	// b 0x822eb7c8
	goto loc_822EB7C8;
loc_822EB83C:
	// lwz r11,432(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 432);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addi r8,r8,24
	ctx.r8.s64 = ctx.r8.s64 + 24;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r7,r10
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eb7b4
	if (cr6.lt) goto loc_822EB7B4;
loc_822EB854:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x822eb868
	if (!cr6.gt) goto loc_822EB868;
	// twi 31,r0,22
loc_822EB868:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eb87c
	if (!cr6.gt) goto loc_822EB87C;
	// twi 31,r0,22
loc_822EB87C:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eb888
	if (cr6.eq) goto loc_822EB888;
	// twi 31,r0,22
loc_822EB888:
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x822eb8d8
	if (cr6.eq) goto loc_822EB8D8;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eb89c
	if (cr6.lt) goto loc_822EB89C;
	// twi 31,r0,22
loc_822EB89C:
	// lwz r10,272(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 272);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822eb8c0
	if (!cr6.eq) goto loc_822EB8C0;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eb8b4
	if (cr6.lt) goto loc_822EB8B4;
	// twi 31,r0,22
loc_822EB8B4:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822eb278
	sub_822EB278(ctx, base);
loc_822EB8C0:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eb8d0
	if (cr6.lt) goto loc_822EB8D0;
	// twi 31,r0,22
loc_822EB8D0:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eb868
	goto loc_822EB868;
loc_822EB8D8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822EB8E0"))) PPC_WEAK_FUNC(sub_822EB8E0);
PPC_FUNC_IMPL(__imp__sub_822EB8E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r31,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r31.u32);
	// addi r11,r1,82
	r11.s64 = ctx.r1.s64 + 82;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_822EB90C:
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x822eb90c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822EB90C;
	// li r8,1001
	ctx.r8.s64 = 1001;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r11,r1,180
	r11.s64 = ctx.r1.s64 + 180;
	// sth r8,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r8.u16);
	// li r9,2
	ctx.r9.s64 = 2;
	// addi r10,r10,564
	ctx.r10.s64 = ctx.r10.s64 + 564;
	// sth r9,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r9.u16);
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822EB93C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822eb95c
	if (!cr0.eq) goto loc_822EB95C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822eb93c
	if (!cr6.eq) goto loc_822EB93C;
loc_822EB95C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822eb970
	if (!cr6.eq) goto loc_822EB970;
	// lis r11,32512
	r11.s64 = 2130706432;
	// ori r10,r11,1
	ctx.r10.u64 = r11.u64 | 1;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822EB970:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r28,r1,80
	r28.s64 = ctx.r1.s64 + 80;
	// bl 0x822eab30
	sub_822EAB30(ctx, base);
	// clrlwi r5,r3,16
	ctx.r5.u64 = ctx.r3.u32 & 0xFFFF;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,496(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 496);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r7,r28
	ctx.r7.u64 = r28.u64;
	// li r8,16
	ctx.r8.s64 = 16;
	// bl 0x82cfabb0
	sub_82CFABB0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822eab30
	sub_822EAB30(ctx, base);
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// cmpw cr6,r28,r11
	cr6.compare<int32_t>(r28.s32, r11.s32, xer);
	// beq cr6,0x822eb9bc
	if (cr6.eq) goto loc_822EB9BC;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822eb278
	sub_822EB278(ctx, base);
loc_822EB9BC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822EB9C8"))) PPC_WEAK_FUNC(sub_822EB9C8);
PPC_FUNC_IMPL(__imp__sub_822EB9C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-1440(r1)
	ea = -1440 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,0
	r11.s64 = 0;
	// li r10,16
	ctx.r10.s64 = 16;
	// stb r11,114(r1)
	PPC_STORE_U8(ctx.r1.u32 + 114, r11.u8);
	// lis r11,0
	r11.s64 = 0;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// li r28,4098
	r28.s64 = 4098;
	// ori r29,r11,65534
	r29.u64 = r11.u64 | 65534;
loc_822EB9F4:
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r3,496(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 496);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1277
	ctx.r5.s64 = 1277;
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// bl 0x82cfab20
	sub_82CFAB20(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// beq cr6,0x822ebc14
	if (cr6.eq) goto loc_822EBC14;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x822ebc14
	if (!cr6.gt) goto loc_822EBC14;
	// lhz r10,82(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// cmplwi cr6,r10,1001
	cr6.compare<uint32_t>(ctx.r10.u32, 1001, xer);
	// bne cr6,0x822ebc08
	if (!cr6.eq) goto loc_822EBC08;
	// lbz r11,114(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 114);
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bgt cr6,0x822ebc08
	if (cr6.gt) goto loc_822EBC08;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-17836
	r12.s64 = r12.s64 + -17836;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822EBA88;
	case 1:
		goto loc_822EBA9C;
	case 2:
		goto loc_822EBAB0;
	case 3:
		goto loc_822EBC08;
	case 4:
		goto loc_822EBAC4;
	case 5:
		goto loc_822EBAD4;
	case 6:
		goto loc_822EBAE8;
	case 7:
		goto loc_822EBC08;
	case 8:
		goto loc_822EBC08;
	case 9:
		goto loc_822EBAFC;
	case 10:
		goto loc_822EBB10;
	case 11:
		goto loc_822EBB20;
	case 12:
		goto loc_822EBBF8;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-17784(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17784);
	// lwz r17,-17764(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17764);
	// lwz r17,-17744(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17744);
	// lwz r17,-17400(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17400);
	// lwz r17,-17724(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17724);
	// lwz r17,-17708(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17708);
	// lwz r17,-17688(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17688);
	// lwz r17,-17400(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17400);
	// lwz r17,-17400(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17400);
	// lwz r17,-17668(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17668);
	// lwz r17,-17648(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17648);
	// lwz r17,-17632(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17632);
	// lwz r17,-17416(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -17416);
loc_822EBA88:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ee2d8
	sub_822EE2D8(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBA9C:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822eeee8
	sub_822EEEE8(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBAB0:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ef408
	sub_822EF408(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBAC4:
	// li r4,4100
	ctx.r4.s64 = 4100;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBAD4:
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r30,232
	ctx.r3.s64 = r30.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// stw r28,456(r30)
	PPC_STORE_U32(r30.u32 + 456, r28.u32);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBAE8:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ebc20
	sub_822EBC20(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBAFC:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822eed38
	sub_822EED38(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBB10:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x822eb278
	sub_822EB278(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBB20:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822eaa98
	sub_822EAA98(ctx, base);
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// lwz r10,236(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 236);
	// subf r9,r11,r31
	ctx.r9.s64 = r31.s64 - r11.s64;
	// li r6,115
	ctx.r6.s64 = 115;
	// ldx r6,r1,r6
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + ctx.r6.u32);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// add r8,r9,r29
	ctx.r8.u64 = ctx.r9.u64 + r29.u64;
	// clrlwi r5,r8,16
	ctx.r5.u64 = ctx.r8.u32 & 0xFFFF;
	// sth r5,1387(r1)
	PPC_STORE_U16(ctx.r1.u32 + 1387, ctx.r5.u16);
	// bne cr6,0x822ebb60
	if (!cr6.eq) goto loc_822EBB60;
	// ld r10,856(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 856);
	// addi r11,r30,856
	r11.s64 = r30.s64 + 856;
	// cmpld cr6,r6,r10
	cr6.compare<uint64_t>(ctx.r6.u64, ctx.r10.u64, xer);
	// beq cr6,0x822ebbd8
	if (cr6.eq) goto loc_822EBBD8;
loc_822EBB60:
	// lwz r11,1156(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 1156);
	// addi r10,r30,1152
	ctx.r10.s64 = r30.s64 + 1152;
	// lwz r8,1160(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 1160);
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// ble cr6,0x822ebb78
	if (!cr6.gt) goto loc_822EBB78;
	// twi 31,r0,22
loc_822EBB78:
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
loc_822EBB80:
	// cmplw cr6,r7,r9
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r9.u32, xer);
	// ble cr6,0x822ebb8c
	if (!cr6.gt) goto loc_822EBB8C;
	// twi 31,r0,22
loc_822EBB8C:
	// cmplw cr6,r10,r10
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r10.u32, xer);
	// beq cr6,0x822ebb98
	if (cr6.eq) goto loc_822EBB98;
	// twi 31,r0,22
loc_822EBB98:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ebc08
	if (cr6.eq) goto loc_822EBC08;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x822ebbac
	if (cr6.lt) goto loc_822EBBAC;
	// twi 31,r0,22
loc_822EBBAC:
	// ld r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// cmpld cr6,r4,r6
	cr6.compare<uint64_t>(ctx.r4.u64, ctx.r6.u64, xer);
	// beq cr6,0x822ebbcc
	if (cr6.eq) goto loc_822EBBCC;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x822ebbc4
	if (cr6.lt) goto loc_822EBBC4;
	// twi 31,r0,22
loc_822EBBC4:
	// addi r11,r11,296
	r11.s64 = r11.s64 + 296;
	// b 0x822ebb80
	goto loc_822EBB80;
loc_822EBBCC:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// blt cr6,0x822ebbd8
	if (cr6.lt) goto loc_822EBBD8;
	// twi 31,r0,22
loc_822EBBD8:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ebc08
	if (cr6.eq) goto loc_822EBC08;
	// li r7,0
	ctx.r7.s64 = 0;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// addi r4,r1,123
	ctx.r4.s64 = ctx.r1.s64 + 123;
	// addi r3,r30,8
	ctx.r3.s64 = r30.s64 + 8;
	// bl 0x823044a0
	sub_823044A0(ctx, base);
	// b 0x822ebc08
	goto loc_822EBC08;
loc_822EBBF8:
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// lwz r5,84(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822ef758
	sub_822EF758(ctx, base);
loc_822EBC08:
	// lhz r10,82(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 82);
	// cmplwi cr6,r10,1001
	cr6.compare<uint32_t>(ctx.r10.u32, 1001, xer);
	// beq cr6,0x822eb9f4
	if (cr6.eq) goto loc_822EB9F4;
loc_822EBC14:
	// addi r1,r1,1440
	ctx.r1.s64 = ctx.r1.s64 + 1440;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822EBC20"))) PPC_WEAK_FUNC(sub_822EBC20);
PPC_FUNC_IMPL(__imp__sub_822EBC20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r6,r31,1152
	ctx.r6.s64 = r31.s64 + 1152;
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// lwz r11,1156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1156);
	// lwz r10,1160(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1160);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ebc58
	if (!cr6.gt) goto loc_822EBC58;
	// twi 31,r0,22
loc_822EBC58:
	// mr r7,r11
	ctx.r7.u64 = r11.u64;
loc_822EBC5C:
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// lwz r9,4(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// ble cr6,0x822ebc70
	if (!cr6.gt) goto loc_822EBC70;
	// twi 31,r0,22
loc_822EBC70:
	// cmplw cr6,r6,r6
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r6.u32, xer);
	// beq cr6,0x822ebc7c
	if (cr6.eq) goto loc_822EBC7C;
	// twi 31,r0,22
loc_822EBC7C:
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// beq cr6,0x822ebcf0
	if (cr6.eq) goto loc_822EBCF0;
	// cmplw cr6,r7,r10
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ebc90
	if (cr6.lt) goto loc_822EBC90;
	// twi 31,r0,22
loc_822EBC90:
	// addi r11,r7,8
	r11.s64 = ctx.r7.s64 + 8;
	// addi r10,r1,132
	ctx.r10.s64 = ctx.r1.s64 + 132;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822EBC9C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r3,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r3.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822ebcbc
	if (!cr0.eq) goto loc_822EBCBC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822ebc9c
	if (!cr6.eq) goto loc_822EBC9C;
loc_822EBCBC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822ebcc8
	if (!cr6.eq) goto loc_822EBCC8;
	// stw r4,272(r7)
	PPC_STORE_U32(ctx.r7.u32 + 272, ctx.r4.u32);
loc_822EBCC8:
	// lwz r11,272(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 272);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822ebcd8
	if (!cr6.eq) goto loc_822EBCD8;
	// li r5,0
	ctx.r5.s64 = 0;
loc_822EBCD8:
	// lwz r10,8(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// cmplw cr6,r7,r10
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ebce8
	if (cr6.lt) goto loc_822EBCE8;
	// twi 31,r0,22
loc_822EBCE8:
	// addi r7,r7,296
	ctx.r7.s64 = ctx.r7.s64 + 296;
	// b 0x822ebc5c
	goto loc_822EBC5C;
loc_822EBCF0:
	// clrlwi r11,r5,24
	r11.u64 = ctx.r5.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ebd10
	if (cr6.eq) goto loc_822EBD10;
	// li r4,4
	ctx.r4.s64 = 4;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// li r11,4098
	r11.s64 = 4098;
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
loc_822EBD10:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EBD28"))) PPC_WEAK_FUNC(sub_822EBD28);
PPC_FUNC_IMPL(__imp__sub_822EBD28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-1504(r1)
	ea = -1504 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r27,0
	r27.s64 = 0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822ebd64
	if (!cr6.eq) goto loc_822EBD64;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r27,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r27.u32);
loc_822EBD64:
	// cmpwi cr6,r26,4096
	cr6.compare<int32_t>(r26.s32, 4096, xer);
	// bgt cr6,0x822ebf58
	if (cr6.gt) goto loc_822EBF58;
	// beq cr6,0x822ebf40
	if (cr6.eq) goto loc_822EBF40;
	// addi r11,r26,-1
	r11.s64 = r26.s64 + -1;
	// cmplwi cr6,r11,6
	cr6.compare<uint32_t>(r11.u32, 6, xer);
	// bgt cr6,0x822ec12c
	if (cr6.gt) goto loc_822EC12C;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-17004
	r12.s64 = r12.s64 + -17004;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822EBDB0;
	case 1:
		goto loc_822EBDDC;
	case 2:
		goto loc_822EC12C;
	case 3:
		goto loc_822EC12C;
	case 4:
		goto loc_822EBE60;
	case 5:
		goto loc_822EC12C;
	case 6:
		goto loc_822EBE78;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-16976(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16976);
	// lwz r17,-16932(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16932);
	// lwz r17,-16084(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16084);
	// lwz r17,-16084(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16084);
	// lwz r17,-16800(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16800);
	// lwz r17,-16084(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16084);
	// lwz r17,-16776(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -16776);
loc_822EBDB0:
	// li r11,46
	r11.s64 = 46;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stw r11,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r11.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r27,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r27.u32);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EBDDC:
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,856
	ctx.r3.s64 = r31.s64 + 856;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r3,r31,1152
	ctx.r3.s64 = r31.s64 + 1152;
	// bl 0x822f06c0
	sub_822F06C0(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// lwz r11,26932(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26932);
	// stw r11,380(r31)
	PPC_STORE_U32(r31.u32 + 380, r11.u32);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r27,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r27.u32);
	// lis r4,0
	ctx.r4.s64 = 0;
	// lwz r3,380(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// li r5,1
	ctx.r5.s64 = 1;
	// ori r4,r4,32779
	ctx.r4.u64 = ctx.r4.u64 | 32779;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// lis r4,0
	ctx.r4.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r3,380(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 380);
	// ori r4,r4,32778
	ctx.r4.u64 = ctx.r4.u64 | 32778;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// li r9,1
	ctx.r9.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// stw r9,236(r31)
	PPC_STORE_U32(r31.u32 + 236, ctx.r9.u32);
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EBE60:
	// addi r3,r31,1152
	ctx.r3.s64 = r31.s64 + 1152;
	// bl 0x822f06c0
	sub_822F06C0(ctx, base);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EBE78:
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,856
	ctx.r3.s64 = r31.s64 + 856;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r3,r31,1152
	ctx.r3.s64 = r31.s64 + 1152;
	// bl 0x822f06c0
	sub_822F06C0(ctx, base);
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r11,8
	r11.s64 = 8;
	// addi r30,r31,8
	r30.s64 = r31.s64 + 8;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 48);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r29,r27
	r29.u64 = r27.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822ebef8
	if (cr6.eq) goto loc_822EBEF8;
	// addi r28,r1,96
	r28.s64 = ctx.r1.s64 + 96;
loc_822EBECC:
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// ld r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U64(r28.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 44);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// cmplw cr6,r29,r9
	cr6.compare<uint32_t>(r29.u32, ctx.r9.u32, xer);
	// blt cr6,0x822ebecc
	if (cr6.lt) goto loc_822EBECC;
loc_822EBEF8:
	// lwz r11,156(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// addi r3,r30,152
	ctx.r3.s64 = r30.s64 + 152;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x82b4f2f8
	sub_82B4F2F8(ctx, base);
	// lwz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// li r4,9
	ctx.r4.s64 = 9;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,156(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// stw r27,160(r30)
	PPC_STORE_U32(r30.u32 + 160, r27.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,156(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EBF40:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EBF58:
	// cmpwi cr6,r26,4099
	cr6.compare<int32_t>(r26.s32, 4099, xer);
	// beq cr6,0x822ec030
	if (cr6.eq) goto loc_822EC030;
	// cmpwi cr6,r26,4100
	cr6.compare<int32_t>(r26.s32, 4100, xer);
	// beq cr6,0x822ebf98
	if (cr6.eq) goto loc_822EBF98;
	// cmpwi cr6,r26,4102
	cr6.compare<int32_t>(r26.s32, 4102, xer);
	// bne cr6,0x822ec12c
	if (!cr6.eq) goto loc_822EC12C;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ec348
	sub_822EC348(ctx, base);
	// li r4,8
	ctx.r4.s64 = 8;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EBF98:
	// std r27,1088(r31)
	PPC_STORE_U64(r31.u32 + 1088, r27.u64);
	// addi r10,r31,1152
	ctx.r10.s64 = r31.s64 + 1152;
	// std r27,1096(r31)
	PPC_STORE_U64(r31.u32 + 1096, r27.u64);
	// std r27,744(r31)
	PPC_STORE_U64(r31.u32 + 744, r27.u64);
	// std r27,752(r31)
	PPC_STORE_U64(r31.u32 + 752, r27.u64);
	// lwz r11,1156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1156);
	// lwz r9,1160(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 1160);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// ble cr6,0x822ebfc0
	if (!cr6.gt) goto loc_822EBFC0;
	// twi 31,r0,22
loc_822EBFC0:
	// lwz r8,8(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r7,r8
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r8.u32, xer);
	// ble cr6,0x822ebfd4
	if (!cr6.gt) goto loc_822EBFD4;
	// twi 31,r0,22
loc_822EBFD4:
	// cmplw cr6,r10,r10
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r10.u32, xer);
	// beq cr6,0x822ebfe0
	if (cr6.eq) goto loc_822EBFE0;
	// twi 31,r0,22
loc_822EBFE0:
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x822ec014
	if (cr6.eq) goto loc_822EC014;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822ebff4
	if (cr6.lt) goto loc_822EBFF4;
	// twi 31,r0,22
loc_822EBFF4:
	// std r27,232(r11)
	PPC_STORE_U64(r11.u32 + 232, r27.u64);
	// std r27,240(r11)
	PPC_STORE_U64(r11.u32 + 240, r27.u64);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822ec00c
	if (cr6.lt) goto loc_822EC00C;
	// twi 31,r0,22
loc_822EC00C:
	// addi r11,r11,296
	r11.s64 = r11.s64 + 296;
	// b 0x822ebfc0
	goto loc_822EBFC0;
loc_822EC014:
	// li r4,6
	ctx.r4.s64 = 6;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EC030:
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ec12c
	if (cr6.eq) goto loc_822EC12C;
	// lwz r11,1156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1156);
	// addi r29,r31,1152
	r29.s64 = r31.s64 + 1152;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec060
	if (cr6.eq) goto loc_822EC060;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// li r9,296
	ctx.r9.s64 = 296;
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw. r7,r8,r9
	ctx.r7.s32 = ctx.r8.s32 / ctx.r9.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x822ec0a0
	if (!cr0.eq) goto loc_822EC0A0;
loc_822EC060:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r30,r11,1752
	r30.s64 = r11.s64 + 1752;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r9,4096
	ctx.r9.s64 = 4096;
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// li r4,2
	ctx.r4.s64 = 2;
	// stw r9,456(r31)
	PPC_STORE_U32(r31.u32 + 456, ctx.r9.u32);
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
loc_822EC0A0:
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// li r10,4
	ctx.r10.s64 = 4;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// stb r10,162(r1)
	PPC_STORE_U8(ctx.r1.u32 + 162, ctx.r10.u8);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x822ec0bc
	if (!cr6.gt) goto loc_822EC0BC;
	// twi 31,r0,22
loc_822EC0BC:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec0d0
	if (!cr6.gt) goto loc_822EC0D0;
	// twi 31,r0,22
loc_822EC0D0:
	// cmplw cr6,r29,r29
	cr6.compare<uint32_t>(r29.u32, r29.u32, xer);
	// beq cr6,0x822ec0dc
	if (cr6.eq) goto loc_822EC0DC;
	// twi 31,r0,22
loc_822EC0DC:
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x822ec118
	if (cr6.eq) goto loc_822EC118;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822ec0f0
	if (cr6.lt) goto loc_822EC0F0;
	// twi 31,r0,22
loc_822EC0F0:
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// lwz r5,8(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822ec110
	if (cr6.lt) goto loc_822EC110;
	// twi 31,r0,22
loc_822EC110:
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// b 0x822ec0bc
	goto loc_822EC0BC;
loc_822EC118:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,564(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 564);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EC12C:
	// stw r26,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r26.u32);
	// stw r27,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r27.u32);
	// addi r1,r1,1504
	ctx.r1.s64 = ctx.r1.s64 + 1504;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822EC140"))) PPC_WEAK_FUNC(sub_822EC140);
PPC_FUNC_IMPL(__imp__sub_822EC140) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r6,24
	r11.u64 = ctx.r6.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec244
	if (cr6.eq) goto loc_822EC244;
	// lwz r11,1360(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1360);
	// addi r8,r3,1356
	ctx.r8.s64 = ctx.r3.s64 + 1356;
	// lwz r10,1364(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1364);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec170
	if (!cr6.gt) goto loc_822EC170;
	// twi 31,r0,22
loc_822EC170:
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
loc_822EC178:
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_822EC17C:
	// lwz r10,8(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmplw cr6,r7,r10
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec190
	if (!cr6.gt) goto loc_822EC190;
	// twi 31,r0,22
loc_822EC190:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822ec1a0
	if (cr6.eq) goto loc_822EC1A0;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// beq cr6,0x822ec1a4
	if (cr6.eq) goto loc_822EC1A4;
loc_822EC1A0:
	// twi 31,r0,22
loc_822EC1A4:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x822ec244
	if (cr6.eq) goto loc_822EC244;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822ec1b8
	if (!cr6.eq) goto loc_822EC1B8;
	// twi 31,r0,22
loc_822EC1B8:
	// lwz r10,8(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ec1c8
	if (cr6.lt) goto loc_822EC1C8;
	// twi 31,r0,22
loc_822EC1C8:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r7,r4
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r4.u32, xer);
	// bne cr6,0x822ec230
	if (!cr6.eq) goto loc_822EC230;
	// lwz r9,8(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ec210
	if (cr6.eq) goto loc_822EC210;
	// addi r10,r11,-12
	ctx.r10.s64 = r11.s64 + -12;
loc_822EC1E8:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r7.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r6,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r6.u32);
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// stw r7,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r7.u32);
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822ec1e8
	if (!cr6.eq) goto loc_822EC1E8;
loc_822EC210:
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lwz r11,8(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// addi r7,r11,-12
	ctx.r7.s64 = r11.s64 + -12;
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r7,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r7.u32);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// b 0x822ec17c
	goto loc_822EC17C;
loc_822EC230:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ec23c
	if (cr6.lt) goto loc_822EC23C;
	// twi 31,r0,22
loc_822EC23C:
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// b 0x822ec178
	goto loc_822EC178;
loc_822EC244:
	// lis r11,8192
	r11.s64 = 536870912;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// ori r10,r11,21
	ctx.r10.u64 = r11.u64 | 21;
	// addi r3,r3,1356
	ctx.r3.s64 = ctx.r3.s64 + 1356;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// bl 0x822f0740
	sub_822F0740(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EC278"))) PPC_WEAK_FUNC(sub_822EC278);
PPC_FUNC_IMPL(__imp__sub_822EC278) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r3,r3,1356
	ctx.r3.s64 = ctx.r3.s64 + 1356;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec2a0
	if (!cr6.gt) goto loc_822EC2A0;
	// twi 31,r0,22
loc_822EC2A0:
	// lwz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,8(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
loc_822EC2A8:
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// ble cr6,0x822ec2b4
	if (!cr6.gt) goto loc_822EC2B4;
	// twi 31,r0,22
loc_822EC2B4:
	// cmplw cr6,r3,r3
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r3.u32, xer);
	// beq cr6,0x822ec2c0
	if (cr6.eq) goto loc_822EC2C0;
	// twi 31,r0,22
loc_822EC2C0:
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ec310
	if (cr6.eq) goto loc_822EC310;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ec2d4
	if (cr6.lt) goto loc_822EC2D4;
	// twi 31,r0,22
loc_822EC2D4:
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r6,r4
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r4.u32, xer);
	// beq cr6,0x822ec2f4
	if (cr6.eq) goto loc_822EC2F4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ec2ec
	if (cr6.lt) goto loc_822EC2EC;
	// twi 31,r0,22
loc_822EC2EC:
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// b 0x822ec2a8
	goto loc_822EC2A8;
loc_822EC2F4:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ec300
	if (cr6.lt) goto loc_822EC300;
	// twi 31,r0,22
loc_822EC300:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// li r7,1
	ctx.r7.s64 = 1;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
loc_822EC310:
	// clrlwi r11,r7,24
	r11.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ec338
	if (!cr6.eq) goto loc_822EC338;
	// lis r11,8192
	r11.s64 = 536870912;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// ori r10,r11,21
	ctx.r10.u64 = r11.u64 | 21;
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// bl 0x822f0740
	sub_822F0740(ctx, base);
loc_822EC338:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EC348"))) PPC_WEAK_FUNC(sub_822EC348);
PPC_FUNC_IMPL(__imp__sub_822EC348) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,468(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 468);
	// addi r27,r31,468
	r27.s64 = r31.s64 + 468;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,997
	cr6.compare<uint32_t>(r11.u32, 997, xer);
	// beq cr6,0x822ec464
	if (cr6.eq) goto loc_822EC464;
	// lwz r11,1372(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1372);
	// li r26,0
	r26.s64 = 0;
	// addis r10,r11,5
	ctx.r10.s64 = r11.s64 + 327680;
	// addi r10,r10,2320
	ctx.r10.s64 = ctx.r10.s64 + 2320;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bgt cr6,0x822ec3a4
	if (cr6.gt) goto loc_822EC3A4;
	// clrlwi r28,r30,24
	r28.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822ec3a4
	if (!cr6.eq) goto loc_822EC3A4;
	// lbz r11,1376(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 1376);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec3d4
	if (cr6.eq) goto loc_822EC3D4;
loc_822EC3A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ec470
	sub_822EC470(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ec580
	sub_822EC580(ctx, base);
	// li r11,1
	r11.s64 = 1;
	// clrlwi r28,r30,24
	r28.u64 = r30.u32 & 0xFF;
	// stw r29,1372(r31)
	PPC_STORE_U32(r31.u32 + 1372, r29.u32);
	// stb r26,1376(r31)
	PPC_STORE_U8(r31.u32 + 1376, r26.u8);
	// stb r11,1377(r31)
	PPC_STORE_U8(r31.u32 + 1377, r11.u8);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822ec464
	if (cr6.eq) goto loc_822EC464;
loc_822EC3D4:
	// lbz r11,1377(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 1377);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec464
	if (cr6.eq) goto loc_822EC464;
	// lbz r11,1378(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 1378);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec464
	if (cr6.eq) goto loc_822EC464;
	// lbz r11,444(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 444);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec404
	if (cr6.eq) goto loc_822EC404;
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// lwz r3,8(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// b 0x822ec408
	goto loc_822EC408;
loc_822EC404:
	// lwz r3,232(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 232);
loc_822EC408:
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x822ec45c
	if (cr6.eq) goto loc_822EC45C;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// li r4,0
	ctx.r4.s64 = 0;
	// bne cr6,0x822ec420
	if (!cr6.eq) goto loc_822EC420;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
loc_822EC420:
	// bl 0x82cf9858
	sub_82CF9858(ctx, base);
	// cmplwi cr6,r3,997
	cr6.compare<uint32_t>(ctx.r3.u32, 997, xer);
	// beq cr6,0x822ec45c
	if (cr6.eq) goto loc_822EC45C;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822ec45c
	if (!cr6.eq) goto loc_822EC45C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r11,1896
	ctx.r3.s64 = r11.s64 + 1896;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82cbc490
	sub_82CBC490(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r10,1932
	ctx.r3.s64 = ctx.r10.s64 + 1932;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EC45C:
	// stb r26,1377(r31)
	PPC_STORE_U8(r31.u32 + 1377, r26.u8);
	// stb r26,1378(r31)
	PPC_STORE_U8(r31.u32 + 1378, r26.u8);
loc_822EC464:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822EC470"))) PPC_WEAK_FUNC(sub_822EC470);
PPC_FUNC_IMPL(__imp__sub_822EC470) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,1344(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1344);
	// addi r28,r3,1340
	r28.s64 = ctx.r3.s64 + 1340;
	// lwz r10,1348(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1348);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec494
	if (!cr6.gt) goto loc_822EC494;
	// twi 31,r0,22
loc_822EC494:
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r27,r11
	r27.u64 = r11.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
	// ld r26,80(r1)
	r26.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_822EC4A8:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r9,4(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// ble cr6,0x822ec4bc
	if (!cr6.gt) goto loc_822EC4BC;
	// twi 31,r0,22
loc_822EC4BC:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ec4cc
	if (cr6.eq) goto loc_822EC4CC;
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// beq cr6,0x822ec4d0
	if (cr6.eq) goto loc_822EC4D0;
loc_822EC4CC:
	// twi 31,r0,22
loc_822EC4D0:
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x822ec574
	if (cr6.eq) goto loc_822EC574;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ec4e4
	if (!cr6.eq) goto loc_822EC4E4;
	// twi 31,r0,22
loc_822EC4E4:
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// blt cr6,0x822ec4f4
	if (cr6.lt) goto loc_822EC4F4;
	// twi 31,r0,22
loc_822EC4F4:
	// lwz r29,0(r27)
	r29.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r31,32(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ec524
	if (cr6.eq) goto loc_822EC524;
	// li r30,0
	r30.s64 = 0;
loc_822EC508:
	// lwz r11,28(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// add r11,r30,r11
	r11.u64 = r30.u64 + r11.u64;
	// lwz r3,8(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r30,r30,12
	r30.s64 = r30.s64 + 12;
	// bne 0x822ec508
	if (!cr0.eq) goto loc_822EC508;
loc_822EC524:
	// lwz r3,28(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 28);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// addi r5,r27,4
	ctx.r5.s64 = r27.s64 + 4;
	// subf r10,r5,r11
	ctx.r10.s64 = r11.s64 - ctx.r5.s64;
	// srawi. r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822ec558
	if (!cr0.gt) goto loc_822EC558;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822EC558:
	// std r26,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r26.u64);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r27,84(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r11,r11,-4
	r11.s64 = r11.s64 + -4;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822ec4a8
	goto loc_822EC4A8;
loc_822EC574:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822EC580"))) PPC_WEAK_FUNC(sub_822EC580);
PPC_FUNC_IMPL(__imp__sub_822EC580) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// mr r14,r4
	r14.u64 = ctx.r4.u64;
	// lbz r11,444(r19)
	r11.u64 = PPC_LOAD_U8(r19.u32 + 444);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec5ac
	if (cr6.eq) goto loc_822EC5AC;
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// lwz r16,8(r3)
	r16.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// b 0x822ec5b0
	goto loc_822EC5B0;
loc_822EC5AC:
	// lwz r16,232(r19)
	r16.u64 = PPC_LOAD_U32(r19.u32 + 232);
loc_822EC5B0:
	// cmpwi cr6,r16,-1
	cr6.compare<int32_t>(r16.s32, -1, xer);
	// beq cr6,0x822ec9bc
	if (cr6.eq) goto loc_822EC9BC;
	// lwz r11,1360(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 1360);
	// addi r24,r19,1356
	r24.s64 = r19.s64 + 1356;
	// li r15,0
	r15.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ec5d8
	if (!cr6.eq) goto loc_822EC5D8;
	// mr r11,r15
	r11.u64 = r15.u64;
	// mr r17,r11
	r17.u64 = r11.u64;
	// b 0x822ec5fc
	goto loc_822EC5FC;
loc_822EC5D8:
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// li r9,12
	ctx.r9.s64 = 12;
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw r11,r8,r9
	r11.s32 = ctx.r8.s32 / ctx.r9.s32;
	// cmplwi cr6,r11,30
	cr6.compare<uint32_t>(r11.u32, 30, xer);
	// bge cr6,0x822ec5f8
	if (!cr6.lt) goto loc_822EC5F8;
	// mr r17,r11
	r17.u64 = r11.u64;
	// b 0x822ec5fc
	goto loc_822EC5FC;
loc_822EC5F8:
	// li r17,30
	r17.s64 = 30;
loc_822EC5FC:
	// li r3,40
	ctx.r3.s64 = 40;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822ec620
	if (cr6.eq) goto loc_822EC620;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// stw r15,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, r15.u32);
	// stw r15,32(r3)
	PPC_STORE_U32(ctx.r3.u32 + 32, r15.u32);
	// stb r15,36(r3)
	PPC_STORE_U8(ctx.r3.u32 + 36, r15.u8);
	// b 0x822ec624
	goto loc_822EC624;
loc_822EC620:
	// mr r25,r15
	r25.u64 = r15.u64;
loc_822EC624:
	// stw r25,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r25.u32);
	// mr r11,r25
	r11.u64 = r25.u64;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
	// li r10,10
	ctx.r10.s64 = 10;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_822EC638:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x822ec638
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822EC638;
	// lis r11,5461
	r11.s64 = 357892096;
	// ori r10,r11,21845
	ctx.r10.u64 = r11.u64 | 21845;
	// cmplw cr6,r17,r10
	cr6.compare<uint32_t>(r17.u32, ctx.r10.u32, xer);
	// bgt cr6,0x822ec664
	if (cr6.gt) goto loc_822EC664;
	// rlwinm r11,r17,1,0,30
	r11.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r17,r11
	r11.u64 = r17.u64 + r11.u64;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x822ec668
	goto loc_822EC668;
loc_822EC664:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_822EC668:
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// rlwinm r11,r17,1,0,30
	r11.u64 = __builtin_rotateleft64(r17.u32 | (r17.u64 << 32), 1) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r3,28(r25)
	PPC_STORE_U32(r25.u32 + 28, ctx.r3.u32);
	// add r11,r17,r11
	r11.u64 = r17.u64 + r11.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r11,4(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// lwz r10,8(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// mr r22,r15
	r22.u64 = r15.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec69c
	if (!cr6.gt) goto loc_822EC69C;
	// twi 31,r0,22
loc_822EC69C:
	// mr r26,r11
	r26.u64 = r11.u64;
	// mr r10,r24
	ctx.r10.u64 = r24.u64;
	// lis r11,8192
	r11.s64 = 536870912;
	// stw r26,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r26.u32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// mr r29,r15
	r29.u64 = r15.u64;
	// ld r21,88(r1)
	r21.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// li r27,2
	r27.s64 = 2;
	// lis r20,-31927
	r20.s64 = -2092367872;
	// ori r23,r11,23
	r23.u64 = r11.u64 | 23;
	// li r18,1
	r18.s64 = 1;
loc_822EC6C8:
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// lwz r9,4(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// ble cr6,0x822ec6dc
	if (!cr6.gt) goto loc_822EC6DC;
	// twi 31,r0,22
loc_822EC6DC:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ec6ec
	if (cr6.eq) goto loc_822EC6EC;
	// cmplw cr6,r10,r24
	cr6.compare<uint32_t>(ctx.r10.u32, r24.u32, xer);
	// beq cr6,0x822ec6f0
	if (cr6.eq) goto loc_822EC6F0;
loc_822EC6EC:
	// twi 31,r0,22
loc_822EC6F0:
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// beq cr6,0x822ec860
	if (cr6.eq) goto loc_822EC860;
	// cmplw cr6,r22,r17
	cr6.compare<uint32_t>(r22.u32, r17.u32, xer);
	// bge cr6,0x822ec860
	if (!cr6.lt) goto loc_822EC860;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ec70c
	if (!cr6.eq) goto loc_822EC70C;
	// twi 31,r0,22
loc_822EC70C:
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// blt cr6,0x822ec71c
	if (cr6.lt) goto loc_822EC71C;
	// twi 31,r0,22
loc_822EC71C:
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r10,28(r25)
	ctx.r10.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// lwz r30,4(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,117
	cr6.compare<uint32_t>(r11.u32, 117, xer);
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stwx r11,r29,r10
	PPC_STORE_U32(r29.u32 + ctx.r10.u32, r11.u32);
	// bne cr6,0x822ec7a4
	if (!cr6.eq) goto loc_822EC7A4;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x8221f3f0
	sub_8221F3F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x822ec76c
	if (!cr6.eq) goto loc_822EC76C;
	// lwz r11,28060(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 28060);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec768
	if (cr6.eq) goto loc_822EC768;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822EC768:
	// bl 0x82cbbb58
	sub_82CBBB58(ctx, base);
loc_822EC76C:
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// neg r10,r30
	ctx.r10.s64 = -r30.s64;
	// extsw r9,r30
	ctx.r9.s64 = r30.s32;
	// add r8,r29,r11
	ctx.r8.u64 = r29.u64 + r11.u64;
	// extsw r7,r10
	ctx.r7.s64 = ctx.r10.s32;
	// stw r27,4(r8)
	PPC_STORE_U32(ctx.r8.u32 + 4, r27.u32);
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// add r6,r29,r11
	ctx.r6.u64 = r29.u64 + r11.u64;
	// stw r31,8(r6)
	PPC_STORE_U32(ctx.r6.u32 + 8, r31.u32);
	// std r9,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r9.u64);
	// stw r23,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r23.u32);
	// stb r27,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r27.u8);
	// std r7,40(r31)
	PPC_STORE_U64(r31.u32 + 40, ctx.r7.u64);
	// b 0x822ec7f8
	goto loc_822EC7F8;
loc_822EC7A4:
	// li r3,24
	ctx.r3.s64 = 24;
	// bl 0x8221f3f0
	sub_8221F3F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x822ec7d8
	if (!cr6.eq) goto loc_822EC7D8;
	// lwz r11,28060(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 28060);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec7d4
	if (cr6.eq) goto loc_822EC7D4;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822EC7D4:
	// bl 0x82cbbb58
	sub_82CBBB58(ctx, base);
loc_822EC7D8:
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// extsw r10,r30
	ctx.r10.s64 = r30.s32;
	// add r9,r29,r11
	ctx.r9.u64 = r29.u64 + r11.u64;
	// stw r18,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r18.u32);
	// lwz r11,28(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// add r8,r29,r11
	ctx.r8.u64 = r29.u64 + r11.u64;
	// stw r31,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, r31.u32);
	// std r10,16(r31)
	PPC_STORE_U64(r31.u32 + 16, ctx.r10.u64);
loc_822EC7F8:
	// stb r27,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r27.u8);
	// addi r11,r26,12
	r11.s64 = r26.s64 + 12;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// addi r22,r22,1
	r22.s64 = r22.s64 + 1;
	// lwz r9,8(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// addi r29,r29,12
	r29.s64 = r29.s64 + 12;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ec844
	if (cr6.eq) goto loc_822EC844;
	// addi r10,r11,-12
	ctx.r10.s64 = r11.s64 + -12;
loc_822EC81C:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r7,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r7.u32);
	// lwz r6,8(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// stw r6,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r6.u32);
	// addi r10,r10,12
	ctx.r10.s64 = ctx.r10.s64 + 12;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822ec81c
	if (!cr6.eq) goto loc_822EC81C;
loc_822EC844:
	// std r21,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r21.u64);
	// lwz r11,8(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 8);
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r11,r11,-12
	r11.s64 = r11.s64 + -12;
	// stw r11,8(r24)
	PPC_STORE_U32(r24.u32 + 8, r11.u32);
	// lwz r26,92(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// b 0x822ec6c8
	goto loc_822EC6C8;
loc_822EC860:
	// cmplwi cr6,r17,0
	cr6.compare<uint32_t>(r17.u32, 0, xer);
	// beq cr6,0x822ec9bc
	if (cr6.eq) goto loc_822EC9BC;
	// stw r22,32(r25)
	PPC_STORE_U32(r25.u32 + 32, r22.u32);
	// lbz r11,560(r19)
	r11.u64 = PPC_LOAD_U8(r19.u32 + 560);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec9bc
	if (cr6.eq) goto loc_822EC9BC;
	// mr r30,r15
	r30.u64 = r15.u64;
	// lis r28,-31927
	r28.s64 = -2092367872;
	// lis r29,-31927
	r29.s64 = -2092367872;
loc_822EC884:
	// addi r11,r30,197
	r11.s64 = r30.s64 + 197;
	// lwz r10,26932(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 26932);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r9,r19
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + r19.u32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ec974
	if (!cr6.eq) goto loc_822EC974;
	// lwz r10,26848(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 26848);
	// slw r9,r18,r11
	ctx.r9.u64 = r11.u8 & 0x20 ? 0 : (r18.u32 << (r11.u8 & 0x3F));
	// and r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 & ctx.r10.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822ec974
	if (cr6.eq) goto loc_822EC974;
	// addi r11,r30,79
	r11.s64 = r30.s64 + 79;
	// lwz r6,28(r25)
	ctx.r6.u64 = PPC_LOAD_U32(r25.u32 + 28);
	// clrlwi r31,r14,24
	r31.u64 = r14.u32 & 0xFF;
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// mr r5,r17
	ctx.r5.u64 = r17.u64;
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// ldx r4,r10,r19
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r10.u32 + r19.u32);
	// bne cr6,0x822ec8dc
	if (!cr6.eq) goto loc_822EC8DC;
	// addi r7,r19,468
	ctx.r7.s64 = r19.s64 + 468;
loc_822EC8DC:
	// bl 0x82cf98f8
	sub_82CF98F8(ctx, base);
	// cmplwi cr6,r3,997
	cr6.compare<uint32_t>(ctx.r3.u32, 997, xer);
	// beq cr6,0x822ec8f0
	if (cr6.eq) goto loc_822EC8F0;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ec994
	if (cr6.eq) goto loc_822EC994;
loc_822EC8F0:
	// lwz r11,1344(r19)
	r11.u64 = PPC_LOAD_U32(r19.u32 + 1344);
	// addi r4,r19,1340
	ctx.r4.s64 = r19.s64 + 1340;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ec908
	if (!cr6.eq) goto loc_822EC908;
	// mr r9,r15
	ctx.r9.u64 = r15.u64;
	// b 0x822ec914
	goto loc_822EC914;
loc_822EC908:
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822EC914:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ec944
	if (cr6.eq) goto loc_822EC944;
	// lwz r10,12(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 12);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// srawi r10,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 2;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x822ec944
	if (!cr6.lt) goto loc_822EC944;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// addi r10,r11,4
	ctx.r10.s64 = r11.s64 + 4;
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// b 0x822ec970
	goto loc_822EC970;
loc_822EC944:
	// lwz r10,8(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ec954
	if (!cr6.gt) goto loc_822EC954;
	// twi 31,r0,22
loc_822EC954:
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x8238eb88
	sub_8238EB88(ctx, base);
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_822EC970:
	// stb r18,1378(r19)
	PPC_STORE_U8(r19.u32 + 1378, r18.u8);
loc_822EC974:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lbz r10,560(r19)
	ctx.r10.u64 = PPC_LOAD_U8(r19.u32 + 560);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822ec884
	if (cr6.lt) goto loc_822EC884;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c00
	return;
loc_822EC994:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r11,1956
	ctx.r3.s64 = r11.s64 + 1956;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r3,r19,468
	ctx.r3.s64 = r19.s64 + 468;
	// bl 0x82cbc490
	sub_82CBC490(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r10,1932
	ctx.r3.s64 = ctx.r10.s64 + 1932;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EC9BC:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c00
	return;
}

__attribute__((alias("__imp__sub_822EC9C8"))) PPC_WEAK_FUNC(sub_822EC9C8);
PPC_FUNC_IMPL(__imp__sub_822EC9C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-1584(r1)
	ea = -1584 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r3,48
	ctx.r3.s64 = 48;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822ec9ec
	if (cr6.eq) goto loc_822EC9EC;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
loc_822EC9EC:
	// addic. r11,r3,4
	xer.ca = ctx.r3.u32 > 4294967291;
	r11.s64 = ctx.r3.s64 + 4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822ec9f8
	if (cr0.eq) goto loc_822EC9F8;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822EC9F8:
	// addic. r11,r3,8
	xer.ca = ctx.r3.u32 > 4294967287;
	r11.s64 = ctx.r3.s64 + 8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822eca04
	if (cr0.eq) goto loc_822ECA04;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822ECA04:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// stb r30,45(r3)
	PPC_STORE_U8(ctx.r3.u32 + 45, r30.u8);
	// li r28,1
	r28.s64 = 1;
	// addi r31,r11,28888
	r31.s64 = r11.s64 + 28888;
	// stb r28,44(r3)
	PPC_STORE_U8(ctx.r3.u32 + 44, r28.u8);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// li r9,33
	ctx.r9.s64 = 33;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// addi r4,r8,1996
	ctx.r4.s64 = ctx.r8.s64 + 1996;
	// stb r28,45(r3)
	PPC_STORE_U8(ctx.r3.u32 + 45, r28.u8);
	// addi r3,r1,804
	ctx.r3.s64 = ctx.r1.s64 + 804;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r9,800(r1)
	PPC_STORE_U32(ctx.r1.u32 + 800, ctx.r9.u32);
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,800
	ctx.r5.s64 = ctx.r1.s64 + 800;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r7,828(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 828);
	// cmplwi cr6,r7,16
	cr6.compare<uint32_t>(ctx.r7.u32, 16, xer);
	// blt cr6,0x822eca7c
	if (cr6.lt) goto loc_822ECA7C;
	// lwz r3,808(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 808);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECA7C:
	// li r11,106
	r11.s64 = 106;
	// stw r30,824(r1)
	PPC_STORE_U32(ctx.r1.u32 + 824, r30.u32);
	// li r29,15
	r29.s64 = 15;
	// stb r30,808(r1)
	PPC_STORE_U8(ctx.r1.u32 + 808, r30.u8);
	// stw r11,544(r1)
	PPC_STORE_U32(ctx.r1.u32 + 544, r11.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stw r29,828(r1)
	PPC_STORE_U32(ctx.r1.u32 + 828, r29.u32);
	// addi r3,r1,548
	ctx.r3.s64 = ctx.r1.s64 + 548;
	// addi r4,r10,2012
	ctx.r4.s64 = ctx.r10.s64 + 2012;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,544
	ctx.r5.s64 = ctx.r1.s64 + 544;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,572(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 572);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecac8
	if (cr6.lt) goto loc_822ECAC8;
	// lwz r3,552(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 552);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECAC8:
	// li r11,35
	r11.s64 = 35;
	// stw r29,572(r1)
	PPC_STORE_U32(ctx.r1.u32 + 572, r29.u32);
	// stw r30,568(r1)
	PPC_STORE_U32(ctx.r1.u32 + 568, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,552(r1)
	PPC_STORE_U8(ctx.r1.u32 + 552, r30.u8);
	// addi r3,r1,196
	ctx.r3.s64 = ctx.r1.s64 + 196;
	// stw r11,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r11.u32);
	// addi r4,r10,2036
	ctx.r4.s64 = ctx.r10.s64 + 2036;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,220(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecb10
	if (cr6.lt) goto loc_822ECB10;
	// lwz r3,200(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECB10:
	// li r11,36
	r11.s64 = 36;
	// stw r29,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r29.u32);
	// stw r30,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,200(r1)
	PPC_STORE_U8(ctx.r1.u32 + 200, r30.u8);
	// addi r3,r1,644
	ctx.r3.s64 = ctx.r1.s64 + 644;
	// stw r11,640(r1)
	PPC_STORE_U32(ctx.r1.u32 + 640, r11.u32);
	// addi r4,r10,2052
	ctx.r4.s64 = ctx.r10.s64 + 2052;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,640
	ctx.r5.s64 = ctx.r1.s64 + 640;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,668(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 668);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecb58
	if (cr6.lt) goto loc_822ECB58;
	// lwz r3,648(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 648);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECB58:
	// li r11,37
	r11.s64 = 37;
	// stw r29,668(r1)
	PPC_STORE_U32(ctx.r1.u32 + 668, r29.u32);
	// stw r30,664(r1)
	PPC_STORE_U32(ctx.r1.u32 + 664, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,648(r1)
	PPC_STORE_U8(ctx.r1.u32 + 648, r30.u8);
	// addi r3,r1,260
	ctx.r3.s64 = ctx.r1.s64 + 260;
	// stw r11,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, r11.u32);
	// addi r4,r10,2076
	ctx.r4.s64 = ctx.r10.s64 + 2076;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,256
	ctx.r5.s64 = ctx.r1.s64 + 256;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,284(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecba0
	if (cr6.lt) goto loc_822ECBA0;
	// lwz r3,264(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 264);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECBA0:
	// li r11,38
	r11.s64 = 38;
	// stw r29,284(r1)
	PPC_STORE_U32(ctx.r1.u32 + 284, r29.u32);
	// stw r30,280(r1)
	PPC_STORE_U32(ctx.r1.u32 + 280, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,264(r1)
	PPC_STORE_U8(ctx.r1.u32 + 264, r30.u8);
	// addi r3,r1,708
	ctx.r3.s64 = ctx.r1.s64 + 708;
	// stw r11,704(r1)
	PPC_STORE_U32(ctx.r1.u32 + 704, r11.u32);
	// addi r4,r10,2104
	ctx.r4.s64 = ctx.r10.s64 + 2104;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,704
	ctx.r5.s64 = ctx.r1.s64 + 704;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,732(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 732);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecbe8
	if (cr6.lt) goto loc_822ECBE8;
	// lwz r3,712(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 712);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECBE8:
	// li r11,39
	r11.s64 = 39;
	// stw r29,732(r1)
	PPC_STORE_U32(ctx.r1.u32 + 732, r29.u32);
	// stw r30,728(r1)
	PPC_STORE_U32(ctx.r1.u32 + 728, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,712(r1)
	PPC_STORE_U8(ctx.r1.u32 + 712, r30.u8);
	// addi r3,r1,324
	ctx.r3.s64 = ctx.r1.s64 + 324;
	// stw r11,320(r1)
	PPC_STORE_U32(ctx.r1.u32 + 320, r11.u32);
	// addi r4,r10,2124
	ctx.r4.s64 = ctx.r10.s64 + 2124;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,320
	ctx.r5.s64 = ctx.r1.s64 + 320;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,348(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 348);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecc30
	if (cr6.lt) goto loc_822ECC30;
	// lwz r3,328(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 328);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECC30:
	// li r11,40
	r11.s64 = 40;
	// stw r29,348(r1)
	PPC_STORE_U32(ctx.r1.u32 + 348, r29.u32);
	// stw r30,344(r1)
	PPC_STORE_U32(ctx.r1.u32 + 344, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,328(r1)
	PPC_STORE_U8(ctx.r1.u32 + 328, r30.u8);
	// addi r3,r1,772
	ctx.r3.s64 = ctx.r1.s64 + 772;
	// stw r11,768(r1)
	PPC_STORE_U32(ctx.r1.u32 + 768, r11.u32);
	// addi r4,r10,2144
	ctx.r4.s64 = ctx.r10.s64 + 2144;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,768
	ctx.r5.s64 = ctx.r1.s64 + 768;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,796(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 796);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecc78
	if (cr6.lt) goto loc_822ECC78;
	// lwz r3,776(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 776);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECC78:
	// li r11,41
	r11.s64 = 41;
	// stw r29,796(r1)
	PPC_STORE_U32(ctx.r1.u32 + 796, r29.u32);
	// stw r30,792(r1)
	PPC_STORE_U32(ctx.r1.u32 + 792, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,776(r1)
	PPC_STORE_U8(ctx.r1.u32 + 776, r30.u8);
	// addi r3,r1,388
	ctx.r3.s64 = ctx.r1.s64 + 388;
	// stw r11,384(r1)
	PPC_STORE_U32(ctx.r1.u32 + 384, r11.u32);
	// addi r4,r10,2172
	ctx.r4.s64 = ctx.r10.s64 + 2172;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,412(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 412);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822eccc0
	if (cr6.lt) goto loc_822ECCC0;
	// lwz r3,392(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 392);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECCC0:
	// li r11,42
	r11.s64 = 42;
	// stw r29,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, r29.u32);
	// stw r30,408(r1)
	PPC_STORE_U32(ctx.r1.u32 + 408, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,392(r1)
	PPC_STORE_U8(ctx.r1.u32 + 392, r30.u8);
	// addi r3,r1,836
	ctx.r3.s64 = ctx.r1.s64 + 836;
	// stw r11,832(r1)
	PPC_STORE_U32(ctx.r1.u32 + 832, r11.u32);
	// addi r4,r10,2192
	ctx.r4.s64 = ctx.r10.s64 + 2192;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,832
	ctx.r5.s64 = ctx.r1.s64 + 832;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,860(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 860);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecd08
	if (cr6.lt) goto loc_822ECD08;
	// lwz r3,840(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 840);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECD08:
	// li r11,43
	r11.s64 = 43;
	// stw r29,860(r1)
	PPC_STORE_U32(ctx.r1.u32 + 860, r29.u32);
	// stw r30,856(r1)
	PPC_STORE_U32(ctx.r1.u32 + 856, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,840(r1)
	PPC_STORE_U8(ctx.r1.u32 + 840, r30.u8);
	// addi r3,r1,452
	ctx.r3.s64 = ctx.r1.s64 + 452;
	// stw r11,448(r1)
	PPC_STORE_U32(ctx.r1.u32 + 448, r11.u32);
	// addi r4,r10,2212
	ctx.r4.s64 = ctx.r10.s64 + 2212;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,448
	ctx.r5.s64 = ctx.r1.s64 + 448;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,476(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecd50
	if (cr6.lt) goto loc_822ECD50;
	// lwz r3,456(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 456);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECD50:
	// li r11,44
	r11.s64 = 44;
	// stw r29,476(r1)
	PPC_STORE_U32(ctx.r1.u32 + 476, r29.u32);
	// stw r30,472(r1)
	PPC_STORE_U32(ctx.r1.u32 + 472, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,456(r1)
	PPC_STORE_U8(ctx.r1.u32 + 456, r30.u8);
	// addi r3,r1,900
	ctx.r3.s64 = ctx.r1.s64 + 900;
	// stw r11,896(r1)
	PPC_STORE_U32(ctx.r1.u32 + 896, r11.u32);
	// addi r4,r10,2236
	ctx.r4.s64 = ctx.r10.s64 + 2236;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,896
	ctx.r5.s64 = ctx.r1.s64 + 896;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,924(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 924);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecd98
	if (cr6.lt) goto loc_822ECD98;
	// lwz r3,904(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 904);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECD98:
	// li r11,45
	r11.s64 = 45;
	// stw r29,924(r1)
	PPC_STORE_U32(ctx.r1.u32 + 924, r29.u32);
	// stw r30,920(r1)
	PPC_STORE_U32(ctx.r1.u32 + 920, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,904(r1)
	PPC_STORE_U8(ctx.r1.u32 + 904, r30.u8);
	// addi r3,r1,516
	ctx.r3.s64 = ctx.r1.s64 + 516;
	// stw r11,512(r1)
	PPC_STORE_U32(ctx.r1.u32 + 512, r11.u32);
	// addi r4,r10,2264
	ctx.r4.s64 = ctx.r10.s64 + 2264;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,512
	ctx.r5.s64 = ctx.r1.s64 + 512;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,540(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 540);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecde0
	if (cr6.lt) goto loc_822ECDE0;
	// lwz r3,520(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 520);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECDE0:
	// li r11,46
	r11.s64 = 46;
	// stw r29,540(r1)
	PPC_STORE_U32(ctx.r1.u32 + 540, r29.u32);
	// stw r30,536(r1)
	PPC_STORE_U32(ctx.r1.u32 + 536, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,520(r1)
	PPC_STORE_U8(ctx.r1.u32 + 520, r30.u8);
	// addi r3,r1,964
	ctx.r3.s64 = ctx.r1.s64 + 964;
	// stw r11,960(r1)
	PPC_STORE_U32(ctx.r1.u32 + 960, r11.u32);
	// addi r4,r10,2284
	ctx.r4.s64 = ctx.r10.s64 + 2284;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,960
	ctx.r5.s64 = ctx.r1.s64 + 960;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,988(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 988);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ece28
	if (cr6.lt) goto loc_822ECE28;
	// lwz r3,968(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 968);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECE28:
	// li r11,47
	r11.s64 = 47;
	// stw r29,988(r1)
	PPC_STORE_U32(ctx.r1.u32 + 988, r29.u32);
	// stw r30,984(r1)
	PPC_STORE_U32(ctx.r1.u32 + 984, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,968(r1)
	PPC_STORE_U8(ctx.r1.u32 + 968, r30.u8);
	// addi r3,r1,580
	ctx.r3.s64 = ctx.r1.s64 + 580;
	// stw r11,576(r1)
	PPC_STORE_U32(ctx.r1.u32 + 576, r11.u32);
	// addi r4,r10,2312
	ctx.r4.s64 = ctx.r10.s64 + 2312;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,576
	ctx.r5.s64 = ctx.r1.s64 + 576;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,604(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 604);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ece70
	if (cr6.lt) goto loc_822ECE70;
	// lwz r3,584(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 584);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECE70:
	// li r11,48
	r11.s64 = 48;
	// stw r29,604(r1)
	PPC_STORE_U32(ctx.r1.u32 + 604, r29.u32);
	// stw r30,600(r1)
	PPC_STORE_U32(ctx.r1.u32 + 600, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,584(r1)
	PPC_STORE_U8(ctx.r1.u32 + 584, r30.u8);
	// addi r3,r1,1028
	ctx.r3.s64 = ctx.r1.s64 + 1028;
	// stw r11,1024(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1024, r11.u32);
	// addi r4,r10,2340
	ctx.r4.s64 = ctx.r10.s64 + 2340;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1024
	ctx.r5.s64 = ctx.r1.s64 + 1024;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,1052(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1052);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822eceb8
	if (cr6.lt) goto loc_822ECEB8;
	// lwz r3,1032(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1032);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECEB8:
	// li r11,49
	r11.s64 = 49;
	// stw r29,1052(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1052, r29.u32);
	// stw r30,1048(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1048, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,1032(r1)
	PPC_STORE_U8(ctx.r1.u32 + 1032, r30.u8);
	// addi r3,r1,932
	ctx.r3.s64 = ctx.r1.s64 + 932;
	// stw r11,928(r1)
	PPC_STORE_U32(ctx.r1.u32 + 928, r11.u32);
	// addi r4,r10,2364
	ctx.r4.s64 = ctx.r10.s64 + 2364;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,928
	ctx.r5.s64 = ctx.r1.s64 + 928;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,956(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 956);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecf00
	if (cr6.lt) goto loc_822ECF00;
	// lwz r3,936(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 936);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECF00:
	// li r11,2
	r11.s64 = 2;
	// stw r29,956(r1)
	PPC_STORE_U32(ctx.r1.u32 + 956, r29.u32);
	// stw r30,952(r1)
	PPC_STORE_U32(ctx.r1.u32 + 952, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,936(r1)
	PPC_STORE_U8(ctx.r1.u32 + 936, r30.u8);
	// addi r3,r1,676
	ctx.r3.s64 = ctx.r1.s64 + 676;
	// stw r11,672(r1)
	PPC_STORE_U32(ctx.r1.u32 + 672, r11.u32);
	// addi r4,r10,2392
	ctx.r4.s64 = ctx.r10.s64 + 2392;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,672
	ctx.r5.s64 = ctx.r1.s64 + 672;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,700(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 700);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecf48
	if (cr6.lt) goto loc_822ECF48;
	// lwz r3,680(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 680);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECF48:
	// li r11,3
	r11.s64 = 3;
	// stw r29,700(r1)
	PPC_STORE_U32(ctx.r1.u32 + 700, r29.u32);
	// stw r30,696(r1)
	PPC_STORE_U32(ctx.r1.u32 + 696, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,680(r1)
	PPC_STORE_U8(ctx.r1.u32 + 680, r30.u8);
	// addi r3,r1,868
	ctx.r3.s64 = ctx.r1.s64 + 868;
	// stw r11,864(r1)
	PPC_STORE_U32(ctx.r1.u32 + 864, r11.u32);
	// addi r4,r10,2420
	ctx.r4.s64 = ctx.r10.s64 + 2420;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,864
	ctx.r5.s64 = ctx.r1.s64 + 864;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,892(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 892);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecf90
	if (cr6.lt) goto loc_822ECF90;
	// lwz r3,872(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 872);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECF90:
	// li r11,4
	r11.s64 = 4;
	// stw r29,892(r1)
	PPC_STORE_U32(ctx.r1.u32 + 892, r29.u32);
	// stw r30,888(r1)
	PPC_STORE_U32(ctx.r1.u32 + 888, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,872(r1)
	PPC_STORE_U8(ctx.r1.u32 + 872, r30.u8);
	// addi r3,r1,740
	ctx.r3.s64 = ctx.r1.s64 + 740;
	// stw r11,736(r1)
	PPC_STORE_U32(ctx.r1.u32 + 736, r11.u32);
	// addi r4,r10,2444
	ctx.r4.s64 = ctx.r10.s64 + 2444;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,736
	ctx.r5.s64 = ctx.r1.s64 + 736;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,764(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 764);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ecfd8
	if (cr6.lt) goto loc_822ECFD8;
	// lwz r3,744(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 744);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ECFD8:
	// li r11,6
	r11.s64 = 6;
	// stw r29,764(r1)
	PPC_STORE_U32(ctx.r1.u32 + 764, r29.u32);
	// stw r30,760(r1)
	PPC_STORE_U32(ctx.r1.u32 + 760, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,744(r1)
	PPC_STORE_U8(ctx.r1.u32 + 744, r30.u8);
	// addi r3,r1,996
	ctx.r3.s64 = ctx.r1.s64 + 996;
	// stw r11,992(r1)
	PPC_STORE_U32(ctx.r1.u32 + 992, r11.u32);
	// addi r4,r10,2476
	ctx.r4.s64 = ctx.r10.s64 + 2476;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,992
	ctx.r5.s64 = ctx.r1.s64 + 992;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,1020(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1020);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed020
	if (cr6.lt) goto loc_822ED020;
	// lwz r3,1000(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 1000);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED020:
	// li r11,7
	r11.s64 = 7;
	// stw r29,1020(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1020, r29.u32);
	// stw r30,1016(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1016, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,1000(r1)
	PPC_STORE_U8(ctx.r1.u32 + 1000, r30.u8);
	// addi r3,r1,132
	ctx.r3.s64 = ctx.r1.s64 + 132;
	// stw r11,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r11.u32);
	// addi r4,r10,2492
	ctx.r4.s64 = ctx.r10.s64 + 2492;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,156(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed068
	if (cr6.lt) goto loc_822ED068;
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED068:
	// li r11,8
	r11.s64 = 8;
	// stw r29,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, r29.u32);
	// stw r30,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,136(r1)
	PPC_STORE_U8(ctx.r1.u32 + 136, r30.u8);
	// addi r3,r1,612
	ctx.r3.s64 = ctx.r1.s64 + 612;
	// stw r11,608(r1)
	PPC_STORE_U32(ctx.r1.u32 + 608, r11.u32);
	// addi r4,r10,2516
	ctx.r4.s64 = ctx.r10.s64 + 2516;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,608
	ctx.r5.s64 = ctx.r1.s64 + 608;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,636(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 636);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed0b0
	if (cr6.lt) goto loc_822ED0B0;
	// lwz r3,616(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 616);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED0B0:
	// li r11,9
	r11.s64 = 9;
	// stw r29,636(r1)
	PPC_STORE_U32(ctx.r1.u32 + 636, r29.u32);
	// stw r30,632(r1)
	PPC_STORE_U32(ctx.r1.u32 + 632, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,616(r1)
	PPC_STORE_U8(ctx.r1.u32 + 616, r30.u8);
	// addi r3,r1,164
	ctx.r3.s64 = ctx.r1.s64 + 164;
	// stw r11,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, r11.u32);
	// addi r4,r10,2532
	ctx.r4.s64 = ctx.r10.s64 + 2532;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,188(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed0f8
	if (cr6.lt) goto loc_822ED0F8;
	// lwz r3,168(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED0F8:
	// li r11,11
	r11.s64 = 11;
	// stw r29,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, r29.u32);
	// stw r30,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,168(r1)
	PPC_STORE_U8(ctx.r1.u32 + 168, r30.u8);
	// addi r3,r1,228
	ctx.r3.s64 = ctx.r1.s64 + 228;
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// addi r4,r10,2556
	ctx.r4.s64 = ctx.r10.s64 + 2556;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,224
	ctx.r5.s64 = ctx.r1.s64 + 224;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,252(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed140
	if (cr6.lt) goto loc_822ED140;
	// lwz r3,232(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED140:
	// li r11,13
	r11.s64 = 13;
	// stw r29,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r29.u32);
	// stw r30,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,232(r1)
	PPC_STORE_U8(ctx.r1.u32 + 232, r30.u8);
	// addi r3,r1,292
	ctx.r3.s64 = ctx.r1.s64 + 292;
	// stw r11,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r11.u32);
	// addi r4,r10,2584
	ctx.r4.s64 = ctx.r10.s64 + 2584;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,288
	ctx.r5.s64 = ctx.r1.s64 + 288;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,316(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed188
	if (cr6.lt) goto loc_822ED188;
	// lwz r3,296(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 296);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED188:
	// li r11,14
	r11.s64 = 14;
	// stw r29,316(r1)
	PPC_STORE_U32(ctx.r1.u32 + 316, r29.u32);
	// stw r30,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,296(r1)
	PPC_STORE_U8(ctx.r1.u32 + 296, r30.u8);
	// addi r3,r1,356
	ctx.r3.s64 = ctx.r1.s64 + 356;
	// stw r11,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, r11.u32);
	// addi r4,r10,2600
	ctx.r4.s64 = ctx.r10.s64 + 2600;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,352
	ctx.r5.s64 = ctx.r1.s64 + 352;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,380(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 380);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed1d0
	if (cr6.lt) goto loc_822ED1D0;
	// lwz r3,360(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 360);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED1D0:
	// stw r29,380(r1)
	PPC_STORE_U32(ctx.r1.u32 + 380, r29.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r30,376(r1)
	PPC_STORE_U32(ctx.r1.u32 + 376, r30.u32);
	// addi r3,r1,420
	ctx.r3.s64 = ctx.r1.s64 + 420;
	// stb r30,360(r1)
	PPC_STORE_U8(ctx.r1.u32 + 360, r30.u8);
	// addi r4,r11,2620
	ctx.r4.s64 = r11.s64 + 2620;
	// stw r29,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, r29.u32);
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,416
	ctx.r5.s64 = ctx.r1.s64 + 416;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r10,444(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 444);
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// blt cr6,0x822ed214
	if (cr6.lt) goto loc_822ED214;
	// lwz r3,424(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 424);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED214:
	// li r11,16
	r11.s64 = 16;
	// stw r29,444(r1)
	PPC_STORE_U32(ctx.r1.u32 + 444, r29.u32);
	// stw r30,440(r1)
	PPC_STORE_U32(ctx.r1.u32 + 440, r30.u32);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stb r30,424(r1)
	PPC_STORE_U8(ctx.r1.u32 + 424, r30.u8);
	// addi r3,r1,484
	ctx.r3.s64 = ctx.r1.s64 + 484;
	// stw r11,480(r1)
	PPC_STORE_U32(ctx.r1.u32 + 480, r11.u32);
	// addi r4,r10,2644
	ctx.r4.s64 = ctx.r10.s64 + 2644;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,480
	ctx.r5.s64 = ctx.r1.s64 + 480;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// lwz r9,508(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 508);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x822ed25c
	if (cr6.lt) goto loc_822ED25C;
	// lwz r3,488(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 488);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822ED25C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r29,508(r1)
	PPC_STORE_U32(ctx.r1.u32 + 508, r29.u32);
	// li r10,54
	ctx.r10.s64 = 54;
	// stw r30,504(r1)
	PPC_STORE_U32(ctx.r1.u32 + 504, r30.u32);
	// addi r9,r11,2660
	ctx.r9.s64 = r11.s64 + 2660;
	// stb r30,488(r1)
	PPC_STORE_U8(ctx.r1.u32 + 488, r30.u8);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,55
	ctx.r7.s64 = 55;
	// addi r6,r8,2676
	ctx.r6.s64 = ctx.r8.s64 + 2676;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r4,56
	ctx.r4.s64 = 56;
	// addi r3,r5,2692
	ctx.r3.s64 = ctx.r5.s64 + 2692;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,57
	ctx.r10.s64 = 57;
	// addi r9,r11,2712
	ctx.r9.s64 = r11.s64 + 2712;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,58
	ctx.r7.s64 = 58;
	// addi r6,r8,2728
	ctx.r6.s64 = ctx.r8.s64 + 2728;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r4,59
	ctx.r4.s64 = 59;
	// addi r3,r5,2748
	ctx.r3.s64 = ctx.r5.s64 + 2748;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,60
	ctx.r10.s64 = 60;
	// addi r9,r11,2764
	ctx.r9.s64 = r11.s64 + 2764;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,61
	ctx.r7.s64 = 61;
	// addi r6,r8,2780
	ctx.r6.s64 = ctx.r8.s64 + 2780;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r4,62
	ctx.r4.s64 = 62;
	// addi r3,r5,2796
	ctx.r3.s64 = ctx.r5.s64 + 2796;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,63
	ctx.r10.s64 = 63;
	// addi r9,r11,2812
	ctx.r9.s64 = r11.s64 + 2812;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,65
	ctx.r7.s64 = 65;
	// addi r6,r8,2836
	ctx.r6.s64 = ctx.r8.s64 + 2836;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r3,66
	ctx.r3.s64 = 66;
	// addi r11,r5,2856
	r11.s64 = ctx.r5.s64 + 2856;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,67
	ctx.r9.s64 = 67;
	// addi r8,r10,2880
	ctx.r8.s64 = ctx.r10.s64 + 2880;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,18
	ctx.r6.s64 = 18;
	// addi r5,r7,2908
	ctx.r5.s64 = ctx.r7.s64 + 2908;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,19
	ctx.r3.s64 = 19;
	// addi r11,r4,2924
	r11.s64 = ctx.r4.s64 + 2924;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,20
	ctx.r9.s64 = 20;
	// addi r8,r10,2940
	ctx.r8.s64 = ctx.r10.s64 + 2940;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,23
	ctx.r6.s64 = 23;
	// addi r5,r7,2960
	ctx.r5.s64 = ctx.r7.s64 + 2960;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,24
	ctx.r3.s64 = 24;
	// addi r11,r4,2984
	r11.s64 = ctx.r4.s64 + 2984;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,25
	ctx.r9.s64 = 25;
	// addi r8,r10,3012
	ctx.r8.s64 = ctx.r10.s64 + 3012;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,22
	ctx.r6.s64 = 22;
	// addi r5,r7,3040
	ctx.r5.s64 = ctx.r7.s64 + 3040;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,27
	ctx.r3.s64 = 27;
	// addi r11,r4,3068
	r11.s64 = ctx.r4.s64 + 3068;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,28
	ctx.r9.s64 = 28;
	// addi r8,r10,3092
	ctx.r8.s64 = ctx.r10.s64 + 3092;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,29
	ctx.r6.s64 = 29;
	// addi r5,r7,3124
	ctx.r5.s64 = ctx.r7.s64 + 3124;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,30
	ctx.r3.s64 = 30;
	// addi r11,r4,3148
	r11.s64 = ctx.r4.s64 + 3148;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,31
	ctx.r9.s64 = 31;
	// addi r8,r10,3176
	ctx.r8.s64 = ctx.r10.s64 + 3176;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r7,3204
	ctx.r5.s64 = ctx.r7.s64 + 3204;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,109
	ctx.r3.s64 = 109;
	// addi r11,r4,3224
	r11.s64 = ctx.r4.s64 + 3224;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,122
	ctx.r9.s64 = 122;
	// addi r8,r10,3240
	ctx.r8.s64 = ctx.r10.s64 + 3240;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,118
	ctx.r6.s64 = 118;
	// addi r5,r7,3260
	ctx.r5.s64 = ctx.r7.s64 + 3260;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,121
	ctx.r3.s64 = 121;
	// addi r11,r4,3276
	r11.s64 = ctx.r4.s64 + 3276;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,113
	ctx.r9.s64 = 113;
	// addi r8,r10,3292
	ctx.r8.s64 = ctx.r10.s64 + 3292;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,112
	ctx.r6.s64 = 112;
	// addi r5,r7,3308
	ctx.r5.s64 = ctx.r7.s64 + 3308;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,117
	ctx.r3.s64 = 117;
	// addi r11,r4,3324
	r11.s64 = ctx.r4.s64 + 3324;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,116
	ctx.r9.s64 = 116;
	// addi r8,r10,3344
	ctx.r8.s64 = ctx.r10.s64 + 3344;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,119
	ctx.r6.s64 = 119;
	// addi r5,r7,3360
	ctx.r5.s64 = ctx.r7.s64 + 3360;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,120
	ctx.r3.s64 = 120;
	// addi r11,r4,3380
	r11.s64 = ctx.r4.s64 + 3380;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,115
	ctx.r9.s64 = 115;
	// addi r8,r10,3404
	ctx.r8.s64 = ctx.r10.s64 + 3404;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,114
	ctx.r6.s64 = 114;
	// addi r5,r7,3420
	ctx.r5.s64 = ctx.r7.s64 + 3420;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,111
	ctx.r3.s64 = 111;
	// addi r11,r4,3436
	r11.s64 = ctx.r4.s64 + 3436;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,110
	ctx.r9.s64 = 110;
	// addi r8,r10,3456
	ctx.r8.s64 = ctx.r10.s64 + 3456;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,123
	ctx.r6.s64 = 123;
	// addi r5,r7,3472
	ctx.r5.s64 = ctx.r7.s64 + 3472;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,74
	ctx.r3.s64 = 74;
	// addi r11,r4,3492
	r11.s64 = ctx.r4.s64 + 3492;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,75
	ctx.r9.s64 = 75;
	// addi r8,r10,3520
	ctx.r8.s64 = ctx.r10.s64 + 3520;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,76
	ctx.r6.s64 = 76;
	// addi r5,r7,3540
	ctx.r5.s64 = ctx.r7.s64 + 3540;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,77
	ctx.r3.s64 = 77;
	// addi r11,r4,3560
	r11.s64 = ctx.r4.s64 + 3560;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,78
	ctx.r9.s64 = 78;
	// addi r8,r10,3580
	ctx.r8.s64 = ctx.r10.s64 + 3580;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r6,79
	ctx.r6.s64 = 79;
	// addi r5,r7,3596
	ctx.r5.s64 = ctx.r7.s64 + 3596;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// addi r3,r1,1056
	ctx.r3.s64 = ctx.r1.s64 + 1056;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1056
	ctx.r5.s64 = ctx.r1.s64 + 1056;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1056
	ctx.r3.s64 = ctx.r1.s64 + 1056;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// li r3,80
	ctx.r3.s64 = 80;
	// addi r11,r4,3616
	r11.s64 = ctx.r4.s64 + 3616;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// addi r3,r1,1408
	ctx.r3.s64 = ctx.r1.s64 + 1408;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1408
	ctx.r5.s64 = ctx.r1.s64 + 1408;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1408
	ctx.r3.s64 = ctx.r1.s64 + 1408;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r9,81
	ctx.r9.s64 = 81;
	// addi r8,r10,3636
	ctx.r8.s64 = ctx.r10.s64 + 3636;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r8,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r8.u32);
	// addi r3,r1,1152
	ctx.r3.s64 = ctx.r1.s64 + 1152;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1152
	ctx.r5.s64 = ctx.r1.s64 + 1152;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1152
	ctx.r3.s64 = ctx.r1.s64 + 1152;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r6,r7,3660
	ctx.r6.s64 = ctx.r7.s64 + 3660;
	// addi r3,r1,1344
	ctx.r3.s64 = ctx.r1.s64 + 1344;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// addi r5,r1,1344
	ctx.r5.s64 = ctx.r1.s64 + 1344;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1344
	ctx.r3.s64 = ctx.r1.s64 + 1344;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r4,84
	ctx.r4.s64 = 84;
	// addi r3,r5,3684
	ctx.r3.s64 = ctx.r5.s64 + 3684;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,1216
	ctx.r3.s64 = ctx.r1.s64 + 1216;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1216
	ctx.r5.s64 = ctx.r1.s64 + 1216;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1216
	ctx.r3.s64 = ctx.r1.s64 + 1216;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r10,r11,3704
	ctx.r10.s64 = r11.s64 + 3704;
	// addi r3,r1,1472
	ctx.r3.s64 = ctx.r1.s64 + 1472;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1472
	ctx.r5.s64 = ctx.r1.s64 + 1472;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1472
	ctx.r3.s64 = ctx.r1.s64 + 1472;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// li r8,69
	ctx.r8.s64 = 69;
	// addi r7,r9,3720
	ctx.r7.s64 = ctx.r9.s64 + 3720;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// addi r3,r1,1280
	ctx.r3.s64 = ctx.r1.s64 + 1280;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1280
	ctx.r5.s64 = ctx.r1.s64 + 1280;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1280
	ctx.r3.s64 = ctx.r1.s64 + 1280;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// li r5,70
	ctx.r5.s64 = 70;
	// addi r3,r6,3740
	ctx.r3.s64 = ctx.r6.s64 + 3740;
	// stw r5,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r5.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,1088
	ctx.r3.s64 = ctx.r1.s64 + 1088;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1088
	ctx.r5.s64 = ctx.r1.s64 + 1088;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1088
	ctx.r3.s64 = ctx.r1.s64 + 1088;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,71
	ctx.r10.s64 = 71;
	// addi r9,r11,3756
	ctx.r9.s64 = r11.s64 + 3756;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,1120
	ctx.r3.s64 = ctx.r1.s64 + 1120;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1120
	ctx.r5.s64 = ctx.r1.s64 + 1120;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1120
	ctx.r3.s64 = ctx.r1.s64 + 1120;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,72
	ctx.r7.s64 = 72;
	// addi r6,r8,3772
	ctx.r6.s64 = ctx.r8.s64 + 3772;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r3,r1,1184
	ctx.r3.s64 = ctx.r1.s64 + 1184;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1184
	ctx.r5.s64 = ctx.r1.s64 + 1184;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1184
	ctx.r3.s64 = ctx.r1.s64 + 1184;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r4,50
	ctx.r4.s64 = 50;
	// addi r3,r5,3804
	ctx.r3.s64 = ctx.r5.s64 + 3804;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,1248
	ctx.r3.s64 = ctx.r1.s64 + 1248;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1248
	ctx.r5.s64 = ctx.r1.s64 + 1248;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1248
	ctx.r3.s64 = ctx.r1.s64 + 1248;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,83
	ctx.r10.s64 = 83;
	// addi r9,r11,3828
	ctx.r9.s64 = r11.s64 + 3828;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,1312
	ctx.r3.s64 = ctx.r1.s64 + 1312;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1312
	ctx.r5.s64 = ctx.r1.s64 + 1312;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1312
	ctx.r3.s64 = ctx.r1.s64 + 1312;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,51
	ctx.r7.s64 = 51;
	// addi r6,r8,3848
	ctx.r6.s64 = ctx.r8.s64 + 3848;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r6,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r6.u32);
	// addi r3,r1,1376
	ctx.r3.s64 = ctx.r1.s64 + 1376;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1376
	ctx.r5.s64 = ctx.r1.s64 + 1376;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1376
	ctx.r3.s64 = ctx.r1.s64 + 1376;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// li r4,52
	ctx.r4.s64 = 52;
	// addi r3,r5,3876
	ctx.r3.s64 = ctx.r5.s64 + 3876;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r3,r1,1440
	ctx.r3.s64 = ctx.r1.s64 + 1440;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1440
	ctx.r5.s64 = ctx.r1.s64 + 1440;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1440
	ctx.r3.s64 = ctx.r1.s64 + 1440;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,53
	ctx.r10.s64 = 53;
	// addi r9,r11,3900
	ctx.r9.s64 = r11.s64 + 3900;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
	// addi r3,r1,1504
	ctx.r3.s64 = ctx.r1.s64 + 1504;
	// bl 0x822f1c98
	sub_822F1C98(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r5,r1,1504
	ctx.r5.s64 = ctx.r1.s64 + 1504;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0878
	sub_822F0878(ctx, base);
	// addi r3,r1,1504
	ctx.r3.s64 = ctx.r1.s64 + 1504;
	// bl 0x822edfc8
	sub_822EDFC8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,1584
	ctx.r1.s64 = ctx.r1.s64 + 1584;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822EDFC8"))) PPC_WEAK_FUNC(sub_822EDFC8);
PPC_FUNC_IMPL(__imp__sub_822EDFC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,28(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r31,r3,4
	r31.s64 = ctx.r3.s64 + 4;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x822edff0
	if (cr6.lt) goto loc_822EDFF0;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822EDFF0:
	// li r11,0
	r11.s64 = 0;
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// stb r11,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r11.u8);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EE018"))) PPC_WEAK_FUNC(sub_822EE018);
PPC_FUNC_IMPL(__imp__sub_822EE018) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r31,0
	r31.s64 = 0;
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r31,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r31.u32);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// mr r30,r31
	r30.u64 = r31.u64;
	// lwz r11,26912(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26912);
	// stb r31,100(r1)
	PPC_STORE_U8(ctx.r1.u32 + 100, r31.u8);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r8,88(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 88);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r11,0(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// addi r3,r11,104
	ctx.r3.s64 = r11.s64 + 104;
	// lwz r11,108(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 108);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ee080
	if (cr6.eq) goto loc_822EE080;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ee07c
	if (cr6.eq) goto loc_822EE07C;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// b 0x822ee084
	goto loc_822EE084;
loc_822EE07C:
	// bl 0x821940c8
	sub_821940C8(ctx, base);
loc_822EE080:
	// mr r11,r31
	r11.u64 = r31.u64;
loc_822EE084:
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// rlwinm r9,r10,7,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 7) & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822ee184
	if (cr6.eq) goto loc_822EE184;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ee0c0
	if (cr6.eq) goto loc_822EE0C0;
	// lbz r10,57(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 57);
	// lwz r11,72(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ee188
	goto loc_822EE188;
loc_822EE0C0:
	// lwz r10,72(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 72);
	// lwz r6,76(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 76);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// subf r11,r10,r6
	r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822ee12c
	if (!cr0.gt) goto loc_822EE12C;
loc_822EE0DC:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,57
	cr6.compare<int32_t>(ctx.r7.s32, 57, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822ee0fc
	if (cr6.lt) goto loc_822EE0FC;
	// mr r7,r31
	ctx.r7.u64 = r31.u64;
loc_822EE0FC:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822ee118
	if (cr6.eq) goto loc_822EE118;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822ee120
	goto loc_822EE120;
loc_822EE118:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822EE120:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822ee0dc
	if (cr6.gt) goto loc_822EE0DC;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822EE12C:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x822ee170
	if (cr6.eq) goto loc_822EE170;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,57
	cr6.compare<int32_t>(r11.s32, 57, xer);
	// li r11,1
	r11.s64 = 1;
	// bgt cr6,0x822ee148
	if (cr6.gt) goto loc_822EE148;
	// mr r11,r31
	r11.u64 = r31.u64;
loc_822EE148:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ee170
	if (!cr6.eq) goto loc_822EE170;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ee188
	goto loc_822EE188;
loc_822EE170:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822ee188
	goto loc_822EE188;
loc_822EE184:
	// mr r11,r31
	r11.u64 = r31.u64;
loc_822EE188:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ee2b0
	if (cr6.eq) goto loc_822EE2B0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,3932
	ctx.r3.s64 = r11.s64 + 3932;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r28,-1
	r28.s64 = -1;
	// addi r27,r11,28888
	r27.s64 = r11.s64 + 28888;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// addi r29,r11,3948
	r29.s64 = r11.s64 + 3948;
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r10,0(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
loc_822EE1CC:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ee1e0
	if (cr6.eq) goto loc_822EE1E0;
	// cmplw cr6,r10,r27
	cr6.compare<uint32_t>(ctx.r10.u32, r27.u32, xer);
	// beq cr6,0x822ee1e4
	if (cr6.eq) goto loc_822EE1E4;
loc_822EE1E0:
	// twi 31,r0,22
loc_822EE1E4:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ee2a4
	if (cr6.eq) goto loc_822EE2A4;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ee1fc
	if (!cr6.eq) goto loc_822EE1FC;
	// twi 31,r0,22
loc_822EE1FC:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ee20c
	if (!cr6.eq) goto loc_822EE20C;
	// twi 31,r0,22
loc_822EE20C:
	// lwz r31,12(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ee21c
	if (!cr6.eq) goto loc_822EE21C;
	// twi 31,r0,22
loc_822EE21C:
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r11,16
	ctx.r4.s64 = r11.s64 + 16;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8218ea38
	sub_8218EA38(ctx, base);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,42
	cr6.compare<int32_t>(r31.s32, 42, xer);
	// beq cr6,0x822ee274
	if (cr6.eq) goto loc_822EE274;
	// cmpwi cr6,r31,44
	cr6.compare<int32_t>(r31.s32, 44, xer);
	// beq cr6,0x822ee274
	if (cr6.eq) goto loc_822EE274;
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x822ee28c
	if (!cr6.lt) goto loc_822EE28C;
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// b 0x822ee28c
	goto loc_822EE28C;
loc_822EE274:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// bge cr6,0x822ee288
	if (!cr6.lt) goto loc_822EE288;
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
loc_822EE288:
	// neg r5,r5
	ctx.r5.s64 = -ctx.r5.s64;
loc_822EE28C:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82c705d8
	sub_82C705D8(ctx, base);
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// b 0x822ee1cc
	goto loc_822EE1CC;
loc_822EE2A4:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,3956
	ctx.r3.s64 = r11.s64 + 3956;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EE2B0:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// stb r10,1376(r26)
	PPC_STORE_U8(r26.u32 + 1376, ctx.r10.u8);
	// blt cr6,0x822ee2cc
	if (cr6.lt) goto loc_822EE2CC;
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822EE2CC:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822EE2D8"))) PPC_WEAK_FUNC(sub_822EE2D8);
PPC_FUNC_IMPL(__imp__sub_822EE2D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// ld r12,-4096(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// stwu r1,-6960(r1)
	ea = -6960 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// addi r3,r11,3972
	ctx.r3.s64 = r11.s64 + 3972;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r20,r5
	r20.u64 = ctx.r5.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r23,-31927
	r23.s64 = -2092367872;
	// li r26,1
	r26.s64 = 1;
	// li r18,0
	r18.s64 = 0;
	// stb r26,434(r1)
	PPC_STORE_U8(ctx.r1.u32 + 434, r26.u8);
	// addi r21,r31,3
	r21.s64 = r31.s64 + 3;
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r26.u32);
	// lwz r11,26912(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 26912);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ee330
	if (cr6.eq) goto loc_822EE330;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lbz r10,26821(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 26821);
	// b 0x822ee334
	goto loc_822EE334;
loc_822EE330:
	// mr r10,r18
	ctx.r10.u64 = r18.u64;
loc_822EE334:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// li r19,2
	r19.s64 = 2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ee6fc
	if (cr6.eq) goto loc_822EE6FC;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lis r22,-31927
	r22.s64 = -2092367872;
	// lwz r10,140(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// lwz r11,184(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 184);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ee398
	if (cr6.eq) goto loc_822EE398;
	// lwz r11,412(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 412);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// bne cr6,0x822ee398
	if (!cr6.eq) goto loc_822EE398;
	// lwz r11,125(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 125);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822ee398
	if (!cr6.eq) goto loc_822EE398;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,26932(r22)
	ctx.r3.u64 = PPC_LOAD_U32(r22.u32 + 26932);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r21,77
	ctx.r4.s64 = r21.s64 + 77;
	// bl 0x82cbc158
	sub_82CBC158(ctx, base);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ee700
	if (cr6.eq) goto loc_822EE700;
loc_822EE398:
	// lwz r11,456(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 456);
	// cmpwi cr6,r11,4096
	cr6.compare<int32_t>(r11.s32, 4096, xer);
	// blt cr6,0x822ee700
	if (cr6.lt) goto loc_822EE700;
	// cmpwi cr6,r11,4101
	cr6.compare<int32_t>(r11.s32, 4101, xer);
	// bgt cr6,0x822ee700
	if (cr6.gt) goto loc_822EE700;
	// lwz r11,236(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ee700
	if (cr6.eq) goto loc_822EE700;
	// lis r24,-31927
	r24.s64 = -2092367872;
	// lwz r11,29136(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 29136);
	// lbz r11,243(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 243);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ee700
	if (cr6.eq) goto loc_822EE700;
	// addi r27,r25,232
	r27.s64 = r25.s64 + 232;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// lwz r11,125(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 125);
	// subf r10,r31,r3
	ctx.r10.s64 = ctx.r3.s64 - r31.s64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// li r4,2
	ctx.r4.s64 = 2;
	// mullw r31,r10,r11
	r31.s64 = int64_t(ctx.r10.s32) * int64_t(r11.s32);
	// bl 0x82334748
	sub_82334748(ctx, base);
	// subf r31,r3,r31
	r31.s64 = r31.s64 - ctx.r3.s64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// add r31,r3,r31
	r31.u64 = ctx.r3.u64 + r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r9,4024
	ctx.r3.s64 = ctx.r9.s64 + 4024;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// li r4,2
	ctx.r4.s64 = 2;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r8,4048
	ctx.r3.s64 = ctx.r8.s64 + 4048;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r7,4072
	ctx.r3.s64 = ctx.r7.s64 + 4072;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r6,4096
	ctx.r3.s64 = ctx.r6.s64 + 4096;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r5,4124
	ctx.r3.s64 = ctx.r5.s64 + 4124;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r3,-32246
	ctx.r3.s64 = -2113273856;
	// lbz r4,8(r21)
	ctx.r4.u64 = PPC_LOAD_U8(r21.u32 + 8);
	// addi r3,r3,4144
	ctx.r3.s64 = ctx.r3.s64 + 4144;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lbz r11,8(r21)
	r11.u64 = PPC_LOAD_U8(r21.u32 + 8);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bgt cr6,0x822eea44
	if (cr6.gt) goto loc_822EEA44;
	// lwz r11,29136(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 29136);
	// lbz r10,243(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 243);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eea44
	if (cr6.eq) goto loc_822EEA44;
	// lbz r11,245(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 245);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eea44
	if (cr6.eq) goto loc_822EEA44;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r31,129(r21)
	r31.u64 = PPC_LOAD_U32(r21.u32 + 129);
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// addi r28,r11,26808
	r28.s64 = r11.s64 + 26808;
	// lwz r29,26808(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 26808);
	// clrlwi r9,r29,24
	ctx.r9.u64 = r29.u32 & 0xFF;
	// cmpw cr6,r10,r9
	cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, xer);
	// bgt cr6,0x822ee818
	if (cr6.gt) goto loc_822EE818;
	// mr r30,r26
	r30.u64 = r26.u64;
loc_822EE4F4:
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,2
	ctx.r6.s64 = 2;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82338878
	sub_82338878(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x822ee818
	if (!cr6.eq) goto loc_822EE818;
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// cmpwi cr6,r30,11
	cr6.compare<int32_t>(r30.s32, 11, xer);
	// ble cr6,0x822ee4f4
	if (!cr6.gt) goto loc_822EE4F4;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4272
	ctx.r3.s64 = r11.s64 + 4272;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// ld r10,368(r25)
	ctx.r10.u64 = PPC_LOAD_U64(r25.u32 + 368);
	// lbz r9,560(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 560);
	// addi r31,r25,512
	r31.s64 = r25.s64 + 512;
	// lwz r8,26932(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 26932);
	// mr r11,r18
	r11.u64 = r18.u64;
	// stw r18,435(r1)
	PPC_STORE_U32(ctx.r1.u32 + 435, r18.u32);
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// stb r26,455(r1)
	PPC_STORE_U8(ctx.r1.u32 + 455, r26.u8);
	// li r12,447
	r12.s64 = 447;
	// stdx r10,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r10.u64);
	// ble cr6,0x822ee578
	if (!cr6.gt) goto loc_822EE578;
	// addi r10,r31,276
	ctx.r10.s64 = r31.s64 + 276;
loc_822EE55C:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r7,r8
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r8.u32, xer);
	// beq cr6,0x822ee57c
	if (cr6.eq) goto loc_822EE57C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x822ee55c
	if (cr6.lt) goto loc_822EE55C;
loc_822EE578:
	// li r11,-1
	r11.s64 = -1;
loc_822EE57C:
	// addi r10,r11,79
	ctx.r10.s64 = r11.s64 + 79;
	// lbz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// mr r11,r18
	r11.u64 = r18.u64;
	// rlwinm r7,r10,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ldx r6,r7,r25
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r7.u32 + r25.u32);
	// std r6,520(r1)
	PPC_STORE_U64(ctx.r1.u32 + 520, ctx.r6.u64);
	// ble cr6,0x822ee5bc
	if (!cr6.gt) goto loc_822EE5BC;
	// addi r10,r31,276
	ctx.r10.s64 = r31.s64 + 276;
loc_822EE5A0:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r7,r8
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r8.u32, xer);
	// beq cr6,0x822ee5c0
	if (cr6.eq) goto loc_822EE5C0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x822ee5a0
	if (cr6.lt) goto loc_822EE5A0;
loc_822EE5BC:
	// li r11,-1
	r11.s64 = -1;
loc_822EE5C0:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// li r6,16
	ctx.r6.s64 = 16;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r5,r11,561
	ctx.r5.s64 = r11.s64 + 561;
	// addi r3,r1,456
	ctx.r3.s64 = ctx.r1.s64 + 456;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// lwz r11,26932(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 26932);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r9,r11,3
	ctx.r9.s64 = r11.s64 + 3;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r4,r10,5664
	ctx.r4.s64 = ctx.r10.s64 + 5664;
	// addi r3,r8,1864
	ctx.r3.s64 = ctx.r8.s64 + 1864;
	// lwzx r6,r7,r25
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + r25.u32);
	// stw r6,552(r1)
	PPC_STORE_U32(ctx.r1.u32 + 552, ctx.r6.u32);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,29136(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 29136);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r18,245(r11)
	PPC_STORE_U8(r11.u32 + 245, r18.u8);
	// bl 0x82334748
	sub_82334748(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82334748
	sub_82334748(ctx, base);
	// lwz r5,376(r25)
	ctx.r5.u64 = PPC_LOAD_U32(r25.u32 + 376);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// rlwinm r6,r5,0,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r18,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r18.u32);
	// add r8,r30,r3
	ctx.r8.u64 = r30.u64 + ctx.r3.u64;
	// addi r7,r9,28344
	ctx.r7.s64 = ctx.r9.s64 + 28344;
	// stw r6,572(r1)
	PPC_STORE_U32(ctx.r1.u32 + 572, ctx.r6.u32);
	// stw r8,568(r1)
	PPC_STORE_U32(ctx.r1.u32 + 568, ctx.r8.u32);
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// li r12,439
	r12.s64 = 439;
	// stdx r10,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r10.u64);
loc_822EE658:
	// mfmsr r11
	// mtmsrd r13,1
	// lwarx r4,0,r5
	reserved.u32 = *(uint32_t*)(base + ctx.r5.u32);
	ctx.r4.u64 = __builtin_bswap32(reserved.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stwcx. r4,0,r5
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r5.u32), reserved.s32, __builtin_bswap32(ctx.r4.s32));
	cr0.so = xer.so;
	// mtmsrd r11,1
	// bne 0x822ee658
	if (!cr0.eq) goto loc_822EE658;
	// stw r18,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r18.u32);
loc_822EE678:
	// mfmsr r3
	// mtmsrd r13,1
	// lwarx r4,0,r7
	reserved.u32 = *(uint32_t*)(base + ctx.r7.u32);
	ctx.r4.u64 = __builtin_bswap32(reserved.u32);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stwcx. r4,0,r7
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r7.u32), reserved.s32, __builtin_bswap32(ctx.r4.s32));
	cr0.so = xer.so;
	// mtmsrd r3,1
	// bne 0x822ee678
	if (!cr0.eq) goto loc_822EE678;
	// lwz r11,26912(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 26912);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,128(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 128);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ee6c0
	if (!cr6.eq) goto loc_822EE6C0;
	// twi 31,r0,22
	// twi 31,r0,22
loc_822EE6C0:
	// lwz r31,8(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// cmpwi cr6,r3,4
	cr6.compare<int32_t>(ctx.r3.s32, 4, xer);
	// addi r30,r11,63
	r30.s64 = r11.s64 + 63;
	// bne cr6,0x822ee9a0
	if (!cr6.eq) goto loc_822EE9A0;
	// addi r4,r31,20
	ctx.r4.s64 = r31.s64 + 20;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// addi r4,r31,24
	ctx.r4.s64 = r31.s64 + 24;
	// b 0x822ee9d8
	goto loc_822EE9D8;
loc_822EE6FC:
	// stw r18,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r18.u32);
loc_822EE700:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,3996
	ctx.r3.s64 = r11.s64 + 3996;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r10,435(r1)
	PPC_STORE_U32(ctx.r1.u32 + 435, ctx.r10.u32);
loc_822EE714:
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// lwz r11,435(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 435);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822eed2c
	if (!cr6.eq) goto loc_822EED2C;
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// ld r11,0(r21)
	r11.u64 = PPC_LOAD_U64(r21.u32 + 0);
	// lbz r10,8(r21)
	ctx.r10.u64 = PPC_LOAD_U8(r21.u32 + 8);
	// stw r20,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r20.u32);
	// std r11,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, r11.u64);
	// stb r10,176(r1)
	PPC_STORE_U8(ctx.r1.u32 + 176, ctx.r10.u8);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,376(r1)
	PPC_STORE_U32(ctx.r1.u32 + 376, ctx.r3.u32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// mr r5,r20
	ctx.r5.u64 = r20.u64;
	// addi r3,r9,4296
	ctx.r3.s64 = ctx.r9.s64 + 4296;
	// li r4,2150
	ctx.r4.s64 = 2150;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,140
	ctx.r4.s64 = ctx.r1.s64 + 140;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82cf9ad0
	sub_82CF9AD0(ctx, base);
	// mr r31,r18
	r31.u64 = r18.u64;
loc_822EE784:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// rlwinm r10,r31,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// addi r9,r1,380
	ctx.r9.s64 = ctx.r1.s64 + 380;
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// addi r8,r3,-5000
	ctx.r8.s64 = ctx.r3.s64 + -5000;
	// cmplwi cr6,r31,4
	cr6.compare<uint32_t>(r31.u32, 4, xer);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// blt cr6,0x822ee784
	if (cr6.lt) goto loc_822EE784;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r21,77
	ctx.r5.s64 = r21.s64 + 77;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,248
	ctx.r3.s64 = ctx.r1.s64 + 248;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,64
	ctx.r6.s64 = 64;
	// addi r5,r21,9
	ctx.r5.s64 = r21.s64 + 9;
	// li r4,64
	ctx.r4.s64 = 64;
	// addi r3,r1,177
	ctx.r3.s64 = ctx.r1.s64 + 177;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r21,109
	ctx.r5.s64 = r21.s64 + 109;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,280
	ctx.r3.s64 = ctx.r1.s64 + 280;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// lwz r11,125(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 125);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// addi r3,r25,8
	ctx.r3.s64 = r25.s64 + 8;
	// stw r11,396(r1)
	PPC_STORE_U32(ctx.r1.u32 + 396, r11.u32);
	// bl 0x823041c8
	sub_823041C8(ctx, base);
	// addi r30,r25,512
	r30.s64 = r25.s64 + 512;
	// addi r10,r1,128
	ctx.r10.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// addi r3,r25,232
	ctx.r3.s64 = r25.s64 + 232;
	// bne cr6,0x822eea58
	if (!cr6.eq) goto loc_822EEA58;
	// bl 0x82334798
	sub_82334798(ctx, base);
	// b 0x822eea5c
	goto loc_822EEA5C;
loc_822EE818:
	// lbz r8,3(r28)
	ctx.r8.u64 = PPC_LOAD_U8(r28.u32 + 3);
	// clrlwi r9,r31,24
	ctx.r9.u64 = r31.u32 & 0xFF;
	// mr r10,r19
	ctx.r10.u64 = r19.u64;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// stw r10,435(r1)
	PPC_STORE_U32(ctx.r1.u32 + 435, ctx.r10.u32);
	// blt cr6,0x822ee838
	if (cr6.lt) goto loc_822EE838;
	// li r10,4
	ctx.r10.s64 = 4;
	// stw r10,435(r1)
	PPC_STORE_U32(ctx.r1.u32 + 435, ctx.r10.u32);
loc_822EE838:
	// stw r29,832(r1)
	PPC_STORE_U32(ctx.r1.u32 + 832, r29.u32);
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822ee850
	if (!cr6.eq) goto loc_822EE850;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4200
	ctx.r3.s64 = r11.s64 + 4200;
	// b 0x822ee858
	goto loc_822EE858;
loc_822EE850:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4240
	ctx.r3.s64 = r11.s64 + 4240;
loc_822EE858:
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,129(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 129);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r21,9
	ctx.r4.s64 = r21.s64 + 9;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// stw r11,26812(r10)
	PPC_STORE_U32(ctx.r10.u32 + 26812, r11.u32);
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r5,832(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 832);
	// lwz r4,129(r21)
	ctx.r4.u64 = PPC_LOAD_U32(r21.u32 + 129);
	// bl 0x823394b8
	sub_823394B8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ee93c
	if (cr6.eq) goto loc_822EE93C;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822ee93c
	if (cr6.eq) goto loc_822EE93C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,3224
	ctx.r4.s64 = r11.s64 + 3224;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lis r10,-32247
	ctx.r10.s64 = -2113339392;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lfs f1,24712(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 24712);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x823319a0
	sub_823319A0(ctx, base);
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ee904
	if (cr6.eq) goto loc_822EE904;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82287710
	sub_82287710(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eab88
	sub_822EAB88(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822ee908
	goto loc_822EE908;
loc_822EE904:
	// mr r31,r18
	r31.u64 = r18.u64;
loc_822EE908:
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// stw r31,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r31.u32);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// li r4,4
	ctx.r4.s64 = 4;
	// lwz r11,26788(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26788);
	// lwz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 32);
	// lwz r3,88(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 88);
	// bl 0x82332d48
	sub_82332D48(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822EE93C:
	// lbz r9,560(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 560);
	// addi r10,r25,512
	ctx.r10.s64 = r25.s64 + 512;
	// mr r11,r18
	r11.u64 = r18.u64;
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// ble cr6,0x822ee974
	if (!cr6.gt) goto loc_822EE974;
	// lwz r8,26932(r22)
	ctx.r8.u64 = PPC_LOAD_U32(r22.u32 + 26932);
	// addi r10,r10,276
	ctx.r10.s64 = ctx.r10.s64 + 276;
loc_822EE958:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r7,r8
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r8.u32, xer);
	// beq cr6,0x822ee978
	if (cr6.eq) goto loc_822EE978;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// blt cr6,0x822ee958
	if (cr6.lt) goto loc_822EE958;
loc_822EE974:
	// li r11,-1
	r11.s64 = -1;
loc_822EE978:
	// rlwinm r11,r11,4,0,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 4) & 0xFFFFFFF0;
	// li r6,16
	ctx.r6.s64 = 16;
	// add r11,r11,r25
	r11.u64 = r11.u64 + r25.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r5,r11,561
	ctx.r5.s64 = r11.s64 + 561;
	// addi r3,r1,456
	ctx.r3.s64 = ctx.r1.s64 + 456;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821c6868
	sub_821C6868(ctx, base);
	// b 0x822ee714
	goto loc_822EE714;
loc_822EE9A0:
	// lwz r11,26912(r23)
	r11.u64 = PPC_LOAD_U32(r23.u32 + 26912);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r10,88(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 88);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// lwz r11,4(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r31,36(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ee9ec
	if (cr6.eq) goto loc_822EE9EC;
	// addi r4,r11,20
	ctx.r4.s64 = r11.s64 + 20;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r4,r11,172
	ctx.r4.s64 = r11.s64 + 172;
loc_822EE9D8:
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ee9f4
	if (!cr6.eq) goto loc_822EE9F4;
loc_822EE9EC:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// b 0x822ee9f8
	goto loc_822EE9F8;
loc_822EE9F4:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822EE9F8:
	// li r5,128
	ctx.r5.s64 = 128;
	// addi r3,r1,576
	ctx.r3.s64 = ctx.r1.s64 + 576;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eea18
	if (cr6.eq) goto loc_822EEA18;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822EEA18:
	// li r5,128
	ctx.r5.s64 = 128;
	// addi r3,r1,704
	ctx.r3.s64 = ctx.r1.s64 + 704;
	// bl 0x822cd828
	sub_822CD828(ctx, base);
	// lwz r11,129(r21)
	r11.u64 = PPC_LOAD_U32(r21.u32 + 129);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// stw r11,26812(r10)
	PPC_STORE_U32(ctx.r10.u32 + 26812, r11.u32);
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// b 0x822ee714
	goto loc_822EE714;
loc_822EEA44:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4172
	ctx.r3.s64 = r11.s64 + 4172;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r26,435(r1)
	PPC_STORE_U32(ctx.r1.u32 + 435, r26.u32);
	// b 0x822ee714
	goto loc_822EE714;
loc_822EEA58:
	// bl 0x823348b0
	sub_823348B0(ctx, base);
loc_822EEA5C:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x822eaca8
	sub_822EACA8(ctx, base);
	// lbz r11,176(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 176);
	// ld r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// addi r3,r1,4283
	ctx.r3.s64 = ctx.r1.s64 + 4283;
	// addi r4,r1,140
	ctx.r4.s64 = ctx.r1.s64 + 140;
	// stb r19,4274(r1)
	PPC_STORE_U8(ctx.r1.u32 + 4274, r19.u8);
	// li r5,36
	ctx.r5.s64 = 36;
	// stb r11,4319(r1)
	PPC_STORE_U8(ctx.r1.u32 + 4319, r11.u8);
	// li r12,4275
	r12.s64 = 4275;
	// stdx r10,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r10.u64);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// li r6,64
	ctx.r6.s64 = 64;
	// addi r5,r1,177
	ctx.r5.s64 = ctx.r1.s64 + 177;
	// li r4,64
	ctx.r4.s64 = 64;
	// addi r3,r1,4320
	ctx.r3.s64 = ctx.r1.s64 + 4320;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r1,248
	ctx.r5.s64 = ctx.r1.s64 + 248;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,4384
	ctx.r3.s64 = ctx.r1.s64 + 4384;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r1,280
	ctx.r5.s64 = ctx.r1.s64 + 280;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,4416
	ctx.r3.s64 = ctx.r1.s64 + 4416;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// lwz r11,1160(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 1160);
	// addi r29,r25,1152
	r29.s64 = r25.s64 + 1152;
	// lwz r31,1156(r25)
	r31.u64 = PPC_LOAD_U32(r25.u32 + 1156);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x822eeae0
	if (!cr6.gt) goto loc_822EEAE0;
	// twi 31,r0,22
loc_822EEAE0:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eeaf4
	if (!cr6.gt) goto loc_822EEAF4;
	// twi 31,r0,22
loc_822EEAF4:
	// cmplw cr6,r29,r29
	cr6.compare<uint32_t>(r29.u32, r29.u32, xer);
	// beq cr6,0x822eeb00
	if (cr6.eq) goto loc_822EEB00;
	// twi 31,r0,22
loc_822EEB00:
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x822eebb0
	if (cr6.eq) goto loc_822EEBB0;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eeb14
	if (cr6.lt) goto loc_822EEB14;
	// twi 31,r0,22
loc_822EEB14:
	// stb r19,2994(r1)
	PPC_STORE_U8(ctx.r1.u32 + 2994, r19.u8);
	// addi r4,r31,12
	ctx.r4.s64 = r31.s64 + 12;
	// lbz r11,48(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// addi r3,r1,3003
	ctx.r3.s64 = ctx.r1.s64 + 3003;
	// stb r11,3039(r1)
	PPC_STORE_U8(ctx.r1.u32 + 3039, r11.u8);
	// li r5,36
	ctx.r5.s64 = 36;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r12,2995
	r12.s64 = 2995;
	// stdx r10,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r10.u64);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// li r6,64
	ctx.r6.s64 = 64;
	// addi r5,r31,49
	ctx.r5.s64 = r31.s64 + 49;
	// li r4,64
	ctx.r4.s64 = 64;
	// addi r3,r1,3040
	ctx.r3.s64 = ctx.r1.s64 + 3040;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r31,120
	ctx.r5.s64 = r31.s64 + 120;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,3104
	ctx.r3.s64 = ctx.r1.s64 + 3104;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r31,152
	ctx.r5.s64 = r31.s64 + 152;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,3136
	ctx.r3.s64 = ctx.r1.s64 + 3136;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// addi r4,r1,2992
	ctx.r4.s64 = ctx.r1.s64 + 2992;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r5,136(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// addi r4,r1,4272
	ctx.r4.s64 = ctx.r1.s64 + 4272;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eeba8
	if (cr6.lt) goto loc_822EEBA8;
	// twi 31,r0,22
loc_822EEBA8:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eeae0
	goto loc_822EEAE0;
loc_822EEBB0:
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822f0600
	sub_822F0600(ctx, base);
	// lwz r11,436(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 436);
	// cmpwi cr6,r11,5
	cr6.compare<int32_t>(r11.s32, 5, xer);
	// ble cr6,0x822eed24
	if (!cr6.gt) goto loc_822EED24;
	// li r11,4
	r11.s64 = 4;
	// lwz r5,136(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// addi r4,r1,5552
	ctx.r4.s64 = ctx.r1.s64 + 5552;
	// stb r11,5554(r1)
	PPC_STORE_U8(ctx.r1.u32 + 5554, r11.u8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// lbz r9,560(r25)
	ctx.r9.u64 = PPC_LOAD_U8(r25.u32 + 560);
	// li r10,8
	ctx.r10.s64 = 8;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stb r10,1714(r1)
	PPC_STORE_U8(ctx.r1.u32 + 1714, ctx.r10.u8);
	// beq cr6,0x822eec54
	if (cr6.eq) goto loc_822EEC54;
	// mr r31,r18
	r31.u64 = r18.u64;
loc_822EEBF8:
	// addi r11,r31,186
	r11.s64 = r31.s64 + 186;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r25
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + r25.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eec40
	if (cr6.eq) goto loc_822EEC40;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// addi r10,r31,79
	ctx.r10.s64 = r31.s64 + 79;
	// stw r11,1731(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1731, r11.u32);
	// addi r4,r1,1712
	ctx.r4.s64 = ctx.r1.s64 + 1712;
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r5,136(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r12,1715
	r12.s64 = 1715;
	// stdx r9,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r9.u64);
	// ldx r7,r8,r25
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + r25.u32);
	// li r12,1723
	r12.s64 = 1723;
	// stdx r7,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r7.u64);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EEC40:
	// addi r11,r31,1
	r11.s64 = r31.s64 + 1;
	// lbz r10,560(r25)
	ctx.r10.u64 = PPC_LOAD_U8(r25.u32 + 560);
	// clrlwi r31,r11,24
	r31.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eebf8
	if (cr6.lt) goto loc_822EEBF8;
loc_822EEC54:
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eec68
	if (!cr6.gt) goto loc_822EEC68;
	// twi 31,r0,22
loc_822EEC68:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// ble cr6,0x822eec7c
	if (!cr6.gt) goto loc_822EEC7C;
	// twi 31,r0,22
loc_822EEC7C:
	// cmplw cr6,r29,r29
	cr6.compare<uint32_t>(r29.u32, r29.u32, xer);
	// beq cr6,0x822eec88
	if (cr6.eq) goto loc_822EEC88;
	// twi 31,r0,22
loc_822EEC88:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822eed24
	if (cr6.eq) goto loc_822EED24;
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eec9c
	if (cr6.lt) goto loc_822EEC9C;
	// twi 31,r0,22
loc_822EEC9C:
	// lbz r11,48(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eed0c
	if (cr6.eq) goto loc_822EED0C;
	// mr r30,r18
	r30.u64 = r18.u64;
loc_822EECAC:
	// addi r11,r30,58
	r11.s64 = r30.s64 + 58;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eecf8
	if (cr6.eq) goto loc_822EECF8;
	// addi r9,r30,15
	ctx.r9.s64 = r30.s64 + 15;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// addi r4,r1,1712
	ctx.r4.s64 = ctx.r1.s64 + 1712;
	// lwz r5,136(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// li r12,1715
	r12.s64 = 1715;
	// stdx r10,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r10.u64);
	// ldx r7,r8,r31
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r8.u32 + r31.u32);
	// li r12,1723
	r12.s64 = 1723;
	// stdx r7,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r7.u64);
	// lwzx r6,r11,r31
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + r31.u32);
	// stw r6,1731(r1)
	PPC_STORE_U32(ctx.r1.u32 + 1731, ctx.r6.u32);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
loc_822EECF8:
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// lbz r10,48(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eecac
	if (cr6.lt) goto loc_822EECAC;
loc_822EED0C:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// blt cr6,0x822eed1c
	if (cr6.lt) goto loc_822EED1C;
	// twi 31,r0,22
loc_822EED1C:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eec68
	goto loc_822EEC68;
loc_822EED24:
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x822eb5c8
	sub_822EB5C8(ctx, base);
loc_822EED2C:
	// addi r1,r1,6960
	ctx.r1.s64 = ctx.r1.s64 + 6960;
	// b 0x82ca2c10
	return;
}

__attribute__((alias("__imp__sub_822EED38"))) PPC_WEAK_FUNC(sub_822EED38);
PPC_FUNC_IMPL(__imp__sub_822EED38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stw r5,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r5.u32);
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822eedb4
	if (!cr6.eq) goto loc_822EEDB4;
	// addi r11,r1,148
	r11.s64 = ctx.r1.s64 + 148;
	// addi r10,r31,864
	ctx.r10.s64 = r31.s64 + 864;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822EED6C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822eed8c
	if (!cr0.eq) goto loc_822EED8C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822eed6c
	if (!cr6.eq) goto loc_822EED6C;
loc_822EED8C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822eed9c
	if (!cr6.eq) goto loc_822EED9C;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,1104(r31)
	PPC_STORE_U32(r31.u32 + 1104, ctx.r3.u32);
loc_822EED9C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822EEDB4:
	// lwz r10,1156(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 1156);
	// addi r30,r31,1152
	r30.s64 = r31.s64 + 1152;
	// lwz r11,1160(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 1160);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822eedcc
	if (!cr6.gt) goto loc_822EEDCC;
	// twi 31,r0,22
loc_822EEDCC:
	// mr r31,r10
	r31.u64 = ctx.r10.u64;
loc_822EEDD0:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822eede4
	if (!cr6.gt) goto loc_822EEDE4;
	// twi 31,r0,22
loc_822EEDE4:
	// cmplw cr6,r30,r30
	cr6.compare<uint32_t>(r30.u32, r30.u32, xer);
	// beq cr6,0x822eedf0
	if (cr6.eq) goto loc_822EEDF0;
	// twi 31,r0,22
loc_822EEDF0:
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x822eed9c
	if (cr6.eq) goto loc_822EED9C;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eee04
	if (cr6.lt) goto loc_822EEE04;
	// twi 31,r0,22
loc_822EEE04:
	// addi r11,r31,8
	r11.s64 = r31.s64 + 8;
	// addi r10,r1,148
	ctx.r10.s64 = ctx.r1.s64 + 148;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822EEE10:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822eee30
	if (!cr0.eq) goto loc_822EEE30;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822eee10
	if (!cr6.eq) goto loc_822EEE10;
loc_822EEE30:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822eee40
	if (!cr6.eq) goto loc_822EEE40;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,248(r31)
	PPC_STORE_U32(r31.u32 + 248, ctx.r3.u32);
loc_822EEE40:
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822eee50
	if (cr6.lt) goto loc_822EEE50;
	// twi 31,r0,22
loc_822EEE50:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// b 0x822eedd0
	goto loc_822EEDD0;
}

__attribute__((alias("__imp__sub_822EEE58"))) PPC_WEAK_FUNC(sub_822EEE58);
PPC_FUNC_IMPL(__imp__sub_822EEE58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r3,29136(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 29136);
	// bl 0x823211f0
	sub_823211F0(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r9,1
	ctx.r9.s64 = 1;
	// stb r9,1476(r31)
	PPC_STORE_U8(r31.u32 + 1476, ctx.r9.u8);
	// lwz r3,26920(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26920);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822eeea4
	if (cr6.eq) goto loc_822EEEA4;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lbz r11,26917(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 26917);
	// b 0x822eeea8
	goto loc_822EEEA8;
loc_822EEEA4:
	// li r11,0
	r11.s64 = 0;
loc_822EEEA8:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eeed4
	if (cr6.eq) goto loc_822EEED4;
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lbz r10,196(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 196);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eeed4
	if (cr6.eq) goto loc_822EEED4;
	// lbz r11,92(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 92);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822eeed4
	if (!cr6.eq) goto loc_822EEED4;
	// bl 0x82356348
	sub_82356348(ctx, base);
loc_822EEED4:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EEEE8"))) PPC_WEAK_FUNC(sub_822EEEE8);
PPC_FUNC_IMPL(__imp__sub_822EEEE8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-896(r1)
	ea = -896 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,4408
	ctx.r3.s64 = r11.s64 + 4408;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,5
	cr6.compare<int32_t>(ctx.r10.s32, 5, xer);
	// bne cr6,0x822ef3fc
	if (!cr6.eq) goto loc_822EF3FC;
	// addi r4,r30,3
	ctx.r4.s64 = r30.s64 + 3;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// li r5,401
	ctx.r5.s64 = 401;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// li r28,0
	r28.s64 = 0;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bgt cr6,0x822ef3c0
	if (cr6.gt) goto loc_822EF3C0;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-4272
	r12.s64 = r12.s64 + -4272;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822EF178;
	case 1:
		goto loc_822EEFC4;
	case 2:
		goto loc_822EF024;
	case 3:
		goto loc_822EEF64;
	case 4:
		goto loc_822EF084;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-3720(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -3720);
	// lwz r17,-4156(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -4156);
	// lwz r17,-4060(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -4060);
	// lwz r17,-4252(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -4252);
	// lwz r17,-3964(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -3964);
loc_822EEF64:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,3996
	ctx.r3.s64 = r11.s64 + 3996;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r30,r10,4432
	r30.s64 = ctx.r10.s64 + 4432;
	// addi r3,r9,1108
	ctx.r3.s64 = ctx.r9.s64 + 1108;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eee58
	sub_822EEE58(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r8,1332
	ctx.r4.s64 = ctx.r8.s64 + 1332;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82303aa0
	sub_82303AA0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// b 0x822ef3c0
	goto loc_822EF3C0;
loc_822EEFC4:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4172
	ctx.r3.s64 = r11.s64 + 4172;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r30,r10,4500
	r30.s64 = ctx.r10.s64 + 4500;
	// addi r3,r9,1108
	ctx.r3.s64 = ctx.r9.s64 + 1108;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eee58
	sub_822EEE58(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r8,4540
	ctx.r4.s64 = ctx.r8.s64 + 4540;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82303aa0
	sub_82303AA0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// b 0x822ef3c0
	goto loc_822EF3C0;
loc_822EF024:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4240
	ctx.r3.s64 = r11.s64 + 4240;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r30,r10,4568
	r30.s64 = ctx.r10.s64 + 4568;
	// addi r3,r9,1108
	ctx.r3.s64 = ctx.r9.s64 + 1108;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eee58
	sub_822EEE58(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r8,4636
	ctx.r4.s64 = ctx.r8.s64 + 4636;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82303aa0
	sub_82303AA0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// b 0x822ef3c0
	goto loc_822EF3C0;
loc_822EF084:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4200
	ctx.r3.s64 = r11.s64 + 4200;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r29,1
	r29.s64 = 1;
	// lwz r10,26920(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26920);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ef0b0
	if (cr6.eq) goto loc_822EF0B0;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lbz r11,26917(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 26917);
	// b 0x822ef0b4
	goto loc_822EF0B4;
loc_822EF0B0:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_822EF0B4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef0d4
	if (cr6.eq) goto loc_822EF0D4;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lbz r10,196(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 196);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ef0d4
	if (cr6.eq) goto loc_822EF0D4;
	// mr r29,r28
	r29.u64 = r28.u64;
loc_822EF0D4:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r30,r11,4568
	r30.s64 = r11.s64 + 4568;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eee58
	sub_822EEE58(ctx, base);
	// lwz r9,509(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 509);
	// li r5,-1
	ctx.r5.s64 = -1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822ef14c
	if (cr6.eq) goto loc_822EF14C;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r4,r1,133
	ctx.r4.s64 = ctx.r1.s64 + 133;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r30,26808(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 26808);
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// lwz r5,509(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 509);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82339090
	sub_82339090(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82303b50
	sub_82303B50(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821c6868
	sub_821C6868(ctx, base);
	// b 0x822ef3c0
	goto loc_822EF3C0;
loc_822EF14C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,4636
	ctx.r4.s64 = r11.s64 + 4636;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82303aa0
	sub_82303AA0(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// b 0x822ef3c0
	goto loc_822EF3C0;
loc_822EF178:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4272
	ctx.r3.s64 = r11.s64 + 4272;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lbz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 132);
	// li r6,64
	ctx.r6.s64 = 64;
	// addi r5,r1,133
	ctx.r5.s64 = ctx.r1.s64 + 133;
	// li r4,64
	ctx.r4.s64 = 64;
	// addi r3,r31,905
	ctx.r3.s64 = r31.s64 + 905;
	// stb r10,904(r31)
	PPC_STORE_U8(r31.u32 + 904, ctx.r10.u8);
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r1,197
	ctx.r5.s64 = ctx.r1.s64 + 197;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r31,976
	ctx.r3.s64 = r31.s64 + 976;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r1,229
	ctx.r5.s64 = ctx.r1.s64 + 229;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,1008
	ctx.r3.s64 = r31.s64 + 1008;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// ld r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 116);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r26,r9,20984
	r26.s64 = ctx.r9.s64 + 20984;
	// li r4,1
	ctx.r4.s64 = 1;
	// std r8,856(r31)
	PPC_STORE_U64(r31.u32 + 856, ctx.r8.u64);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r6,420(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 420);
	// addi r30,r31,232
	r30.s64 = r31.s64 + 232;
	// addi r29,r31,856
	r29.s64 = r31.s64 + 856;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r28,420(r31)
	PPC_STORE_U32(r31.u32 + 420, r28.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r6,428(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 428);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,3
	ctx.r4.s64 = 3;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r28,428(r31)
	PPC_STORE_U32(r31.u32 + 428, r28.u32);
	// lwz r5,245(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 245);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,416(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 416);
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r25,416(r31)
	PPC_STORE_U32(r31.u32 + 416, r25.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r6,424(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 424);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,2
	ctx.r4.s64 = 2;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r28,424(r31)
	PPC_STORE_U32(r31.u32 + 424, r28.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,249(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 249);
	// bl 0x82334678
	sub_82334678(ctx, base);
	// ld r7,124(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 124);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// std r7,368(r31)
	PPC_STORE_U64(r31.u32 + 368, ctx.r7.u64);
	// bl 0x823041c8
	sub_823041C8(ctx, base);
	// addi r6,r31,512
	ctx.r6.s64 = r31.s64 + 512;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// cmplw cr6,r6,r29
	cr6.compare<uint32_t>(ctx.r6.u32, r29.u32, xer);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bne cr6,0x822ef280
	if (!cr6.eq) goto loc_822EF280;
	// bl 0x82334798
	sub_82334798(ctx, base);
	// b 0x822ef284
	goto loc_822EF284;
loc_822EF280:
	// bl 0x823348b0
	sub_823348B0(ctx, base);
loc_822EF284:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eaca8
	sub_822EACA8(ctx, base);
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,528
	ctx.r3.s64 = ctx.r1.s64 + 528;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// ld r11,116(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 116);
	// lbz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 132);
	// stw r27,536(r1)
	PPC_STORE_U32(ctx.r1.u32 + 536, r27.u32);
	// std r11,528(r1)
	PPC_STORE_U64(ctx.r1.u32 + 528, r11.u64);
	// stb r10,576(r1)
	PPC_STORE_U8(ctx.r1.u32 + 576, ctx.r10.u8);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,776(r1)
	PPC_STORE_U32(ctx.r1.u32 + 776, ctx.r3.u32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// addi r3,r9,4296
	ctx.r3.s64 = ctx.r9.s64 + 4296;
	// li r4,2435
	ctx.r4.s64 = 2435;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,540
	ctx.r4.s64 = ctx.r1.s64 + 540;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82cf9ad0
	sub_82CF9AD0(ctx, base);
	// mr r30,r28
	r30.u64 = r28.u64;
loc_822EF2E0:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// addi r9,r1,780
	ctx.r9.s64 = ctx.r1.s64 + 780;
	// clrlwi r30,r11,24
	r30.u64 = r11.u32 & 0xFF;
	// addi r8,r3,-5000
	ctx.r8.s64 = ctx.r3.s64 + -5000;
	// cmplwi cr6,r30,4
	cr6.compare<uint32_t>(r30.u32, 4, xer);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// blt cr6,0x822ef2e0
	if (cr6.lt) goto loc_822EF2E0;
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r1,197
	ctx.r5.s64 = ctx.r1.s64 + 197;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r1,648
	ctx.r3.s64 = ctx.r1.s64 + 648;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,64
	ctx.r6.s64 = 64;
	// addi r5,r1,133
	ctx.r5.s64 = ctx.r1.s64 + 133;
	// li r4,64
	ctx.r4.s64 = 64;
	// addi r3,r1,577
	ctx.r3.s64 = ctx.r1.s64 + 577;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r1,229
	ctx.r5.s64 = ctx.r1.s64 + 229;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,680
	ctx.r3.s64 = ctx.r1.s64 + 680;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// addi r4,r1,528
	ctx.r4.s64 = ctx.r1.s64 + 528;
	// addi r3,r31,1152
	ctx.r3.s64 = r31.s64 + 1152;
	// bl 0x822f0600
	sub_822F0600(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eb5c8
	sub_822EB5C8(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x827ad8c0
	sub_827AD8C0(ctx, base);
	// stb r28,94(r1)
	PPC_STORE_U8(ctx.r1.u32 + 94, r28.u8);
	// sth r28,92(r1)
	PPC_STORE_U16(ctx.r1.u32 + 92, r28.u16);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82c816e8
	sub_82C816E8(ctx, base);
	// li r11,18248
	r11.s64 = 18248;
	// lis r29,-31927
	r29.s64 = -2092367872;
	// sth r11,92(r1)
	PPC_STORE_U16(ctx.r1.u32 + 92, r11.u16);
	// addi r4,r1,253
	ctx.r4.s64 = ctx.r1.s64 + 253;
	// lwz r30,29136(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 29136);
	// addi r3,r30,64
	ctx.r3.s64 = r30.s64 + 64;
	// bl 0x82275368
	sub_82275368(ctx, base);
	// addi r3,r30,68
	ctx.r3.s64 = r30.s64 + 68;
	// addi r4,r1,381
	ctx.r4.s64 = ctx.r1.s64 + 381;
	// bl 0x82275368
	sub_82275368(ctx, base);
	// lwz r11,29136(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 29136);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stb r10,55(r11)
	PPC_STORE_U8(r11.u32 + 55, ctx.r10.u8);
	// stb r10,218(r11)
	PPC_STORE_U8(r11.u32 + 218, ctx.r10.u8);
	// stb r10,1328(r31)
	PPC_STORE_U8(r31.u32 + 1328, ctx.r10.u8);
	// lwz r3,29136(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 29136);
	// bl 0x82317970
	sub_82317970(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x829ce870
	sub_829CE870(ctx, base);
loc_822EF3C0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,4096
	ctx.r4.s64 = 4096;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822ef3e8
	if (!cr6.eq) goto loc_822EF3E8;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r28,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r28.u32);
loc_822EF3E8:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,4096
	r11.s64 = 4096;
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r28,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r28.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
loc_822EF3FC:
	// addi r1,r1,896
	ctx.r1.s64 = ctx.r1.s64 + 896;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822EF408"))) PPC_WEAK_FUNC(sub_822EF408);
PPC_FUNC_IMPL(__imp__sub_822EF408) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-608(r1)
	ea = -608 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r4,r4,3
	ctx.r4.s64 = ctx.r4.s64 + 3;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// li r5,157
	ctx.r5.s64 = 157;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,1156(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 1156);
	// lwz r10,1160(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 1160);
	// addi r28,r27,1152
	r28.s64 = r27.s64 + 1152;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ef440
	if (!cr6.gt) goto loc_822EF440;
	// twi 31,r0,22
loc_822EF440:
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lwz r8,4(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r9,8(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// ld r7,96(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r29,84(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_822EF464:
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// ble cr6,0x822ef470
	if (!cr6.gt) goto loc_822EF470;
	// twi 31,r0,22
loc_822EF470:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ef480
	if (cr6.eq) goto loc_822EF480;
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// beq cr6,0x822ef484
	if (cr6.eq) goto loc_822EF484;
loc_822EF480:
	// twi 31,r0,22
loc_822EF484:
	// cmplw cr6,r29,r9
	cr6.compare<uint32_t>(r29.u32, ctx.r9.u32, xer);
	// beq cr6,0x822ef4e0
	if (cr6.eq) goto loc_822EF4E0;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ef498
	if (!cr6.eq) goto loc_822EF498;
	// twi 31,r0,22
loc_822EF498:
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x822ef4a8
	if (cr6.lt) goto loc_822EF4A8;
	// twi 31,r0,22
loc_822EF4A8:
	// ld r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// cmpld cr6,r6,r7
	cr6.compare<uint64_t>(ctx.r6.u64, ctx.r7.u64, xer);
	// beq cr6,0x822ef4c8
	if (cr6.eq) goto loc_822EF4C8;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x822ef4c0
	if (cr6.lt) goto loc_822EF4C0;
	// twi 31,r0,22
loc_822EF4C0:
	// addi r29,r29,296
	r29.s64 = r29.s64 + 296;
	// b 0x822ef464
	goto loc_822EF464;
loc_822EF4C8:
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x822ef4d8
	if (cr6.lt) goto loc_822EF4D8;
	// twi 31,r0,22
loc_822EF4D8:
	// mr r31,r29
	r31.u64 = r29.u64;
	// b 0x822ef56c
	goto loc_822EF56C;
loc_822EF4E0:
	// lbz r10,140(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 140);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ef744
	if (cr6.eq) goto loc_822EF744;
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// addi r4,r1,256
	ctx.r4.s64 = ctx.r1.s64 + 256;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822f0600
	sub_822F0600(ctx, base);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822ef51c
	if (!cr6.gt) goto loc_822EF51C;
	// twi 31,r0,22
loc_822EF51C:
	// lwz r9,8(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// addi r10,r11,-296
	ctx.r10.s64 = r11.s64 + -296;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// bgt cr6,0x822ef548
	if (cr6.gt) goto loc_822EF548;
	// lwz r9,4(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bge cr6,0x822ef54c
	if (!cr6.lt) goto loc_822EF54C;
loc_822EF548:
	// twi 31,r0,22
loc_822EF54C:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r31,r11,-296
	r31.s64 = r11.s64 + -296;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// blt cr6,0x822ef564
	if (cr6.lt) goto loc_822EF564;
	// twi 31,r0,22
loc_822EF564:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822ef744
	if (cr6.eq) goto loc_822EF744;
loc_822EF56C:
	// lbz r11,140(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef66c
	if (cr6.eq) goto loc_822EF66C;
	// stb r11,48(r31)
	PPC_STORE_U8(r31.u32 + 48, r11.u8);
	// addi r3,r31,12
	ctx.r3.s64 = r31.s64 + 12;
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// li r5,36
	ctx.r5.s64 = 36;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r4,2514
	ctx.r4.s64 = 2514;
	// ld r7,244(r27)
	ctx.r7.u64 = PPC_LOAD_U64(r27.u32 + 244);
	// addi r3,r10,4672
	ctx.r3.s64 = ctx.r10.s64 + 4672;
	// lwz r5,104(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lhz r6,112(r1)
	ctx.r6.u64 = PPC_LOAD_U16(ctx.r1.u32 + 112);
	// addi r30,r27,244
	r30.s64 = r27.s64 + 244;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r5,r31,8
	ctx.r5.s64 = r31.s64 + 8;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82cf9ab8
	sub_82CF9AB8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r4,2516
	ctx.r4.s64 = 2516;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r3,r8,4792
	ctx.r3.s64 = ctx.r8.s64 + 4792;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r6,64
	ctx.r6.s64 = 64;
	// addi r5,r1,141
	ctx.r5.s64 = ctx.r1.s64 + 141;
	// li r4,64
	ctx.r4.s64 = 64;
	// addi r3,r31,49
	ctx.r3.s64 = r31.s64 + 49;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,32
	ctx.r6.s64 = 32;
	// addi r5,r1,205
	ctx.r5.s64 = ctx.r1.s64 + 205;
	// li r4,32
	ctx.r4.s64 = 32;
	// addi r3,r31,120
	ctx.r3.s64 = r31.s64 + 120;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// li r6,16
	ctx.r6.s64 = 16;
	// addi r5,r1,237
	ctx.r5.s64 = ctx.r1.s64 + 237;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,152
	ctx.r3.s64 = r31.s64 + 152;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r27,8
	ctx.r3.s64 = r27.s64 + 8;
	// bl 0x823041c8
	sub_823041C8(ctx, base);
	// addi r7,r27,512
	ctx.r7.s64 = r27.s64 + 512;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// cmplw cr6,r7,r31
	cr6.compare<uint32_t>(ctx.r7.u32, r31.u32, xer);
	// addi r3,r27,232
	ctx.r3.s64 = r27.s64 + 232;
	// bne cr6,0x822ef650
	if (!cr6.eq) goto loc_822EF650;
	// bl 0x82334798
	sub_82334798(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822eaca8
	sub_822EACA8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822eb5c8
	sub_822EB5C8(ctx, base);
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// b 0x82ca2c34
	return;
loc_822EF650:
	// bl 0x823348b0
	sub_823348B0(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822eaca8
	sub_822EACA8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822eb5c8
	sub_822EB5C8(ctx, base);
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// b 0x82ca2c34
	return;
loc_822EF66C:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r27,8
	ctx.r3.s64 = r27.s64 + 8;
	// bl 0x82304368
	sub_82304368(ctx, base);
	// addi r11,r27,512
	r11.s64 = r27.s64 + 512;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bne cr6,0x822ef6f8
	if (!cr6.eq) goto loc_822EF6F8;
	// lwz r3,232(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 232);
	// addi r30,r27,232
	r30.s64 = r27.s64 + 232;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x822ef704
	if (cr6.eq) goto loc_822EF704;
	// li r6,0
	ctx.r6.s64 = 0;
	// lbz r4,48(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// bl 0x82cf9330
	sub_82CF9330(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822ef6c8
	if (cr6.eq) goto loc_822EF6C8;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x82cbc490
	sub_82CBC490(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r11,21212
	ctx.r3.s64 = r11.s64 + 21212;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// b 0x822ef704
	goto loc_822EF704;
loc_822EF6C8:
	// lwz r11,268(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 268);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// lbz r11,48(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 48);
	// beq cr6,0x822ef6e8
	if (cr6.eq) goto loc_822EF6E8;
	// lwz r10,196(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 196);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,196(r30)
	PPC_STORE_U32(r30.u32 + 196, ctx.r9.u32);
	// b 0x822ef704
	goto loc_822EF704;
loc_822EF6E8:
	// lwz r10,192(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 192);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// stw r9,192(r30)
	PPC_STORE_U32(r30.u32 + 192, ctx.r9.u32);
	// b 0x822ef704
	goto loc_822EF704;
loc_822EF6F8:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r27,232
	ctx.r3.s64 = r27.s64 + 232;
	// bl 0x823349d0
	sub_823349D0(ctx, base);
loc_822EF704:
	// addi r31,r29,296
	r31.s64 = r29.s64 + 296;
	// lwz r29,8(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// beq cr6,0x822ef738
	if (cr6.eq) goto loc_822EF738;
	// addi r30,r31,-296
	r30.s64 = r31.s64 + -296;
loc_822EF718:
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x822ef718
	if (!cr6.eq) goto loc_822EF718;
loc_822EF738:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// addi r11,r11,-296
	r11.s64 = r11.s64 + -296;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
loc_822EF744:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822eb5c8
	sub_822EB5C8(ctx, base);
	// addi r1,r1,608
	ctx.r1.s64 = ctx.r1.s64 + 608;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822EF758"))) PPC_WEAK_FUNC(sub_822EF758);
PPC_FUNC_IMPL(__imp__sub_822EF758) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-576(r1)
	ea = -576 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// addi r4,r4,3
	ctx.r4.s64 = ctx.r4.s64 + 3;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// li r5,301
	ctx.r5.s64 = 301;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r22,r20,632
	r22.s64 = r20.s64 + 632;
	// addi r23,r20,788
	r23.s64 = r20.s64 + 788;
	// addi r26,r20,808
	r26.s64 = r20.s64 + 808;
	// li r19,4
	r19.s64 = 4;
	// li r18,-1
	r18.s64 = -1;
	// lbz r21,152(r1)
	r21.u64 = PPC_LOAD_U8(ctx.r1.u32 + 152);
loc_822EF790:
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r21,0
	cr6.compare<uint32_t>(r21.u32, 0, xer);
	// beq cr6,0x822ef8c0
	if (cr6.eq) goto loc_822EF8C0;
	// addi r27,r20,8
	r27.s64 = r20.s64 + 8;
	// addi r28,r1,153
	r28.s64 = ctx.r1.s64 + 153;
	// addi r24,r1,189
	r24.s64 = ctx.r1.s64 + 189;
loc_822EF7A8:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f0ae8
	sub_822F0AE8(ctx, base);
	// ld r30,96(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lwz r29,108(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// lwz r31,104(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// std r30,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r30.u64);
loc_822EF7C8:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef7dc
	if (cr6.eq) goto loc_822EF7DC;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822ef7e0
	if (cr6.eq) goto loc_822EF7E0;
loc_822EF7DC:
	// twi 31,r0,22
loc_822EF7E0:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// beq cr6,0x822ef7f8
	if (cr6.eq) goto loc_822EF7F8;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823d4f20
	sub_823D4F20(ctx, base);
	// b 0x822ef7c8
	goto loc_822EF7C8;
loc_822EF7F8:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// ld r6,104(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822f09d8
	sub_822F09D8(ctx, base);
	// addi r11,r1,185
	r11.s64 = ctx.r1.s64 + 185;
	// lbzx r11,r25,r11
	r11.u64 = PPC_LOAD_U8(r25.u32 + r11.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef850
	if (cr6.eq) goto loc_822EF850;
	// mr r31,r24
	r31.u64 = r24.u64;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_822EF824:
	// ld r11,0(r22)
	r11.u64 = PPC_LOAD_U64(r22.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// cmpld cr6,r11,r10
	cr6.compare<uint64_t>(r11.u64, ctx.r10.u64, xer);
	// bne cr6,0x822ef844
	if (!cr6.eq) goto loc_822EF844;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x822f03c0
	sub_822F03C0(ctx, base);
loc_822EF844:
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// addi r31,r31,8
	r31.s64 = r31.s64 + 8;
	// bne 0x822ef824
	if (!cr0.eq) goto loc_822EF824;
loc_822EF850:
	// lwz r30,0(r23)
	r30.u64 = PPC_LOAD_U32(r23.u32 + 0);
	// ld r31,0(r28)
	r31.u64 = PPC_LOAD_U64(r28.u32 + 0);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,148(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 148);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// bl 0x822f8648
	sub_822F8648(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x822ef884
	if (!cr6.eq) goto loc_822EF884;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82304740
	sub_82304740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822ef88c
	if (cr6.eq) goto loc_822EF88C;
loc_822EF884:
	// mr r6,r18
	ctx.r6.u64 = r18.u64;
	// b 0x822ef890
	goto loc_822EF890;
loc_822EF88C:
	// li r6,0
	ctx.r6.s64 = 0;
loc_822EF890:
	// lwz r3,0(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,80(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 80);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r25,r25,1
	r25.s64 = r25.s64 + 1;
	// addi r24,r24,64
	r24.s64 = r24.s64 + 64;
	// addi r28,r28,8
	r28.s64 = r28.s64 + 8;
	// cmplw cr6,r25,r21
	cr6.compare<uint32_t>(r25.u32, r21.u32, xer);
	// blt cr6,0x822ef7a8
	if (cr6.lt) goto loc_822EF7A8;
loc_822EF8C0:
	// addic. r19,r19,-1
	xer.ca = r19.u32 > 0;
	r19.s64 = r19.s64 + -1;
	cr0.compare<int32_t>(r19.s32, 0, xer);
	// addi r23,r23,4
	r23.s64 = r23.s64 + 4;
	// addi r26,r26,12
	r26.s64 = r26.s64 + 12;
	// addi r22,r22,8
	r22.s64 = r22.s64 + 8;
	// bne 0x822ef790
	if (!cr0.eq) goto loc_822EF790;
	// addi r1,r1,576
	ctx.r1.s64 = ctx.r1.s64 + 576;
	// b 0x82ca2c10
	return;
}

__attribute__((alias("__imp__sub_822EF8E0"))) PPC_WEAK_FUNC(sub_822EF8E0);
PPC_FUNC_IMPL(__imp__sub_822EF8E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,1156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1156);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef928
	if (cr6.eq) goto loc_822EF928;
	// lwz r9,1160(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1160);
	// li r10,296
	ctx.r10.s64 = 296;
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// divw. r7,r8,r10
	ctx.r7.s32 = ctx.r8.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x822ef928
	if (cr0.eq) goto loc_822EF928;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef91c
	if (cr6.eq) goto loc_822EF91C;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// divw. r7,r8,r10
	ctx.r7.s32 = ctx.r8.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x822ef920
	if (!cr0.eq) goto loc_822EF920;
loc_822EF91C:
	// twi 31,r0,22
loc_822EF920:
	// ld r3,120(r11)
	ctx.r3.u64 = PPC_LOAD_U64(r11.u32 + 120);
	// blr 
	return;
loc_822EF928:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EF930"))) PPC_WEAK_FUNC(sub_822EF930);
PPC_FUNC_IMPL(__imp__sub_822EF930) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,1156(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1156);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef98c
	if (cr6.eq) goto loc_822EF98C;
	// lwz r9,1160(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 1160);
	// li r10,296
	ctx.r10.s64 = 296;
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// divw. r7,r8,r10
	ctx.r7.s32 = ctx.r8.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x822ef98c
	if (cr0.eq) goto loc_822EF98C;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ef980
	if (cr6.eq) goto loc_822EF980;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// divw. r7,r8,r10
	ctx.r7.s32 = ctx.r8.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x822ef984
	if (!cr0.eq) goto loc_822EF984;
loc_822EF980:
	// twi 31,r0,22
loc_822EF984:
	// addi r4,r11,49
	ctx.r4.s64 = r11.s64 + 49;
	// b 0x822ef994
	goto loc_822EF994;
loc_822EF98C:
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r4,r11,3224
	ctx.r4.s64 = r11.s64 + 3224;
loc_822EF994:
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822EF9B8"))) PPC_WEAK_FUNC(sub_822EF9B8);
PPC_FUNC_IMPL(__imp__sub_822EF9B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r17{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-2704(r1)
	ea = -2704 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r12,-32209
	r12.s64 = -2110849024;
	// addi r12,r12,-1568
	r12.s64 = r12.s64 + -1568;
	// rlwinm r0,r4,2,0,29
	r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	return;
	// lwz r17,-1536(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -1536);
	// lwz r17,-904(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -904);
	// lwz r17,-800(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -800);
	// lwz r17,-704(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -704);
	// lwz r17,-616(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -616);
	// lwz r17,-520(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -520);
	// lwz r17,-464(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -464);
	// lwz r17,-376(r14)
	r17.u64 = PPC_LOAD_U32(r14.u32 + -376);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,4896
	ctx.r3.s64 = r11.s64 + 4896;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// cmpwi cr6,r10,2
	cr6.compare<int32_t>(ctx.r10.s32, 2, xer);
	// bne cr6,0x822efae8
	if (!cr6.eq) goto loc_822EFAE8;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822efa84
	if (cr6.eq) goto loc_822EFA84;
	// li r28,1
	r28.s64 = 1;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r28,780(r31)
	PPC_STORE_U32(r31.u32 + 780, r28.u32);
	// addi r29,r11,1056
	r29.s64 = r11.s64 + 1056;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8217ab30
	sub_8217AB30(ctx, base);
	// lwz r11,240(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 240);
	// addi r30,r31,232
	r30.s64 = r31.s64 + 232;
	// rlwinm r5,r3,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 1) & 0xFFFFFFFE;
	// li r7,4
	ctx.r7.s64 = 4;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822efa58
	if (!cr6.eq) goto loc_822EFA58;
	// li r7,5
	ctx.r7.s64 = 5;
loc_822EFA58:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r3,r30,12
	ctx.r3.s64 = r30.s64 + 12;
	// bl 0x82cf9b28
	sub_82CF9B28(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822efa80
	if (cr6.eq) goto loc_822EFA80;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,21016
	ctx.r3.s64 = r11.s64 + 21016;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822EFA80:
	// stw r28,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r28.u32);
loc_822EFA84:
	// addi r4,r31,512
	ctx.r4.s64 = r31.s64 + 512;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334798
	sub_82334798(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eaca8
	sub_822EACA8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eb5c8
	sub_822EB5C8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,4096
	ctx.r4.s64 = 4096;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822efacc
	if (!cr6.eq) goto loc_822EFACC;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r29,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r29.u32);
loc_822EFACC:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,4096
	r11.s64 = 4096;
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
loc_822EFAE8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// ld r7,244(r31)
	ctx.r7.u64 = PPC_LOAD_U64(r31.u32 + 244);
	// li r4,2736
	ctx.r4.s64 = 2736;
	// lhz r6,260(r31)
	ctx.r6.u64 = PPC_LOAD_U16(r31.u32 + 260);
	// addi r3,r11,4672
	ctx.r3.s64 = r11.s64 + 4672;
	// lwz r5,252(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// addi r29,r31,244
	r29.s64 = r31.s64 + 244;
	// addi r28,r31,252
	r28.s64 = r31.s64 + 252;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r30,r31,864
	r30.s64 = r31.s64 + 864;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82cf9ab8
	sub_82CF9AB8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822efb94
	if (cr6.eq) goto loc_822EFB94;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r30,r11,4952
	r30.s64 = r11.s64 + 4952;
	// addi r3,r10,1108
	ctx.r3.s64 = ctx.r10.s64 + 1108;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// li r4,4096
	ctx.r4.s64 = 4096;
	// addi r3,r9,1716
	ctx.r3.s64 = ctx.r9.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r8,456(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r8,4
	cr6.compare<int32_t>(ctx.r8.s32, 4, xer);
	// bne cr6,0x822efb78
	if (!cr6.eq) goto loc_822EFB78;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r29,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r29.u32);
loc_822EFB78:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,4096
	r11.s64 = 4096;
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
loc_822EFB94:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r4,2745
	ctx.r4.s64 = 2745;
	// addi r3,r11,4792
	ctx.r3.s64 = r11.s64 + 4792;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r4,r31,512
	ctx.r4.s64 = r31.s64 + 512;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334798
	sub_82334798(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822eaca8
	sub_822EACA8(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lis r28,-31927
	r28.s64 = -2092367872;
	// ld r5,512(r31)
	ctx.r5.u64 = PPC_LOAD_U64(r31.u32 + 512);
	// li r9,1
	ctx.r9.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// stb r9,107(r1)
	PPC_STORE_U8(ctx.r1.u32 + 107, ctx.r9.u8);
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// lwz r11,26808(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26808);
	// lwz r3,26932(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 26932);
	// stb r29,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, r29.u8);
	// addi r8,r3,3
	ctx.r8.s64 = ctx.r3.s64 + 3;
	// li r12,99
	r12.s64 = 99;
	// stdx r5,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r5.u64);
	// mr r7,r3
	ctx.r7.u64 = ctx.r3.u64;
	// stw r11,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r11.u32);
	// rlwinm r6,r8,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,780(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 780);
	// stw r11,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, r11.u32);
	// lwzx r10,r6,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + r31.u32);
	// stw r10,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r10.u32);
	// stb r7,172(r1)
	PPC_STORE_U8(ctx.r1.u32 + 172, ctx.r7.u8);
	// bl 0x82cbc440
	sub_82CBC440(ctx, base);
	// li r5,16
	ctx.r5.s64 = 16;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r3,26932(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 26932);
	// bl 0x82cbc148
	sub_82CBC148(ctx, base);
	// stb r29,95(r1)
	PPC_STORE_U8(ctx.r1.u32 + 95, r29.u8);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,108
	ctx.r3.s64 = ctx.r1.s64 + 108;
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// li r4,2770
	ctx.r4.s64 = 2770;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r3,r9,5032
	ctx.r3.s64 = ctx.r9.s64 + 5032;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,500(r31)
	PPC_STORE_U32(r31.u32 + 500, ctx.r3.u32);
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5140
	ctx.r3.s64 = r11.s64 + 5140;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4098
	cr6.compare<int32_t>(ctx.r10.s32, 4098, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// lwz r11,236(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 236);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822efcc8
	if (!cr6.eq) goto loc_822EFCC8;
	// li r11,6
	r11.s64 = 6;
	// lwz r5,864(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 864);
	// addi r4,r1,1376
	ctx.r4.s64 = ctx.r1.s64 + 1376;
	// stb r11,1378(r1)
	PPC_STORE_U8(ctx.r1.u32 + 1378, r11.u8);
	// bl 0x822eb8e0
	sub_822EB8E0(ctx, base);
	// li r4,4099
	ctx.r4.s64 = 4099;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
loc_822EFCC8:
	// bl 0x822eb720
	sub_822EB720(ctx, base);
	// li r4,4099
	ctx.r4.s64 = 4099;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5200
	ctx.r3.s64 = r11.s64 + 5200;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4098
	cr6.compare<int32_t>(ctx.r10.s32, 4098, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,4096
	ctx.r4.s64 = 4096;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822efd24
	if (!cr6.eq) goto loc_822EFD24;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r29,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r29.u32);
loc_822EFD24:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,4096
	r11.s64 = 4096;
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5264
	ctx.r3.s64 = r11.s64 + 5264;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4100
	cr6.compare<int32_t>(ctx.r10.s32, 4100, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,4101
	ctx.r4.s64 = 4101;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822efd84
	if (!cr6.eq) goto loc_822EFD84;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r29,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r29.u32);
loc_822EFD84:
	// li r11,4101
	r11.s64 = 4101;
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5320
	ctx.r3.s64 = r11.s64 + 5320;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4100
	cr6.compare<int32_t>(ctx.r10.s32, 4100, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,4096
	ctx.r4.s64 = 4096;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822efddc
	if (!cr6.eq) goto loc_822EFDDC;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// stw r29,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r29.u32);
loc_822EFDDC:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,4096
	r11.s64 = 4096;
	// stw r3,504(r31)
	PPC_STORE_U32(r31.u32 + 504, ctx.r3.u32);
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5380
	ctx.r3.s64 = r11.s64 + 5380;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4102
	cr6.compare<int32_t>(ctx.r10.s32, 4102, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// li r4,2
	ctx.r4.s64 = 2;
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x82334b98
	sub_82334B98(ctx, base);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5436
	ctx.r3.s64 = r11.s64 + 5436;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// cmpwi cr6,r10,4102
	cr6.compare<int32_t>(ctx.r10.s32, 4102, xer);
	// bne cr6,0x822eff50
	if (!cr6.eq) goto loc_822EFF50;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r4,4101
	ctx.r4.s64 = 4101;
	// addi r3,r11,1716
	ctx.r3.s64 = r11.s64 + 1716;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,4
	cr6.compare<int32_t>(ctx.r10.s32, 4, xer);
	// bne cr6,0x822efd84
	if (!cr6.eq) goto loc_822EFD84;
	// lwz r3,1204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1204);
	// bl 0x82cf9b48
	sub_82CF9B48(ctx, base);
	// li r11,4101
	r11.s64 = 4101;
	// stw r29,1168(r31)
	PPC_STORE_U32(r31.u32 + 1168, r29.u32);
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
	// stw r11,456(r31)
	PPC_STORE_U32(r31.u32 + 456, r11.u32);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,5496
	ctx.r3.s64 = r11.s64 + 5496;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,456(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 456);
	// li r29,0
	r29.s64 = 0;
	// cmpwi cr6,r10,7
	cr6.compare<int32_t>(ctx.r10.s32, 7, xer);
	// bne cr6,0x822efedc
	if (!cr6.eq) goto loc_822EFEDC;
	// lbz r11,6(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 6);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822efec8
	if (cr6.eq) goto loc_822EFEC8;
	// lwz r4,780(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 780);
	// bl 0x822eae30
	sub_822EAE30(ctx, base);
	// stb r29,1476(r31)
	PPC_STORE_U8(r31.u32 + 1476, r29.u8);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
loc_822EFEC8:
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
	// stb r29,1476(r31)
	PPC_STORE_U8(r31.u32 + 1476, r29.u8);
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
loc_822EFEDC:
	// lbz r11,1476(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 1476);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eff4c
	if (cr6.eq) goto loc_822EFF4C;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r10,26912(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 26912);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822eff04
	if (cr6.eq) goto loc_822EFF04;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lbz r11,26821(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 26821);
	// b 0x822eff08
	goto loc_822EFF08;
loc_822EFF04:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822EFF08:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822eff4c
	if (cr6.eq) goto loc_822EFF4C;
	// lwz r11,12(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// lbz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 256);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822eff4c
	if (!cr6.eq) goto loc_822EFF4C;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r9,1
	ctx.r9.s64 = 1;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r11,29136(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 29136);
	// stb r9,245(r11)
	PPC_STORE_U8(r11.u32 + 245, ctx.r9.u8);
	// lwz r3,29136(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 29136);
	// bl 0x8230f690
	sub_8230F690(ctx, base);
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822ebd28
	sub_822EBD28(ctx, base);
loc_822EFF4C:
	// stb r29,1476(r31)
	PPC_STORE_U8(r31.u32 + 1476, r29.u8);
loc_822EFF50:
	// addi r1,r1,2704
	ctx.r1.s64 = ctx.r1.s64 + 2704;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822EFF58"))) PPC_WEAK_FUNC(sub_822EFF58);
PPC_FUNC_IMPL(__imp__sub_822EFF58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// li r30,0
	r30.s64 = 0;
	// addi r31,r10,6904
	r31.s64 = ctx.r10.s64 + 6904;
	// mr r11,r30
	r11.u64 = r30.u64;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// li r3,40
	ctx.r3.s64 = 40;
	// stb r10,4(r31)
	PPC_STORE_U8(r31.u32 + 4, ctx.r10.u8);
	// stb r9,5(r31)
	PPC_STORE_U8(r31.u32 + 5, ctx.r9.u8);
	// stb r8,6(r31)
	PPC_STORE_U8(r31.u32 + 6, ctx.r8.u8);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822effa4
	if (cr6.eq) goto loc_822EFFA4;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
loc_822EFFA4:
	// addic. r11,r3,4
	xer.ca = ctx.r3.u32 > 4294967291;
	r11.s64 = ctx.r3.s64 + 4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822effb0
	if (cr0.eq) goto loc_822EFFB0;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822EFFB0:
	// addic. r11,r3,8
	xer.ca = ctx.r3.u32 > 4294967287;
	r11.s64 = ctx.r3.s64 + 8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822effbc
	if (cr0.eq) goto loc_822EFFBC;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822EFFBC:
	// li r29,1
	r29.s64 = 1;
	// stb r30,33(r3)
	PPC_STORE_U8(ctx.r3.u32 + 33, r30.u8);
	// stb r29,32(r3)
	PPC_STORE_U8(ctx.r3.u32 + 32, r29.u8);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r3,164(r31)
	PPC_STORE_U32(r31.u32 + 164, ctx.r3.u32);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// stb r29,33(r3)
	PPC_STORE_U8(ctx.r3.u32 + 33, r29.u8);
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r19,r29
	r19.u64 = r29.u64;
	// mr r28,r30
	r28.u64 = r30.u64;
	// li r27,-1
	r27.s64 = -1;
	// mr r26,r30
	r26.u64 = r30.u64;
	// mr r25,r30
	r25.u64 = r30.u64;
	// mr r24,r30
	r24.u64 = r30.u64;
	// mr r23,r30
	r23.u64 = r30.u64;
	// mr r22,r30
	r22.u64 = r30.u64;
	// li r21,46
	r21.s64 = 46;
	// mr r20,r30
	r20.u64 = r30.u64;
	// mr r18,r29
	r18.u64 = r29.u64;
	// li r5,60
	ctx.r5.s64 = 60;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,244
	ctx.r3.s64 = r31.s64 + 244;
	// addi r10,r31,28
	ctx.r10.s64 = r31.s64 + 28;
	// addi r10,r31,12
	ctx.r10.s64 = r31.s64 + 12;
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r11,164(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// stw r11,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r11.u32);
	// stw r9,168(r31)
	PPC_STORE_U32(r31.u32 + 168, ctx.r9.u32);
	// stw r8,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r8.u32);
	// stw r30,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r30.u32);
	// stw r30,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r30.u32);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// stw r7,176(r31)
	PPC_STORE_U32(r31.u32 + 176, ctx.r7.u32);
	// stw r6,180(r31)
	PPC_STORE_U32(r31.u32 + 180, ctx.r6.u32);
	// stb r29,444(r31)
	PPC_STORE_U8(r31.u32 + 444, r29.u8);
	// stb r30,445(r31)
	PPC_STORE_U8(r31.u32 + 445, r30.u8);
	// stw r28,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r28.u32);
	// stw r27,232(r31)
	PPC_STORE_U32(r31.u32 + 232, r27.u32);
	// stw r26,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r26.u32);
	// stw r25,432(r31)
	PPC_STORE_U32(r31.u32 + 432, r25.u32);
	// stw r24,236(r31)
	PPC_STORE_U32(r31.u32 + 236, r24.u32);
	// stw r23,240(r31)
	PPC_STORE_U32(r31.u32 + 240, r23.u32);
	// stw r22,380(r31)
	PPC_STORE_U32(r31.u32 + 380, r22.u32);
	// stw r21,376(r31)
	PPC_STORE_U32(r31.u32 + 376, r21.u32);
	// stw r20,436(r31)
	PPC_STORE_U32(r31.u32 + 436, r20.u32);
	// stw r19,416(r31)
	PPC_STORE_U32(r31.u32 + 416, r19.u32);
	// stw r18,420(r31)
	PPC_STORE_U32(r31.u32 + 420, r18.u32);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r5,296
	ctx.r5.s64 = 296;
	// std r30,368(r31)
	PPC_STORE_U64(r31.u32 + 368, r30.u64);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,512
	ctx.r3.s64 = r31.s64 + 512;
	// addi r11,r31,368
	r11.s64 = r31.s64 + 368;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r28,3
	r28.s64 = 3;
	// addi r27,r31,808
	r27.s64 = r31.s64 + 808;
loc_822F00C0:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822f0330
	sub_822F0330(ctx, base);
	// addic. r28,r28,-1
	xer.ca = r28.u32 > 0;
	r28.s64 = r28.s64 + -1;
	cr0.compare<int32_t>(r28.s32, 0, xer);
	// addi r27,r27,12
	r27.s64 = r27.s64 + 12;
	// bge 0x822f00c0
	if (!cr0.lt) goto loc_822F00C0;
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r31,856
	ctx.r3.s64 = r31.s64 + 856;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r11,r30
	r11.u64 = r30.u64;
	// addi r28,r10,29136
	r28.s64 = ctx.r10.s64 + 29136;
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// stw r11,1156(r31)
	PPC_STORE_U32(r31.u32 + 1156, r11.u32);
	// mr r9,r30
	ctx.r9.u64 = r30.u64;
	// stw r11,1164(r31)
	PPC_STORE_U32(r31.u32 + 1164, r11.u32);
	// mr r8,r30
	ctx.r8.u64 = r30.u64;
	// stw r10,1160(r31)
	PPC_STORE_U32(r31.u32 + 1160, ctx.r10.u32);
	// stb r9,1328(r31)
	PPC_STORE_U8(r31.u32 + 1328, ctx.r9.u8);
	// lwz r7,0(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// stb r10,1329(r31)
	PPC_STORE_U8(r31.u32 + 1329, ctx.r10.u8);
	// stw r11,1344(r31)
	PPC_STORE_U32(r31.u32 + 1344, r11.u32);
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// stw r8,1348(r31)
	PPC_STORE_U32(r31.u32 + 1348, ctx.r8.u32);
	// stw r9,1352(r31)
	PPC_STORE_U32(r31.u32 + 1352, ctx.r9.u32);
	// stw r10,1360(r31)
	PPC_STORE_U32(r31.u32 + 1360, ctx.r10.u32);
	// stw r11,1364(r31)
	PPC_STORE_U32(r31.u32 + 1364, r11.u32);
	// stw r8,1368(r31)
	PPC_STORE_U32(r31.u32 + 1368, ctx.r8.u32);
	// stw r9,1372(r31)
	PPC_STORE_U32(r31.u32 + 1372, ctx.r9.u32);
	// stb r10,1376(r31)
	PPC_STORE_U8(r31.u32 + 1376, ctx.r10.u8);
	// stb r11,1377(r31)
	PPC_STORE_U8(r31.u32 + 1377, r11.u8);
	// stb r8,1378(r31)
	PPC_STORE_U8(r31.u32 + 1378, ctx.r8.u8);
	// stw r9,1464(r31)
	PPC_STORE_U32(r31.u32 + 1464, ctx.r9.u32);
	// stb r10,1468(r31)
	PPC_STORE_U8(r31.u32 + 1468, ctx.r10.u8);
	// stw r11,1472(r31)
	PPC_STORE_U32(r31.u32 + 1472, r11.u32);
	// stb r8,1476(r31)
	PPC_STORE_U8(r31.u32 + 1476, ctx.r8.u8);
	// bne cr6,0x822f0184
	if (!cr6.eq) goto loc_822F0184;
	// li r3,328
	ctx.r3.s64 = 328;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f0170
	if (cr6.eq) goto loc_822F0170;
	// bl 0x8231e908
	sub_8231E908(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822f0174
	goto loc_822F0174;
loc_822F0170:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
loc_822F0174:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822f0518
	sub_822F0518(ctx, base);
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// bl 0x82309f00
	sub_82309F00(ctx, base);
loc_822F0184:
	// mr r11,r29
	r11.u64 = r29.u64;
	// stw r29,1336(r31)
	PPC_STORE_U32(r31.u32 + 1336, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,1332(r31)
	PPC_STORE_U32(r31.u32 + 1332, r11.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c10
	return;
}

__attribute__((alias("__imp__sub_822F01A0"))) PPC_WEAK_FUNC(sub_822F01A0);
PPC_FUNC_IMPL(__imp__sub_822F01A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r31,r3,344
	r31.s64 = ctx.r3.s64 + 344;
	// li r30,3
	r30.s64 = 3;
	// li r29,0
	r29.s64 = 0;
loc_822F01B8:
	// addi r31,r31,-12
	r31.s64 = r31.s64 + -12;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x822f09d8
	sub_822F09D8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// stw r29,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r29.u32);
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
	// bge 0x822f01b8
	if (!cr0.lt) goto loc_822F01B8;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F0208"))) PPC_WEAK_FUNC(sub_822F0208);
PPC_FUNC_IMPL(__imp__sub_822F0208) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mullw r11,r29,r5
	r11.s64 = int64_t(r29.s32) * int64_t(ctx.r5.s32);
	// addic. r31,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	r31.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// add r30,r11,r3
	r30.u64 = r11.u64 + ctx.r3.u64;
	// blt 0x822f0244
	if (cr0.lt) goto loc_822F0244;
loc_822F022C:
	// subf r30,r29,r30
	r30.s64 = r30.s64 - r29.s64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r28
	ctr.u64 = r28.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bge 0x822f022c
	if (!cr0.lt) goto loc_822F022C;
loc_822F0244:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F0250"))) PPC_WEAK_FUNC(sub_822F0250);
PPC_FUNC_IMPL(__imp__sub_822F0250) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// addi r31,r11,6904
	r31.s64 = r11.s64 + 6904;
	// lwz r3,1360(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1360);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f0278
	if (cr6.eq) goto loc_822F0278;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F0278:
	// lwz r3,1344(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1344);
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,1360(r31)
	PPC_STORE_U32(r31.u32 + 1360, r11.u32);
	// stw r10,1364(r31)
	PPC_STORE_U32(r31.u32 + 1364, ctx.r10.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r9,1368(r31)
	PPC_STORE_U32(r31.u32 + 1368, ctx.r9.u32);
	// beq cr6,0x822f02a0
	if (cr6.eq) goto loc_822F02A0;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F02A0:
	// lwz r3,1156(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 1156);
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,1344(r31)
	PPC_STORE_U32(r31.u32 + 1344, r11.u32);
	// stw r10,1348(r31)
	PPC_STORE_U32(r31.u32 + 1348, ctx.r10.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r9,1352(r31)
	PPC_STORE_U32(r31.u32 + 1352, ctx.r9.u32);
	// beq cr6,0x822f02c8
	if (cr6.eq) goto loc_822F02C8;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F02C8:
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r11,1156(r31)
	PPC_STORE_U32(r31.u32 + 1156, r11.u32);
	// stw r10,1160(r31)
	PPC_STORE_U32(r31.u32 + 1160, ctx.r10.u32);
	// addi r3,r31,512
	ctx.r3.s64 = r31.s64 + 512;
	// stw r9,1164(r31)
	PPC_STORE_U32(r31.u32 + 1164, ctx.r9.u32);
	// bl 0x822f01a0
	sub_822F01A0(ctx, base);
	// addi r3,r31,232
	ctx.r3.s64 = r31.s64 + 232;
	// bl 0x823344a0
	sub_823344A0(ctx, base);
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f0314
	if (cr6.eq) goto loc_822F0314;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r11,0
	r11.s64 = 0;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_822F0314:
	// addi r3,r31,160
	ctx.r3.s64 = r31.s64 + 160;
	// bl 0x8233ddd8
	sub_8233DDD8(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F0330"))) PPC_WEAK_FUNC(sub_822F0330);
PPC_FUNC_IMPL(__imp__sub_822F0330) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f0360
	if (cr6.eq) goto loc_822F0360;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_822F0360:
	// addic. r9,r11,4
	xer.ca = r11.u32 > 4294967291;
	ctx.r9.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x822f036c
	if (cr0.eq) goto loc_822F036C;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
loc_822F036C:
	// addic. r9,r11,8
	xer.ca = r11.u32 > 4294967287;
	ctx.r9.s64 = r11.s64 + 8;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq 0x822f0378
	if (cr0.eq) goto loc_822F0378;
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
loc_822F0378:
	// li r9,1
	ctx.r9.s64 = 1;
	// stb r10,25(r11)
	PPC_STORE_U8(r11.u32 + 25, ctx.r10.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r9,24(r11)
	PPC_STORE_U8(r11.u32 + 24, ctx.r9.u8);
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stb r9,25(r11)
	PPC_STORE_U8(r11.u32 + 25, ctx.r9.u8);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F03C0"))) PPC_WEAK_FUNC(sub_822F03C0);
PPC_FUNC_IMPL(__imp__sub_822F03C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r26,1
	r26.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r30,4(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lbz r10,25(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f0434
	if (!cr6.eq) goto loc_822F0434;
	// ld r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U64(r27.u32 + 0);
loc_822F03F8:
	// ld r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmpld cr6,r9,r10
	cr6.compare<uint64_t>(ctx.r9.u64, ctx.r10.u64, xer);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// blt cr6,0x822f0410
	if (cr6.lt) goto loc_822F0410;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822F0410:
	// clrlwi r29,r10,24
	r29.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f0424
	if (cr6.eq) goto loc_822F0424;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822f0428
	goto loc_822F0428;
loc_822F0424:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_822F0428:
	// lbz r10,25(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f03f8
	if (cr6.eq) goto loc_822F03F8;
loc_822F0434:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f0498
	if (cr6.eq) goto loc_822F0498;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f0494
	if (!cr6.eq) goto loc_822F0494;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x822f0bb8
	sub_822F0BB8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F0494:
	// bl 0x82498700
	sub_82498700(ctx, base);
loc_822F0498:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// ld r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U64(r27.u32 + 0);
	// ld r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// mr r11,r26
	r11.u64 = r26.u64;
	// cmpld cr6,r9,r10
	cr6.compare<uint64_t>(ctx.r9.u64, ctx.r10.u64, xer);
	// blt cr6,0x822f04b4
	if (cr6.lt) goto loc_822F04B4;
	// li r11,0
	r11.s64 = 0;
loc_822F04B4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f04fc
	if (cr6.eq) goto loc_822F04FC;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0bb8
	sub_822F0BB8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F04FC:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r10,8(r31)
	PPC_STORE_U8(r31.u32 + 8, ctx.r10.u8);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F0518"))) PPC_WEAK_FUNC(sub_822F0518);
PPC_FUNC_IMPL(__imp__sub_822F0518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f0580
	if (cr6.eq) goto loc_822F0580;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f0578
	if (cr6.eq) goto loc_822F0578;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lis r9,-32209
	ctx.r9.s64 = -2110849024;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r7,r9,4504
	ctx.r7.s64 = ctx.r9.s64 + 4504;
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// b 0x822f057c
	goto loc_822F057C;
loc_822F0578:
	// li r11,0
	r11.s64 = 0;
loc_822F057C:
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
loc_822F0580:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F0598"))) PPC_WEAK_FUNC(sub_822F0598);
PPC_FUNC_IMPL(__imp__sub_822F0598) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,254
	cr6.compare<uint32_t>(r31.u32, 254, xer);
	// blt cr6,0x822f05c0
	if (cr6.lt) goto loc_822F05C0;
	// bl 0x824ee4c0
	sub_824EE4C0(ctx, base);
loc_822F05C0:
	// rlwinm r11,r31,29,3,29
	r11.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 29) & 0x1FFFFFFC;
	// clrlwi r10,r31,27
	ctx.r10.u64 = r31.u32 & 0x1F;
	// li r9,1
	ctx.r9.s64 = 1;
	// slw r8,r9,r10
	ctx.r8.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// lwzx r7,r11,r30
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// and r6,r8,r7
	ctx.r6.u64 = ctx.r8.u64 & ctx.r7.u64;
	// cntlzw r5,r6
	ctx.r5.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// rlwinm r4,r5,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 27) & 0x1;
	// xori r3,r4,1
	ctx.r3.u64 = ctx.r4.u64 ^ 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F0600"))) PPC_WEAK_FUNC(sub_822F0600);
PPC_FUNC_IMPL(__imp__sub_822F0600) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r9,296
	ctx.r9.s64 = 296;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f0630
	if (!cr6.eq) goto loc_822F0630;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x822f063c
	goto loc_822F063C;
loc_822F0630:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw r10,r8,r9
	ctx.r10.s32 = ctx.r8.s32 / ctx.r9.s32;
loc_822F063C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f067c
	if (cr6.eq) goto loc_822F067C;
	// lwz r8,12(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - r11.s64;
	// divw r6,r7,r9
	ctx.r6.s32 = ctx.r7.s32 / ctx.r9.s32;
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// bge cr6,0x822f067c
	if (!cr6.lt) goto loc_822F067C;
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f0670
	if (cr6.eq) goto loc_822F0670;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F0670:
	// addi r11,r30,296
	r11.s64 = r30.s64 + 296;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x822f06a8
	goto loc_822F06A8;
loc_822F067C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f068c
	if (!cr6.gt) goto loc_822F068C;
	// twi 31,r0,22
loc_822F068C:
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x822f0e20
	sub_822F0E20(ctx, base);
loc_822F06A8:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F06C0"))) PPC_WEAK_FUNC(sub_822F06C0);
PPC_FUNC_IMPL(__imp__sub_822F06C0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r31,8(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// ble cr6,0x822f06ec
	if (!cr6.gt) goto loc_822F06EC;
	// twi 31,r0,22
	// twi 31,r0,22
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
loc_822F06EC:
	// beq cr6,0x822f0734
	if (cr6.eq) goto loc_822F0734;
	// subf r10,r31,r31
	ctx.r10.s64 = r31.s64 - r31.s64;
	// li r9,296
	ctx.r9.s64 = 296;
	// mr r30,r31
	r30.u64 = r31.u64;
	// divw r8,r10,r9
	ctx.r8.s32 = ctx.r10.s32 / ctx.r9.s32;
	// cmplw cr6,r31,r31
	cr6.compare<uint32_t>(r31.u32, r31.u32, xer);
	// mulli r10,r8,296
	ctx.r10.s64 = ctx.r8.s64 * 296;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// beq cr6,0x822f0730
	if (cr6.eq) goto loc_822F0730;
	// subf r29,r31,r11
	r29.s64 = r11.s64 - r31.s64;
loc_822F0714:
	// add r3,r29,r30
	ctx.r3.u64 = r29.u64 + r30.u64;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x822f0714
	if (!cr6.eq) goto loc_822F0714;
loc_822F0730:
	// stw r28,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r28.u32);
loc_822F0734:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F0740"))) PPC_WEAK_FUNC(sub_822F0740);
PPC_FUNC_IMPL(__imp__sub_822F0740) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// li r9,12
	ctx.r9.s64 = 12;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f0768
	if (!cr6.eq) goto loc_822F0768;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x822f0774
	goto loc_822F0774;
loc_822F0768:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw r10,r8,r9
	ctx.r10.s32 = ctx.r8.s32 / ctx.r9.s32;
loc_822F0774:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f07cc
	if (cr6.eq) goto loc_822F07CC;
	// lwz r8,12(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - r11.s64;
	// divw r5,r7,r9
	ctx.r5.s32 = ctx.r7.s32 / ctx.r9.s32;
	// cmplw cr6,r10,r5
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, xer);
	// bge cr6,0x822f07cc
	if (!cr6.lt) goto loc_822F07CC;
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f07b4
	if (cr6.eq) goto loc_822F07B4;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r9,4(r6)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// lwz r8,8(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// stw r8,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r8.u32);
loc_822F07B4:
	// addi r11,r11,12
	r11.s64 = r11.s64 + 12;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_822F07CC:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f07dc
	if (!cr6.gt) goto loc_822F07DC;
	// twi 31,r0,22
loc_822F07DC:
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x8272e828
	sub_8272E828(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F0808"))) PPC_WEAK_FUNC(sub_822F0808);
PPC_FUNC_IMPL(__imp__sub_822F0808) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// addi r31,r11,28888
	r31.s64 = r11.s64 + 28888;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x822f15b8
	sub_822F15B8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F0878"))) PPC_WEAK_FUNC(sub_822F0878);
PPC_FUNC_IMPL(__imp__sub_822F0878) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r26,1
	r26.s64 = 1;
	// addi r28,r11,28888
	r28.s64 = r11.s64 + 28888;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r30,r26
	r30.u64 = r26.u64;
	// lwz r9,4(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f08f4
	if (!cr6.eq) goto loc_822F08F4;
	// lwz r8,0(r27)
	ctx.r8.u64 = PPC_LOAD_U32(r27.u32 + 0);
loc_822F08B8:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmpw cr6,r8,r10
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r10.s32, xer);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// blt cr6,0x822f08d0
	if (cr6.lt) goto loc_822F08D0;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822F08D0:
	// clrlwi r30,r10,24
	r30.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f08e4
	if (cr6.eq) goto loc_822F08E4;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822f08e8
	goto loc_822F08E8;
loc_822F08E4:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_822F08E8:
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f08b8
	if (cr6.eq) goto loc_822F08B8;
loc_822F08F4:
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f0954
	if (cr6.eq) goto loc_822F0954;
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x822f0950
	if (!cr6.eq) goto loc_822F0950;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// bl 0x822f0f00
	sub_822F0F00(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F0950:
	// bl 0x82370fd8
	sub_82370FD8(ctx, base);
loc_822F0954:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r11,r26
	r11.u64 = r26.u64;
	// cmpw cr6,r9,r10
	cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, xer);
	// blt cr6,0x822f0970
	if (cr6.lt) goto loc_822F0970;
	// li r11,0
	r11.s64 = 0;
loc_822F0970:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f09b8
	if (cr6.eq) goto loc_822F09B8;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f0f00
	sub_822F0F00(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F09B8:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r10,8(r31)
	PPC_STORE_U8(r31.u32 + 8, ctx.r10.u8);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F09D8"))) PPC_WEAK_FUNC(sub_822F09D8);
PPC_FUNC_IMPL(__imp__sub_822F09D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// std r5,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r5.u64);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r6,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r6.u64);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x822f0a10
	if (cr6.eq) goto loc_822F0A10;
	// cmplw cr6,r9,r31
	cr6.compare<uint32_t>(ctx.r9.u32, r31.u32, xer);
	// beq cr6,0x822f0a14
	if (cr6.eq) goto loc_822F0A14;
loc_822F0A10:
	// twi 31,r0,22
loc_822F0A14:
	// lwz r8,180(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r28,188(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r30,184(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f0a8c
	if (!cr6.eq) goto loc_822F0A8C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f0a38
	if (cr6.eq) goto loc_822F0A38;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x822f0a3c
	if (cr6.eq) goto loc_822F0A3C;
loc_822F0A38:
	// twi 31,r0,22
loc_822F0A3C:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822f0a8c
	if (!cr6.eq) goto loc_822F0A8C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x824267e0
	sub_824267E0(ctx, base);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// stw r6,4(r29)
	PPC_STORE_U32(r29.u32 + 4, ctx.r6.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
loc_822F0A8C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f0a9c
	if (cr6.eq) goto loc_822F0A9C;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// beq cr6,0x822f0aa0
	if (cr6.eq) goto loc_822F0AA0;
loc_822F0A9C:
	// twi 31,r0,22
loc_822F0AA0:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822f0ad4
	if (cr6.eq) goto loc_822F0AD4;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// bl 0x823d4f20
	sub_823D4F20(ctx, base);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82365a88
	sub_82365A88(ctx, base);
	// ld r5,176(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// b 0x822f0a8c
	goto loc_822F0A8C;
loc_822F0AD4:
	// std r5,0(r29)
	PPC_STORE_U64(r29.u32 + 0, ctx.r5.u64);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F0AE8"))) PPC_WEAK_FUNC(sub_822F0AE8);
PPC_FUNC_IMPL(__imp__sub_822F0AE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lbz r10,25(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f0b3c
	if (!cr6.eq) goto loc_822F0B3C;
	// ld r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r5.u32 + 0);
loc_822F0B00:
	// ld r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// cmpld cr6,r8,r10
	cr6.compare<uint64_t>(ctx.r8.u64, ctx.r10.u64, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// blt cr6,0x822f0b14
	if (cr6.lt) goto loc_822F0B14;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822F0B14:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f0b2c
	if (cr6.eq) goto loc_822F0B2C;
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822f0b30
	goto loc_822F0B30;
loc_822F0B2C:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_822F0B30:
	// lbz r10,25(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f0b00
	if (cr6.eq) goto loc_822F0B00;
loc_822F0B3C:
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// stw r4,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r4.u32);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lbz r10,25(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f0b9c
	if (!cr6.eq) goto loc_822F0B9C;
	// ld r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r5.u32 + 0);
loc_822F0B60:
	// ld r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// cmpld cr6,r10,r8
	cr6.compare<uint64_t>(ctx.r10.u64, ctx.r8.u64, xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// blt cr6,0x822f0b74
	if (cr6.lt) goto loc_822F0B74;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822F0B74:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f0b88
	if (cr6.eq) goto loc_822F0B88;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x822f0b90
	goto loc_822F0B90;
loc_822F0B88:
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F0B90:
	// lbz r10,25(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f0b60
	if (cr6.eq) goto loc_822F0B60;
loc_822F0B9C:
	// ld r11,-16(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// stw r4,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r4.u32);
	// ld r10,-16(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// std r10,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r10.u64);
	// std r11,8(r3)
	PPC_STORE_U64(ctx.r3.u32 + 8, r11.u64);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F0BB8"))) PPC_WEAK_FUNC(sub_822F0BB8);
PPC_FUNC_IMPL(__imp__sub_822F0BB8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r11,8191
	r11.s64 = 536805376;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// ori r9,r11,65534
	ctx.r9.u64 = r11.u64 | 65534;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f0c30
	if (cr6.lt) goto loc_822F0C30;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5552
	ctx.r4.s64 = r11.s64 + 5552;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822F0C30:
	// li r3,32
	ctx.r3.s64 = 32;
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822f0c68
	if (cr6.eq) goto loc_822F0C68;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
	// stw r30,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r30.u32);
	// ld r11,0(r28)
	r11.u64 = PPC_LOAD_U64(r28.u32 + 0);
	// std r11,16(r27)
	PPC_STORE_U64(r27.u32 + 16, r11.u64);
	// stb r25,24(r27)
	PPC_STORE_U8(r27.u32 + 24, r25.u8);
	// stb r25,25(r27)
	PPC_STORE_U8(r27.u32 + 25, r25.u8);
loc_822F0C68:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// bne cr6,0x822f0c98
	if (!cr6.eq) goto loc_822F0C98;
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r27.u32);
	// b 0x822f0cd8
	goto loc_822F0CD8;
loc_822F0C98:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f0cc0
	if (cr6.eq) goto loc_822F0CC0;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f0cd8
	if (!cr6.eq) goto loc_822F0CD8;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// b 0x822f0cd8
	goto loc_822F0CD8;
loc_822F0CC0:
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f0cd8
	if (!cr6.eq) goto loc_822F0CD8;
	// stw r27,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r27.u32);
loc_822F0CD8:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// li r30,1
	r30.s64 = 1;
	// mr r31,r27
	r31.u64 = r27.u64;
	// lbz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 24);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f0e00
	if (!cr6.eq) goto loc_822F0E00;
loc_822F0CF4:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f0d7c
	if (!cr6.eq) goto loc_822F0D7C;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 24);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f0d3c
	if (!cr6.eq) goto loc_822F0D3C;
	// rotlwi r9,r4,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// stb r30,24(r9)
	PPC_STORE_U8(ctx.r9.u32 + 24, r30.u8);
	// stb r30,24(r10)
	PPC_STORE_U8(ctx.r10.u32 + 24, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,24(r7)
	PPC_STORE_U8(ctx.r7.u32 + 24, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822f0dec
	goto loc_822F0DEC;
loc_822F0D3C:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f0d54
	if (!cr6.eq) goto loc_822F0D54;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x827c6448
	sub_827C6448(ctx, base);
loc_822F0D54:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,24(r11)
	PPC_STORE_U8(r11.u32 + 24, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,24(r9)
	PPC_STORE_U8(ctx.r9.u32 + 24, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x827c64c0
	sub_827C64C0(ctx, base);
	// b 0x822f0dec
	goto loc_822F0DEC;
loc_822F0D7C:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 24);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f0db0
	if (!cr6.eq) goto loc_822F0DB0;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r30,24(r9)
	PPC_STORE_U8(ctx.r9.u32 + 24, r30.u8);
	// stb r30,24(r10)
	PPC_STORE_U8(ctx.r10.u32 + 24, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,24(r7)
	PPC_STORE_U8(ctx.r7.u32 + 24, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822f0dec
	goto loc_822F0DEC;
loc_822F0DB0:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f0dc8
	if (!cr6.eq) goto loc_822F0DC8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x827c64c0
	sub_827C64C0(ctx, base);
loc_822F0DC8:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,24(r11)
	PPC_STORE_U8(r11.u32 + 24, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,24(r9)
	PPC_STORE_U8(ctx.r9.u32 + 24, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x827c6448
	sub_827C6448(ctx, base);
loc_822F0DEC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lbz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 24);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f0cf4
	if (cr6.eq) goto loc_822F0CF4;
loc_822F0E00:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r27,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r27.u32);
	// stw r29,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r29.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stb r30,24(r10)
	PPC_STORE_U8(ctx.r10.u32 + 24, r30.u8);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822F0E20"))) PPC_WEAK_FUNC(sub_822F0E20);
PPC_FUNC_IMPL(__imp__sub_822F0E20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r4,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r4.u64);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f0e5c
	if (cr6.eq) goto loc_822F0E5C;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// li r9,296
	ctx.r9.s64 = 296;
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw. r7,r8,r9
	ctx.r7.s32 = ctx.r8.s32 / ctx.r9.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x822f0e64
	if (!cr0.eq) goto loc_822F0E64;
loc_822F0E5C:
	// li r30,0
	r30.s64 = 0;
	// b 0x822f0e94
	goto loc_822F0E94;
loc_822F0E64:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f0e70
	if (!cr6.gt) goto loc_822F0E70;
	// twi 31,r0,22
loc_822F0E70:
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f0e84
	if (cr6.eq) goto loc_822F0E84;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// beq cr6,0x822f0e88
	if (cr6.eq) goto loc_822F0E88;
loc_822F0E84:
	// twi 31,r0,22
loc_822F0E88:
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw r30,r8,r9
	r30.s32 = ctx.r8.s32 / ctx.r9.s32;
loc_822F0E94:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f11d8
	sub_822F11D8(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f0eb4
	if (!cr6.gt) goto loc_822F0EB4;
	// twi 31,r0,22
loc_822F0EB4:
	// mulli r10,r30,296
	ctx.r10.s64 = r30.s64 * 296;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bgt cr6,0x822f0ee4
	if (cr6.gt) goto loc_822F0EE4;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f0ee8
	if (!cr6.lt) goto loc_822F0EE8;
loc_822F0EE4:
	// twi 31,r0,22
loc_822F0EE8:
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r11.u64);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F0F00"))) PPC_WEAK_FUNC(sub_822F0F00);
PPC_FUNC_IMPL(__imp__sub_822F0F00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lis r10,2047
	ctx.r10.s64 = 134152192;
	// addi r29,r11,28888
	r29.s64 = r11.s64 + 28888;
	// ori r9,r10,65534
	ctx.r9.u64 = ctx.r10.u64 | 65534;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// mr r25,r5
	r25.u64 = ctx.r5.u64;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r27,r7
	r27.u64 = ctx.r7.u64;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f0f7c
	if (cr6.lt) goto loc_822F0F7C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5552
	ctx.r4.s64 = r11.s64 + 5552;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822F0F7C:
	// li r3,48
	ctx.r3.s64 = 48;
	// lwz r28,4(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r26,0
	r26.s64 = 0;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f0fdc
	if (cr6.eq) goto loc_822F0FDC;
	// stw r28,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r28.u32);
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r31,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r31.u32);
	// addi r11,r30,12
	r11.s64 = r30.s64 + 12;
	// stw r28,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r28.u32);
	// addi r4,r27,4
	ctx.r4.s64 = r27.s64 + 4;
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// stw r9,12(r30)
	PPC_STORE_U32(r30.u32 + 12, ctx.r9.u32);
	// li r6,-1
	ctx.r6.s64 = -1;
	// stw r10,40(r30)
	PPC_STORE_U32(r30.u32 + 40, ctx.r10.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r26,36(r30)
	PPC_STORE_U32(r30.u32 + 36, r26.u32);
	// stb r26,20(r30)
	PPC_STORE_U8(r30.u32 + 20, r26.u8);
	// bl 0x8218ea38
	sub_8218EA38(ctx, base);
	// stb r26,44(r30)
	PPC_STORE_U8(r30.u32 + 44, r26.u8);
	// stb r26,45(r30)
	PPC_STORE_U8(r30.u32 + 45, r26.u8);
loc_822F0FDC:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// bne cr6,0x822f1008
	if (!cr6.eq) goto loc_822F1008;
	// stw r30,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r30.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// b 0x822f1048
	goto loc_822F1048;
loc_822F1008:
	// clrlwi r11,r25,24
	r11.u64 = r25.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1034
	if (cr6.eq) goto loc_822F1034;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f104c
	if (!cr6.eq) goto loc_822F104C;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
	// b 0x822f104c
	goto loc_822F104C;
loc_822F1034:
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f104c
	if (!cr6.eq) goto loc_822F104C;
loc_822F1048:
	// stw r30,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r30.u32);
loc_822F104C:
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// li r28,1
	r28.s64 = 1;
	// mr r31,r30
	r31.u64 = r30.u64;
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f1174
	if (!cr6.eq) goto loc_822F1174;
loc_822F1068:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f10f0
	if (!cr6.eq) goto loc_822F10F0;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f10b0
	if (!cr6.eq) goto loc_822F10B0;
	// rotlwi r9,r4,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// stb r28,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r28.u8);
	// stb r28,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r28.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r26,44(r7)
	PPC_STORE_U8(ctx.r7.u32 + 44, r26.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822f1160
	goto loc_822F1160;
loc_822F10B0:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f10c8
	if (!cr6.eq) goto loc_822F10C8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x822f16c8
	sub_822F16C8(ctx, base);
loc_822F10C8:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r28,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r28.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r26,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r26.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x822f1748
	sub_822F1748(ctx, base);
	// b 0x822f1160
	goto loc_822F1160;
loc_822F10F0:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f1124
	if (!cr6.eq) goto loc_822F1124;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r28,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r28.u8);
	// stb r28,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r28.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r26,44(r7)
	PPC_STORE_U8(ctx.r7.u32 + 44, r26.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822f1160
	goto loc_822F1160;
loc_822F1124:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f113c
	if (!cr6.eq) goto loc_822F113C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x822f1748
	sub_822F1748(ctx, base);
loc_822F113C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r28,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r28.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r26,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r26.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x822f16c8
	sub_822F16C8(ctx, base);
loc_822F1160:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f1068
	if (cr6.eq) goto loc_822F1068;
loc_822F1174:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r30,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r30.u32);
	// stw r29,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r29.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stb r28,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r28.u8);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822F1198"))) PPC_WEAK_FUNC(sub_822F1198);
PPC_FUNC_IMPL(__imp__sub_822F1198) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f11c0
	if (cr6.eq) goto loc_822F11C0;
	// bl 0x8231eb10
	sub_8231EB10(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F11C0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F11D8"))) PPC_WEAK_FUNC(sub_822F11D8);
PPC_FUNC_IMPL(__imp__sub_822F11D8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// std r4,472(r1)
	PPC_STORE_U64(ctx.r1.u32 + 472, ctx.r4.u64);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// li r24,296
	r24.s64 = 296;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f1214
	if (!cr6.eq) goto loc_822F1214;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f1220
	goto loc_822F1220;
loc_822F1214:
	// lwz r10,12(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// divw r9,r9,r24
	ctx.r9.s32 = ctx.r9.s32 / r24.s32;
loc_822F1220:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f1230
	if (!cr6.eq) goto loc_822F1230;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x822f123c
	goto loc_822F123C;
loc_822F1230:
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// divw r10,r8,r24
	ctx.r10.s32 = ctx.r8.s32 / r24.s32;
loc_822F123C:
	// lis r8,221
	ctx.r8.s64 = 14483456;
	// ori r8,r8,26568
	ctx.r8.u64 = ctx.r8.u64 | 26568;
	// subf r7,r10,r8
	ctx.r7.s64 = ctx.r8.s64 - ctx.r10.s64;
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bge cr6,0x822f125c
	if (!cr6.lt) goto loc_822F125C;
	// bl 0x82684b38
	sub_82684B38(ctx, base);
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x82ca2c28
	return;
loc_822F125C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f126c
	if (!cr6.eq) goto loc_822F126C;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x822f1278
	goto loc_822F1278;
loc_822F126C:
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - r11.s64;
	// divw r10,r7,r24
	ctx.r10.s32 = ctx.r7.s32 / r24.s32;
loc_822F1278:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f13d8
	if (!cr6.lt) goto loc_822F13D8;
	// rlwinm r10,r9,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// li r25,0
	r25.s64 = 0;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f129c
	if (cr6.lt) goto loc_822F129C;
	// add r25,r10,r9
	r25.u64 = ctx.r10.u64 + ctx.r9.u64;
loc_822F129C:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f12ac
	if (!cr6.eq) goto loc_822F12AC;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x822f12b8
	goto loc_822F12B8;
loc_822F12AC:
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// divw r10,r9,r24
	ctx.r10.s32 = ctx.r9.s32 / r24.s32;
loc_822F12B8:
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r25,r10
	cr6.compare<uint32_t>(r25.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f12dc
	if (!cr6.lt) goto loc_822F12DC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f12d8
	if (cr6.eq) goto loc_822F12D8;
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// divw r11,r9,r24
	r11.s32 = ctx.r9.s32 / r24.s32;
loc_822F12D8:
	// addi r25,r11,1
	r25.s64 = r11.s64 + 1;
loc_822F12DC:
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822f1548
	sub_822F1548(ctx, base);
	// lwz r30,4(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r29,476(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// mr r31,r27
	r31.u64 = r27.u64;
	// beq cr6,0x822f1328
	if (cr6.eq) goto loc_822F1328;
loc_822F1300:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f1318
	if (cr6.eq) goto loc_822F1318;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F1318:
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// bne cr6,0x822f1300
	if (!cr6.eq) goto loc_822F1300;
loc_822F1328:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f1340
	if (cr6.eq) goto loc_822F1340;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F1340:
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// addi r30,r31,296
	r30.s64 = r31.s64 + 296;
	// cmplw cr6,r29,r28
	cr6.compare<uint32_t>(r29.u32, r28.u32, xer);
	// beq cr6,0x822f1384
	if (cr6.eq) goto loc_822F1384;
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// addi r31,r11,-296
	r31.s64 = r11.s64 + -296;
loc_822F135C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f1374
	if (cr6.eq) goto loc_822F1374;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F1374:
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// cmplw cr6,r31,r28
	cr6.compare<uint32_t>(r31.u32, r28.u32, xer);
	// bne cr6,0x822f135c
	if (!cr6.eq) goto loc_822F135C;
loc_822F1384:
	// lwz r3,4(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822f1398
	if (!cr6.eq) goto loc_822F1398;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f13a4
	goto loc_822F13A4;
loc_822F1398:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r10,r3,r11
	ctx.r10.s64 = r11.s64 - ctx.r3.s64;
	// divw r11,r10,r24
	r11.s32 = ctx.r10.s32 / r24.s32;
loc_822F13A4:
	// addi r31,r11,1
	r31.s64 = r11.s64 + 1;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f13b4
	if (cr6.eq) goto loc_822F13B4;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F13B4:
	// mulli r10,r25,296
	ctx.r10.s64 = r25.s64 * 296;
	// stw r27,4(r26)
	PPC_STORE_U32(r26.u32 + 4, r27.u32);
	// mulli r11,r31,296
	r11.s64 = r31.s64 * 296;
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + r27.u64;
	// add r9,r11,r27
	ctx.r9.u64 = r11.u64 + r27.u64;
	// stw r10,12(r26)
	PPC_STORE_U32(r26.u32 + 12, ctx.r10.u32);
	// stw r9,8(r26)
	PPC_STORE_U32(r26.u32 + 8, ctx.r9.u32);
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x82ca2c28
	return;
loc_822F13D8:
	// lwz r27,476(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 476);
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r11,r27,r28
	r11.s64 = r28.s64 - r27.s64;
	// divw r10,r11,r24
	ctx.r10.s32 = r11.s32 / r24.s32;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bge cr6,0x822f14a4
	if (!cr6.lt) goto loc_822F14A4;
	// addi r31,r27,296
	r31.s64 = r27.s64 + 296;
	// cmplw cr6,r27,r28
	cr6.compare<uint32_t>(r27.u32, r28.u32, xer);
	// beq cr6,0x822f1428
	if (cr6.eq) goto loc_822F1428;
	// addi r30,r31,-296
	r30.s64 = r31.s64 + -296;
loc_822F1400:
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f1418
	if (cr6.eq) goto loc_822F1418;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F1418:
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// bne cr6,0x822f1400
	if (!cr6.eq) goto loc_822F1400;
loc_822F1428:
	// lwz r30,8(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r11,r27,r30
	r11.s64 = r30.s64 - r27.s64;
	// divw r10,r11,r24
	ctx.r10.s32 = r11.s32 / r24.s32;
	// subfic r31,r10,1
	xer.ca = ctx.r10.u32 <= 1;
	r31.s64 = 1 - ctx.r10.s64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f1464
	if (cr6.eq) goto loc_822F1464;
loc_822F1440:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f1458
	if (cr6.eq) goto loc_822F1458;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F1458:
	// addic. r31,r31,-1
	xer.ca = r31.u32 > 0;
	r31.s64 = r31.s64 + -1;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// bne 0x822f1440
	if (!cr0.eq) goto loc_822F1440;
loc_822F1464:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// mr r31,r27
	r31.u64 = r27.u64;
	// addi r11,r11,296
	r11.s64 = r11.s64 + 296;
	// addi r30,r11,-296
	r30.s64 = r11.s64 + -296;
	// stw r11,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r11.u32);
	// cmplw cr6,r27,r30
	cr6.compare<uint32_t>(r27.u32, r30.u32, xer);
	// beq cr6,0x822f153c
	if (cr6.eq) goto loc_822F153C;
loc_822F1480:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822f1480
	if (!cr6.eq) goto loc_822F1480;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x82ca2c28
	return;
loc_822F14A4:
	// addi r31,r28,-296
	r31.s64 = r28.s64 + -296;
	// mr r30,r28
	r30.u64 = r28.u64;
	// mr r29,r31
	r29.u64 = r31.u64;
	// cmplw cr6,r31,r28
	cr6.compare<uint32_t>(r31.u32, r28.u32, xer);
	// beq cr6,0x822f14e0
	if (cr6.eq) goto loc_822F14E0;
loc_822F14B8:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f14d0
	if (cr6.eq) goto loc_822F14D0;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822F14D0:
	// addi r29,r29,296
	r29.s64 = r29.s64 + 296;
	// addi r30,r30,296
	r30.s64 = r30.s64 + 296;
	// cmplw cr6,r29,r28
	cr6.compare<uint32_t>(r29.u32, r28.u32, xer);
	// bne cr6,0x822f14b8
	if (!cr6.eq) goto loc_822F14B8;
loc_822F14E0:
	// stw r30,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r30.u32);
	// cmplw cr6,r27,r31
	cr6.compare<uint32_t>(r27.u32, r31.u32, xer);
	// beq cr6,0x822f1510
	if (cr6.eq) goto loc_822F1510;
	// addi r30,r31,296
	r30.s64 = r31.s64 + 296;
loc_822F14F0:
	// addi r31,r31,-296
	r31.s64 = r31.s64 + -296;
	// addi r30,r30,-296
	r30.s64 = r30.s64 + -296;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// cmplw cr6,r31,r27
	cr6.compare<uint32_t>(r31.u32, r27.u32, xer);
	// bne cr6,0x822f14f0
	if (!cr6.eq) goto loc_822F14F0;
loc_822F1510:
	// addi r30,r27,296
	r30.s64 = r27.s64 + 296;
	// mr r31,r27
	r31.u64 = r27.u64;
	// cmplw cr6,r27,r30
	cr6.compare<uint32_t>(r27.u32, r30.u32, xer);
	// beq cr6,0x822f153c
	if (cr6.eq) goto loc_822F153C;
loc_822F1520:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// li r5,296
	ctx.r5.s64 = 296;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r31,r31,296
	r31.s64 = r31.s64 + 296;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822f1520
	if (!cr6.eq) goto loc_822F1520;
loc_822F153C:
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822F1548"))) PPC_WEAK_FUNC(sub_822F1548);
PPC_FUNC_IMPL(__imp__sub_822F1548) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f159c
	if (cr6.eq) goto loc_822F159C;
	// li r11,-1
	r11.s64 = -1;
	// divwu r10,r11,r31
	ctx.r10.u32 = r11.u32 / r31.u32;
	// cmplwi cr6,r10,296
	cr6.compare<uint32_t>(ctx.r10.u32, 296, xer);
	// bge cr6,0x822f159c
	if (!cr6.lt) goto loc_822F159C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r9,r11,5684
	ctx.r9.s64 = r11.s64 + 5684;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// addi r7,r8,5672
	ctx.r7.s64 = ctx.r8.s64 + 5672;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
loc_822F159C:
	// mulli r3,r31,296
	ctx.r3.s64 = r31.s64 * 296;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F15B8"))) PPC_WEAK_FUNC(sub_822F15B8);
PPC_FUNC_IMPL(__imp__sub_822F15B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// std r5,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r5.u64);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r31,r11,28888
	r31.s64 = r11.s64 + 28888;
	// std r6,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r6.u64);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x822f15f4
	if (cr6.eq) goto loc_822F15F4;
	// cmplw cr6,r9,r31
	cr6.compare<uint32_t>(ctx.r9.u32, r31.u32, xer);
	// beq cr6,0x822f15f8
	if (cr6.eq) goto loc_822F15F8;
loc_822F15F4:
	// twi 31,r0,22
loc_822F15F8:
	// lwz r8,180(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r28,188(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r29,184(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f1670
	if (!cr6.eq) goto loc_822F1670;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f161c
	if (cr6.eq) goto loc_822F161C;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// beq cr6,0x822f1620
	if (cr6.eq) goto loc_822F1620;
loc_822F161C:
	// twi 31,r0,22
loc_822F1620:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822f1670
	if (!cr6.eq) goto loc_822F1670;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822f1c18
	sub_822F1C18(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r31,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r31.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
loc_822F1670:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f1680
	if (cr6.eq) goto loc_822F1680;
	// cmplw cr6,r9,r29
	cr6.compare<uint32_t>(ctx.r9.u32, r29.u32, xer);
	// beq cr6,0x822f1684
	if (cr6.eq) goto loc_822F1684;
loc_822F1680:
	// twi 31,r0,22
loc_822F1684:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822f16b8
	if (cr6.eq) goto loc_822F16B8;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// bl 0x82c705d8
	sub_82C705D8(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f17c8
	sub_822F17C8(ctx, base);
	// ld r5,176(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// b 0x822f1670
	goto loc_822F1670;
loc_822F16B8:
	// std r5,0(r30)
	PPC_STORE_U64(r30.u32 + 0, ctx.r5.u64);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F16C8"))) PPC_WEAK_FUNC(sub_822F16C8);
PPC_FUNC_IMPL(__imp__sub_822F16C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,45(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 45);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f16e8
	if (!cr6.eq) goto loc_822F16E8;
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
loc_822F16E8:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// addi r8,r10,28888
	ctx.r8.s64 = ctx.r10.s64 + 28888;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r4,r7
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, xer);
	// bne cr6,0x822f1718
	if (!cr6.eq) goto loc_822F1718;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_822F1718:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f1738
	if (!cr6.eq) goto loc_822F1738;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_822F1738:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1748"))) PPC_WEAK_FUNC(sub_822F1748);
PPC_FUNC_IMPL(__imp__sub_822F1748) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,45(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 45);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f1768
	if (!cr6.eq) goto loc_822F1768;
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
loc_822F1768:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lwz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// addi r8,r10,28888
	ctx.r8.s64 = ctx.r10.s64 + 28888;
	// stw r9,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r9.u32);
	// lwz r10,4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// lwz r7,4(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r4,r7
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r7.u32, xer);
	// bne cr6,0x822f1798
	if (!cr6.eq) goto loc_822F1798;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_822F1798:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f17b8
	if (!cr6.eq) goto loc_822F17B8;
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_822F17B8:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F17C8"))) PPC_WEAK_FUNC(sub_822F17C8);
PPC_FUNC_IMPL(__imp__sub_822F17C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r5,256(r1)
	PPC_STORE_U64(ctx.r1.u32 + 256, ctx.r5.u64);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r31,260(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// lbz r11,45(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1830
	if (cr6.eq) goto loc_822F1830;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5600
	ctx.r4.s64 = r11.s64 + 5600;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822F1830:
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// mr r26,r31
	r26.u64 = r31.u64;
	// bl 0x82c705d8
	sub_82C705D8(ctx, base);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lbz r11,45(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1854
	if (cr6.eq) goto loc_822F1854;
	// lwz r27,8(r26)
	r27.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// b 0x822f187c
	goto loc_822F187C;
loc_822F1854:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lbz r9,45(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f186c
	if (cr6.eq) goto loc_822F186C;
	// mr r27,r10
	r27.u64 = ctx.r10.u64;
	// b 0x822f187c
	goto loc_822F187C;
loc_822F186C:
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// lwz r27,8(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bne cr6,0x822f1974
	if (!cr6.eq) goto loc_822F1974;
loc_822F187C:
	// lbz r11,45(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 45);
	// lwz r31,4(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f1890
	if (!cr6.eq) goto loc_822F1890;
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
loc_822F1890:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r28,r11,28888
	r28.s64 = r11.s64 + 28888;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x822f18b0
	if (!cr6.eq) goto loc_822F18B0;
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// b 0x822f18c8
	goto loc_822F18C8;
loc_822F18B0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f18c4
	if (!cr6.eq) goto loc_822F18C4;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// b 0x822f18c8
	goto loc_822F18C8;
loc_822F18C4:
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
loc_822F18C8:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x822f191c
	if (!cr6.eq) goto loc_822F191C;
	// lbz r11,45(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f18ec
	if (cr6.eq) goto loc_822F18EC;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// b 0x822f1914
	goto loc_822F1914;
loc_822F18EC:
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// lbz r9,45(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f1914
	if (!cr6.eq) goto loc_822F1914;
loc_822F1900:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,45(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f1900
	if (cr6.eq) goto loc_822F1900;
loc_822F1914:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
loc_822F191C:
	// lwz r9,4(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f1a10
	if (!cr6.eq) goto loc_822F1A10;
	// lbz r11,45(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1944
	if (cr6.eq) goto loc_822F1944;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822f1a10
	goto loc_822F1A10;
loc_822F1944:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r10,r27
	ctx.r10.u64 = r27.u64;
	// lbz r8,45(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f196c
	if (!cr6.eq) goto loc_822F196C;
loc_822F1958:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r8,45(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f1958
	if (cr6.eq) goto loc_822F1958;
loc_822F196C:
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822f1a10
	goto loc_822F1A10;
loc_822F1974:
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f1994
	if (!cr6.eq) goto loc_822F1994;
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x822f19bc
	goto loc_822F19BC;
loc_822F1994:
	// lbz r10,45(r27)
	ctx.r10.u64 = PPC_LOAD_U8(r27.u32 + 45);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f19a8
	if (!cr6.eq) goto loc_822F19A8;
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
loc_822F19A8:
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
loc_822F19BC:
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r28,r10,28888
	r28.s64 = ctx.r10.s64 + 28888;
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822f19dc
	if (!cr6.eq) goto loc_822F19DC;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// b 0x822f19f8
	goto loc_822F19F8;
loc_822F19DC:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822f19f4
	if (!cr6.eq) goto loc_822F19F4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x822f19f8
	goto loc_822F19F8;
loc_822F19F4:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
loc_822F19F8:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lbz r9,44(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 44);
	// lbz r8,44(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// stb r9,44(r11)
	PPC_STORE_U8(r11.u32 + 44, ctx.r9.u8);
	// stb r8,44(r26)
	PPC_STORE_U8(r26.u32 + 44, ctx.r8.u8);
loc_822F1A10:
	// lbz r11,44(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 44);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822f1bb0
	if (!cr6.eq) goto loc_822F1BB0;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// li r29,1
	r29.s64 = 1;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f1bac
	if (cr6.eq) goto loc_822F1BAC;
loc_822F1A34:
	// lbz r11,44(r27)
	r11.u64 = PPC_LOAD_U8(r27.u32 + 44);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822f1bac
	if (!cr6.eq) goto loc_822F1BAC;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bne cr6,0x822f1af0
	if (!cr6.eq) goto loc_822F1AF0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f1a74
	if (!cr6.eq) goto loc_822F1A74;
	// stb r29,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r29.u8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stb r30,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r30.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822f16c8
	sub_822F16C8(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822F1A74:
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f1b44
	if (!cr6.eq) goto loc_822F1B44;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f1aa0
	if (!cr6.eq) goto loc_822F1AA0;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x822f1b40
	if (cr6.eq) goto loc_822F1B40;
loc_822F1AA0:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f1acc
	if (!cr6.eq) goto loc_822F1ACC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stb r29,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r29.u8);
	// stb r30,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r30.u8);
	// bl 0x822f1748
	sub_822F1748(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822F1ACC:
	// lbz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 44);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r10,44(r11)
	PPC_STORE_U8(r11.u32 + 44, ctx.r10.u8);
	// stb r29,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r29.u8);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stb r29,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r29.u8);
	// bl 0x822f16c8
	sub_822F16C8(ctx, base);
	// b 0x822f1bac
	goto loc_822F1BAC;
loc_822F1AF0:
	// lbz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f1b14
	if (!cr6.eq) goto loc_822F1B14;
	// stb r29,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r29.u8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stb r30,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r30.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822f1748
	sub_822F1748(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F1B14:
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f1b44
	if (!cr6.eq) goto loc_822F1B44;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f1b60
	if (!cr6.eq) goto loc_822F1B60;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f1b60
	if (!cr6.eq) goto loc_822F1B60;
loc_822F1B40:
	// stb r30,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r30.u8);
loc_822F1B44:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r27,r31
	r27.u64 = r31.u64;
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f1a34
	if (!cr6.eq) goto loc_822F1A34;
	// b 0x822f1bac
	goto loc_822F1BAC;
loc_822F1B60:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f1b8c
	if (!cr6.eq) goto loc_822F1B8C;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// stb r29,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r29.u8);
	// stb r30,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r30.u8);
	// bl 0x822f16c8
	sub_822F16C8(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F1B8C:
	// lbz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 44);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r10,44(r11)
	PPC_STORE_U8(r11.u32 + 44, ctx.r10.u8);
	// stb r29,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r29.u8);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r29,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r29.u8);
	// bl 0x822f1748
	sub_822F1748(ctx, base);
loc_822F1BAC:
	// stb r29,44(r27)
	PPC_STORE_U8(r27.u32 + 44, r29.u8);
loc_822F1BB0:
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// addi r31,r26,16
	r31.s64 = r26.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x822f1bc8
	if (cr6.lt) goto loc_822F1BC8;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F1BC8:
	// li r11,15
	r11.s64 = 15;
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// stb r30,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r30.u8);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1c08
	if (cr6.eq) goto loc_822F1C08;
	// ld r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stw r11,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r11.u32);
	// std r10,0(r25)
	PPC_STORE_U64(r25.u32 + 0, ctx.r10.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
loc_822F1C08:
	// ld r11,256(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// std r11,0(r25)
	PPC_STORE_U64(r25.u32 + 0, r11.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F1C18"))) PPC_WEAK_FUNC(sub_822F1C18);
PPC_FUNC_IMPL(__imp__sub_822F1C18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// lbz r11,45(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f1c90
	if (!cr6.eq) goto loc_822F1C90;
	// li r27,15
	r27.s64 = 15;
	// li r28,0
	r28.s64 = 0;
loc_822F1C44:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r4,8(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// bl 0x822f1c18
	sub_822F1C18(ctx, base);
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// addi r31,r29,16
	r31.s64 = r29.s64 + 16;
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// lwz r30,0(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// blt cr6,0x822f1c6c
	if (cr6.lt) goto loc_822F1C6C;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F1C6C:
	// stw r27,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r27.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r28,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r28.u32);
	// stb r28,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r28.u8);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lbz r11,45(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 45);
	// mr r29,r30
	r29.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1c44
	if (cr6.eq) goto loc_822F1C44;
loc_822F1C90:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F1C98"))) PPC_WEAK_FUNC(sub_822F1C98);
PPC_FUNC_IMPL(__imp__sub_822F1C98) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// lwz r4,4(r4)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1CD8"))) PPC_WEAK_FUNC(sub_822F1CD8);
PPC_FUNC_IMPL(__imp__sub_822F1CD8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lbz r11,0(r4)
	r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, r11.u8);
	// stb r10,0(r4)
	PPC_STORE_U8(ctx.r4.u32 + 0, ctx.r10.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1CF0"))) PPC_WEAK_FUNC(sub_822F1CF0);
PPC_FUNC_IMPL(__imp__sub_822F1CF0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r11,5672
	ctx.r10.s64 = r11.s64 + 5672;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1D00"))) PPC_WEAK_FUNC(sub_822F1D00);
PPC_FUNC_IMPL(__imp__sub_822F1D00) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,5672
	ctx.r9.s64 = r11.s64 + 5672;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x822f1d34
	if (cr6.eq) goto loc_822F1D34;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_822F1D34:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1D48"))) PPC_WEAK_FUNC(sub_822F1D48);
PPC_FUNC_IMPL(__imp__sub_822F1D48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r4,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r4.u32);
	// addi r10,r11,5684
	ctx.r10.s64 = r11.s64 + 5684;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1D60"))) PPC_WEAK_FUNC(sub_822F1D60);
PPC_FUNC_IMPL(__imp__sub_822F1D60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// li r10,15
	ctx.r10.s64 = 15;
	// li r6,-1
	ctx.r6.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// stb r11,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r11.u8);
	// bl 0x8218ea38
	sub_8218EA38(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1DB0"))) PPC_WEAK_FUNC(sub_822F1DB0);
PPC_FUNC_IMPL(__imp__sub_822F1DB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f1e04
	if (cr6.eq) goto loc_822F1E04;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x822f1e04
	if (cr6.lt) goto loc_822F1E04;
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f1dfc
	if (cr6.eq) goto loc_822F1DFC;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
loc_822F1DFC:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F1E04:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
	// li r10,15
	ctx.r10.s64 = 15;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r10,24(r30)
	PPC_STORE_U32(r30.u32 + 24, ctx.r10.u32);
	// stbx r9,r11,r31
	PPC_STORE_U8(r11.u32 + r31.u32, ctx.r9.u8);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F1E28"))) PPC_WEAK_FUNC(sub_822F1E28);
PPC_FUNC_IMPL(__imp__sub_822F1E28) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r11,5696
	ctx.r10.s64 = r11.s64 + 5696;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x826c3fa8
	sub_826C3FA8(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822F1E38"))) PPC_WEAK_FUNC(sub_822F1E38);
PPC_FUNC_IMPL(__imp__sub_822F1E38) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r10,r11,5696
	ctx.r10.s64 = r11.s64 + 5696;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// clrlwi r9,r30,31
	ctx.r9.u64 = r30.u32 & 0x1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f1e7c
	if (cr6.eq) goto loc_822F1E7C;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_822F1E7C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1E98"))) PPC_WEAK_FUNC(sub_822F1E98);
PPC_FUNC_IMPL(__imp__sub_822F1E98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// addi r11,r3,8
	r11.s64 = ctx.r3.s64 + 8;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// blt cr6,0x822f1eb0
	if (cr6.lt) goto loc_822F1EB0;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// blr 
	return;
loc_822F1EB0:
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1EB8"))) PPC_WEAK_FUNC(sub_822F1EB8);
PPC_FUNC_IMPL(__imp__sub_822F1EB8) {
	PPC_FUNC_PROLOGUE();
	// b 0x82cd1408
	sub_82CD1408(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822F1EC0"))) PPC_WEAK_FUNC(sub_822F1EC0);
PPC_FUNC_IMPL(__imp__sub_822F1EC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r10,r11,5696
	ctx.r10.s64 = r11.s64 + 5696;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1F00"))) PPC_WEAK_FUNC(sub_822F1F00);
PPC_FUNC_IMPL(__imp__sub_822F1F00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31949
	r11.s64 = -2093809664;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r10,28456(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 28456);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f1f30
	if (cr6.eq) goto loc_822F1F30;
	// rotlwi r11,r10,0
	r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F1F30:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cd1408
	sub_82CD1408(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F1F60"))) PPC_WEAK_FUNC(sub_822F1F60);
PPC_FUNC_IMPL(__imp__sub_822F1F60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// bge cr6,0x822f1f88
	if (!cr6.lt) goto loc_822F1F88;
	// bl 0x82cd12c8
	sub_82CD12C8(ctx, base);
loc_822F1F88:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// subf r9,r29,r11
	ctx.r9.s64 = r11.s64 - r29.s64;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// bge cr6,0x822f1f9c
	if (!cr6.lt) goto loc_822F1F9C;
	// mr r30,r9
	r30.u64 = ctx.r9.u64;
loc_822F1F9C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f2010
	if (cr6.eq) goto loc_822F2010;
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r28,r31,4
	r28.s64 = r31.s64 + 4;
	// cmplwi cr6,r8,16
	cr6.compare<uint32_t>(ctx.r8.u32, 16, xer);
	// blt cr6,0x822f1fbc
	if (cr6.lt) goto loc_822F1FBC;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x822f1fc0
	goto loc_822F1FC0;
loc_822F1FBC:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_822F1FC0:
	// cmplwi cr6,r8,16
	cr6.compare<uint32_t>(ctx.r8.u32, 16, xer);
	// blt cr6,0x822f1fd0
	if (cr6.lt) goto loc_822F1FD0;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// b 0x822f1fd4
	goto loc_822F1FD4;
loc_822F1FD0:
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
loc_822F1FD4:
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// subf r6,r30,r9
	ctx.r6.s64 = ctx.r9.s64 - r30.s64;
	// add r5,r11,r30
	ctx.r5.u64 = r11.u64 + r30.u64;
	// subf r4,r29,r8
	ctx.r4.s64 = ctx.r8.s64 - r29.s64;
	// add r3,r10,r29
	ctx.r3.u64 = ctx.r10.u64 + r29.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// cmplwi cr6,r10,16
	cr6.compare<uint32_t>(ctx.r10.u32, 16, xer);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// blt cr6,0x822f2008
	if (cr6.lt) goto loc_822F2008;
	// lwz r28,0(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_822F2008:
	// li r10,0
	ctx.r10.s64 = 0;
	// stbx r10,r28,r11
	PPC_STORE_U8(r28.u32 + r11.u32, ctx.r10.u8);
loc_822F2010:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F2020"))) PPC_WEAK_FUNC(sub_822F2020);
PPC_FUNC_IMPL(__imp__sub_822F2020) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,15
	ctx.r9.s64 = 15;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// stw r9,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r9.u32);
	// stb r10,4(r31)
	PPC_STORE_U8(r31.u32 + 4, ctx.r10.u8);
loc_822F204C:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f204c
	if (!cr6.eq) goto loc_822F204C;
	// subf r11,r4,r11
	r11.s64 = r11.s64 - ctx.r4.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(r11.u32, 0);
	// bl 0x821a8f68
	sub_821A8F68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F2088"))) PPC_WEAK_FUNC(sub_822F2088);
PPC_FUNC_IMPL(__imp__sub_822F2088) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,28(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f20d4
	if (!cr6.eq) goto loc_822F20D4;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f20d4
	if (cr6.eq) goto loc_822F20D4;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f20c8
	if (cr6.lt) goto loc_822F20C8;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x822f20cc
	goto loc_822F20CC;
loc_822F20C8:
	// addi r3,r31,4
	ctx.r3.s64 = r31.s64 + 4;
loc_822F20CC:
	// bl 0x822ca580
	sub_822CA580(ctx, base);
	// stw r3,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r3.u32);
loc_822F20D4:
	// lwz r3,28(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 28);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F20F0"))) PPC_WEAK_FUNC(sub_822F20F0);
PPC_FUNC_IMPL(__imp__sub_822F20F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// addi r9,r3,16
	ctx.r9.s64 = ctx.r3.s64 + 16;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmplw cr6,r3,r9
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f216c
	if (cr6.eq) goto loc_822F216C;
loc_822F2100:
	// lhz r11,0(r3)
	r11.u64 = PPC_LOAD_U16(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,48
	cr6.compare<uint32_t>(r11.u32, 48, xer);
	// blt cr6,0x822f2124
	if (cr6.lt) goto loc_822F2124;
	// cmplwi cr6,r11,57
	cr6.compare<uint32_t>(r11.u32, 57, xer);
	// bgt cr6,0x822f2124
	if (cr6.gt) goto loc_822F2124;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r10,r11,-48
	ctx.r10.s64 = r11.s64 + -48;
	// b 0x822f2160
	goto loc_822F2160;
loc_822F2124:
	// cmplwi cr6,r11,97
	cr6.compare<uint32_t>(r11.u32, 97, xer);
	// blt cr6,0x822f2144
	if (cr6.lt) goto loc_822F2144;
	// cmplwi cr6,r11,102
	cr6.compare<uint32_t>(r11.u32, 102, xer);
	// bgt cr6,0x822f2144
	if (cr6.gt) goto loc_822F2144;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r10,r11,-87
	ctx.r10.s64 = r11.s64 + -87;
	// b 0x822f2160
	goto loc_822F2160;
loc_822F2144:
	// cmplwi cr6,r11,65
	cr6.compare<uint32_t>(r11.u32, 65, xer);
	// blt cr6,0x822f2174
	if (cr6.lt) goto loc_822F2174;
	// cmplwi cr6,r11,70
	cr6.compare<uint32_t>(r11.u32, 70, xer);
	// bgt cr6,0x822f2174
	if (cr6.gt) goto loc_822F2174;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r10,r11,-55
	ctx.r10.s64 = r11.s64 + -55;
loc_822F2160:
	// addi r3,r3,2
	ctx.r3.s64 = ctx.r3.s64 + 2;
	// cmplw cr6,r3,r9
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f2100
	if (!cr6.eq) goto loc_822F2100;
loc_822F216C:
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// blr 
	return;
loc_822F2174:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F2180"))) PPC_WEAK_FUNC(sub_822F2180);
PPC_FUNC_IMPL(__imp__sub_822F2180) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f2238
	sub_822F2238(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F21B8"))) PPC_WEAK_FUNC(sub_822F21B8);
PPC_FUNC_IMPL(__imp__sub_822F21B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// ld r12,-4096(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// stwu r1,-4208(r1)
	ea = -4208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// li r7,-1
	ctx.r7.s64 = -1;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// li r5,2049
	ctx.r5.s64 = 2049;
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x8217a978
	sub_8217A978(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8217ab30
	sub_8217AB30(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x8217a6c0
	sub_8217A6C0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f2238
	sub_822F2238(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,4208
	ctx.r1.s64 = ctx.r1.s64 + 4208;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F2238"))) PPC_WEAK_FUNC(sub_822F2238);
PPC_FUNC_IMPL(__imp__sub_822F2238) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r28,r30,4
	r28.s64 = r30.s64 + 4;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f2260
	if (cr6.lt) goto loc_822F2260;
	// lwz r4,4(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x822f2264
	goto loc_822F2264;
loc_822F2260:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_822F2264:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822f6ee8
	sub_822F6EE8(ctx, base);
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f2284
	if (cr6.lt) goto loc_822F2284;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// b 0x822f2288
	goto loc_822F2288;
loc_822F2284:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_822F2288:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f6ee8
	sub_822F6EE8(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x8217a7f8
	sub_8217A7F8(ctx, base);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r10,0
	ctx.r10.s64 = 0;
	// lbz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// ld r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lis r7,-32232
	ctx.r7.s64 = -2112356352;
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r7,r7,-22144
	ctx.r7.s64 = ctx.r7.s64 + -22144;
	// ld r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// std r9,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r9.u64);
	// lwz r5,100(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lbz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// bl 0x8217a9a8
	sub_8217A9A8(ctx, base);
	// li r29,47
	r29.s64 = 47;
	// li r6,1
	ctx.r6.s64 = 1;
	// sth r29,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r29.u16);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8217a858
	sub_8217A858(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// beq cr6,0x822f233c
	if (cr6.eq) goto loc_822F233C;
	// sth r29,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r29.u16);
loc_822F2304:
	// li r7,92
	ctx.r7.s64 = 92;
	// li r6,1
	ctx.r6.s64 = 1;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f52a8
	sub_822F52A8(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r31,1
	ctx.r5.s64 = r31.s64 + 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8217a858
	sub_8217A858(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// bne cr6,0x822f2304
	if (!cr6.eq) goto loc_822F2304;
loc_822F233C:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f2390
	if (cr6.eq) goto loc_822F2390;
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822f2358
	if (!cr6.gt) goto loc_822F2358;
	// twi 31,r0,22
loc_822F2358:
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f236c
	if (cr6.lt) goto loc_822F236C;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// b 0x822f2370
	goto loc_822F2370;
loc_822F236C:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_822F2370:
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + r11.u32);
	// cmplwi cr6,r9,92
	cr6.compare<uint32_t>(ctx.r9.u32, 92, xer);
	// beq cr6,0x822f2390
	if (cr6.eq) goto loc_822F2390;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,5760
	ctx.r4.s64 = r11.s64 + 5760;
	// bl 0x822f5258
	sub_822F5258(ctx, base);
loc_822F2390:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F2398"))) PPC_WEAK_FUNC(sub_822F2398);
PPC_FUNC_IMPL(__imp__sub_822F2398) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// li r6,-1
	ctx.r6.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// bl 0x822f6380
	sub_822F6380(ctx, base);
	// lwz r9,28(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r9,28(r31)
	PPC_STORE_U32(r31.u32 + 28, ctx.r9.u32);
	// lwz r8,32(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// stw r8,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r8.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F2400"))) PPC_WEAK_FUNC(sub_822F2400);
PPC_FUNC_IMPL(__imp__sub_822F2400) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r8,28(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// subf r31,r3,r31
	r31.s64 = r31.s64 - ctx.r3.s64;
	// addi r3,r31,1
	ctx.r3.s64 = r31.s64 + 1;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// lwz r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,12(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 12);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stbx r10,r28,r31
	PPC_STORE_U8(r28.u32 + r31.u32, ctx.r10.u8);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F24A8"))) PPC_WEAK_FUNC(sub_822F24A8);
PPC_FUNC_IMPL(__imp__sub_822F24A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32240
	r11.s64 = -2112880640;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,-13180
	ctx.r9.s64 = r11.s64 + -13180;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x822f24dc
	if (cr6.eq) goto loc_822F24DC;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_822F24DC:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F24F0"))) PPC_WEAK_FUNC(sub_822F24F0);
PPC_FUNC_IMPL(__imp__sub_822F24F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// lwz r11,-8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + -8);
	// subf r3,r11,r3
	ctx.r3.s64 = ctx.r3.s64 - r11.s64;
	// b 0x8221be68
	sub_8221BE68(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822F2504"))) PPC_WEAK_FUNC(sub_822F2504);
PPC_FUNC_IMPL(__imp__sub_822F2504) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F2508"))) PPC_WEAK_FUNC(sub_822F2508);
PPC_FUNC_IMPL(__imp__sub_822F2508) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r3,26932(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 26932);
	// b 0x82cbc658
	sub_82CBC658(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822F2518"))) PPC_WEAK_FUNC(sub_822F2518);
PPC_FUNC_IMPL(__imp__sub_822F2518) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// stb r30,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r30.u8);
	// stb r30,5(r31)
	PPC_STORE_U8(r31.u32 + 5, r30.u8);
	// bl 0x832b258c
	__imp__RtlInitializeCriticalSection(ctx, base);
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// li r3,36
	ctx.r3.s64 = 36;
	// stw r30,44(r31)
	PPC_STORE_U32(r31.u32 + 44, r30.u32);
	// stw r30,48(r31)
	PPC_STORE_U32(r31.u32 + 48, r30.u32);
	// stw r30,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r30.u32);
	// stw r30,60(r31)
	PPC_STORE_U32(r31.u32 + 60, r30.u32);
	// stw r30,64(r31)
	PPC_STORE_U32(r31.u32 + 64, r30.u32);
	// stw r30,68(r31)
	PPC_STORE_U32(r31.u32 + 68, r30.u32);
	// stw r30,72(r31)
	PPC_STORE_U32(r31.u32 + 72, r30.u32);
	// stw r30,76(r31)
	PPC_STORE_U32(r31.u32 + 76, r30.u32);
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// stw r30,84(r31)
	PPC_STORE_U32(r31.u32 + 84, r30.u32);
	// stw r30,88(r31)
	PPC_STORE_U32(r31.u32 + 88, r30.u32);
	// stw r30,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r30.u32);
	// stw r30,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r30.u32);
	// stw r30,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r30.u32);
	// stw r30,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r30.u32);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f25a4
	if (cr6.eq) goto loc_822F25A4;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822F25A4:
	// addic. r10,r11,4
	xer.ca = r11.u32 > 4294967291;
	ctx.r10.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822f25b0
	if (cr0.eq) goto loc_822F25B0;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
loc_822F25B0:
	// addic. r10,r11,8
	xer.ca = r11.u32 > 4294967287;
	ctx.r10.s64 = r11.s64 + 8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822f25bc
	if (cr0.eq) goto loc_822F25BC;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
loc_822F25BC:
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r30,33(r11)
	PPC_STORE_U8(r11.u32 + 33, r30.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r10,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r10.u8);
	// stw r11,112(r31)
	PPC_STORE_U32(r31.u32 + 112, r11.u32);
	// stb r10,33(r11)
	PPC_STORE_U8(r11.u32 + 33, ctx.r10.u8);
	// lwz r11,112(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// lwz r10,112(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
	// lwz r9,112(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// stw r9,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r9.u32);
	// stw r30,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r30.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F2608"))) PPC_WEAK_FUNC(sub_822F2608);
PPC_FUNC_IMPL(__imp__sub_822F2608) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r29,r31,72
	r29.s64 = r31.s64 + 72;
	// lwz r3,72(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 72);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2648
	if (cr6.eq) goto loc_822F2648;
	// bl 0x8236cc90
	sub_8236CC90(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f2640
	if (cr6.eq) goto loc_822F2640;
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x8236ca90
	sub_8236CA90(ctx, base);
loc_822F2640:
	// lwz r3,0(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// bl 0x8236c940
	sub_8236C940(ctx, base);
loc_822F2648:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2668
	if (cr6.eq) goto loc_822F2668;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2668:
	// li r30,0
	r30.s64 = 0;
	// addi r3,r31,108
	ctx.r3.s64 = r31.s64 + 108;
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// bl 0x822f5718
	sub_822F5718(ctx, base);
	// addi r3,r31,100
	ctx.r3.s64 = r31.s64 + 100;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// addi r3,r31,92
	ctx.r3.s64 = r31.s64 + 92;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// addi r3,r31,84
	ctx.r3.s64 = r31.s64 + 84;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f26b0
	if (cr6.eq) goto loc_822F26B0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F26B0:
	// stw r30,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r30.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// addi r3,r31,56
	ctx.r3.s64 = r31.s64 + 56;
	// bl 0x82356698
	sub_82356698(ctx, base);
	// lwz r3,48(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 48);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f26e0
	if (cr6.eq) goto loc_822F26E0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F26E0:
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 44);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f26fc
	if (cr6.eq) goto loc_822F26FC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F26FC:
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 40);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f271c
	if (cr6.eq) goto loc_822F271C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F271C:
	// stw r30,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r30.u32);
	// lwz r3,36(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2740
	if (cr6.eq) goto loc_822F2740;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2740:
	// stw r30,36(r31)
	PPC_STORE_U32(r31.u32 + 36, r30.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F2750"))) PPC_WEAK_FUNC(sub_822F2750);
PPC_FUNC_IMPL(__imp__sub_822F2750) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-656(r1)
	ea = -656 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r10,260
	ctx.r10.s64 = 260;
	// addi r3,r11,5800
	ctx.r3.s64 = r11.s64 + 5800;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r8,r1,88
	ctx.r8.s64 = ctx.r1.s64 + 88;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r4,r1,368
	ctx.r4.s64 = ctx.r1.s64 + 368;
	// li r31,0
	r31.s64 = 0;
	// bl 0x82cbc6c0
	sub_82CBC6C0(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f27a4
	if (cr6.eq) goto loc_822F27A4;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,1
	ctx.r3.s64 = 1;
	// cmplwi cr6,r11,42
	cr6.compare<uint32_t>(r11.u32, 42, xer);
	// bgt cr6,0x822f27a8
	if (cr6.gt) goto loc_822F27A8;
loc_822F27A4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_822F27A8:
	// addi r1,r1,656
	ctx.r1.s64 = ctx.r1.s64 + 656;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F27C0"))) PPC_WEAK_FUNC(sub_822F27C0);
PPC_FUNC_IMPL(__imp__sub_822F27C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-1152(r1)
	ea = -1152 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// bl 0x82b3d380
	sub_82B3D380(ctx, base);
	// li r28,0
	r28.s64 = 0;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stb r28,4(r27)
	PPC_STORE_U8(r27.u32 + 4, r28.u8);
	// stw r28,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r28.u32);
	// bl 0x82b3d3f0
	sub_82B3D3F0(ctx, base);
	// lis r11,-32209
	r11.s64 = -2110849024;
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r5,r11,9480
	ctx.r5.s64 = r11.s64 + 9480;
	// lwz r25,80(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82b3d680
	sub_82B3D680(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,5820
	ctx.r4.s64 = ctx.r10.s64 + 5820;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r6,2
	ctx.r6.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x82c63fb8
	sub_82C63FB8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x82c62de8
	sub_82C62DE8(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f2858
	if (cr6.eq) goto loc_822F2858;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// addi r4,r11,28912
	ctx.r4.s64 = r11.s64 + 28912;
	// bl 0x822f2400
	sub_822F2400(ctx, base);
	// b 0x822f28b4
	goto loc_822F28B4;
loc_822F2858:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,5848
	ctx.r4.s64 = r11.s64 + 5848;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r6,2
	ctx.r6.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c63fb8
	sub_82C63FB8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c62de8
	sub_82C62DE8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f28ac
	if (cr6.eq) goto loc_822F28AC;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r4,r11,28912
	ctx.r4.s64 = r11.s64 + 28912;
	// bl 0x822f2400
	sub_822F2400(ctx, base);
loc_822F28AC:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c62a08
	sub_82C62A08(ctx, base);
loc_822F28B4:
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x82c65418
	sub_82C65418(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82c645a8
	sub_82C645A8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// addi r4,r11,5868
	ctx.r4.s64 = r11.s64 + 5868;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x822f2238
	sub_822F2238(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82b3e1b0
	sub_82B3E1B0(ctx, base);
	// lwz r10,216(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f2904
	if (cr6.lt) goto loc_822F2904;
	// lwz r3,196(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F2904:
	// li r29,7
	r29.s64 = 7;
	// stw r28,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, r28.u32);
	// sth r28,196(r1)
	PPC_STORE_U16(ctx.r1.u32 + 196, r28.u16);
	// stw r29,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, r29.u32);
	// bl 0x822f2750
	sub_822F2750(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f2b94
	if (cr6.eq) goto loc_822F2B94;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r4,r11,5880
	ctx.r4.s64 = r11.s64 + 5880;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,164
	ctx.r3.s64 = ctx.r1.s64 + 164;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x82c68b50
	sub_82C68B50(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// lwz r3,164(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f296c
	if (cr6.eq) goto loc_822F296C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F296C:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82b40a18
	sub_82B40A18(ctx, base);
	// stw r29,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r29.u32);
	// stw r28,324(r1)
	PPC_STORE_U32(ctx.r1.u32 + 324, r28.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// sth r28,308(r1)
	PPC_STORE_U16(ctx.r1.u32 + 308, r28.u16);
	// li r6,2
	ctx.r6.s64 = 2;
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r5,r1,304
	ctx.r5.s64 = ctx.r1.s64 + 304;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r3,r1,304
	ctx.r3.s64 = ctx.r1.s64 + 304;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r4,r9,5896
	ctx.r4.s64 = ctx.r9.s64 + 5896;
	// bl 0x822f21b8
	sub_822F21B8(ctx, base);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// addi r4,r8,5908
	ctx.r4.s64 = ctx.r8.s64 + 5908;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// bl 0x822f2238
	sub_822F2238(ctx, base);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,464
	ctx.r4.s64 = ctx.r1.s64 + 464;
	// addi r3,r1,152
	ctx.r3.s64 = ctx.r1.s64 + 152;
	// bl 0x82b3e1b0
	sub_82B3E1B0(ctx, base);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,2
	ctx.r6.s64 = 2;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r10,36(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 36);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,152(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2a3c
	if (cr6.eq) goto loc_822F2A3C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2A3C:
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// addi r4,r11,5936
	ctx.r4.s64 = r11.s64 + 5936;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// stw r28,412(r1)
	PPC_STORE_U32(ctx.r1.u32 + 412, r28.u32);
	// stw r28,416(r1)
	PPC_STORE_U32(ctx.r1.u32 + 416, r28.u32);
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x8217aa20
	sub_8217AA20(ctx, base);
	// stw r29,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r29.u32);
	// stw r28,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, r28.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// sth r28,244(r1)
	PPC_STORE_U16(ctx.r1.u32 + 244, r28.u16);
	// addi r5,r1,384
	ctx.r5.s64 = ctx.r1.s64 + 384;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x82c686b0
	sub_82C686B0(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,2
	ctx.r6.s64 = 2;
	// addi r5,r1,240
	ctx.r5.s64 = ctx.r1.s64 + 240;
	// lwz r8,36(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,160(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2ad0
	if (cr6.eq) goto loc_822F2AD0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2AD0:
	// addi r3,r1,240
	ctx.r3.s64 = ctx.r1.s64 + 240;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// addi r4,r11,5960
	ctx.r4.s64 = r11.s64 + 5960;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// stw r28,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r28.u32);
	// stw r28,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, r28.u32);
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// bl 0x8217aa20
	sub_8217AA20(ctx, base);
	// stw r29,296(r1)
	PPC_STORE_U32(ctx.r1.u32 + 296, r29.u32);
	// stw r28,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r28.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// sth r28,276(r1)
	PPC_STORE_U16(ctx.r1.u32 + 276, r28.u16);
	// addi r5,r1,336
	ctx.r5.s64 = ctx.r1.s64 + 336;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,132
	ctx.r3.s64 = ctx.r1.s64 + 132;
	// bl 0x82c686b0
	sub_82C686B0(ctx, base);
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// li r6,2
	ctx.r6.s64 = 2;
	// addi r5,r1,272
	ctx.r5.s64 = ctx.r1.s64 + 272;
	// lwz r8,36(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 36);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,0(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,132(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2b64
	if (cr6.eq) goto loc_822F2B64;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2B64:
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r3,r1,336
	ctx.r3.s64 = ctx.r1.s64 + 336;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x822f54c8
	sub_822F54C8(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2B94:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// li r10,260
	ctx.r10.s64 = 260;
	// addi r3,r11,5808
	ctx.r3.s64 = r11.s64 + 5808;
	// addi r9,r1,544
	ctx.r9.s64 = ctx.r1.s64 + 544;
	// addi r8,r1,220
	ctx.r8.s64 = ctx.r1.s64 + 220;
	// addi r7,r1,224
	ctx.r7.s64 = ctx.r1.s64 + 224;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// li r5,260
	ctx.r5.s64 = 260;
	// addi r4,r1,816
	ctx.r4.s64 = ctx.r1.s64 + 816;
	// bl 0x82cbc6c0
	sub_82CBC6C0(ctx, base);
	// lwz r26,84(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r3,1
	cr6.compare<int32_t>(ctx.r3.s32, 1, xer);
	// bne cr6,0x822f2cd0
	if (!cr6.eq) goto loc_822F2CD0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,432
	ctx.r3.s64 = ctx.r1.s64 + 432;
	// addi r4,r11,5988
	ctx.r4.s64 = r11.s64 + 5988;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// addi r3,r1,432
	ctx.r3.s64 = ctx.r1.s64 + 432;
	// bl 0x822f2238
	sub_822F2238(ctx, base);
	// addi r3,r1,148
	ctx.r3.s64 = ctx.r1.s64 + 148;
	// bl 0x8237ab90
	sub_8237AB90(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r31,0(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x82b44788
	sub_82B44788(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// addi r4,r1,432
	ctx.r4.s64 = ctx.r1.s64 + 432;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lwz r5,0(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// bl 0x82b3e1b0
	sub_82B3E1B0(ctx, base);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2c30
	if (cr6.eq) goto loc_822F2C30;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2C30:
	// lwz r3,148(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2c4c
	if (cr6.eq) goto loc_822F2C4C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2C4C:
	// addi r3,r1,432
	ctx.r3.s64 = ctx.r1.s64 + 432;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r4,r11,6016
	ctx.r4.s64 = r11.s64 + 6016;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,136
	ctx.r3.s64 = ctx.r1.s64 + 136;
	// bl 0x82c686b0
	sub_82C686B0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x822f54c8
	sub_822F54C8(ctx, base);
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2ca8
	if (cr6.eq) goto loc_822F2CA8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2CA8:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f2d28
	if (cr6.eq) goto loc_822F2D28;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// b 0x822f2d28
	goto loc_822F2D28;
loc_822F2CD0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r4,r11,6016
	ctx.r4.s64 = r11.s64 + 6016;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,140
	ctx.r3.s64 = ctx.r1.s64 + 140;
	// bl 0x82c686b0
	sub_82C686B0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x822f54c8
	sub_822F54C8(ctx, base);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2d20
	if (cr6.eq) goto loc_822F2D20;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2D20:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
loc_822F2D28:
	// bl 0x82369ee0
	sub_82369EE0(ctx, base);
	// lwz r29,80(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x8236a058
	sub_8236A058(ctx, base);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82c634c0
	sub_82C634C0(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r30,r11,6032
	r30.s64 = r11.s64 + 6032;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82c62848
	sub_82C62848(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// bl 0x82c634f8
	sub_82C634F8(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82c62900
	sub_82C62900(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r5,r11,28900
	ctx.r5.s64 = r11.s64 + 28900;
	// addi r4,r10,29960
	ctx.r4.s64 = ctx.r10.s64 + 29960;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821e2cc8
	sub_821E2CC8(ctx, base);
	// bl 0x82357df8
	sub_82357DF8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// bl 0x822f2750
	sub_822F2750(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f2dd8
	if (!cr6.eq) goto loc_822F2DD8;
	// lbz r10,-27427(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -27427);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f2de0
	if (cr6.eq) goto loc_822F2DE0;
loc_822F2DD8:
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r10,-27427(r11)
	PPC_STORE_U8(r11.u32 + -27427, ctx.r10.u8);
loc_822F2DE0:
	// lis r9,-31953
	ctx.r9.s64 = -2094071808;
	// lis r8,-31953
	ctx.r8.s64 = -2094071808;
	// lis r11,-32076
	r11.s64 = -2102132736;
	// lis r10,-32209
	ctx.r10.s64 = -2110849024;
	// addi r11,r11,-32528
	r11.s64 = r11.s64 + -32528;
	// addi r10,r10,9456
	ctx.r10.s64 = ctx.r10.s64 + 9456;
	// stw r11,27684(r9)
	PPC_STORE_U32(ctx.r9.u32 + 27684, r11.u32);
	// lis r31,-31926
	r31.s64 = -2092302336;
	// stw r10,27688(r8)
	PPC_STORE_U32(ctx.r8.u32 + 27688, ctx.r10.u32);
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// addi r4,r7,29592
	ctx.r4.s64 = ctx.r7.s64 + 29592;
	// lwz r24,-6420(r31)
	r24.u64 = PPC_LOAD_U32(r31.u32 + -6420);
	// bl 0x82b39978
	sub_82B39978(ctx, base);
	// lis r6,-31950
	ctx.r6.s64 = -2093875200;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// addi r10,r6,-27076
	ctx.r10.s64 = ctx.r6.s64 + -27076;
	// lis r5,-31950
	ctx.r5.s64 = -2093875200;
	// lis r3,-31950
	ctx.r3.s64 = -2093875200;
	// lwz r4,-27076(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + -27076);
	// addi r9,r1,92
	ctx.r9.s64 = ctx.r1.s64 + 92;
	// lwz r8,-27420(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + -27420);
	// lwz r7,-27424(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + -27424);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lbz r6,-27428(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + -27428);
	// lwz r11,0(r24)
	r11.u64 = PPC_LOAD_U32(r24.u32 + 0);
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x821c6868
	sub_821C6868(ctx, base);
	// clrlwi r9,r24,24
	ctx.r9.u64 = r24.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f3334
	if (cr6.eq) goto loc_822F3334;
	// lwz r3,-6420(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + -6420);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lbz r4,26927(r11)
	ctx.r4.u64 = PPC_LOAD_U8(r11.u32 + 26927);
	// lwz r9,40(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 40);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f2ed4
	if (cr6.eq) goto loc_822F2ED4;
	// lis r11,-32241
	r11.s64 = -2112946176;
	// li r3,300
	ctx.r3.s64 = 300;
	// addi r10,r11,7524
	ctx.r10.s64 = r11.s64 + 7524;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2ec8
	if (cr6.eq) goto loc_822F2EC8;
	// bl 0x82a153f0
	sub_82A153F0(ctx, base);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// b 0x822f2ed8
	goto loc_822F2ED8;
loc_822F2EC8:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// b 0x822f2ed8
	goto loc_822F2ED8;
loc_822F2ED4:
	// mr r31,r28
	r31.u64 = r28.u64;
loc_822F2ED8:
	// stw r31,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r31.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f2ef0
	if (cr6.eq) goto loc_822F2EF0;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x826324e0
	sub_826324E0(ctx, base);
	// b 0x822f2ef4
	goto loc_822F2EF4;
loc_822F2EF0:
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
loc_822F2EF4:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82b38998
	sub_82B38998(ctx, base);
	// lis r9,-31927
	ctx.r9.s64 = -2092367872;
	// lis r10,-31951
	ctx.r10.s64 = -2093940736;
	// mr r11,r28
	r11.u64 = r28.u64;
	// li r3,1
	ctx.r3.s64 = 1;
	// lbz r8,26938(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 26938);
	// stb r11,17931(r10)
	PPC_STORE_U8(ctx.r10.u32 + 17931, r11.u8);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f2f20
	if (!cr6.eq) goto loc_822F2F20;
	// li r3,0
	ctx.r3.s64 = 0;
loc_822F2F20:
	// bl 0x829f7b08
	sub_829F7B08(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,496
	ctx.r3.s64 = ctx.r1.s64 + 496;
	// addi r4,r11,6040
	ctx.r4.s64 = r11.s64 + 6040;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,496
	ctx.r5.s64 = ctx.r1.s64 + 496;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// addi r3,r1,156
	ctx.r3.s64 = ctx.r1.s64 + 156;
	// bl 0x82c686b0
	sub_82C686B0(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// addi r3,r27,48
	ctx.r3.s64 = r27.s64 + 48;
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// bl 0x822f54c8
	sub_822F54C8(ctx, base);
	// lwz r3,156(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2f74
	if (cr6.eq) goto loc_822F2F74;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F2F74:
	// addi r3,r1,496
	ctx.r3.s64 = ctx.r1.s64 + 496;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// bl 0x82b608b8
	sub_82B608B8(ctx, base);
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f2f9c
	if (cr6.eq) goto loc_822F2F9C;
	// bl 0x82b619d0
	sub_82B619D0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822f2fa0
	goto loc_822F2FA0;
loc_822F2F9C:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
loc_822F2FA0:
	// lis r31,-31926
	r31.s64 = -2092302336;
	// addi r3,r31,23652
	ctx.r3.s64 = r31.s64 + 23652;
	// bl 0x82b63988
	sub_82B63988(ctx, base);
	// lwz r11,23652(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 23652);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x82b61338
	sub_82B61338(ctx, base);
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r31,r11,-20604
	r31.s64 = r11.s64 + -20604;
	// li r11,1
	r11.s64 = 1;
	// stb r11,28071(r10)
	PPC_STORE_U8(ctx.r10.u32 + 28071, r11.u8);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f2fec
	if (cr6.eq) goto loc_822F2FEC;
	// lwz r10,-8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + -8);
	// subf r3,r10,r11
	ctx.r3.s64 = r11.s64 - ctx.r10.s64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F2FEC:
	// mr r11,r28
	r11.u64 = r28.u64;
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// li r4,4096
	ctx.r4.s64 = 4096;
	// stw r9,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r9.u32);
	// lis r3,16
	ctx.r3.s64 = 1048576;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// bl 0x82b380f0
	sub_82B380F0(ctx, base);
	// lis r11,16
	r11.s64 = 1048576;
	// li r10,4096
	ctx.r10.s64 = 4096;
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r10,12(r31)
	PPC_STORE_U32(r31.u32 + 12, ctx.r10.u32);
	// stw r9,16(r31)
	PPC_STORE_U32(r31.u32 + 16, ctx.r9.u32);
	// bl 0x82b66bc8
	sub_82B66BC8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f3054
	if (!cr6.eq) goto loc_822F3054;
	// lis r11,-32074
	r11.s64 = -2102001664;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r11,32184
	ctx.r3.s64 = r11.s64 + 32184;
	// bl 0x82ca34b0
	sub_82CA34B0(ctx, base);
loc_822F3054:
	// lis r31,-31927
	r31.s64 = -2092367872;
	// lbz r11,28070(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 28070);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f3068
	if (!cr6.eq) goto loc_822F3068;
	// bl 0x82b6ec08
	sub_82B6EC08(ctx, base);
loc_822F3068:
	// li r11,1
	r11.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stb r11,28070(r31)
	PPC_STORE_U8(r31.u32 + 28070, r11.u8);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lis r11,-31926
	r11.s64 = -2092302336;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r11,23644
	ctx.r3.s64 = r11.s64 + 23644;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f30e8
	if (cr6.eq) goto loc_822F30E8;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r3,608
	ctx.r3.s64 = 608;
	// addi r10,r11,31216
	ctx.r10.s64 = r11.s64 + 31216;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f30d8
	if (cr6.eq) goto loc_822F30D8;
	// bl 0x8236b988
	sub_8236B988(ctx, base);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// mr r30,r31
	r30.u64 = r31.u64;
	// b 0x822f30ec
	goto loc_822F30EC;
loc_822F30D8:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r30,r31
	r30.u64 = r31.u64;
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// b 0x822f30ec
	goto loc_822F30EC;
loc_822F30E8:
	// mr r30,r28
	r30.u64 = r28.u64;
loc_822F30EC:
	// addi r31,r27,72
	r31.s64 = r27.s64 + 72;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r30,72(r27)
	PPC_STORE_U32(r27.u32 + 72, r30.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f3140
	if (cr6.eq) goto loc_822F3140;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3138
	if (cr6.eq) goto loc_822F3138;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lis r9,-32105
	ctx.r9.s64 = -2104033280;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r7,r9,-32320
	ctx.r7.s64 = ctx.r9.s64 + -32320;
	// stw r8,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r8.u32);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r7,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r7.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// b 0x822f313c
	goto loc_822F313C;
loc_822F3138:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_822F313C:
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
loc_822F3140:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// lwz r3,4(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x8236baa8
	sub_8236BAA8(ctx, base);
	// bl 0x82b68f60
	sub_82B68F60(ctx, base);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// bl 0x8236c9f8
	sub_8236C9F8(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822f4958
	sub_822F4958(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822f4e98
	sub_822F4E98(ctx, base);
	// stb r28,5(r27)
	PPC_STORE_U8(r27.u32 + 5, r28.u8);
	// lis r10,-32209
	ctx.r10.s64 = -2110849024;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// lis r4,4
	ctx.r4.s64 = 262144;
	// addi r3,r10,13240
	ctx.r3.s64 = ctx.r10.s64 + 13240;
	// bl 0x82ca34b0
	sub_82CA34B0(ctx, base);
	// stw r3,52(r27)
	PPC_STORE_U32(r27.u32 + 52, ctx.r3.u32);
	// li r3,24
	ctx.r3.s64 = 24;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f3208
	if (cr6.eq) goto loc_822F3208;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r3,28
	ctx.r3.s64 = 28;
	// addi r10,r11,29964
	ctx.r10.s64 = r11.s64 + 29964;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f31cc
	if (cr6.eq) goto loc_822F31CC;
	// bl 0x832b258c
	__imp__RtlInitializeCriticalSection(ctx, base);
	// b 0x822f31d0
	goto loc_822F31D0;
loc_822F31CC:
	// mr r30,r28
	r30.u64 = r28.u64;
loc_822F31D0:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stb r28,12(r31)
	PPC_STORE_U8(r31.u32 + 12, r28.u8);
	// addi r8,r11,28344
	ctx.r8.s64 = r11.s64 + 28344;
	// stw r28,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r28.u32);
loc_822F31E4:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822f31e4
	if (!cr0.eq) goto loc_822F31E4;
	// stw r28,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r28.u32);
	// b 0x822f320c
	goto loc_822F320C;
loc_822F3208:
	// mr r31,r28
	r31.u64 = r28.u64;
loc_822F320C:
	// lwz r3,36(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 36);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f322c
	if (cr6.eq) goto loc_822F322C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F322C:
	// stw r31,36(r27)
	PPC_STORE_U32(r27.u32 + 36, r31.u32);
	// li r3,8
	ctx.r3.s64 = 8;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3258
	if (cr6.eq) goto loc_822F3258;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r10,r11,31220
	ctx.r10.s64 = r11.s64 + 31220;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stw r28,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r28.u32);
	// b 0x822f325c
	goto loc_822F325C;
loc_822F3258:
	// mr r31,r28
	r31.u64 = r28.u64;
loc_822F325C:
	// lwz r3,40(r27)
	ctx.r3.u64 = PPC_LOAD_U32(r27.u32 + 40);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f327c
	if (cr6.eq) goto loc_822F327C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F327C:
	// lis r11,-31950
	r11.s64 = -2093875200;
	// stw r31,40(r27)
	PPC_STORE_U32(r27.u32 + 40, r31.u32);
	// lbz r10,-27440(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -27440);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f32a8
	if (cr6.eq) goto loc_822F32A8;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// addi r31,r11,-20468
	r31.s64 = r11.s64 + -20468;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82b68cb8
	sub_82B68CB8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82a392f8
	sub_82A392F8(ctx, base);
loc_822F32A8:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f32cc
	if (cr6.eq) goto loc_822F32CC;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F32CC:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x822f32e8
	if (cr6.eq) goto loc_822F32E8;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F32E8:
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3304
	if (cr6.eq) goto loc_822F3304;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3304:
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x82c62a08
	sub_82C62A08(ctx, base);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x822f3328
	if (cr6.eq) goto loc_822F3328;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3328:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,1152
	ctx.r1.s64 = ctx.r1.s64 + 1152;
	// b 0x82ca2c28
	return;
loc_822F3334:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f3350
	if (cr6.eq) goto loc_822F3350;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3350:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x822f336c
	if (cr6.eq) goto loc_822F336C;
	// lwz r11,0(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F336C:
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3388
	if (cr6.eq) goto loc_822F3388;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3388:
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// bl 0x82c62a08
	sub_82C62A08(ctx, base);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x822f33ac
	if (cr6.eq) goto loc_822F33AC;
	// lwz r11,0(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 0);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F33AC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,1152
	ctx.r1.s64 = ctx.r1.s64 + 1152;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822F33B8"))) PPC_WEAK_FUNC(sub_822F33B8);
PPC_FUNC_IMPL(__imp__sub_822F33B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r3,r11,6076
	ctx.r3.s64 = r11.s64 + 6076;
	// bl 0x82b38798
	sub_82B38798(ctx, base);
	// li r4,3
	ctx.r4.s64 = 3;
	// li r3,-2
	ctx.r3.s64 = -2;
	// bl 0x82cbbe20
	sub_82CBBE20(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822f3b18
	sub_822F3B18(ctx, base);
	// li r3,208
	ctx.r3.s64 = 208;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3408
	if (cr6.eq) goto loc_822F3408;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,50
	ctx.r4.s64 = 50;
	// bl 0x8236d4e0
	sub_8236D4E0(ctx, base);
	// b 0x822f340c
	goto loc_822F340C;
loc_822F3408:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822F340C:
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r29,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r29.u32);
	// beq cr6,0x822f3430
	if (cr6.eq) goto loc_822F3430;
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x822f55a0
	sub_822F55A0(ctx, base);
	// lwz r30,124(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r29,120(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// b 0x822f3438
	goto loc_822F3438;
loc_822F3430:
	// li r30,0
	r30.s64 = 0;
	// stw r30,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r30.u32);
loc_822F3438:
	// lwz r11,88(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 88);
	// addi r31,r28,84
	r31.s64 = r28.s64 + 84;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x822f346c
	if (cr6.eq) goto loc_822F346C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f346c
	if (cr6.eq) goto loc_822F346C;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_822F346C:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x823726f8
	sub_823726F8(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,29984
	ctx.r4.s64 = r11.s64 + 29984;
	// bl 0x821f0108
	sub_821F0108(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r10,6324
	ctx.r4.s64 = ctx.r10.s64 + 6324;
	// bl 0x821da550
	sub_821DA550(ctx, base);
	// li r6,2
	ctx.r6.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c64018
	sub_82C64018(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// li r5,16384
	ctx.r5.s64 = 16384;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x82a1cdd8
	sub_82A1CDD8(ctx, base);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x8322f940
	sub_8322F940(ctx, base);
	// lis r9,-32241
	ctx.r9.s64 = -2112946176;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// addi r8,r9,7776
	ctx.r8.s64 = ctx.r9.s64 + 7776;
	// stw r8,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r8.u32);
	// bl 0x82a1cef0
	sub_82A1CEF0(ctx, base);
	// lis r7,-32241
	ctx.r7.s64 = -2112946176;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r6,r7,-1124
	ctx.r6.s64 = ctx.r7.s64 + -1124;
	// stw r6,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r6.u32);
	// bl 0x82c63398
	sub_82C63398(ctx, base);
	// lis r5,-31950
	ctx.r5.s64 = -2093875200;
	// lbz r4,-27440(r5)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r5.u32 + -27440);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f3550
	if (cr6.eq) goto loc_822F3550;
	// li r3,288
	ctx.r3.s64 = 288;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f351c
	if (cr6.eq) goto loc_822F351C;
	// bl 0x82378cd0
	sub_82378CD0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822f3520
	goto loc_822F3520;
loc_822F351C:
	// li r31,0
	r31.s64 = 0;
loc_822F3520:
	// lwz r3,80(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3540
	if (cr6.eq) goto loc_822F3540;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3540:
	// stw r31,80(r28)
	PPC_STORE_U32(r28.u32 + 80, r31.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// rotlwi r3,r31,0
	ctx.r3.u64 = __builtin_rotateleft32(r31.u32, 0);
	// bl 0x82378fa0
	sub_82378FA0(ctx, base);
loc_822F3550:
	// bl 0x829faf40
	sub_829FAF40(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,5732
	ctx.r4.s64 = r11.s64 + 5732;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// bl 0x82b3aea0
	sub_82B3AEA0(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r30,r11,63
	r30.s64 = r11.s64 + 63;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// beq cr6,0x822f3598
	if (cr6.eq) goto loc_822F3598;
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
loc_822F3598:
	// lis r31,-31927
	r31.s64 = -2092367872;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,27600(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 27600);
	// bl 0x829fb018
	sub_829FB018(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,5708
	ctx.r4.s64 = r11.s64 + 5708;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82b3aea0
	sub_82B3AEA0(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f35e4
	if (cr6.eq) goto loc_822F35E4;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F35E4:
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,27600(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 27600);
	// bl 0x829fb018
	sub_829FB018(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r28,8
	ctx.r4.s64 = r28.s64 + 8;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82200688
	sub_82200688(ctx, base);
	// lbz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 100);
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// stb r11,5(r28)
	PPC_STORE_U8(r28.u32 + 5, r11.u8);
	// beq cr6,0x822f361c
	if (cr6.eq) goto loc_822F361C;
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
loc_822F361C:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F3640"))) PPC_WEAK_FUNC(sub_822F3640);
PPC_FUNC_IMPL(__imp__sub_822F3640) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r31,8
	ctx.r4.s64 = r31.s64 + 8;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82200688
	sub_82200688(ctx, base);
	// lwz r11,52(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3684
	if (cr6.eq) goto loc_822F3684;
	// lbz r11,5(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 5);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3684
	if (cr6.eq) goto loc_822F3684;
	// li r11,0
	r11.s64 = 0;
	// stw r11,52(r31)
	PPC_STORE_U32(r31.u32 + 52, r11.u32);
loc_822F3684:
	// lbz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 84);
	// lbz r31,5(r31)
	r31.u64 = PPC_LOAD_U8(r31.u32 + 5);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f369c
	if (cr6.eq) goto loc_822F369C;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
loc_822F369C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F36B8"))) PPC_WEAK_FUNC(sub_822F36B8);
PPC_FUNC_IMPL(__imp__sub_822F36B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x82375100
	sub_82375100(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823754c8
	sub_823754C8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f380c
	if (cr6.eq) goto loc_822F380C;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// addi r4,r11,3224
	ctx.r4.s64 = r11.s64 + 3224;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r6,36(r30)
	ctx.r6.u64 = PPC_LOAD_U32(r30.u32 + 36);
	// bl 0x82c680f8
	sub_82C680F8(ctx, base);
	// lwz r31,0(r3)
	r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
	// beq cr6,0x822f3740
	if (cr6.eq) goto loc_822F3740;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3740:
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f375c
	if (cr6.eq) goto loc_822F375C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F375C:
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82c645a8
	sub_82C645A8(ctx, base);
	// lwz r3,0(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r11.u32);
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// li r6,2
	ctx.r6.s64 = 2;
	// sth r11,100(r1)
	PPC_STORE_U16(ctx.r1.u32 + 100, r11.u16);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r9,0(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r8,36(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f37c8
	if (cr6.eq) goto loc_822F37C8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F37C8:
	// addi r4,r1,92
	ctx.r4.s64 = ctx.r1.s64 + 92;
	// addi r3,r30,40
	ctx.r3.s64 = r30.s64 + 40;
	// bl 0x822f6188
	sub_822F6188(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f37f0
	if (cr6.eq) goto loc_822F37F0;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F37F0:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f380c
	if (cr6.eq) goto loc_822F380C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F380C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F3828"))) PPC_WEAK_FUNC(sub_822F3828);
PPC_FUNC_IMPL(__imp__sub_822F3828) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82375100
	sub_82375100(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823754c8
	sub_823754C8(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3890
	if (cr6.eq) goto loc_822F3890;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x822f6258
	sub_822F6258(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3890
	if (cr6.eq) goto loc_822F3890;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3890:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F38B0"))) PPC_WEAK_FUNC(sub_822F38B0);
PPC_FUNC_IMPL(__imp__sub_822F38B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f3b08
	if (cr6.eq) goto loc_822F3B08;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// addi r5,r11,28904
	ctx.r5.s64 = r11.s64 + 28904;
	// addi r4,r10,29960
	ctx.r4.s64 = ctx.r10.s64 + 29960;
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x821e2cc8
	sub_821E2CC8(ctx, base);
	// addi r4,r1,100
	ctx.r4.s64 = ctx.r1.s64 + 100;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82b3ada0
	sub_82B3ADA0(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82b40708
	sub_82B40708(ctx, base);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821f0108
	sub_821F0108(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r9,6032
	ctx.r4.s64 = ctx.r9.s64 + 6032;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lis r11,-32247
	r11.s64 = -2113339392;
	// li r29,0
	r29.s64 = 0;
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f393c
	if (!cr6.eq) goto loc_822F393C;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822f3944
	goto loc_822F3944;
loc_822F393C:
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F3944:
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f3954
	if (cr6.eq) goto loc_822F3954;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F3954:
	// bl 0x82ca3920
	sub_82CA3920(ctx, base);
	// cntlzw r11,r3
	r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// rlwinm r28,r11,27,31,31
	r28.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 27) & 0x1;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822f39a4
	if (cr6.eq) goto loc_822F39A4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// beq cr6,0x822f3980
	if (cr6.eq) goto loc_822F3980;
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 4);
loc_822F3980:
	// li r5,5
	ctx.r5.s64 = 5;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x821e3950
	sub_821E3950(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
loc_822F39A4:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82b39978
	sub_82B39978(ctx, base);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f39c8
	if (!cr6.eq) goto loc_822F39C8;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r4,r11,-28040
	ctx.r4.s64 = r11.s64 + -28040;
	// b 0x822f39cc
	goto loc_822F39CC;
loc_822F39C8:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F39CC:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822f5150
	sub_822F5150(ctx, base);
	// stw r29,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r29.u32);
	// stw r29,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r29.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x8217aa20
	sub_8217AA20(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x821c6868
	sub_821C6868(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// li r6,1
	ctx.r6.s64 = 1;
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3ae0
	if (cr6.eq) goto loc_822F3AE0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rotlwi r31,r3,0
	r31.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x822f3ac4
	if (!cr6.gt) goto loc_822F3AC4;
	// addi r3,r31,1
	ctx.r3.s64 = r31.s64 + 1;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r30,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r30.u32);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// stw r29,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r29.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x822f3abc
	if (!cr6.eq) goto loc_822F3ABC;
	// bl 0x824151e8
	sub_824151E8(ctx, base);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// bl 0x823577c0
	sub_823577C0(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,28768
	ctx.r4.s64 = r11.s64 + 28768;
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r7,r1,92
	ctx.r7.s64 = ctx.r1.s64 + 92;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x824155c8
	sub_824155C8(ctx, base);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82415428
	sub_82415428(ctx, base);
loc_822F3ABC:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F3AC4:
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3ae0
	if (cr6.eq) goto loc_822F3AE0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3AE0:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,100
	ctx.r3.s64 = ctx.r1.s64 + 100;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
loc_822F3B08:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F3B18"))) PPC_WEAK_FUNC(sub_822F3B18);
PPC_FUNC_IMPL(__imp__sub_822F3B18) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x82373e08
	sub_82373E08(ctx, base);
	// bl 0x82372e88
	sub_82372E88(ctx, base);
	// bl 0x82369ee0
	sub_82369EE0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c62940
	sub_82C62940(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,6100
	ctx.r4.s64 = r11.s64 + 6100;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r6,2
	ctx.r6.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c635d8
	sub_82C635D8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c62de8
	sub_82C62DE8(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r11,26948(r10)
	PPC_STORE_U8(ctx.r10.u32 + 26948, r11.u8);
	// bl 0x82368538
	sub_82368538(ctx, base);
	// lis r9,-32209
	ctx.r9.s64 = -2110849024;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r5,r9,14512
	ctx.r5.s64 = ctx.r9.s64 + 14512;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8236a530
	sub_8236A530(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3bc0
	if (cr6.eq) goto loc_822F3BC0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3BC0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r4,r11,6120
	ctx.r4.s64 = r11.s64 + 6120;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8236a460
	sub_8236A460(ctx, base);
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f3bf0
	if (cr6.lt) goto loc_822F3BF0;
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F3BF0:
	// li r31,0
	r31.s64 = 0;
	// li r29,7
	r29.s64 = 7;
	// stw r31,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r31.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r29,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r29.u32);
	// li r5,-1
	ctx.r5.s64 = -1;
	// sth r31,116(r1)
	PPC_STORE_U16(ctx.r1.u32 + 116, r31.u16);
	// addi r4,r11,6168
	ctx.r4.s64 = r11.s64 + 6168;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,6140
	ctx.r4.s64 = ctx.r10.s64 + 6140;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// bl 0x82a492b0
	sub_82A492B0(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x821f0108
	sub_821F0108(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82a48b68
	sub_82A48B68(ctx, base);
	// bl 0x82a493a8
	sub_82A493A8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// addi r4,r9,6196
	ctx.r4.s64 = ctx.r9.s64 + 6196;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822f4458
	sub_822F4458(ctx, base);
	// lwz r8,184(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplwi cr6,r8,8
	cr6.compare<uint32_t>(ctx.r8.u32, 8, xer);
	// blt cr6,0x822f3c94
	if (cr6.lt) goto loc_822F3C94;
	// lwz r3,164(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F3C94:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// stw r31,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r31.u32);
	// sth r31,164(r1)
	PPC_STORE_U16(ctx.r1.u32 + 164, r31.u16);
	// addi r30,r11,29696
	r30.s64 = r11.s64 + 29696;
	// stw r29,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r29.u32);
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822f3cd0
	if (cr6.eq) goto loc_822F3CD0;
loc_822F3CB8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f3d20
	sub_822F3D20(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r31,r31,20
	r31.s64 = r31.s64 + 20;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f3cb8
	if (!cr6.eq) goto loc_822F3CB8;
loc_822F3CD0:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r30,r11,29680
	r30.s64 = r11.s64 + 29680;
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822f3d10
	if (cr6.eq) goto loc_822F3D10;
loc_822F3CE8:
	// lbz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 12);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f3d04
	if (!cr6.eq) goto loc_822F3D04;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822f4260
	sub_822F4260(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
loc_822F3D04:
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f3ce8
	if (!cr6.eq) goto loc_822F3CE8;
loc_822F3D10:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82c62a08
	sub_82C62A08(ctx, base);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F3D20"))) PPC_WEAK_FUNC(sub_822F3D20);
PPC_FUNC_IMPL(__imp__sub_822F3D20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd0
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r31,r11,29696
	r31.s64 = r11.s64 + 29696;
	// lwz r22,4(r31)
	r22.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r22,r10
	cr6.compare<uint32_t>(r22.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f40f8
	if (cr6.eq) goto loc_822F40F8;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r28,r11,63
	r28.s64 = r11.s64 + 63;
loc_822F3D50:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3d64
	if (cr6.eq) goto loc_822F3D64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F3D64:
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3d78
	if (cr6.eq) goto loc_822F3D78;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F3D78:
	// bl 0x82ca6320
	sub_82CA6320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f3d9c
	if (cr6.eq) goto loc_822F3D9C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r22,r22,20
	r22.s64 = r22.s64 + 20;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// bne cr6,0x822f3d50
	if (!cr6.eq) goto loc_822F3D50;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c20
	return;
loc_822F3D9C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,6228
	ctx.r4.s64 = r11.s64 + 6228;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r31,r28
	r31.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3dc4
	if (cr6.eq) goto loc_822F3DC4;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F3DC4:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r30,r28
	r30.u64 = r28.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3dd8
	if (cr6.eq) goto loc_822F3DD8;
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F3DD8:
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822F3DDC:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f3ddc
	if (!cr6.eq) goto loc_822F3DDC;
	// subf r11,r30,r11
	r11.s64 = r11.s64 - r30.s64;
	// li r27,0
	r27.s64 = 0;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// rotlwi r29,r11,0
	r29.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x822f3e0c
	if (!cr6.eq) goto loc_822F3E0C;
loc_822F3E04:
	// li r31,1
	r31.s64 = 1;
	// b 0x822f3e68
	goto loc_822F3E68;
loc_822F3E0C:
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3e64
	if (cr6.eq) goto loc_822F3E64;
loc_822F3E18:
	// lbz r11,0(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 0);
	// extsb r3,r11
	ctx.r3.s64 = r11.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// lbz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// extsb r3,r10
	ctx.r3.s64 = ctx.r10.s8;
	// bl 0x821eeb10
	sub_821EEB10(ctx, base);
	// cmpw cr6,r3,r26
	cr6.compare<int32_t>(ctx.r3.s32, r26.s32, xer);
	// bne cr6,0x822f3e54
	if (!cr6.eq) goto loc_822F3E54;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cab438
	sub_82CAB438(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f3e04
	if (cr6.eq) goto loc_822F3E04;
loc_822F3E54:
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// lbz r11,0(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f3e18
	if (!cr6.eq) goto loc_822F3E18;
loc_822F3E64:
	// mr r31,r27
	r31.u64 = r27.u64;
loc_822F3E68:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821c67d8
	sub_821C67D8(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r24,r11,28344
	r24.s64 = r11.s64 + 28344;
	// mr r8,r24
	ctx.r8.u64 = r24.u64;
loc_822F3E7C:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822f3e7c
	if (!cr0.eq) goto loc_822F3E7C;
	// clrlwi r7,r31,24
	ctx.r7.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822f40d8
	if (cr6.eq) goto loc_822F40D8;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r23,r11,3224
	r23.s64 = r11.s64 + 3224;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r27.u32);
	// stw r27,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r27.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// stw r27,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r27.u32);
	// lwz r11,0(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f3ed8
	if (cr6.eq) goto loc_822F3ED8;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F3ED8:
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// li r6,-1
	ctx.r6.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f6380
	sub_822F6380(ctx, base);
	// lwz r11,188(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r10,192(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// lwz r9,184(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r11.u32);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// blt cr6,0x822f3f18
	if (cr6.lt) goto loc_822F3F18;
	// lwz r3,164(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F3F18:
	// li r25,7
	r25.s64 = 7;
	// stw r27,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, r27.u32);
	// sth r27,164(r1)
	PPC_STORE_U16(ctx.r1.u32 + 164, r27.u16);
	// lis r11,-32209
	r11.s64 = -2110849024;
	// stw r25,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, r25.u32);
	// addi r31,r1,96
	r31.s64 = ctx.r1.s64 + 96;
	// addi r30,r11,14376
	r30.s64 = r11.s64 + 14376;
	// bl 0x82369ee0
	sub_82369EE0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// bl 0x8236a530
	sub_8236A530(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f3f68
	if (cr6.eq) goto loc_822F3F68;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F3F68:
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r4,140(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplw cr6,r3,r4
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, xer);
	// ble cr6,0x822f3f7c
	if (!cr6.gt) goto loc_822F3F7C;
	// twi 31,r0,22
loc_822F3F7C:
	// lis r11,-31926
	r11.s64 = -2092302336;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r26,-31927
	r26.s64 = -2092367872;
	// addi r28,r11,-7876
	r28.s64 = r11.s64 + -7876;
loc_822F3F8C:
	// cmplw cr6,r3,r4
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, xer);
	// ble cr6,0x822f3f98
	if (!cr6.gt) goto loc_822F3F98;
	// twi 31,r0,22
loc_822F3F98:
	// cmplw cr6,r29,r4
	cr6.compare<uint32_t>(r29.u32, ctx.r4.u32, xer);
	// beq cr6,0x822f4098
	if (cr6.eq) goto loc_822F4098;
	// mr r4,r23
	ctx.r4.u64 = r23.u64;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// lwz r11,140(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// blt cr6,0x822f3fbc
	if (cr6.lt) goto loc_822F3FBC;
	// twi 31,r0,22
loc_822F3FBC:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r30,0(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r31,27596(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 27596);
	// bl 0x832b227c
	__imp__RtlEnterCriticalSection(ctx, base);
	// addi r5,r1,208
	ctx.r5.s64 = ctx.r1.s64 + 208;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// bl 0x82bfd4a8
	sub_82BFD4A8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x822f4044
	if (!cr6.eq) goto loc_822F4044;
	// mr r9,r24
	ctx.r9.u64 = r24.u64;
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
loc_822F3FF0:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822f3ff0
	if (!cr0.eq) goto loc_822F3FF0;
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// bl 0x82bfc9c0
	sub_82BFC9C0(ctx, base);
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x821c67d8
	sub_821C67D8(ctx, base);
	// mr r6,r24
	ctx.r6.u64 = r24.u64;
loc_822F4024:
	// mfmsr r7
	// mtmsrd r13,1
	// lwarx r8,0,r6
	reserved.u32 = *(uint32_t*)(base + ctx.r6.u32);
	ctx.r8.u64 = __builtin_bswap32(reserved.u32);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stwcx. r8,0,r6
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r6.u32), reserved.s32, __builtin_bswap32(ctx.r8.s32));
	cr0.so = xer.so;
	// mtmsrd r7,1
	// bne 0x822f4024
	if (!cr0.eq) goto loc_822F4024;
	// stw r27,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r27.u32);
loc_822F4044:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r22,4
	ctx.r3.s64 = r22.s64 + 4;
	// bl 0x822f5c68
	sub_822F5C68(ctx, base);
	// lwz r11,232(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f4070
	if (cr6.lt) goto loc_822F4070;
	// lwz r3,212(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F4070:
	// lwz r4,140(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// stw r25,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r25.u32);
	// stw r27,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, r27.u32);
	// cmplw cr6,r29,r4
	cr6.compare<uint32_t>(r29.u32, ctx.r4.u32, xer);
	// sth r27,212(r1)
	PPC_STORE_U16(ctx.r1.u32 + 212, r27.u16);
	// blt cr6,0x822f408c
	if (cr6.lt) goto loc_822F408C;
	// twi 31,r0,22
loc_822F408C:
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// b 0x822f3f8c
	goto loc_822F3F8C;
loc_822F4098:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f40b0
	if (cr6.eq) goto loc_822F40B0;
	// addi r5,r1,132
	ctx.r5.s64 = ctx.r1.s64 + 132;
	// bl 0x822f84d8
	sub_822F84D8(ctx, base);
	// lwz r3,136(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F40B0:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, r27.u32);
	// stw r27,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r27.u32);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// stw r27,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r27.u32);
	// blt cr6,0x822f40f8
	if (cr6.lt) goto loc_822F40F8;
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c20
	return;
loc_822F40D8:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r4,r22
	ctx.r4.u64 = r22.u64;
	// lwz r3,27596(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 27596);
	// bl 0x829f84e0
	sub_829F84E0(ctx, base);
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r22,4
	ctx.r3.s64 = r22.s64 + 4;
	// bl 0x822f5c68
	sub_822F5C68(ctx, base);
loc_822F40F8:
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c20
	return;
}

__attribute__((alias("__imp__sub_822F4100"))) PPC_WEAK_FUNC(sub_822F4100);
PPC_FUNC_IMPL(__imp__sub_822F4100) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r6,-1
	ctx.r6.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x822f6380
	sub_822F6380(ctx, base);
	// lwz r11,28(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 28);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F4158"))) PPC_WEAK_FUNC(sub_822F4158);
PPC_FUNC_IMPL(__imp__sub_822F4158) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r11,29696
	r31.s64 = r11.s64 + 29696;
	// lwz r30,4(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// beq cr6,0x822f4258
	if (cr6.eq) goto loc_822F4258;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r29,r11,63
	r29.s64 = r11.s64 + 63;
loc_822F4188:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f419c
	if (cr6.eq) goto loc_822F419C;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F419C:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f41b0
	if (cr6.eq) goto loc_822F41B0;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F41B0:
	// bl 0x82ca6320
	sub_82CA6320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f41d4
	if (cr6.eq) goto loc_822F41D4;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r30,r30,20
	r30.s64 = r30.s64 + 20;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822f4188
	if (!cr6.eq) goto loc_822F4188;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F41D4:
	// lwz r31,8(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822f4224
	if (cr6.eq) goto loc_822F4224;
	// lis r11,-31926
	r11.s64 = -2092302336;
	// lis r27,-31927
	r27.s64 = -2092367872;
	// addi r29,r11,-7876
	r29.s64 = r11.s64 + -7876;
loc_822F41F0:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// lwz r26,0(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r28,27596(r27)
	r28.u64 = PPC_LOAD_U32(r27.u32 + 27596);
	// bl 0x832b227c
	__imp__RtlEnterCriticalSection(ctx, base);
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// lwz r3,12(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 12);
	// bl 0x82bfd430
	sub_82BFD430(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// lwz r11,12(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f41f0
	if (!cr6.eq) goto loc_822F41F0;
loc_822F4224:
	// lwz r3,8(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r29,r30,4
	r29.s64 = r30.s64 + 4;
	// lwz r5,12(r30)
	ctx.r5.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// cmplw cr6,r3,r5
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r5.u32, xer);
	// beq cr6,0x822f4258
	if (cr6.eq) goto loc_822F4258;
	// subf r11,r5,r5
	r11.s64 = ctx.r5.s64 - ctx.r5.s64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r6,r3
	r31.u64 = ctx.r6.u64 + ctx.r3.u64;
	// ble 0x822f4254
	if (!cr0.gt) goto loc_822F4254;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F4254:
	// stw r31,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r31.u32);
loc_822F4258:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F4260"))) PPC_WEAK_FUNC(sub_822F4260);
PPC_FUNC_IMPL(__imp__sub_822F4260) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r30,r11,29680
	r30.s64 = r11.s64 + 29680;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822f435c
	if (cr6.eq) goto loc_822F435C;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r29,r11,63
	r29.s64 = r11.s64 + 63;
loc_822F4294:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f42a8
	if (cr6.eq) goto loc_822F42A8;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F42A8:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f42bc
	if (cr6.eq) goto loc_822F42BC;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F42BC:
	// bl 0x82ca6320
	sub_82CA6320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f42e0
	if (cr6.eq) goto loc_822F42E0;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f4294
	if (!cr6.eq) goto loc_822F4294;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
loc_822F42E0:
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lbz r10,-27427(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -27427);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f4338
	if (cr6.eq) goto loc_822F4338;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f4304
	if (cr6.eq) goto loc_822F4304;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F4304:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// lbz r11,4(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r5,r9,1
	ctx.r5.u64 = ctx.r9.u64 ^ 1;
	// bl 0x822f4458
	sub_822F4458(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
loc_822F4338:
	// lbz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r6,1
	ctx.r6.s64 = 1;
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// rlwinm r5,r9,27,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// lwz r3,27596(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 27596);
	// bl 0x829f85b0
	sub_829F85B0(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r3.u32);
loc_822F435C:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F4368"))) PPC_WEAK_FUNC(sub_822F4368);
PPC_FUNC_IMPL(__imp__sub_822F4368) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// addi r30,r11,29680
	r30.s64 = r11.s64 + 29680;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f4450
	if (cr6.eq) goto loc_822F4450;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r29,r11,63
	r29.s64 = r11.s64 + 63;
loc_822F439C:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f43b0
	if (cr6.eq) goto loc_822F43B0;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F43B0:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f43c4
	if (cr6.eq) goto loc_822F43C4;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F43C4:
	// bl 0x82ca6320
	sub_82CA6320(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f43e8
	if (cr6.eq) goto loc_822F43E8;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r31,r31,16
	r31.s64 = r31.s64 + 16;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f439c
	if (!cr6.eq) goto loc_822F439C;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
loc_822F43E8:
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lbz r10,-27427(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + -27427);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f4438
	if (cr6.eq) goto loc_822F4438;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f440c
	if (cr6.eq) goto loc_822F440C;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F440C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8217ab58
	sub_8217AB58(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x822f4578
	sub_822F4578(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f5208
	sub_822F5208(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
loc_822F4438:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r3,27596(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 27596);
	// bl 0x829f8630
	sub_829F8630(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
loc_822F4450:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F4458"))) PPC_WEAK_FUNC(sub_822F4458);
PPC_FUNC_IMPL(__imp__sub_822F4458) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x822f2088
	sub_822F2088(ctx, base);
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// addi r31,r31,108
	r31.s64 = r31.s64 + 108;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822f5bc8
	sub_822F5BC8(ctx, base);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f44a8
	if (cr6.eq) goto loc_822F44A8;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f44ac
	if (cr6.eq) goto loc_822F44AC;
loc_822F44A8:
	// twi 31,r0,22
loc_822F44AC:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f456c
	if (!cr6.eq) goto loc_822F456C;
	// bl 0x82369ee0
	sub_82369EE0(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f2398
	sub_822F2398(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// stw r29,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r29.u32);
	// lis r11,-32209
	r11.s64 = -2110849024;
	// stw r30,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r30.u32);
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// stw r30,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r30.u32);
	// addi r5,r11,14008
	ctx.r5.s64 = r11.s64 + 14008;
	// stw r30,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r30.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x8236a530
	sub_8236A530(ctx, base);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4514
	if (cr6.eq) goto loc_822F4514;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4514:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r31,r1,136
	r31.s64 = ctx.r1.s64 + 136;
	// bl 0x822f5608
	sub_822F5608(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822f5e78
	sub_822F5E78(ctx, base);
	// lwz r4,140(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f454c
	if (cr6.eq) goto loc_822F454C;
	// addi r3,r1,136
	ctx.r3.s64 = ctx.r1.s64 + 136;
	// lwz r5,144(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,140(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F454C:
	// lwz r11,120(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r30,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, r30.u32);
	// stw r30,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r30.u32);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// stw r30,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, r30.u32);
	// blt cr6,0x822f456c
	if (cr6.lt) goto loc_822F456C;
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F456C:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F4578"))) PPC_WEAK_FUNC(sub_822F4578);
PPC_FUNC_IMPL(__imp__sub_822F4578) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x822f2088
	sub_822F2088(ctx, base);
	// stw r3,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r3.u32);
	// addi r29,r31,108
	r29.s64 = r31.s64 + 108;
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822f5bc8
	sub_822F5BC8(ctx, base);
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r9,112(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f45c0
	if (cr6.eq) goto loc_822F45C0;
	// cmplw cr6,r10,r29
	cr6.compare<uint32_t>(ctx.r10.u32, r29.u32, xer);
	// beq cr6,0x822f45c4
	if (cr6.eq) goto loc_822F45C4;
loc_822F45C0:
	// twi 31,r0,22
loc_822F45C4:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f4688
	if (cr6.eq) goto loc_822F4688;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f45dc
	if (!cr6.eq) goto loc_822F45DC;
	// twi 31,r0,22
loc_822F45DC:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f45ec
	if (!cr6.eq) goto loc_822F45EC;
	// twi 31,r0,22
loc_822F45EC:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r30,r11,16
	r30.s64 = r11.s64 + 16;
	// bl 0x82c645a8
	sub_82C645A8(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// ble cr6,0x822f460c
	if (!cr6.gt) goto loc_822F460C;
	// twi 31,r0,22
loc_822F460C:
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822f461c
	if (!cr6.gt) goto loc_822F461C;
	// twi 31,r0,22
loc_822F461C:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822f465c
	if (cr6.eq) goto loc_822F465C;
	// blt cr6,0x822f462c
	if (cr6.lt) goto loc_822F462C;
	// twi 31,r0,22
loc_822F462C:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r4,0(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,40(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 40);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// blt cr6,0x822f4654
	if (cr6.lt) goto loc_822F4654;
	// twi 31,r0,22
loc_822F4654:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// b 0x822f460c
	goto loc_822F460C;
loc_822F465C:
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822f5780
	sub_822F5780(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4688
	if (cr6.eq) goto loc_822F4688;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4688:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F4690"))) PPC_WEAK_FUNC(sub_822F4690);
PPC_FUNC_IMPL(__imp__sub_822F4690) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// lwz r3,0(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,0(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f46d4
	if (cr6.eq) goto loc_822F46D4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F46D4:
	// li r11,0
	r11.s64 = 0;
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// stw r11,0(r26)
	PPC_STORE_U32(r26.u32 + 0, r11.u32);
	// addi r3,r10,23620
	ctx.r3.s64 = ctx.r10.s64 + 23620;
	// bl 0x82b5e040
	sub_82B5E040(ctx, base);
	// lis r9,-31926
	ctx.r9.s64 = -2092302336;
	// addi r3,r9,23608
	ctx.r3.s64 = ctx.r9.s64 + 23608;
	// bl 0x82b5dd60
	sub_82B5DD60(ctx, base);
	// lis r8,-31927
	ctx.r8.s64 = -2092367872;
	// li r11,0
	r11.s64 = 0;
	// lis r10,-31926
	ctx.r10.s64 = -2092302336;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r31,r10,23660
	r31.s64 = ctx.r10.s64 + 23660;
	// stb r11,28070(r8)
	PPC_STORE_U8(ctx.r8.u32 + 28070, r11.u8);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82200688
	sub_82200688(ctx, base);
	// bl 0x82b66bc8
	sub_82B66BC8(ctx, base);
	// clrlwi r7,r3,24
	ctx.r7.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822f4734
	if (cr6.eq) goto loc_822F4734;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// li r11,1
	r11.s64 = 1;
	// stb r11,28396(r10)
	PPC_STORE_U8(ctx.r10.u32 + 28396, r11.u8);
loc_822F4734:
	// lbz r28,84(r1)
	r28.u64 = PPC_LOAD_U8(ctx.r1.u32 + 84);
	// lwz r27,80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822f4750
	if (cr6.eq) goto loc_822F4750;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// li r28,0
	r28.s64 = 0;
loc_822F4750:
	// lis r29,-31927
	r29.s64 = -2092367872;
loc_822F4754:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x832b227c
	__imp__RtlEnterCriticalSection(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lbz r30,28335(r29)
	r30.u64 = PPC_LOAD_U8(r29.u32 + 28335);
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f4778
	if (cr6.eq) goto loc_822F4778;
	// bl 0x82cbd098
	sub_82CBD098(ctx, base);
	// b 0x822f4754
	goto loc_822F4754;
loc_822F4778:
	// clrlwi r11,r28,24
	r11.u64 = r28.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f478c
	if (cr6.eq) goto loc_822F478C;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x832b226c
	__imp__RtlLeaveCriticalSection(ctx, base);
loc_822F478C:
	// bl 0x82b66ae0
	sub_82B66AE0(ctx, base);
	// lis r11,-31926
	r11.s64 = -2092302336;
	// lwz r3,-6420(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + -6420);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lis r31,-31926
	r31.s64 = -2092302336;
	// addi r3,r31,23652
	ctx.r3.s64 = r31.s64 + 23652;
	// bl 0x82b65078
	sub_82B65078(ctx, base);
	// lis r8,-31950
	ctx.r8.s64 = -2093875200;
	// li r11,0
	r11.s64 = 0;
	// addi r7,r8,-26692
	ctx.r7.s64 = ctx.r8.s64 + -26692;
	// stw r11,23652(r31)
	PPC_STORE_U32(r31.u32 + 23652, r11.u32);
	// lwz r3,20(r7)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r7.u32 + 20);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f47d4
	if (cr6.eq) goto loc_822F47D4;
	// bl 0x82cbbf60
	sub_82CBBF60(ctx, base);
loc_822F47D4:
	// bl 0x82374b38
	sub_82374B38(ctx, base);
	// bl 0x82bf2a18
	sub_82BF2A18(ctx, base);
	// bl 0x8322f768
	sub_8322F768(ctx, base);
	// li r3,10
	ctx.r3.s64 = 10;
	// bl 0x82cbc6b0
	sub_82CBC6B0(ctx, base);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822f4ca0
	sub_822F4CA0(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F47F8"))) PPC_WEAK_FUNC(sub_822F47F8);
PPC_FUNC_IMPL(__imp__sub_822F47F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,16
	ctx.r3.s64 = 16;
	// lbz r10,26919(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 26919);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f4838
	if (cr6.eq) goto loc_822F4838;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4858
	if (cr6.eq) goto loc_822F4858;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82356180
	sub_82356180(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x822f485c
	goto loc_822F485C;
loc_822F4838:
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4858
	if (cr6.eq) goto loc_822F4858;
	// addi r5,r31,56
	ctx.r5.s64 = r31.s64 + 56;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x823789d0
	sub_823789D0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// b 0x822f485c
	goto loc_822F485C;
loc_822F4858:
	// li r30,0
	r30.s64 = 0;
loc_822F485C:
	// addi r3,r31,56
	ctx.r3.s64 = r31.s64 + 56;
	// bl 0x822f5540
	sub_822F5540(ctx, base);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f48a4
	if (cr6.eq) goto loc_822F48A4;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f48a0
	if (cr6.eq) goto loc_822F48A0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F48A0:
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
loc_822F48A4:
	// lbz r11,4(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f494c
	if (!cr6.eq) goto loc_822F494C;
	// lis r29,-31927
	r29.s64 = -2092367872;
	// li r28,1
	r28.s64 = 1;
loc_822F48B8:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lbz r9,26781(r29)
	ctx.r9.u64 = PPC_LOAD_U8(r29.u32 + 26781);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f48e4
	if (cr6.eq) goto loc_822F48E4;
	// li r3,10000
	ctx.r3.s64 = 10000;
	// bl 0x82cbc6b0
	sub_82CBC6B0(ctx, base);
loc_822F48E4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x822f48f4
	if (!cr6.eq) goto loc_822F48F4;
	// stb r28,4(r31)
	PPC_STORE_U8(r31.u32 + 4, r28.u8);
	// b 0x822f4940
	goto loc_822F4940;
loc_822F48F4:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mtctr r8
	ctr.u64 = ctx.r8.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f493c
	if (cr6.eq) goto loc_822F493C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F493C:
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
loc_822F4940:
	// lbz r11,4(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f48b8
	if (cr6.eq) goto loc_822F48B8;
loc_822F494C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F4958"))) PPC_WEAK_FUNC(sub_822F4958);
PPC_FUNC_IMPL(__imp__sub_822F4958) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r11,6240
	ctx.r4.s64 = r11.s64 + 6240;
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r10,6284
	ctx.r4.s64 = ctx.r10.s64 + 6284;
	// addi r3,r1,124
	ctx.r3.s64 = ctx.r1.s64 + 124;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r28,r11,3224
	r28.s64 = r11.s64 + 3224;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// li r25,0
	r25.s64 = 0;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,120
	ctx.r3.s64 = ctx.r1.s64 + 120;
	// mr r29,r25
	r29.u64 = r25.u64;
	// bl 0x8229ad78
	sub_8229AD78(ctx, base);
	// clrlwi r9,r3,24
	ctx.r9.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f4bb8
	if (cr6.eq) goto loc_822F4BB8;
	// addi r31,r1,120
	r31.s64 = ctx.r1.s64 + 120;
	// addi r30,r30,56
	r30.s64 = r30.s64 + 56;
	// li r26,1
	r26.s64 = 1;
	// lis r27,-31927
	r27.s64 = -2092367872;
loc_822F49D8:
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8221f3f0
	sub_8221F3F0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822f4a44
	if (!cr6.eq) goto loc_822F4A44;
	// lwz r11,28060(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 28060);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f4a04
	if (cr6.eq) goto loc_822F4A04;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4A04:
	// bl 0x82cbbb58
	sub_82CBBB58(ctx, base);
	// mr r31,r25
	r31.u64 = r25.u64;
loc_822F4A0C:
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f4a5c
	if (cr6.eq) goto loc_822F4A5C;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4a5c
	if (cr6.eq) goto loc_822F4A5C;
	// lis r11,-32105
	r11.s64 = -2104033280;
	// stw r31,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r31.u32);
	// stw r3,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r3.u32);
	// addi r10,r11,-32320
	ctx.r10.s64 = r11.s64 + -32320;
	// stw r26,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r26.u32);
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// b 0x822f4a60
	goto loc_822F4A60;
loc_822F4A44:
	// li r6,2
	ctx.r6.s64 = 2;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82c64018
	sub_82C64018(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822f4a0c
	goto loc_822F4A0C;
loc_822F4A5C:
	// stw r25,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r25.u32);
loc_822F4A60:
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x827f18f0
	sub_827F18F0(ctx, base);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822f4a88
	if (!cr6.gt) goto loc_822F4A88;
	// twi 31,r0,22
loc_822F4A88:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// addi r10,r11,-8
	ctx.r10.s64 = r11.s64 + -8;
	// stw r30,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r30.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ld r8,96(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// bgt cr6,0x822f4ab4
	if (cr6.gt) goto loc_822F4AB4;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bge cr6,0x822f4ab8
	if (!cr6.lt) goto loc_822F4AB8;
loc_822F4AB4:
	// twi 31,r0,22
loc_822F4AB8:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f4ad0
	if (cr6.lt) goto loc_822F4AD0;
	// twi 31,r0,22
loc_822F4AD0:
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82c63098
	sub_82C63098(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822f4af0
	if (!cr6.gt) goto loc_822F4AF0;
	// twi 31,r0,22
loc_822F4AF0:
	// stw r30,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r30.u32);
	// addi r10,r11,-8
	ctx.r10.s64 = r11.s64 + -8;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// ld r8,104(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// std r8,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r8.u64);
	// bgt cr6,0x822f4b1c
	if (cr6.gt) goto loc_822F4B1C;
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bge cr6,0x822f4b20
	if (!cr6.lt) goto loc_822F4B20;
loc_822F4B1C:
	// twi 31,r0,22
loc_822F4B20:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r11,r11,-8
	r11.s64 = r11.s64 + -8;
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f4b38
	if (cr6.lt) goto loc_822F4B38;
	// twi 31,r0,22
loc_822F4B38:
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,36(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 36);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// rlwinm r10,r3,30,2,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 30) & 0x3FFFFFFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f4b90
	if (cr6.eq) goto loc_822F4B90;
	// addi r11,r31,2
	r11.s64 = r31.s64 + 2;
loc_822F4B5C:
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 1);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,-1(r11)
	ctx.r7.u64 = PPC_LOAD_U8(r11.u32 + -1);
	// lbz r6,-2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(r11.u32 + -2);
	// stb r9,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r9.u8);
	// stb r8,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, ctx.r8.u8);
	// stb r7,82(r1)
	PPC_STORE_U8(ctx.r1.u32 + 82, ctx.r7.u8);
	// stb r6,83(r1)
	PPC_STORE_U8(ctx.r1.u32 + 83, ctx.r6.u8);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r5,-2(r11)
	PPC_STORE_U32(r11.u32 + -2, ctx.r5.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bne 0x822f4b5c
	if (!cr0.eq) goto loc_822F4B5C;
loc_822F4B90:
	// addi r29,r29,1
	r29.s64 = r29.s64 + 1;
	// addi r11,r1,120
	r11.s64 = ctx.r1.s64 + 120;
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8229ad78
	sub_8229AD78(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f49d8
	if (!cr6.eq) goto loc_822F49D8;
loc_822F4BB8:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r31,r1,132
	r31.s64 = ctx.r1.s64 + 132;
	// li r30,2
	r30.s64 = 2;
	// addi r29,r11,28344
	r29.s64 = r11.s64 + 28344;
loc_822F4BC8:
	// addi r31,r31,-4
	r31.s64 = r31.s64 + -4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x821c67d8
	sub_821C67D8(ctx, base);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
loc_822F4BD8:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822f4bd8
	if (!cr0.eq) goto loc_822F4BD8;
	// addic. r30,r30,-1
	xer.ca = r30.u32 > 0;
	r30.s64 = r30.s64 + -1;
	cr0.compare<int32_t>(r30.s32, 0, xer);
	// stw r25,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r25.u32);
	// bge 0x822f4bc8
	if (!cr0.lt) goto loc_822F4BC8;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F4C08"))) PPC_WEAK_FUNC(sub_822F4C08);
PPC_FUNC_IMPL(__imp__sub_822F4C08) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r11,80(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822f4c70
	if (!cr6.eq) goto loc_822F4C70;
	// li r3,288
	ctx.r3.s64 = 288;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4c48
	if (cr6.eq) goto loc_822F4C48;
	// bl 0x82378cd0
	sub_82378CD0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822f4c4c
	goto loc_822F4C4C;
loc_822F4C48:
	// li r31,0
	r31.s64 = 0;
loc_822F4C4C:
	// lwz r3,80(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4c6c
	if (cr6.eq) goto loc_822F4C6C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4C6C:
	// stw r31,80(r30)
	PPC_STORE_U32(r30.u32 + 80, r31.u32);
loc_822F4C70:
	// lwz r3,80(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 80);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 20);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F4CA0"))) PPC_WEAK_FUNC(sub_822F4CA0);
PPC_FUNC_IMPL(__imp__sub_822F4CA0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f4cf8
	if (cr6.eq) goto loc_822F4CF8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4cf0
	if (cr6.eq) goto loc_822F4CF0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4CF0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,80(r31)
	PPC_STORE_U32(r31.u32 + 80, r11.u32);
loc_822F4CF8:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F4D10"))) PPC_WEAK_FUNC(sub_822F4D10);
PPC_FUNC_IMPL(__imp__sub_822F4D10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f4de8
	if (cr6.eq) goto loc_822F4DE8;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// ble cr6,0x822f4de8
	if (!cr6.gt) goto loc_822F4DE8;
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82b43b88
	sub_82B43B88(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82366210
	sub_82366210(ctx, base);
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4d7c
	if (cr6.eq) goto loc_822F4D7C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4D7C:
	// lwz r31,80(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f4de8
	if (cr6.eq) goto loc_822F4DE8;
	// li r5,16384
	ctx.r5.s64 = 16384;
	// li r4,50
	ctx.r4.s64 = 50;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8236d4e0
	sub_8236D4E0(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x8236db60
	sub_8236DB60(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f4dcc
	if (cr6.eq) goto loc_822F4DCC;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// lwz r3,84(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 84);
	// bl 0x8236dd48
	sub_8236DD48(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f4dcc
	if (cr6.eq) goto loc_822F4DCC;
	// li r30,1
	r30.s64 = 1;
loc_822F4DCC:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f4df8
	sub_822F4DF8(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F4DE8:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F4DF8"))) PPC_WEAK_FUNC(sub_822F4DF8);
PPC_FUNC_IMPL(__imp__sub_822F4DF8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r30,r31,180
	r30.s64 = r31.s64 + 180;
	// lwz r11,204(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 204);
	// cmplwi cr6,r11,16
	cr6.compare<uint32_t>(r11.u32, 16, xer);
	// blt cr6,0x822f4e20
	if (cr6.lt) goto loc_822F4E20;
	// lwz r3,4(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F4E20:
	// li r29,0
	r29.s64 = 0;
	// li r11,15
	r11.s64 = 15;
	// stw r29,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r29.u32);
	// addi r3,r31,168
	ctx.r3.s64 = r31.s64 + 168;
	// stw r11,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r11.u32);
	// stb r29,4(r30)
	PPC_STORE_U8(r30.u32 + 4, r29.u8);
	// bl 0x823fb150
	sub_823FB150(ctx, base);
	// addi r30,r31,108
	r30.s64 = r31.s64 + 108;
	// addi r3,r30,16
	ctx.r3.s64 = r30.s64 + 16;
	// bl 0x8233ddd8
	sub_8233DDD8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x823fb150
	sub_823FB150(ctx, base);
	// addi r3,r31,88
	ctx.r3.s64 = r31.s64 + 88;
	// bl 0x822f69f8
	sub_822F69F8(ctx, base);
	// addi r3,r31,76
	ctx.r3.s64 = r31.s64 + 76;
	// bl 0x822f6660
	sub_822F6660(ctx, base);
	// addi r3,r31,64
	ctx.r3.s64 = r31.s64 + 64;
	// bl 0x822f6990
	sub_822F6990(ctx, base);
	// lwz r3,52(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 52);
	// addi r30,r31,48
	r30.s64 = r31.s64 + 48;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4e7c
	if (cr6.eq) goto loc_822F4E7C;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F4E7C:
	// stw r29,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r29,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r29.u32);
	// stw r29,12(r30)
	PPC_STORE_U32(r30.u32 + 12, r29.u32);
	// bl 0x822f5d48
	sub_822F5D48(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F4E98"))) PPC_WEAK_FUNC(sub_822F4E98);
PPC_FUNC_IMPL(__imp__sub_822F4E98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4ecc
	if (cr6.eq) goto loc_822F4ECC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r10,r11,6392
	ctx.r10.s64 = r11.s64 + 6392;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x822f4ed0
	goto loc_822F4ED0;
loc_822F4ECC:
	// li r30,0
	r30.s64 = 0;
loc_822F4ED0:
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4ef4
	if (cr6.eq) goto loc_822F4EF4;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r10,r11,6384
	ctx.r10.s64 = r11.s64 + 6384;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x822f4ef8
	goto loc_822F4EF8;
loc_822F4EF4:
	// li r31,0
	r31.s64 = 0;
loc_822F4EF8:
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4f18
	if (cr6.eq) goto loc_822F4F18;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r10,r11,6376
	ctx.r10.s64 = r11.s64 + 6376;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x822f4f1c
	goto loc_822F4F1C;
loc_822F4F18:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822F4F1C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lfs f1,-27340(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27340);
	ctx.f1.f64 = double(temp.f32);
	// bl 0x8322f808
	sub_8322F808(ctx, base);
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r28,1
	r28.s64 = 1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f4f68
	if (cr6.eq) goto loc_822F4F68;
	// lis r11,-32245
	r11.s64 = -2113208320;
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// addi r10,r11,-32324
	ctx.r10.s64 = r11.s64 + -32324;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// bl 0x8237c3f0
	sub_8237C3F0(ctx, base);
	// b 0x822f4f6c
	goto loc_822F4F6C;
loc_822F4F68:
	// li r31,0
	r31.s64 = 0;
loc_822F4F6C:
	// addi r30,r27,92
	r30.s64 = r27.s64 + 92;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r31,92(r27)
	PPC_STORE_U32(r27.u32 + 92, r31.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f4fbc
	if (cr6.eq) goto loc_822F4FBC;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f4fb4
	if (cr6.eq) goto loc_822F4FB4;
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// lis r9,-32105
	ctx.r9.s64 = -2104033280;
	// stw r28,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r28.u32);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// addi r8,r9,-32320
	ctx.r8.s64 = ctx.r9.s64 + -32320;
	// stw r8,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r8.u32);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// b 0x822f4fb8
	goto loc_822F4FB8;
loc_822F4FB4:
	// li r11,0
	r11.s64 = 0;
loc_822F4FB8:
	// stw r11,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r11.u32);
loc_822F4FBC:
	// lwz r31,4(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// beq cr6,0x822f4ff0
	if (cr6.eq) goto loc_822F4FF0;
loc_822F4FD4:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r31
	reserved.u32 = *(uint32_t*)(base + r31.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwcx. r11,0,r31
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r31.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822f4fd4
	if (!cr0.eq) goto loc_822F4FD4;
loc_822F4FF0:
	// lis r11,-31926
	r11.s64 = -2092302336;
	// addi r30,r11,-7704
	r30.s64 = r11.s64 + -7704;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f5028
	if (cr6.eq) goto loc_822F5028;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r29,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r29.u32);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// stw r31,4(r30)
	PPC_STORE_U32(r30.u32 + 4, r31.u32);
	// beq cr6,0x822f5028
	if (cr6.eq) goto loc_822F5028;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_822F5028:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// li r3,4
	ctx.r3.s64 = 4;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f5054
	if (cr6.eq) goto loc_822F5054;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r10,r11,6368
	ctx.r10.s64 = r11.s64 + 6368;
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// b 0x822f5058
	goto loc_822F5058;
loc_822F5054:
	// li r29,0
	r29.s64 = 0;
loc_822F5058:
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f5090
	if (cr6.eq) goto loc_822F5090;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f5090
	if (cr6.eq) goto loc_822F5090;
	// lis r11,-32105
	r11.s64 = -2104033280;
	// stw r29,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, r29.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r28,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r28.u32);
	// addi r10,r11,-32320
	ctx.r10.s64 = r11.s64 + -32320;
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// b 0x822f5094
	goto loc_822F5094;
loc_822F5090:
	// li r30,0
	r30.s64 = 0;
loc_822F5094:
	// lwz r11,104(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 104);
	// addi r31,r27,100
	r31.s64 = r27.s64 + 100;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x822f50cc
	if (cr6.eq) goto loc_822F50CC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f50cc
	if (cr6.eq) goto loc_822F50CC;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_822F50CC:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// lwz r30,4(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r29,0(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// beq cr6,0x822f5108
	if (cr6.eq) goto loc_822F5108;
loc_822F50EC:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r30
	reserved.u32 = *(uint32_t*)(base + r30.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stwcx. r11,0,r30
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r30.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822f50ec
	if (!cr0.eq) goto loc_822F50EC;
loc_822F5108:
	// lis r11,-31926
	r11.s64 = -2092302336;
	// addi r31,r11,-7712
	r31.s64 = r11.s64 + -7712;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x822f5140
	if (cr6.eq) goto loc_822F5140;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// stw r29,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r29.u32);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// beq cr6,0x822f5140
	if (cr6.eq) goto loc_822F5140;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
loc_822F5140:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x829ff648
	sub_829FF648(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F5150"))) PPC_WEAK_FUNC(sub_822F5150);
PPC_FUNC_IMPL(__imp__sub_822F5150) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// bl 0x8217ab30
	sub_8217AB30(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8217a6c0
	sub_8217A6C0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F51B8"))) PPC_WEAK_FUNC(sub_822F51B8);
PPC_FUNC_IMPL(__imp__sub_822F51B8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// li r6,-1
	ctx.r6.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// bl 0x822f6380
	sub_822F6380(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5208"))) PPC_WEAK_FUNC(sub_822F5208);
PPC_FUNC_IMPL(__imp__sub_822F5208) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f5230
	if (cr6.lt) goto loc_822F5230;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F5230:
	// li r11,0
	r11.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// stw r10,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r10.u32);
	// sth r11,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r11.u16);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5258"))) PPC_WEAK_FUNC(sub_822F5258);
PPC_FUNC_IMPL(__imp__sub_822F5258) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8217ab30
	sub_8217AB30(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f6cb0
	sub_822F6CB0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F52A8"))) PPC_WEAK_FUNC(sub_822F52A8);
PPC_FUNC_IMPL(__imp__sub_822F52A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r25,r7
	r25.u64 = ctx.r7.u64;
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// bge cr6,0x822f52d8
	if (!cr6.lt) goto loc_822F52D8;
	// bl 0x82cd12c8
	sub_82CD12C8(ctx, base);
loc_822F52D8:
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// subf r11,r31,r10
	r11.s64 = ctx.r10.s64 - r31.s64;
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// bge cr6,0x822f52ec
	if (!cr6.lt) goto loc_822F52EC;
	// mr r28,r11
	r28.u64 = r11.u64;
loc_822F52EC:
	// subf r11,r28,r10
	r11.s64 = ctx.r10.s64 - r28.s64;
	// subfic r10,r29,-1
	xer.ca = r29.u32 <= 4294967295;
	ctx.r10.s64 = -1 - r29.s64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x822f5300
	if (cr6.gt) goto loc_822F5300;
	// bl 0x82cd11d0
	sub_82CD11D0(ctx, base);
loc_822F5300:
	// lwz r11,20(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// cmplw cr6,r29,r28
	cr6.compare<uint32_t>(r29.u32, r28.u32, xer);
	// subf r10,r31,r11
	ctx.r10.s64 = r11.s64 - r31.s64;
	// subf r26,r28,r10
	r26.s64 = ctx.r10.s64 - r28.s64;
	// bge cr6,0x822f5370
	if (!cr6.lt) goto loc_822F5370;
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f532c
	if (cr6.lt) goto loc_822F532C;
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x822f5330
	goto loc_822F5330;
loc_822F532C:
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_822F5330:
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f5340
	if (cr6.lt) goto loc_822F5340;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822f5344
	goto loc_822F5344;
loc_822F5340:
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_822F5344:
	// subf r7,r31,r10
	ctx.r7.s64 = ctx.r10.s64 - r31.s64;
	// add r6,r31,r28
	ctx.r6.u64 = r31.u64 + r28.u64;
	// add r5,r31,r29
	ctx.r5.u64 = r31.u64 + r29.u64;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r29,r7
	ctx.r4.s64 = ctx.r7.s64 - r29.s64;
	// rlwinm r6,r26,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F5370:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x822f5380
	if (!cr6.eq) goto loc_822F5380;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822f5448
	if (cr6.eq) goto loc_822F5448;
loc_822F5380:
	// lwz r10,20(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// subf r11,r28,r29
	r11.s64 = r29.s64 - r28.s64;
	// li r5,0
	ctx.r5.s64 = 0;
	// add r27,r11,r10
	r27.u64 = r11.u64 + ctx.r10.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x8217a5e0
	sub_8217A5E0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f5448
	if (cr6.eq) goto loc_822F5448;
	// cmplw cr6,r28,r29
	cr6.compare<uint32_t>(r28.u32, r29.u32, xer);
	// bge cr6,0x822f540c
	if (!cr6.lt) goto loc_822F540C;
	// lwz r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f53c8
	if (cr6.lt) goto loc_822F53C8;
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x822f53cc
	goto loc_822F53CC;
loc_822F53C8:
	// mr r8,r11
	ctx.r8.u64 = r11.u64;
loc_822F53CC:
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f53dc
	if (cr6.lt) goto loc_822F53DC;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822f53e0
	goto loc_822F53E0;
loc_822F53DC:
	// mr r9,r11
	ctx.r9.u64 = r11.u64;
loc_822F53E0:
	// subf r7,r31,r10
	ctx.r7.s64 = ctx.r10.s64 - r31.s64;
	// add r6,r31,r28
	ctx.r6.u64 = r31.u64 + r28.u64;
	// add r5,r31,r29
	ctx.r5.u64 = r31.u64 + r29.u64;
	// rlwinm r10,r6,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r5,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r29,r7
	ctx.r4.s64 = ctx.r7.s64 - r29.s64;
	// rlwinm r6,r26,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F540C:
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f6578
	sub_822F6578(ctx, base);
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// stw r27,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r27.u32);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f5438
	if (cr6.lt) goto loc_822F5438;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// b 0x822f543c
	goto loc_822F543C;
loc_822F5438:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
loc_822F543C:
	// rlwinm r10,r27,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// sthx r9,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r9.u16);
loc_822F5448:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F5458"))) PPC_WEAK_FUNC(sub_822F5458);
PPC_FUNC_IMPL(__imp__sub_822F5458) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f546c
	if (cr6.lt) goto loc_822F546C;
	// lwz r3,4(r3)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// blr 
	return;
loc_822F546C:
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5478"))) PPC_WEAK_FUNC(sub_822F5478);
PPC_FUNC_IMPL(__imp__sub_822F5478) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,0(r4)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r3.u32);
	// beq cr6,0x822f54ac
	if (cr6.eq) goto loc_822F54AC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F54AC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F54C8"))) PPC_WEAK_FUNC(sub_822F54C8);
PPC_FUNC_IMPL(__imp__sub_822F54C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f5500
	if (cr6.eq) goto loc_822F5500;
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F5500:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f551c
	if (cr6.eq) goto loc_822F551C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F551C:
	// stw r30,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5540"))) PPC_WEAK_FUNC(sub_822F5540);
PPC_FUNC_IMPL(__imp__sub_822F5540) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// ble cr6,0x822f5564
	if (!cr6.gt) goto loc_822F5564;
	// twi 31,r0,22
loc_822F5564:
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ble cr6,0x822f5578
	if (!cr6.gt) goto loc_822F5578;
	// twi 31,r0,22
loc_822F5578:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x827425c0
	sub_827425C0(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F55A0"))) PPC_WEAK_FUNC(sub_822F55A0);
PPC_FUNC_IMPL(__imp__sub_822F55A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,12
	ctx.r3.s64 = 12;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f55ec
	if (cr6.eq) goto loc_822F55EC;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lis r9,-32209
	ctx.r9.s64 = -2110849024;
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r7,r9,26312
	ctx.r7.s64 = ctx.r9.s64 + 26312;
	// stw r8,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r8.u32);
	// stw r7,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r7.u32);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// b 0x822f55f0
	goto loc_822F55F0;
loc_822F55EC:
	// li r11,0
	r11.s64 = 0;
loc_822F55F0:
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5608"))) PPC_WEAK_FUNC(sub_822F5608);
PPC_FUNC_IMPL(__imp__sub_822F5608) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lbz r9,33(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f565c
	if (!cr6.eq) goto loc_822F565C;
	// lwz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
loc_822F5634:
	// lwz r8,12(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bge cr6,0x822f5648
	if (!cr6.lt) goto loc_822F5648;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x822f5650
	goto loc_822F5650;
loc_822F5648:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F5650:
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f5634
	if (cr6.eq) goto loc_822F5634;
loc_822F565C:
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// mr r9,r31
	ctx.r9.u64 = r31.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// beq cr6,0x822f5688
	if (cr6.eq) goto loc_822F5688;
	// lwz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r7,12(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplw cr6,r8,r7
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r7.u32, xer);
	// bge cr6,0x822f56e4
	if (!cr6.lt) goto loc_822F56E4;
loc_822F5688:
	// lwz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// li r11,0
	r11.s64 = 0;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// addi r3,r1,116
	ctx.r3.s64 = ctx.r1.s64 + 116;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r11.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, r11.u32);
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// bl 0x822f77a0
	sub_822F77A0(ctx, base);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822f6708
	sub_822F6708(ctx, base);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// ld r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// bl 0x822f6328
	sub_822F6328(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822f5e20
	sub_822F5E20(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_822F56E4:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f56f0
	if (!cr6.eq) goto loc_822F56F0;
	// twi 31,r0,22
loc_822F56F0:
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f5700
	if (!cr6.eq) goto loc_822F5700;
	// twi 31,r0,22
loc_822F5700:
	// addi r3,r11,16
	ctx.r3.s64 = r11.s64 + 16;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5718"))) PPC_WEAK_FUNC(sub_822F5718);
PPC_FUNC_IMPL(__imp__sub_822F5718) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x822f71a8
	sub_822F71A8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5780"))) PPC_WEAK_FUNC(sub_822F5780);
PPC_FUNC_IMPL(__imp__sub_822F5780) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r5,256(r1)
	PPC_STORE_U64(ctx.r1.u32 + 256, ctx.r5.u64);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r31,260(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lbz r11,33(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f57ec
	if (cr6.eq) goto loc_822F57EC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5600
	ctx.r4.s64 = r11.s64 + 5600;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822F57EC:
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// mr r26,r31
	r26.u64 = r31.u64;
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lbz r11,33(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f5810
	if (cr6.eq) goto loc_822F5810;
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// b 0x822f5838
	goto loc_822F5838;
loc_822F5810:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lbz r9,33(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f5828
	if (cr6.eq) goto loc_822F5828;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// b 0x822f5838
	goto loc_822F5838;
loc_822F5828:
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// lwz r28,8(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bne cr6,0x822f5924
	if (!cr6.eq) goto loc_822F5924;
loc_822F5838:
	// lbz r11,33(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// lwz r31,4(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f584c
	if (!cr6.eq) goto loc_822F584C;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822F584C:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x822f5864
	if (!cr6.eq) goto loc_822F5864;
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// b 0x822f587c
	goto loc_822F587C;
loc_822F5864:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f5878
	if (!cr6.eq) goto loc_822F5878;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// b 0x822f587c
	goto loc_822F587C;
loc_822F5878:
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
loc_822F587C:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f58cc
	if (!cr6.eq) goto loc_822F58CC;
	// lbz r11,33(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f58a0
	if (cr6.eq) goto loc_822F58A0;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// b 0x822f58c8
	goto loc_822F58C8;
loc_822F58A0:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f58c8
	if (!cr6.eq) goto loc_822F58C8;
loc_822F58B4:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f58b4
	if (cr6.eq) goto loc_822F58B4;
loc_822F58C8:
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
loc_822F58CC:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f59b8
	if (!cr6.eq) goto loc_822F59B8;
	// lbz r11,33(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f58f4
	if (cr6.eq) goto loc_822F58F4;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822f59b8
	goto loc_822F59B8;
loc_822F58F4:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f591c
	if (!cr6.eq) goto loc_822F591C;
loc_822F5908:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f5908
	if (cr6.eq) goto loc_822F5908;
loc_822F591C:
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822f59b8
	goto loc_822F59B8;
loc_822F5924:
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f5944
	if (!cr6.eq) goto loc_822F5944;
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x822f596c
	goto loc_822F596C;
loc_822F5944:
	// lbz r10,33(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5958
	if (!cr6.eq) goto loc_822F5958;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822F5958:
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
loc_822F596C:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822f5984
	if (!cr6.eq) goto loc_822F5984;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// b 0x822f59a0
	goto loc_822F59A0;
loc_822F5984:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822f599c
	if (!cr6.eq) goto loc_822F599C;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x822f59a0
	goto loc_822F59A0;
loc_822F599C:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
loc_822F59A0:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lbz r8,32(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 32);
	// lbz r9,32(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 32);
	// stb r9,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r9.u8);
	// stb r8,32(r26)
	PPC_STORE_U8(r26.u32 + 32, ctx.r8.u8);
loc_822F59B8:
	// lbz r11,32(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 32);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822f5b58
	if (!cr6.eq) goto loc_822F5B58;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// li r29,1
	r29.s64 = 1;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f5b54
	if (cr6.eq) goto loc_822F5B54;
loc_822F59DC:
	// lbz r11,32(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822f5b54
	if (!cr6.eq) goto loc_822F5B54;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822f5a98
	if (!cr6.eq) goto loc_822F5A98;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5a1c
	if (!cr6.eq) goto loc_822F5A1C;
	// stb r29,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822F5A1C:
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5aec
	if (!cr6.eq) goto loc_822F5AEC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f5a48
	if (!cr6.eq) goto loc_822F5A48;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x822f5ae8
	if (cr6.eq) goto loc_822F5AE8;
loc_822F5A48:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f5a74
	if (!cr6.eq) goto loc_822F5A74;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r29.u8);
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// bl 0x8234d600
	sub_8234D600(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822F5A74:
	// lbz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r10.u8);
	// stb r29,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r29.u8);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stb r29,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r29.u8);
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
	// b 0x822f5b54
	goto loc_822F5B54;
loc_822F5A98:
	// lbz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5abc
	if (!cr6.eq) goto loc_822F5ABC;
	// stb r29,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8234d600
	sub_8234D600(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F5ABC:
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5aec
	if (!cr6.eq) goto loc_822F5AEC;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f5b08
	if (!cr6.eq) goto loc_822F5B08;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f5b08
	if (!cr6.eq) goto loc_822F5B08;
loc_822F5AE8:
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
loc_822F5AEC:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// mr r28,r31
	r28.u64 = r31.u64;
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f59dc
	if (!cr6.eq) goto loc_822F59DC;
	// b 0x822f5b54
	goto loc_822F5B54;
loc_822F5B08:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f5b34
	if (!cr6.eq) goto loc_822F5B34;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r29.u8);
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F5B34:
	// lbz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r10.u8);
	// stb r29,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r29.u8);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r29,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r29.u8);
	// bl 0x8234d600
	sub_8234D600(ctx, base);
loc_822F5B54:
	// stb r29,32(r28)
	PPC_STORE_U8(r28.u32 + 32, r29.u8);
loc_822F5B58:
	// lwz r4,20(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 20);
	// addi r31,r26,16
	r31.s64 = r26.s64 + 16;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f5b7c
	if (cr6.eq) goto loc_822F5B7C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F5B7C:
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f5bb8
	if (cr6.eq) goto loc_822F5BB8;
	// ld r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// stw r9,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r9.u32);
	// std r10,0(r25)
	PPC_STORE_U64(r25.u32 + 0, ctx.r10.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
loc_822F5BB8:
	// ld r11,256(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// std r11,0(r25)
	PPC_STORE_U64(r25.u32 + 0, r11.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F5BC8"))) PPC_WEAK_FUNC(sub_822F5BC8);
PPC_FUNC_IMPL(__imp__sub_822F5BC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lbz r9,33(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f5c08
	if (!cr6.eq) goto loc_822F5C08;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
loc_822F5BE0:
	// lwz r8,12(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bge cr6,0x822f5bf4
	if (!cr6.lt) goto loc_822F5BF4;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x822f5bfc
	goto loc_822F5BFC;
loc_822F5BF4:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F5BFC:
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f5be0
	if (cr6.eq) goto loc_822F5BE0;
loc_822F5C08:
	// lwz r11,4(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r10,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r10.u32);
	// stw r4,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r4.u32);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822f5c44
	if (cr6.eq) goto loc_822F5C44;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r8,12(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 12);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// blt cr6,0x822f5c44
	if (cr6.lt) goto loc_822F5C44;
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// blr 
	return;
loc_822F5C44:
	// stw r11,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, r11.u32);
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// stw r4,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r4.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5C68"))) PPC_WEAK_FUNC(sub_822F5C68);
PPC_FUNC_IMPL(__imp__sub_822F5C68) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5c8c
	if (!cr6.eq) goto loc_822F5C8C;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f5c98
	goto loc_822F5C98;
loc_822F5C8C:
	// lwz r11,12(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822F5C98:
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// srawi r8,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// bge cr6,0x822f5ccc
	if (!cr6.lt) goto loc_822F5CCC;
	// lwz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
loc_822F5CCC:
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x822f7530
	sub_822F7530(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5CF8"))) PPC_WEAK_FUNC(sub_822F5CF8);
PPC_FUNC_IMPL(__imp__sub_822F5CF8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x822f6928
	sub_822F6928(ctx, base);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5D48"))) PPC_WEAK_FUNC(sub_822F5D48);
PPC_FUNC_IMPL(__imp__sub_822F5D48) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r31,r28,20
	r31.s64 = r28.s64 + 20;
	// lwz r30,24(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 24);
	// lwz r11,28(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 28);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// ble cr6,0x822f5d70
	if (!cr6.gt) goto loc_822F5D70;
	// twi 31,r0,22
loc_822F5D70:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f5d84
	if (!cr6.gt) goto loc_822F5D84;
	// twi 31,r0,22
loc_822F5D84:
	// cmplw cr6,r31,r31
	cr6.compare<uint32_t>(r31.u32, r31.u32, xer);
	// beq cr6,0x822f5d90
	if (cr6.eq) goto loc_822F5D90;
	// twi 31,r0,22
loc_822F5D90:
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f5dd8
	if (cr6.eq) goto loc_822F5DD8;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822f5da4
	if (cr6.lt) goto loc_822F5DA4;
	// twi 31,r0,22
loc_822F5DA4:
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f5dc0
	if (cr6.eq) goto loc_822F5DC0;
	// lwz r3,12(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F5DC0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822f5dd0
	if (cr6.lt) goto loc_822F5DD0;
	// twi 31,r0,22
loc_822F5DD0:
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// b 0x822f5d70
	goto loc_822F5D70;
loc_822F5DD8:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f5de8
	if (cr6.eq) goto loc_822F5DE8;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F5DE8:
	// li r30,0
	r30.s64 = 0;
	// addi r29,r28,4
	r29.s64 = r28.s64 + 4;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// lwz r3,8(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f5e0c
	if (cr6.eq) goto loc_822F5E0C;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F5E0C:
	// stw r30,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r30.u32);
	// stw r30,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r30.u32);
	// stw r30,12(r29)
	PPC_STORE_U32(r29.u32 + 12, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F5E20"))) PPC_WEAK_FUNC(sub_822F5E20);
PPC_FUNC_IMPL(__imp__sub_822F5E20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f5e50
	if (cr6.eq) goto loc_822F5E50;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F5E50:
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F5E78"))) PPC_WEAK_FUNC(sub_822F5E78);
PPC_FUNC_IMPL(__imp__sub_822F5E78) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// cmplw cr6,r26,r27
	cr6.compare<uint32_t>(r26.u32, r27.u32, xer);
	// beq cr6,0x822f6178
	if (cr6.eq) goto loc_822F6178;
	// lwz r31,4(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f5eb4
	if (cr6.eq) goto loc_822F5EB4;
	// lwz r30,8(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f5f2c
	if (!cr6.eq) goto loc_822F5F2C;
loc_822F5EB4:
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r31,8(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// ble cr6,0x822f5ed0
	if (!cr6.gt) goto loc_822F5ED0;
	// twi 31,r0,22
	// twi 31,r0,22
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
loc_822F5ED0:
	// beq cr6,0x822f6178
	if (cr6.eq) goto loc_822F6178;
	// subf r10,r31,r31
	ctx.r10.s64 = r31.s64 - r31.s64;
	// mr r30,r31
	r30.u64 = r31.u64;
	// srawi r9,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 2;
	// cmplw cr6,r31,r31
	cr6.compare<uint32_t>(r31.u32, r31.u32, xer);
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r10,r11
	r28.u64 = ctx.r10.u64 + r11.u64;
	// beq cr6,0x822f5f0c
	if (cr6.eq) goto loc_822F5F0C;
	// subf r29,r31,r11
	r29.s64 = r11.s64 - r31.s64;
loc_822F5EF4:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// add r3,r29,r30
	ctx.r3.u64 = r29.u64 + r30.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x822f5ef4
	if (!cr6.eq) goto loc_822F5EF4;
loc_822F5F0C:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r5,8(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// stw r28,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r28.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F5F2C:
	// lwz r4,4(r26)
	ctx.r4.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x822f5f44
	if (!cr6.eq) goto loc_822F5F44;
	// mr r11,r25
	r11.u64 = r25.u64;
	// b 0x822f5f50
	goto loc_822F5F50;
loc_822F5F44:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r9,r4,r11
	ctx.r9.s64 = r11.s64 - ctx.r4.s64;
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
loc_822F5F50:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x822f5ff0
	if (cr6.gt) goto loc_822F5FF0;
	// subf r11,r31,r30
	r11.s64 = r30.s64 - r31.s64;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// rlwinm r11,r10,2,0,29
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r4
	r28.u64 = r11.u64 + ctx.r4.u64;
	// beq cr6,0x822f5f8c
	if (cr6.eq) goto loc_822F5F8C;
	// subf r29,r31,r4
	r29.s64 = ctx.r4.s64 - r31.s64;
loc_822F5F74:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = r29.u64 + r31.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822f5f74
	if (!cr6.eq) goto loc_822F5F74;
loc_822F5F8C:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// lwz r5,8(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f5fc8
	if (!cr6.eq) goto loc_822F5FC8;
	// mr r11,r25
	r11.u64 = r25.u64;
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F5FC8:
	// lwz r10,8(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// stw r11,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F5FF0:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x822f6000
	if (!cr6.eq) goto loc_822F6000;
	// mr r11,r25
	r11.u64 = r25.u64;
	// b 0x822f600c
	goto loc_822F600C;
loc_822F6000:
	// lwz r11,12(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 12);
	// subf r9,r4,r11
	ctx.r9.s64 = r11.s64 - ctx.r4.s64;
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
loc_822F600C:
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x822f60bc
	if (cr6.gt) goto loc_822F60BC;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x822f6024
	if (!cr6.eq) goto loc_822F6024;
	// mr r11,r25
	r11.u64 = r25.u64;
	// b 0x822f6030
	goto loc_822F6030;
loc_822F6024:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// subf r10,r4,r11
	ctx.r10.s64 = r11.s64 - ctx.r4.s64;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
loc_822F6030:
	// lwz r30,4(r27)
	r30.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// add r31,r11,r30
	r31.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x822f6064
	if (cr6.eq) goto loc_822F6064;
loc_822F6048:
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x822f6048
	if (!cr6.eq) goto loc_822F6048;
loc_822F6064:
	// lwz r29,8(r27)
	r29.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// lwz r30,8(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// beq cr6,0x822f6174
	if (cr6.eq) goto loc_822F6174;
loc_822F6074:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f609c
	if (cr6.eq) goto loc_822F609C;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// beq cr6,0x822f609c
	if (cr6.eq) goto loc_822F609C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F609C:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x822f6074
	if (!cr6.eq) goto loc_822F6074;
	// stw r30,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r30.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F60BC:
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f60d8
	if (cr6.eq) goto loc_822F60D8;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r5,8(r26)
	ctx.r5.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,4(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F60D8:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f60ec
	if (!cr6.eq) goto loc_822F60EC;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// b 0x822f60f8
	goto loc_822F60F8;
loc_822F60EC:
	// lwz r10,8(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r4,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 2;
loc_822F60F8:
	// stw r25,4(r26)
	PPC_STORE_U32(r26.u32 + 4, r25.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r25,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r25.u32);
	// stw r25,12(r26)
	PPC_STORE_U32(r26.u32 + 12, r25.u32);
	// bne cr6,0x822f6114
	if (!cr6.eq) goto loc_822F6114;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// b 0x822f611c
	goto loc_822F611C;
loc_822F6114:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x825813e8
	sub_825813E8(ctx, base);
loc_822F611C:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6178
	if (cr6.eq) goto loc_822F6178;
	// lwz r29,8(r27)
	r29.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// lwz r31,4(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r30,4(r26)
	r30.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// beq cr6,0x822f6174
	if (cr6.eq) goto loc_822F6174;
loc_822F613C:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f6164
	if (cr6.eq) goto loc_822F6164;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// beq cr6,0x822f6164
	if (cr6.eq) goto loc_822F6164;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F6164:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x822f613c
	if (!cr6.eq) goto loc_822F613C;
loc_822F6174:
	// stw r30,8(r26)
	PPC_STORE_U32(r26.u32 + 8, r30.u32);
loc_822F6178:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F6188"))) PPC_WEAK_FUNC(sub_822F6188);
PPC_FUNC_IMPL(__imp__sub_822F6188) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f61b8
	if (!cr6.eq) goto loc_822F61B8;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f61c4
	goto loc_822F61C4;
loc_822F61B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822F61C4:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6218
	if (cr6.eq) goto loc_822F6218;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// srawi r10,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 2;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f6218
	if (!cr6.lt) goto loc_822F6218;
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f620c
	if (cr6.eq) goto loc_822F620C;
	// lwz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// beq cr6,0x822f620c
	if (cr6.eq) goto loc_822F620C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F620C:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x822f6240
	goto loc_822F6240;
loc_822F6218:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f6228
	if (!cr6.gt) goto loc_822F6228;
	// twi 31,r0,22
loc_822F6228:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x822f6a88
	sub_822F6A88(ctx, base);
loc_822F6240:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F6258"))) PPC_WEAK_FUNC(sub_822F6258);
PPC_FUNC_IMPL(__imp__sub_822F6258) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f6288
	if (!cr6.eq) goto loc_822F6288;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f6294
	goto loc_822F6294;
loc_822F6288:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822F6294:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f62e8
	if (cr6.eq) goto loc_822F62E8;
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r8,r11,r10
	ctx.r8.s64 = ctx.r10.s64 - r11.s64;
	// srawi r10,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r8.s32 >> 2;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f62e8
	if (!cr6.lt) goto loc_822F62E8;
	// lwz r30,8(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f62dc
	if (cr6.eq) goto loc_822F62DC;
	// lwz r3,0(r6)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// beq cr6,0x822f62dc
	if (cr6.eq) goto loc_822F62DC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F62DC:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// b 0x822f6310
	goto loc_822F6310;
loc_822F62E8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f62f8
	if (!cr6.gt) goto loc_822F62F8;
	// twi 31,r0,22
loc_822F62F8:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x822f6bd0
	sub_822F6BD0(ctx, base);
loc_822F6310:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F6328"))) PPC_WEAK_FUNC(sub_822F6328);
PPC_FUNC_IMPL(__imp__sub_822F6328) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r4,8(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r31,r3,4
	r31.s64 = ctx.r3.s64 + 4;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f635c
	if (cr6.eq) goto loc_822F635C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F635C:
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F6380"))) PPC_WEAK_FUNC(sub_822F6380);
PPC_FUNC_IMPL(__imp__sub_822F6380) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bge cr6,0x822f63ac
	if (!cr6.lt) goto loc_822F63AC;
	// bl 0x82cd12c8
	sub_82CD12C8(ctx, base);
loc_822F63AC:
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// subf r30,r27,r11
	r30.s64 = r11.s64 - r27.s64;
	// cmplw cr6,r28,r30
	cr6.compare<uint32_t>(r28.u32, r30.u32, xer);
	// bge cr6,0x822f63c0
	if (!cr6.lt) goto loc_822F63C0;
	// mr r30,r28
	r30.u64 = r28.u64;
loc_822F63C0:
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bne cr6,0x822f63f4
	if (!cr6.eq) goto loc_822F63F4;
	// li r5,-1
	ctx.r5.s64 = -1;
	// add r4,r30,r27
	ctx.r4.u64 = r30.u64 + r27.u64;
	// bl 0x822f6e18
	sub_822F6E18(ctx, base);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f6e18
	sub_822F6E18(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_822F63F4:
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8217a5e0
	sub_8217A5E0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6474
	if (cr6.eq) goto loc_822F6474;
	// lwz r11,24(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f6420
	if (cr6.lt) goto loc_822F6420;
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// b 0x822f6424
	goto loc_822F6424;
loc_822F6420:
	// addi r9,r29,4
	ctx.r9.s64 = r29.s64 + 4;
loc_822F6424:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r28,r31,4
	r28.s64 = r31.s64 + 4;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// blt cr6,0x822f643c
	if (cr6.lt) goto loc_822F643C;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x822f6440
	goto loc_822F6440;
loc_822F643C:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
loc_822F6440:
	// rlwinm r11,r27,1,0,30
	r11.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r29,r30,1,0,30
	r29.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// rlwinm r4,r10,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f646c
	if (cr6.lt) goto loc_822F646C;
	// lwz r28,0(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_822F646C:
	// li r11,0
	r11.s64 = 0;
	// sthx r11,r29,r28
	PPC_STORE_U16(r29.u32 + r28.u32, r11.u16);
loc_822F6474:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F6480"))) PPC_WEAK_FUNC(sub_822F6480);
PPC_FUNC_IMPL(__imp__sub_822F6480) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r27,r6
	r27.u64 = ctx.r6.u64;
	// mr r26,r4
	r26.u64 = ctx.r4.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// bne cr6,0x822f64bc
	if (!cr6.eq) goto loc_822F64BC;
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// bge cr6,0x822f64b0
	if (!cr6.lt) goto loc_822F64B0;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
loc_822F64B0:
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F64BC:
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bgt cr6,0x822f6568
	if (cr6.gt) goto loc_822F6568;
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// cmplw cr6,r5,r11
	cr6.compare<uint32_t>(ctx.r5.u32, r11.u32, xer);
	// blt cr6,0x822f64d4
	if (cr6.lt) goto loc_822F64D4;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
loc_822F64D4:
	// lwz r25,24(r29)
	r25.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// addi r28,r29,4
	r28.s64 = r29.s64 + 4;
	// cmplwi cr6,r25,8
	cr6.compare<uint32_t>(r25.u32, 8, xer);
	// blt cr6,0x822f64ec
	if (cr6.lt) goto loc_822F64EC;
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// b 0x822f64f0
	goto loc_822F64F0;
loc_822F64EC:
	// mr r11,r28
	r11.u64 = r28.u64;
loc_822F64F0:
	// rlwinm r10,r5,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// lhz r30,0(r26)
	r30.u64 = PPC_LOAD_U16(r26.u32 + 0);
	// add r31,r10,r11
	r31.u64 = ctx.r10.u64 + r11.u64;
loc_822F64FC:
	// lhz r11,0(r31)
	r11.u64 = PPC_LOAD_U16(r31.u32 + 0);
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bne cr6,0x822f6520
	if (!cr6.eq) goto loc_822F6520;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f85c8
	sub_822F85C8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f6538
	if (cr6.eq) goto loc_822F6538;
loc_822F6520:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822f5458
	sub_822F5458(ctx, base);
	// cmplw cr6,r31,r3
	cr6.compare<uint32_t>(r31.u32, ctx.r3.u32, xer);
	// beq cr6,0x822f6568
	if (cr6.eq) goto loc_822F6568;
	// addi r31,r31,-2
	r31.s64 = r31.s64 + -2;
	// b 0x822f64fc
	goto loc_822F64FC;
loc_822F6538:
	// cmplwi cr6,r25,8
	cr6.compare<uint32_t>(r25.u32, 8, xer);
	// blt cr6,0x822f6554
	if (cr6.lt) goto loc_822F6554;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F6554:
	// mr r11,r28
	r11.u64 = r28.u64;
	// subf r11,r11,r31
	r11.s64 = r31.s64 - r11.s64;
	// srawi r3,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r3.s64 = r11.s32 >> 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F6568:
	// li r3,-1
	ctx.r3.s64 = -1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F6578"))) PPC_WEAK_FUNC(sub_822F6578);
PPC_FUNC_IMPL(__imp__sub_822F6578) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,24(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmplwi cr6,r5,1
	cr6.compare<uint32_t>(ctx.r5.u32, 1, xer);
	// bne cr6,0x822f65ac
	if (!cr6.eq) goto loc_822F65AC;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f659c
	if (cr6.lt) goto loc_822F659C;
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r6,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r6.u16);
	// blr 
	return;
loc_822F659C:
	// addi r11,r3,4
	r11.s64 = ctx.r3.s64 + 4;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r6,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r6.u16);
	// blr 
	return;
loc_822F65AC:
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f65bc
	if (cr6.lt) goto loc_822F65BC;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// b 0x822f65c0
	goto loc_822F65C0;
loc_822F65BC:
	// addi r10,r3,4
	ctx.r10.s64 = ctx.r3.s64 + 4;
loc_822F65C0:
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// beqlr cr6
	if (cr6.eq) return;
	// mtctr r5
	ctr.u64 = ctx.r5.u64;
loc_822F65D4:
	// sth r6,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r6.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x822f65d4
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822F65D4;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F65E8"))) PPC_WEAK_FUNC(sub_822F65E8);
PPC_FUNC_IMPL(__imp__sub_822F65E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f663c
	if (cr6.eq) goto loc_822F663C;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f663c
	if (cr6.lt) goto loc_822F663C;
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r3,r30,4
	ctx.r3.s64 = r30.s64 + 4;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f6634
	if (cr6.eq) goto loc_822F6634;
	// rlwinm r6,r31,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,16
	ctx.r4.s64 = 16;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
loc_822F6634:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F663C:
	// addi r11,r30,4
	r11.s64 = r30.s64 + 4;
	// stw r31,20(r30)
	PPC_STORE_U32(r30.u32 + 20, r31.u32);
	// rlwinm r10,r31,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,7
	ctx.r9.s64 = 7;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r9,24(r30)
	PPC_STORE_U32(r30.u32 + 24, ctx.r9.u32);
	// sthx r8,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r8.u16);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F6660"))) PPC_WEAK_FUNC(sub_822F6660);
PPC_FUNC_IMPL(__imp__sub_822F6660) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x822f6f58
	sub_822F6F58(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F66C8"))) PPC_WEAK_FUNC(sub_822F66C8);
PPC_FUNC_IMPL(__imp__sub_822F66C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822f66f0
	if (cr6.eq) goto loc_822F66F0;
	// bl 0x822f4df8
	sub_822F4DF8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F66F0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F6708"))) PPC_WEAK_FUNC(sub_822F6708);
PPC_FUNC_IMPL(__imp__sub_822F6708) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// li r11,0
	r11.s64 = 0;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// std r28,192(r1)
	PPC_STORE_U64(ctx.r1.u32 + 192, r28.u64);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f675c
	if (!cr6.eq) goto loc_822F675C;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r6,4(r31)
	ctx.r6.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r5,1
	ctx.r5.s64 = 1;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
loc_822F675C:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,192(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// beq cr6,0x822f6778
	if (cr6.eq) goto loc_822F6778;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f677c
	if (cr6.eq) goto loc_822F677C;
loc_822F6778:
	// twi 31,r0,22
loc_822F677C:
	// lwz r27,196(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// cmplw cr6,r27,r9
	cr6.compare<uint32_t>(r27.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f67bc
	if (!cr6.eq) goto loc_822F67BC;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r10,12(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f68f4
	if (!cr6.lt) goto loc_822F68F4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
loc_822F67BC:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f67cc
	if (cr6.eq) goto loc_822F67CC;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f67d0
	if (cr6.eq) goto loc_822F67D0;
loc_822F67CC:
	// twi 31,r0,22
loc_822F67D0:
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// cmplw cr6,r27,r10
	cr6.compare<uint32_t>(r27.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f680c
	if (!cr6.eq) goto loc_822F680C;
	// lwz r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,12(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f68f4
	if (!cr6.lt) goto loc_822F68F4;
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
loc_822F680C:
	// lwz r10,12(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f686c
	if (!cr6.lt) goto loc_822F686C;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// std r28,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r28.u64);
	// bl 0x828836e8
	sub_828836E8(ctx, base);
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r10,12(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f686c
	if (!cr6.lt) goto loc_822F686C;
	// lwz r11,8(r6)
	r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f68e0
	if (!cr6.eq) goto loc_822F68E0;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
loc_822F6858:
	// li r5,1
	ctx.r5.s64 = 1;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
loc_822F686C:
	// lwz r10,12(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 12);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f68f4
	if (!cr6.lt) goto loc_822F68F4;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// std r28,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r28.u64);
	// lwz r28,4(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f689c
	if (cr6.eq) goto loc_822F689C;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f68a0
	if (cr6.eq) goto loc_822F68A0;
loc_822F689C:
	// twi 31,r0,22
loc_822F68A0:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r6,84(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822f68c0
	if (cr6.eq) goto loc_822F68C0;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// lwz r10,12(r6)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r6.u32 + 12);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f68f4
	if (!cr6.lt) goto loc_822F68F4;
loc_822F68C0:
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f6858
	if (cr6.eq) goto loc_822F6858;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
loc_822F68E0:
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
loc_822F68F4:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822f7068
	sub_822F7068(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r9,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r9.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F6928"))) PPC_WEAK_FUNC(sub_822F6928);
PPC_FUNC_IMPL(__imp__sub_822F6928) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// std r28,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, r28.u64);
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// std r6,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r6.u64);
	// lwz r3,164(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// lwz r5,172(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// cmplw cr6,r3,r5
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r5.u32, xer);
	// beq cr6,0x822f697c
	if (cr6.eq) goto loc_822F697C;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r5,r11
	ctx.r10.s64 = r11.s64 - ctx.r5.s64;
	// srawi. r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r6,r3
	r29.u64 = ctx.r6.u64 + ctx.r3.u64;
	// ble 0x822f6978
	if (!cr0.gt) goto loc_822F6978;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F6978:
	// stw r29,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r29.u32);
loc_822F697C:
	// std r28,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r28.u64);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F6990"))) PPC_WEAK_FUNC(sub_822F6990);
PPC_FUNC_IMPL(__imp__sub_822F6990) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x82b4cc70
	sub_82B4CC70(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F69F8"))) PPC_WEAK_FUNC(sub_822F69F8);
PPC_FUNC_IMPL(__imp__sub_822F69F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r27,0
	r27.s64 = 0;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r30,0(r11)
	r30.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// stw r27,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r27.u32);
	// beq cr6,0x822f6a74
	if (cr6.eq) goto loc_822F6A74;
	// li r26,7
	r26.s64 = 7;
loc_822F6A34:
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// addi r31,r30,8
	r31.s64 = r30.s64 + 8;
	// lwz r29,0(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f6a50
	if (cr6.lt) goto loc_822F6A50;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F6A50:
	// stw r26,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r26.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stw r27,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r27.u32);
	// sth r27,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r27.u16);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r30,r29
	r30.u64 = r29.u64;
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x822f6a34
	if (!cr6.eq) goto loc_822F6A34;
loc_822F6A74:
	// lwz r3,4(r28)
	ctx.r3.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// stw r27,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r27.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F6A88"))) PPC_WEAK_FUNC(sub_822F6A88);
PPC_FUNC_IMPL(__imp__sub_822F6A88) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r4,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r4.u64);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6ac0
	if (cr6.eq) goto loc_822F6AC0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi. r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822f6ac8
	if (!cr0.eq) goto loc_822F6AC8;
loc_822F6AC0:
	// li r30,0
	r30.s64 = 0;
	// b 0x822f6af8
	goto loc_822F6AF8;
loc_822F6AC8:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f6ad4
	if (!cr6.gt) goto loc_822F6AD4;
	// twi 31,r0,22
loc_822F6AD4:
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f6ae8
	if (cr6.eq) goto loc_822F6AE8;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// beq cr6,0x822f6aec
	if (cr6.eq) goto loc_822F6AEC;
loc_822F6AE8:
	// twi 31,r0,22
loc_822F6AEC:
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r30,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r30.s64 = ctx.r9.s32 >> 2;
loc_822F6AF8:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f7870
	sub_822F7870(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f6b18
	if (!cr6.gt) goto loc_822F6B18;
	// twi 31,r0,22
loc_822F6B18:
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// bgt cr6,0x822f6b48
	if (cr6.gt) goto loc_822F6B48;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f6b4c
	if (!cr6.lt) goto loc_822F6B4C;
loc_822F6B48:
	// twi 31,r0,22
loc_822F6B4C:
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r11.u64);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F6B68"))) PPC_WEAK_FUNC(sub_822F6B68);
PPC_FUNC_IMPL(__imp__sub_822F6B68) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// cmplw cr6,r4,r30
	cr6.compare<uint32_t>(ctx.r4.u32, r30.u32, xer);
	// beq cr6,0x822f6bb4
	if (cr6.eq) goto loc_822F6BB4;
loc_822F6B8C:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f6ba8
	if (cr6.eq) goto loc_822F6BA8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F6BA8:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822f6b8c
	if (!cr6.eq) goto loc_822F6B8C;
loc_822F6BB4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F6BD0"))) PPC_WEAK_FUNC(sub_822F6BD0);
PPC_FUNC_IMPL(__imp__sub_822F6BD0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r4,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r4.u64);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6c08
	if (cr6.eq) goto loc_822F6C08;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi. r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822f6c10
	if (!cr0.eq) goto loc_822F6C10;
loc_822F6C08:
	// li r30,0
	r30.s64 = 0;
	// b 0x822f6c40
	goto loc_822F6C40;
loc_822F6C10:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f6c1c
	if (!cr6.gt) goto loc_822F6C1C;
	// twi 31,r0,22
loc_822F6C1C:
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f6c30
	if (cr6.eq) goto loc_822F6C30;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// beq cr6,0x822f6c34
	if (cr6.eq) goto loc_822F6C34;
loc_822F6C30:
	// twi 31,r0,22
loc_822F6C34:
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r30,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r30.s64 = ctx.r9.s32 >> 2;
loc_822F6C40:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f7b70
	sub_822F7B70(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822f6c60
	if (!cr6.gt) goto loc_822F6C60;
	// twi 31,r0,22
loc_822F6C60:
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// bgt cr6,0x822f6c90
	if (cr6.gt) goto loc_822F6C90;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f6c94
	if (!cr6.lt) goto loc_822F6C94;
loc_822F6C90:
	// twi 31,r0,22
loc_822F6C94:
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r11.u64);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F6CB0"))) PPC_WEAK_FUNC(sub_822F6CB0);
PPC_FUNC_IMPL(__imp__sub_822F6CB0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// addi r29,r31,4
	r29.s64 = r31.s64 + 4;
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// blt cr6,0x822f6ce0
	if (cr6.lt) goto loc_822F6CE0;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x822f6ce4
	goto loc_822F6CE4;
loc_822F6CE0:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822F6CE4:
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// blt cr6,0x822f6d18
	if (cr6.lt) goto loc_822F6D18;
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// blt cr6,0x822f6cfc
	if (cr6.lt) goto loc_822F6CFC;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// b 0x822f6d00
	goto loc_822F6D00;
loc_822F6CFC:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822F6D00:
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// li r11,1
	r11.s64 = 1;
	// cmplw cr6,r8,r27
	cr6.compare<uint32_t>(ctx.r8.u32, r27.u32, xer);
	// bgt cr6,0x822f6d1c
	if (cr6.gt) goto loc_822F6D1C;
loc_822F6D18:
	// li r11,0
	r11.s64 = 0;
loc_822F6D1C:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6d5c
	if (cr6.eq) goto loc_822F6D5C;
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// blt cr6,0x822f6d38
	if (cr6.lt) goto loc_822F6D38;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// b 0x822f6d3c
	goto loc_822F6D3C;
loc_822F6D38:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822F6D3C:
	// subf r11,r11,r27
	r11.s64 = r27.s64 - r11.s64;
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// srawi r5,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r5.s64 = r11.s32 >> 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f7e28
	sub_822F7E28(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
loc_822F6D5C:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// subfic r10,r11,-1
	xer.ca = r11.u32 <= 4294967295;
	ctx.r10.s64 = -1 - r11.s64;
	// cmplw cr6,r10,r28
	cr6.compare<uint32_t>(ctx.r10.u32, r28.u32, xer);
	// ble cr6,0x822f6d78
	if (!cr6.gt) goto loc_822F6D78;
	// add r10,r11,r28
	ctx.r10.u64 = r11.u64 + r28.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f6d7c
	if (!cr6.lt) goto loc_822F6D7C;
loc_822F6D78:
	// bl 0x82cd11d0
	sub_82CD11D0(ctx, base);
loc_822F6D7C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822f6e08
	if (cr6.eq) goto loc_822F6E08;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r30,r28,r11
	r30.u64 = r28.u64 + r11.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x8217a5e0
	sub_8217A5E0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f6e08
	if (cr6.eq) goto loc_822F6E08;
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f6dbc
	if (cr6.lt) goto loc_822F6DBC;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// b 0x822f6dc0
	goto loc_822F6DC0;
loc_822F6DBC:
	// mr r10,r29
	ctx.r10.u64 = r29.u64;
loc_822F6DC0:
	// lwz r9,20(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rlwinm r6,r28,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// subf r8,r9,r11
	ctx.r8.s64 = r11.s64 - ctx.r9.s64;
	// rlwinm r11,r9,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r8,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r10
	ctx.r3.u64 = r11.u64 + ctx.r10.u64;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// cmplwi cr6,r7,8
	cr6.compare<uint32_t>(ctx.r7.u32, 8, xer);
	// blt cr6,0x822f6df8
	if (cr6.lt) goto loc_822F6DF8;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// b 0x822f6dfc
	goto loc_822F6DFC;
loc_822F6DF8:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822F6DFC:
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// sthx r9,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r9.u16);
loc_822F6E08:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F6E18"))) PPC_WEAK_FUNC(sub_822F6E18);
PPC_FUNC_IMPL(__imp__sub_822F6E18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// bge cr6,0x822f6e40
	if (!cr6.lt) goto loc_822F6E40;
	// bl 0x82cd12c8
	sub_82CD12C8(ctx, base);
loc_822F6E40:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// subf r11,r29,r11
	r11.s64 = r11.s64 - r29.s64;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bge cr6,0x822f6e54
	if (!cr6.lt) goto loc_822F6E54;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_822F6E54:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f6edc
	if (cr6.eq) goto loc_822F6EDC;
	// lwz r7,24(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r28,r31,4
	r28.s64 = r31.s64 + 4;
	// cmplwi cr6,r7,8
	cr6.compare<uint32_t>(ctx.r7.u32, 8, xer);
	// blt cr6,0x822f6e74
	if (cr6.lt) goto loc_822F6E74;
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x822f6e78
	goto loc_822F6E78;
loc_822F6E74:
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
loc_822F6E78:
	// cmplwi cr6,r7,8
	cr6.compare<uint32_t>(ctx.r7.u32, 8, xer);
	// blt cr6,0x822f6e88
	if (cr6.lt) goto loc_822F6E88;
	// lwz r9,0(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// b 0x822f6e8c
	goto loc_822F6E8C;
loc_822F6E88:
	// mr r9,r28
	ctx.r9.u64 = r28.u64;
loc_822F6E8C:
	// add r10,r29,r30
	ctx.r10.u64 = r29.u64 + r30.u64;
	// subf r6,r30,r11
	ctx.r6.s64 = r11.s64 - r30.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r29,r7
	ctx.r4.s64 = ctx.r7.s64 - r29.s64;
	// rlwinm r11,r29,1,0,30
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r6,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// subf r11,r30,r3
	r11.s64 = ctx.r3.s64 - r30.s64;
	// cmplwi cr6,r10,8
	cr6.compare<uint32_t>(ctx.r10.u32, 8, xer);
	// stw r11,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r11.u32);
	// blt cr6,0x822f6ed0
	if (cr6.lt) goto loc_822F6ED0;
	// lwz r28,0(r28)
	r28.u64 = PPC_LOAD_U32(r28.u32 + 0);
loc_822F6ED0:
	// rlwinm r11,r11,1,0,30
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r10,0
	ctx.r10.s64 = 0;
	// sthx r10,r11,r28
	PPC_STORE_U16(r11.u32 + r28.u32, ctx.r10.u16);
loc_822F6EDC:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F6EE8"))) PPC_WEAK_FUNC(sub_822F6EE8);
PPC_FUNC_IMPL(__imp__sub_822F6EE8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
	// beq cr6,0x822f6f44
	if (cr6.eq) goto loc_822F6F44;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f6f44
	if (cr6.eq) goto loc_822F6F44;
	// lwz r9,24(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 24);
	// addi r11,r5,4
	r11.s64 = ctx.r5.s64 + 4;
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// blt cr6,0x822f6f18
	if (cr6.lt) goto loc_822F6F18;
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// b 0x822f6f1c
	goto loc_822F6F1C;
loc_822F6F18:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
loc_822F6F1C:
	// cmplw cr6,r10,r4
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r4.u32, xer);
	// bgt cr6,0x822f6f44
	if (cr6.gt) goto loc_822F6F44;
	// cmplwi cr6,r9,8
	cr6.compare<uint32_t>(ctx.r9.u32, 8, xer);
	// blt cr6,0x822f6f30
	if (cr6.lt) goto loc_822F6F30;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F6F30:
	// lwz r10,20(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 20);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// ble cr6,0x822f6f48
	if (!cr6.gt) goto loc_822F6F48;
loc_822F6F44:
	// twi 31,r0,22
loc_822F6F48:
	// stw r5,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r5.u32);
	// stw r4,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F6F58"))) PPC_WEAK_FUNC(sub_822F6F58);
PPC_FUNC_IMPL(__imp__sub_822F6F58) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// std r5,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r5.u64);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r6,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r6.u64);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x822f6f90
	if (cr6.eq) goto loc_822F6F90;
	// cmplw cr6,r9,r31
	cr6.compare<uint32_t>(ctx.r9.u32, r31.u32, xer);
	// beq cr6,0x822f6f94
	if (cr6.eq) goto loc_822F6F94;
loc_822F6F90:
	// twi 31,r0,22
loc_822F6F94:
	// lwz r8,180(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r28,188(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r30,184(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f700c
	if (!cr6.eq) goto loc_822F700C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f6fb8
	if (cr6.eq) goto loc_822F6FB8;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x822f6fbc
	if (cr6.eq) goto loc_822F6FBC;
loc_822F6FB8:
	// twi 31,r0,22
loc_822F6FBC:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822f700c
	if (!cr6.eq) goto loc_822F700C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822f8380
	sub_822F8380(ctx, base);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// stw r6,4(r29)
	PPC_STORE_U32(r29.u32 + 4, ctx.r6.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
loc_822F700C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f701c
	if (cr6.eq) goto loc_822F701C;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// beq cr6,0x822f7020
	if (cr6.eq) goto loc_822F7020;
loc_822F701C:
	// twi 31,r0,22
loc_822F7020:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822f7054
	if (cr6.eq) goto loc_822F7054;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// bl 0x82c705d8
	sub_82C705D8(ctx, base);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f7f40
	sub_822F7F40(ctx, base);
	// ld r5,176(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// b 0x822f700c
	goto loc_822F700C;
loc_822F7054:
	// std r5,0(r29)
	PPC_STORE_U64(r29.u32 + 0, ctx.r5.u64);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F7068"))) PPC_WEAK_FUNC(sub_822F7068);
PPC_FUNC_IMPL(__imp__sub_822F7068) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r26,1
	r26.s64 = 1;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r31,4(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f70d4
	if (!cr6.eq) goto loc_822F70D4;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
loc_822F70A0:
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r31,r11
	r31.u64 = r11.u64;
	// subfc r8,r9,r10
	xer.ca = ctx.r10.u32 >= ctx.r9.u32;
	ctx.r8.s64 = ctx.r10.s64 - ctx.r9.s64;
	// subfe r7,r8,r8
	temp.u8 = (~ctx.r8.u32 + ctx.r8.u32 < ~ctx.r8.u32) | (~ctx.r8.u32 + ctx.r8.u32 + xer.ca < xer.ca);
	ctx.r7.u64 = ~ctx.r8.u64 + ctx.r8.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r29,r7,31
	r29.u64 = ctx.r7.u32 & 0x1;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f70c4
	if (cr6.eq) goto loc_822F70C4;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822f70c8
	goto loc_822F70C8;
loc_822F70C4:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_822F70C8:
	// lbz r9,33(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f70a0
	if (cr6.eq) goto loc_822F70A0;
loc_822F70D4:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f7138
	if (cr6.eq) goto loc_822F7138;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f7134
	if (!cr6.eq) goto loc_822F7134;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r30)
	PPC_STORE_U8(r30.u32 + 8, r26.u8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// stw r9,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F7134:
	// bl 0x828836e8
	sub_828836E8(ctx, base);
loc_822F7138:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bge cr6,0x822f7188
	if (!cr6.lt) goto loc_822F7188;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f72b8
	sub_822F72B8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r30)
	PPC_STORE_U8(r30.u32 + 8, r26.u8);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r10.u32);
	// stw r9,4(r30)
	PPC_STORE_U32(r30.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822F7188:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stb r10,8(r30)
	PPC_STORE_U8(r30.u32 + 8, ctx.r10.u8);
	// std r11,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F71A8"))) PPC_WEAK_FUNC(sub_822F71A8);
PPC_FUNC_IMPL(__imp__sub_822F71A8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// std r5,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r5.u64);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r6,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r6.u64);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x822f71e0
	if (cr6.eq) goto loc_822F71E0;
	// cmplw cr6,r9,r31
	cr6.compare<uint32_t>(ctx.r9.u32, r31.u32, xer);
	// beq cr6,0x822f71e4
	if (cr6.eq) goto loc_822F71E4;
loc_822F71E0:
	// twi 31,r0,22
loc_822F71E4:
	// lwz r8,180(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r28,188(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r30,184(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f725c
	if (!cr6.eq) goto loc_822F725C;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f7208
	if (cr6.eq) goto loc_822F7208;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x822f720c
	if (cr6.eq) goto loc_822F720C;
loc_822F7208:
	// twi 31,r0,22
loc_822F720C:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822f725c
	if (!cr6.eq) goto loc_822F725C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822f8400
	sub_822F8400(ctx, base);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// stw r6,4(r29)
	PPC_STORE_U32(r29.u32 + 4, ctx.r6.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
loc_822F725C:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f726c
	if (cr6.eq) goto loc_822F726C;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// beq cr6,0x822f7270
	if (cr6.eq) goto loc_822F7270;
loc_822F726C:
	// twi 31,r0,22
loc_822F7270:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822f72a4
	if (cr6.eq) goto loc_822F72A4;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822f5780
	sub_822F5780(ctx, base);
	// ld r5,176(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// b 0x822f725c
	goto loc_822F725C;
loc_822F72A4:
	// std r5,0(r29)
	PPC_STORE_U64(r29.u32 + 0, ctx.r5.u64);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F72B8"))) PPC_WEAK_FUNC(sub_822F72B8);
PPC_FUNC_IMPL(__imp__sub_822F72B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r11,3276
	r11.s64 = 214695936;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// ori r9,r11,52427
	ctx.r9.u64 = r11.u64 | 52427;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f7330
	if (cr6.lt) goto loc_822F7330;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5552
	ctx.r4.s64 = r11.s64 + 5552;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822F7330:
	// li r3,36
	ctx.r3.s64 = 36;
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822f7378
	if (cr6.eq) goto loc_822F7378;
	// addi r11,r27,12
	r11.s64 = r27.s64 + 12;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
	// addi r4,r28,4
	ctx.r4.s64 = r28.s64 + 4;
	// stw r30,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r30.u32);
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// stw r11,12(r27)
	PPC_STORE_U32(r27.u32 + 12, r11.u32);
	// bl 0x822f77a0
	sub_822F77A0(ctx, base);
	// stb r25,32(r27)
	PPC_STORE_U8(r27.u32 + 32, r25.u8);
	// stb r25,33(r27)
	PPC_STORE_U8(r27.u32 + 33, r25.u8);
loc_822F7378:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// bne cr6,0x822f73a8
	if (!cr6.eq) goto loc_822F73A8;
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r27.u32);
	// b 0x822f73e8
	goto loc_822F73E8;
loc_822F73A8:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f73d0
	if (cr6.eq) goto loc_822F73D0;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f73e8
	if (!cr6.eq) goto loc_822F73E8;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// b 0x822f73e8
	goto loc_822F73E8;
loc_822F73D0:
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f73e8
	if (!cr6.eq) goto loc_822F73E8;
	// stw r27,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r27.u32);
loc_822F73E8:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// li r30,1
	r30.s64 = 1;
	// mr r31,r27
	r31.u64 = r27.u64;
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f7510
	if (!cr6.eq) goto loc_822F7510;
loc_822F7404:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f748c
	if (!cr6.eq) goto loc_822F748C;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f744c
	if (!cr6.eq) goto loc_822F744C;
	// rotlwi r9,r4,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// stb r30,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r30.u8);
	// stb r30,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,32(r7)
	PPC_STORE_U8(ctx.r7.u32 + 32, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822f74fc
	goto loc_822F74FC;
loc_822F744C:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f7464
	if (!cr6.eq) goto loc_822F7464;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
loc_822F7464:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x8234d600
	sub_8234D600(ctx, base);
	// b 0x822f74fc
	goto loc_822F74FC;
loc_822F748C:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f74c0
	if (!cr6.eq) goto loc_822F74C0;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r30,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r30.u8);
	// stb r30,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,32(r7)
	PPC_STORE_U8(ctx.r7.u32 + 32, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822f74fc
	goto loc_822F74FC;
loc_822F74C0:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822f74d8
	if (!cr6.eq) goto loc_822F74D8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x8234d600
	sub_8234D600(ctx, base);
loc_822F74D8:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
loc_822F74FC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f7404
	if (cr6.eq) goto loc_822F7404;
loc_822F7510:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r27,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r27.u32);
	// stw r29,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r29.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stb r30,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r30.u8);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822F7530"))) PPC_WEAK_FUNC(sub_822F7530);
PPC_FUNC_IMPL(__imp__sub_822F7530) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r25,0(r6)
	r25.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// std r4,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r4.u64);
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f7560
	if (!cr6.eq) goto loc_822F7560;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f756c
	goto loc_822F756C;
loc_822F7560:
	// lwz r10,12(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822F756C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f7798
	if (cr6.eq) goto loc_822F7798;
	// lwz r28,8(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lis r10,16383
	ctx.r10.s64 = 1073676288;
	// subf r8,r11,r28
	ctx.r8.s64 = r28.s64 - r11.s64;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// srawi r11,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r11.s64 = ctx.r8.s32 >> 2;
	// subf r7,r11,r10
	ctx.r7.s64 = ctx.r10.s64 - r11.s64;
	// cmplw cr6,r7,r29
	cr6.compare<uint32_t>(ctx.r7.u32, r29.u32, xer);
	// bge cr6,0x822f75a0
	if (!cr6.lt) goto loc_822F75A0;
	// bl 0x82a97648
	sub_82A97648(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F75A0:
	// add r8,r11,r29
	ctx.r8.u64 = r11.u64 + r29.u64;
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bge cr6,0x822f768c
	if (!cr6.lt) goto loc_822F768C;
	// rlwinm r11,r9,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// li r26,0
	r26.s64 = 0;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f75c4
	if (cr6.lt) goto loc_822F75C4;
	// add r26,r11,r9
	r26.u64 = r11.u64 + ctx.r9.u64;
loc_822F75C4:
	// cmplw cr6,r26,r8
	cr6.compare<uint32_t>(r26.u32, ctx.r8.u32, xer);
	// bge cr6,0x822f75d0
	if (!cr6.lt) goto loc_822F75D0;
	// mr r26,r8
	r26.u64 = ctx.r8.u64;
loc_822F75D0:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x823093f8
	sub_823093F8(ctx, base);
	// lwz r5,4(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r28,172(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// subf r11,r5,r28
	r11.s64 = r28.s64 - ctx.r5.s64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r27,r6,r30
	r27.u64 = ctx.r6.u64 + r30.u64;
	// beq 0x822f7604
	if (cr0.eq) goto loc_822F7604;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F7604:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822f7620
	if (cr6.eq) goto loc_822F7620;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mtctr r29
	ctr.u64 = r29.u64;
loc_822F7614:
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x822f7614
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822F7614;
loc_822F7620:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r28,r11
	ctx.r10.s64 = r11.s64 - r28.s64;
	// srawi. r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822f7648
	if (cr0.eq) goto loc_822F7648;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// add r3,r11,r27
	ctx.r3.u64 = r11.u64 + r27.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F7648:
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// subf r10,r3,r11
	ctx.r10.s64 = r11.s64 - ctx.r3.s64;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
	// add r29,r11,r29
	r29.u64 = r11.u64 + r29.u64;
	// beq cr6,0x822f7668
	if (cr6.eq) goto loc_822F7668;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F7668:
	// rlwinm r11,r26,2,0,29
	r11.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F768C:
	// lwz r30,172(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// subf r11,r30,r28
	r11.s64 = r28.s64 - r30.s64;
	// srawi r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// bge cr6,0x822f7724
	if (!cr6.lt) goto loc_822F7724;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f76c0
	if (cr6.eq) goto loc_822F76C0;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r29,2,0,29
	r11.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// add r3,r11,r30
	ctx.r3.u64 = r11.u64 + r30.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F76C0:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r30,r11
	ctx.r10.s64 = r11.s64 - r30.s64;
	// srawi r9,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 2;
	// subf. r10,r9,r29
	ctx.r10.s64 = r29.s64 - ctx.r9.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822f76ec
	if (cr0.eq) goto loc_822F76EC;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f76ec
	if (cr6.eq) goto loc_822F76EC;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_822F76E0:
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x822f76e0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822F76E0;
loc_822F76EC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// rlwinm r9,r29,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r30
	r11.u64 = r30.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// subf r9,r9,r10
	ctx.r9.s64 = ctx.r10.s64 - ctx.r9.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// cmplw cr6,r30,r9
	cr6.compare<uint32_t>(r30.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f7798
	if (cr6.eq) goto loc_822F7798;
loc_822F770C:
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f770c
	if (!cr6.eq) goto loc_822F770C;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
loc_822F7724:
	// rlwinm r27,r29,2,0,29
	r27.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r3,8(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r29,r27,r28
	r29.s64 = r28.s64 - r27.s64;
	// subf r11,r29,r28
	r11.s64 = r28.s64 - r29.s64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r26,r6,r3
	r26.u64 = ctx.r6.u64 + ctx.r3.u64;
	// beq 0x822f7750
	if (cr0.eq) goto loc_822F7750;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F7750:
	// subf r11,r30,r29
	r11.s64 = r29.s64 - r30.s64;
	// stw r26,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r26.u32);
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822f7778
	if (!cr0.gt) goto loc_822F7778;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r6,r11
	ctx.r6.u64 = r11.u64;
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// subf r3,r11,r28
	ctx.r3.s64 = r28.s64 - r11.s64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F7778:
	// add r10,r27,r30
	ctx.r10.u64 = r27.u64 + r30.u64;
	// mr r11,r30
	r11.u64 = r30.u64;
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f7798
	if (cr6.eq) goto loc_822F7798;
loc_822F7788:
	// stw r25,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r25.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f7788
	if (!cr6.eq) goto loc_822F7788;
loc_822F7798:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F77A0"))) PPC_WEAK_FUNC(sub_822F77A0);
PPC_FUNC_IMPL(__imp__sub_822F77A0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f77cc
	if (!cr6.eq) goto loc_822F77CC;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// b 0x822f77d8
	goto loc_822F77D8;
loc_822F77CC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r4,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 2;
loc_822F77D8:
	// stw r3,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r3.u32);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// stw r3,8(r28)
	PPC_STORE_U32(r28.u32 + 8, ctx.r3.u32);
	// stw r3,12(r28)
	PPC_STORE_U32(r28.u32 + 12, ctx.r3.u32);
	// beq cr6,0x822f77f4
	if (cr6.eq) goto loc_822F77F4;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x825813e8
	sub_825813E8(ctx, base);
loc_822F77F4:
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f7860
	if (cr6.eq) goto loc_822F7860;
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// ble cr6,0x822f7818
	if (!cr6.gt) goto loc_822F7818;
	// twi 31,r0,22
	// twi 31,r0,22
loc_822F7818:
	// lwz r30,4(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// beq cr6,0x822f785c
	if (cr6.eq) goto loc_822F785C;
loc_822F7824:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f784c
	if (cr6.eq) goto loc_822F784C;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// stw r3,0(r30)
	PPC_STORE_U32(r30.u32 + 0, ctx.r3.u32);
	// beq cr6,0x822f784c
	if (cr6.eq) goto loc_822F784C;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F784C:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x822f7824
	if (!cr6.eq) goto loc_822F7824;
loc_822F785C:
	// stw r30,8(r28)
	PPC_STORE_U32(r28.u32 + 8, r30.u32);
loc_822F7860:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F7870"))) PPC_WEAK_FUNC(sub_822F7870);
PPC_FUNC_IMPL(__imp__sub_822F7870) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r27,0(r6)
	r27.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// std r4,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r4.u64);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// beq cr6,0x822f78a8
	if (cr6.eq) goto loc_822F78A8;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F78A8:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f78bc
	if (!cr6.eq) goto loc_822F78BC;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f78c8
	goto loc_822F78C8;
loc_822F78BC:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822F78C8:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f78d8
	if (!cr6.eq) goto loc_822F78D8;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f78e4
	goto loc_822F78E4;
loc_822F78D8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r11.s64 = ctx.r8.s32 >> 2;
loc_822F78E4:
	// lis r8,16383
	ctx.r8.s64 = 1073676288;
	// ori r8,r8,65535
	ctx.r8.u64 = ctx.r8.u64 | 65535;
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - r11.s64;
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bge cr6,0x822f7900
	if (!cr6.lt) goto loc_822F7900;
	// bl 0x82684b38
	sub_82684B38(ctx, base);
	// b 0x822f7b4c
	goto loc_822F7B4C;
loc_822F7900:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7910
	if (!cr6.eq) goto loc_822F7910;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f791c
	goto loc_822F791C;
loc_822F7910:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r7,r10,r11
	ctx.r7.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	r11.s64 = ctx.r7.s32 >> 2;
loc_822F791C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x822f7a38
	if (!cr6.lt) goto loc_822F7A38;
	// rlwinm r11,r9,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// li r28,0
	r28.s64 = 0;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f7940
	if (cr6.lt) goto loc_822F7940;
	// add r28,r11,r9
	r28.u64 = r11.u64 + ctx.r9.u64;
loc_822F7940:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7950
	if (!cr6.eq) goto loc_822F7950;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f795c
	goto loc_822F795C;
loc_822F7950:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
loc_822F795C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bge cr6,0x822f7988
	if (!cr6.lt) goto loc_822F7988;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7978
	if (!cr6.eq) goto loc_822F7978;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f7984
	goto loc_822F7984;
loc_822F7978:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
loc_822F7984:
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
loc_822F7988:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82333f78
	sub_82333F78(ctx, base);
	// lwz r29,172(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f4e30
	sub_825F4E30(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// bne cr6,0x822f79ec
	if (!cr6.eq) goto loc_822F79EC;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f79f8
	goto loc_822F79F8;
loc_822F79EC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r4,r11
	ctx.r10.s64 = r11.s64 - ctx.r4.s64;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
loc_822F79F8:
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// beq cr6,0x822f7a18
	if (cr6.eq) goto loc_822F7A18;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F7A18:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// b 0x822f7b4c
	goto loc_822F7B4C;
loc_822F7A38:
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r30,172(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// subf r11,r30,r29
	r11.s64 = r29.s64 - r30.s64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bge cr6,0x822f7ab4
	if (!cr6.lt) goto loc_822F7AB4;
	// addi r6,r30,4
	ctx.r6.s64 = r30.s64 + 4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// subf r11,r30,r4
	r11.s64 = ctx.r4.s64 - r30.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// subfic r5,r10,1
	xer.ca = ctx.r10.u32 <= 1;
	ctx.r5.s64 = 1 - ctx.r10.s64;
	// bl 0x825f4e30
	sub_825F4E30(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r29,r11,-4
	r29.s64 = r11.s64 + -4;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// beq cr6,0x822f7b4c
	if (cr6.eq) goto loc_822F7B4C;
loc_822F7A98:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// bne cr6,0x822f7a98
	if (!cr6.eq) goto loc_822F7A98;
	// b 0x822f7b4c
	goto loc_822F7B4C;
loc_822F7AB4:
	// addi r28,r29,-4
	r28.s64 = r29.s64 + -4;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r3.u32);
	// mr r31,r28
	r31.u64 = r28.u64;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// beq cr6,0x822f7b24
	if (cr6.eq) goto loc_822F7B24;
	// subf r29,r28,r29
	r29.s64 = r29.s64 - r28.s64;
loc_822F7AD8:
	// addi r31,r31,-4
	r31.s64 = r31.s64 + -4;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f7af8
	if (cr6.eq) goto loc_822F7AF8;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F7AF8:
	// lwzx r3,r29,r31
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + r31.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f7b14
	if (cr6.eq) goto loc_822F7B14;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F7B14:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// stwx r11,r29,r31
	PPC_STORE_U32(r29.u32 + r31.u32, r11.u32);
	// bne cr6,0x822f7ad8
	if (!cr6.eq) goto loc_822F7AD8;
loc_822F7B24:
	// addi r29,r30,4
	r29.s64 = r30.s64 + 4;
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// beq cr6,0x822f7b4c
	if (cr6.eq) goto loc_822F7B4C;
loc_822F7B34:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x822f7b34
	if (!cr6.eq) goto loc_822F7B34;
loc_822F7B4C:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822f7b68
	if (cr6.eq) goto loc_822F7B68;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F7B68:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F7B70"))) PPC_WEAK_FUNC(sub_822F7B70);
PPC_FUNC_IMPL(__imp__sub_822F7B70) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r27,0(r6)
	r27.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// std r4,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, ctx.r4.u64);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// beq cr6,0x822f7ba8
	if (cr6.eq) goto loc_822F7BA8;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F7BA8:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7bbc
	if (!cr6.eq) goto loc_822F7BBC;
	// li r9,0
	ctx.r9.s64 = 0;
	// b 0x822f7bc8
	goto loc_822F7BC8;
loc_822F7BBC:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// srawi r9,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
loc_822F7BC8:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7bd8
	if (!cr6.eq) goto loc_822F7BD8;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f7be4
	goto loc_822F7BE4;
loc_822F7BD8:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r8,r10,r11
	ctx.r8.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r11.s64 = ctx.r8.s32 >> 2;
loc_822F7BE4:
	// lis r8,16383
	ctx.r8.s64 = 1073676288;
	// ori r8,r8,65535
	ctx.r8.u64 = ctx.r8.u64 | 65535;
	// subf r7,r11,r8
	ctx.r7.s64 = ctx.r8.s64 - r11.s64;
	// cmplwi cr6,r7,1
	cr6.compare<uint32_t>(ctx.r7.u32, 1, xer);
	// bge cr6,0x822f7c00
	if (!cr6.lt) goto loc_822F7C00;
	// bl 0x82684b38
	sub_82684B38(ctx, base);
	// b 0x822f7e00
	goto loc_822F7E00;
loc_822F7C00:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7c10
	if (!cr6.eq) goto loc_822F7C10;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f7c1c
	goto loc_822F7C1C;
loc_822F7C10:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r7,r10,r11
	ctx.r7.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r7,2
	xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	r11.s64 = ctx.r7.s32 >> 2;
loc_822F7C1C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bge cr6,0x822f7d38
	if (!cr6.lt) goto loc_822F7D38;
	// rlwinm r11,r9,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// li r28,0
	r28.s64 = 0;
	// subf r8,r11,r8
	ctx.r8.s64 = ctx.r8.s64 - r11.s64;
	// cmplw cr6,r8,r9
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r9.u32, xer);
	// blt cr6,0x822f7c40
	if (cr6.lt) goto loc_822F7C40;
	// add r28,r11,r9
	r28.u64 = r11.u64 + ctx.r9.u64;
loc_822F7C40:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7c50
	if (!cr6.eq) goto loc_822F7C50;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f7c5c
	goto loc_822F7C5C;
loc_822F7C50:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r9,2
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	r11.s64 = ctx.r9.s32 >> 2;
loc_822F7C5C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bge cr6,0x822f7c88
	if (!cr6.lt) goto loc_822F7C88;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f7c78
	if (!cr6.eq) goto loc_822F7C78;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f7c84
	goto loc_822F7C84;
loc_822F7C78:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r10,r11
	ctx.r10.s64 = r11.s64 - ctx.r10.s64;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
loc_822F7C84:
	// addi r28,r11,1
	r28.s64 = r11.s64 + 1;
loc_822F7C88:
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82333f78
	sub_82333F78(ctx, base);
	// lwz r29,172(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r4,4(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f4e30
	sub_825F4E30(ctx, base);
	// mr r6,r3
	ctx.r6.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822f7cec
	if (!cr6.eq) goto loc_822F7CEC;
	// li r11,0
	r11.s64 = 0;
	// b 0x822f7cf8
	goto loc_822F7CF8;
loc_822F7CEC:
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r3,r11
	ctx.r10.s64 = r11.s64 - ctx.r3.s64;
	// srawi r11,r10,2
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	r11.s64 = ctx.r10.s32 >> 2;
loc_822F7CF8:
	// addi r29,r11,1
	r29.s64 = r11.s64 + 1;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f7d18
	if (cr6.eq) goto loc_822F7D18;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822f84d8
	sub_822F84D8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F7D18:
	// rlwinm r11,r28,2,0,29
	r11.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r29.u32 | (r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + r30.u64;
	// stw r11,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r11.u32);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// b 0x822f7e00
	goto loc_822F7E00;
loc_822F7D38:
	// lwz r29,8(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r30,172(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// subf r11,r30,r29
	r11.s64 = r29.s64 - r30.s64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// cmplwi cr6,r10,1
	cr6.compare<uint32_t>(ctx.r10.u32, 1, xer);
	// bge cr6,0x822f7db4
	if (!cr6.lt) goto loc_822F7DB4;
	// addi r6,r30,4
	ctx.r6.s64 = r30.s64 + 4;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// subf r11,r30,r4
	r11.s64 = ctx.r4.s64 - r30.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// subfic r5,r10,1
	xer.ca = ctx.r10.u32 <= 1;
	ctx.r5.s64 = 1 - ctx.r10.s64;
	// bl 0x825f4e30
	sub_825F4E30(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// addi r29,r11,-4
	r29.s64 = r11.s64 + -4;
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// beq cr6,0x822f7e00
	if (cr6.eq) goto loc_822F7E00;
loc_822F7D98:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r30,r30,4
	r30.s64 = r30.s64 + 4;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// bne cr6,0x822f7d98
	if (!cr6.eq) goto loc_822F7D98;
	// b 0x822f7e00
	goto loc_822F7E00;
loc_822F7DB4:
	// addi r28,r29,-4
	r28.s64 = r29.s64 + -4;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x825f51d0
	sub_825F51D0(ctx, base);
	// stw r3,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r3.u32);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f8540
	sub_822F8540(ctx, base);
	// addi r29,r30,4
	r29.s64 = r30.s64 + 4;
	// mr r31,r30
	r31.u64 = r30.u64;
	// cmplw cr6,r30,r29
	cr6.compare<uint32_t>(r30.u32, r29.u32, xer);
	// beq cr6,0x822f7e00
	if (cr6.eq) goto loc_822F7E00;
loc_822F7DE8:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8260c130
	sub_8260C130(ctx, base);
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// bne cr6,0x822f7de8
	if (!cr6.eq) goto loc_822F7DE8;
loc_822F7E00:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822f7e1c
	if (cr6.eq) goto loc_822F7E1C;
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F7E1C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F7E28"))) PPC_WEAK_FUNC(sub_822F7E28);
PPC_FUNC_IMPL(__imp__sub_822F7E28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r6
	r30.u64 = ctx.r6.u64;
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// cmplw cr6,r11,r27
	cr6.compare<uint32_t>(r11.u32, r27.u32, xer);
	// bge cr6,0x822f7e54
	if (!cr6.lt) goto loc_822F7E54;
	// bl 0x82cd12c8
	sub_82CD12C8(ctx, base);
loc_822F7E54:
	// lwz r11,20(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// subf r11,r27,r11
	r11.s64 = r11.s64 - r27.s64;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// bge cr6,0x822f7e68
	if (!cr6.lt) goto loc_822F7E68;
	// mr r30,r11
	r30.u64 = r11.u64;
loc_822F7E68:
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// subfic r10,r11,-1
	xer.ca = r11.u32 <= 4294967295;
	ctx.r10.s64 = -1 - r11.s64;
	// cmplw cr6,r10,r30
	cr6.compare<uint32_t>(ctx.r10.u32, r30.u32, xer);
	// ble cr6,0x822f7e84
	if (!cr6.gt) goto loc_822F7E84;
	// add r10,r11,r30
	ctx.r10.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822f7e88
	if (!cr6.lt) goto loc_822F7E88;
loc_822F7E84:
	// bl 0x82cd11d0
	sub_82CD11D0(ctx, base);
loc_822F7E88:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822f7f34
	if (cr6.eq) goto loc_822F7F34;
	// lwz r11,20(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// add r28,r11,r30
	r28.u64 = r11.u64 + r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8217a5e0
	sub_8217A5E0(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f7f34
	if (cr6.eq) goto loc_822F7F34;
	// lwz r11,24(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f7ec8
	if (cr6.lt) goto loc_822F7EC8;
	// lwz r8,4(r29)
	ctx.r8.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// b 0x822f7ecc
	goto loc_822F7ECC;
loc_822F7EC8:
	// addi r8,r29,4
	ctx.r8.s64 = r29.s64 + 4;
loc_822F7ECC:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r29,r31,4
	r29.s64 = r31.s64 + 4;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f7ee4
	if (cr6.lt) goto loc_822F7EE4;
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// b 0x822f7ee8
	goto loc_822F7EE8;
loc_822F7EE4:
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
loc_822F7EE8:
	// lwz r7,20(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 20);
	// rlwinm r10,r27,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r6,r30,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r4,r7,r11
	ctx.r4.s64 = r11.s64 - ctx.r7.s64;
	// rlwinm r11,r7,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r4,r4,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r3,r11,r9
	ctx.r3.u64 = r11.u64 + ctx.r9.u64;
	// bl 0x82ca3730
	sub_82CA3730(ctx, base);
	// lwz r3,24(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stw r28,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r28.u32);
	// cmplwi cr6,r3,8
	cr6.compare<uint32_t>(ctx.r3.u32, 8, xer);
	// blt cr6,0x822f7f24
	if (cr6.lt) goto loc_822F7F24;
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// b 0x822f7f28
	goto loc_822F7F28;
loc_822F7F24:
	// mr r11,r29
	r11.u64 = r29.u64;
loc_822F7F28:
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(r28.u32 | (r28.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,0
	ctx.r9.s64 = 0;
	// sthx r9,r10,r11
	PPC_STORE_U16(ctx.r10.u32 + r11.u32, ctx.r9.u16);
loc_822F7F34:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F7F40"))) PPC_WEAK_FUNC(sub_822F7F40);
PPC_FUNC_IMPL(__imp__sub_822F7F40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r5,256(r1)
	PPC_STORE_U64(ctx.r1.u32 + 256, ctx.r5.u64);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r31,260(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lbz r11,45(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f7fac
	if (cr6.eq) goto loc_822F7FAC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5600
	ctx.r4.s64 = r11.s64 + 5600;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822F7FAC:
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// mr r26,r31
	r26.u64 = r31.u64;
	// bl 0x82c705d8
	sub_82C705D8(ctx, base);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lbz r11,45(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f7fd0
	if (cr6.eq) goto loc_822F7FD0;
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// b 0x822f7ff8
	goto loc_822F7FF8;
loc_822F7FD0:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lbz r9,45(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f7fe8
	if (cr6.eq) goto loc_822F7FE8;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// b 0x822f7ff8
	goto loc_822F7FF8;
loc_822F7FE8:
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// lwz r28,8(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bne cr6,0x822f80e4
	if (!cr6.eq) goto loc_822F80E4;
loc_822F7FF8:
	// lbz r11,45(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 45);
	// lwz r31,4(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f800c
	if (!cr6.eq) goto loc_822F800C;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822F800C:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x822f8024
	if (!cr6.eq) goto loc_822F8024;
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// b 0x822f803c
	goto loc_822F803C;
loc_822F8024:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f8038
	if (!cr6.eq) goto loc_822F8038;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// b 0x822f803c
	goto loc_822F803C;
loc_822F8038:
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
loc_822F803C:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f808c
	if (!cr6.eq) goto loc_822F808C;
	// lbz r11,45(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8060
	if (cr6.eq) goto loc_822F8060;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// b 0x822f8088
	goto loc_822F8088;
loc_822F8060:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,45(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f8088
	if (!cr6.eq) goto loc_822F8088;
loc_822F8074:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r8,45(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f8074
	if (cr6.eq) goto loc_822F8074;
loc_822F8088:
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
loc_822F808C:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822f8178
	if (!cr6.eq) goto loc_822F8178;
	// lbz r11,45(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f80b4
	if (cr6.eq) goto loc_822F80B4;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822f8178
	goto loc_822F8178;
loc_822F80B4:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,45(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f80dc
	if (!cr6.eq) goto loc_822F80DC;
loc_822F80C8:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r8,45(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822f80c8
	if (cr6.eq) goto loc_822F80C8;
loc_822F80DC:
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822f8178
	goto loc_822F8178;
loc_822F80E4:
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f8104
	if (!cr6.eq) goto loc_822F8104;
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x822f812c
	goto loc_822F812C;
loc_822F8104:
	// lbz r10,45(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 45);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f8118
	if (!cr6.eq) goto loc_822F8118;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822F8118:
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
loc_822F812C:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822f8144
	if (!cr6.eq) goto loc_822F8144;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// b 0x822f8160
	goto loc_822F8160;
loc_822F8144:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822f815c
	if (!cr6.eq) goto loc_822F815C;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x822f8160
	goto loc_822F8160;
loc_822F815C:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
loc_822F8160:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lbz r8,44(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// lbz r9,44(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 44);
	// stb r9,44(r11)
	PPC_STORE_U8(r11.u32 + 44, ctx.r9.u8);
	// stb r8,44(r26)
	PPC_STORE_U8(r26.u32 + 44, ctx.r8.u8);
loc_822F8178:
	// lbz r11,44(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 44);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822f8318
	if (!cr6.eq) goto loc_822F8318;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// li r29,1
	r29.s64 = 1;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f8314
	if (cr6.eq) goto loc_822F8314;
loc_822F819C:
	// lbz r11,44(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 44);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822f8314
	if (!cr6.eq) goto loc_822F8314;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822f8258
	if (!cr6.eq) goto loc_822F8258;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f81dc
	if (!cr6.eq) goto loc_822F81DC;
	// stb r29,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82c70680
	sub_82C70680(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822F81DC:
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f82ac
	if (!cr6.eq) goto loc_822F82AC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f8208
	if (!cr6.eq) goto loc_822F8208;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x822f82a8
	if (cr6.eq) goto loc_822F82A8;
loc_822F8208:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f8234
	if (!cr6.eq) goto loc_822F8234;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r29.u8);
	// stb r30,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r30.u8);
	// bl 0x82c70560
	sub_82C70560(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822F8234:
	// lbz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 44);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,44(r11)
	PPC_STORE_U8(r11.u32 + 44, ctx.r10.u8);
	// stb r29,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r29.u8);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stb r29,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r29.u8);
	// bl 0x82c70680
	sub_82C70680(ctx, base);
	// b 0x822f8314
	goto loc_822F8314;
loc_822F8258:
	// lbz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f827c
	if (!cr6.eq) goto loc_822F827C;
	// stb r29,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82c70560
	sub_82C70560(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F827C:
	// lbz r10,45(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 45);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f82ac
	if (!cr6.eq) goto loc_822F82AC;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f82c8
	if (!cr6.eq) goto loc_822F82C8;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f82c8
	if (!cr6.eq) goto loc_822F82C8;
loc_822F82A8:
	// stb r30,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r30.u8);
loc_822F82AC:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// mr r28,r31
	r28.u64 = r31.u64;
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f819c
	if (!cr6.eq) goto loc_822F819C;
	// b 0x822f8314
	goto loc_822F8314;
loc_822F82C8:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,44(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 44);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822f82f4
	if (!cr6.eq) goto loc_822F82F4;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,44(r10)
	PPC_STORE_U8(ctx.r10.u32 + 44, r29.u8);
	// stb r30,44(r11)
	PPC_STORE_U8(r11.u32 + 44, r30.u8);
	// bl 0x82c70680
	sub_82C70680(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822F82F4:
	// lbz r10,44(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 44);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,44(r11)
	PPC_STORE_U8(r11.u32 + 44, ctx.r10.u8);
	// stb r29,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r29.u8);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r29,44(r9)
	PPC_STORE_U8(ctx.r9.u32 + 44, r29.u8);
	// bl 0x82c70560
	sub_82C70560(ctx, base);
loc_822F8314:
	// stb r29,44(r28)
	PPC_STORE_U8(r28.u32 + 44, r29.u8);
loc_822F8318:
	// lwz r11,40(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 40);
	// addi r31,r26,16
	r31.s64 = r26.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// blt cr6,0x822f8330
	if (cr6.lt) goto loc_822F8330;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F8330:
	// li r11,7
	r11.s64 = 7;
	// stw r30,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r30.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// sth r30,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r30.u16);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8370
	if (cr6.eq) goto loc_822F8370;
	// ld r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// stw r9,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r9.u32);
	// std r10,0(r25)
	PPC_STORE_U64(r25.u32 + 0, ctx.r10.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
loc_822F8370:
	// ld r11,256(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// std r11,0(r25)
	PPC_STORE_U64(r25.u32 + 0, r11.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822F8380"))) PPC_WEAK_FUNC(sub_822F8380);
PPC_FUNC_IMPL(__imp__sub_822F8380) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// lbz r11,45(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 45);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f83f8
	if (!cr6.eq) goto loc_822F83F8;
	// li r27,7
	r27.s64 = 7;
	// li r28,0
	r28.s64 = 0;
loc_822F83AC:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// lwz r4,8(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// bl 0x822f8380
	sub_822F8380(ctx, base);
	// lwz r11,40(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 40);
	// addi r31,r29,16
	r31.s64 = r29.s64 + 16;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// lwz r30,0(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// blt cr6,0x822f83d4
	if (cr6.lt) goto loc_822F83D4;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F83D4:
	// stw r27,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r27.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r28,20(r31)
	PPC_STORE_U32(r31.u32 + 20, r28.u32);
	// sth r28,4(r31)
	PPC_STORE_U16(r31.u32 + 4, r28.u16);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lbz r11,45(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 45);
	// mr r29,r30
	r29.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f83ac
	if (cr6.eq) goto loc_822F83AC;
loc_822F83F8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822F8400"))) PPC_WEAK_FUNC(sub_822F8400);
PPC_FUNC_IMPL(__imp__sub_822F8400) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r30,r29
	r30.u64 = r29.u64;
	// lbz r11,33(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f8480
	if (!cr6.eq) goto loc_822F8480;
	// li r28,0
	r28.s64 = 0;
loc_822F8428:
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r4,8(r30)
	ctx.r4.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// bl 0x822f8400
	sub_822F8400(ctx, base);
	// lwz r4,20(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 20);
	// addi r31,r29,16
	r31.s64 = r29.s64 + 16;
	// cmplwi cr6,r4,0
	cr6.compare<uint32_t>(ctx.r4.u32, 0, xer);
	// lwz r30,0(r30)
	r30.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// beq cr6,0x822f845c
	if (cr6.eq) goto loc_822F845C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r5,8(r31)
	ctx.r5.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822f6b68
	sub_822F6B68(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F845C:
	// stw r28,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r28.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
	// stw r28,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r28.u32);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lbz r11,33(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 33);
	// mr r29,r30
	r29.u64 = r30.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8428
	if (cr6.eq) goto loc_822F8428;
loc_822F8480:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F8488"))) PPC_WEAK_FUNC(sub_822F8488);
PPC_FUNC_IMPL(__imp__sub_822F8488) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r3,r5
	ctx.r3.u64 = ctx.r5.u64;
	// subf r11,r10,r4
	r11.s64 = ctx.r4.s64 - ctx.r10.s64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r6,r3
	r31.u64 = ctx.r6.u64 + ctx.r3.u64;
	// ble 0x822f84c0
	if (!cr0.gt) goto loc_822F84C0;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_822F84C0:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F84D8"))) PPC_WEAK_FUNC(sub_822F84D8);
PPC_FUNC_IMPL(__imp__sub_822F84D8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplw cr6,r3,r30
	cr6.compare<uint32_t>(ctx.r3.u32, r30.u32, xer);
	// beq cr6,0x822f8524
	if (cr6.eq) goto loc_822F8524;
loc_822F84FC:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f8518
	if (cr6.eq) goto loc_822F8518;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F8518:
	// addi r31,r31,4
	r31.s64 = r31.s64 + 4;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822f84fc
	if (!cr6.eq) goto loc_822F84FC;
loc_822F8524:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8540"))) PPC_WEAK_FUNC(sub_822F8540);
PPC_FUNC_IMPL(__imp__sub_822F8540) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// subf r11,r29,r4
	r11.s64 = ctx.r4.s64 - r29.s64;
	// cmplw cr6,r29,r4
	cr6.compare<uint32_t>(r29.u32, ctx.r4.u32, xer);
	// srawi r10,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	ctx.r10.s64 = r11.s32 >> 2;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r28,r9,r5
	r28.s64 = ctx.r5.s64 - ctx.r9.s64;
	// beq cr6,0x822f85bc
	if (cr6.eq) goto loc_822F85BC;
	// subf r30,r4,r5
	r30.s64 = ctx.r5.s64 - ctx.r4.s64;
loc_822F8570:
	// addi r31,r31,-4
	r31.s64 = r31.s64 + -4;
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f8590
	if (cr6.eq) goto loc_822F8590;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F8590:
	// lwzx r3,r30,r31
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + r31.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f85ac
	if (cr6.eq) goto loc_822F85AC;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F85AC:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// stwx r11,r30,r31
	PPC_STORE_U32(r30.u32 + r31.u32, r11.u32);
	// bne cr6,0x822f8570
	if (!cr6.eq) goto loc_822F8570;
loc_822F85BC:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F85C8"))) PPC_WEAK_FUNC(sub_822F85C8);
PPC_FUNC_IMPL(__imp__sub_822F85C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822f85f8
	if (cr6.eq) goto loc_822F85F8;
loc_822F85D8:
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r8
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, xer);
	// bne cr6,0x822f8600
	if (!cr6.eq) goto loc_822F8600;
	// addic. r5,r5,-1
	xer.ca = ctx.r5.u32 > 0;
	ctx.r5.s64 = ctx.r5.s64 + -1;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// bne 0x822f85d8
	if (!cr0.eq) goto loc_822F85D8;
loc_822F85F8:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_822F8600:
	// lhz r11,0(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + 0);
	// lhz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// subfc r9,r10,r11
	xer.ca = r11.u32 >= ctx.r10.u32;
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// subfe r8,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r8.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// rlwinm r11,r8,0,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r3,r11,1
	ctx.r3.s64 = r11.s64 + 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8620"))) PPC_WEAK_FUNC(sub_822F8620);
PPC_FUNC_IMPL(__imp__sub_822F8620) {
	PPC_FUNC_PROLOGUE();
	// b 0x8217ab30
	sub_8217AB30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822F8628"))) PPC_WEAK_FUNC(sub_822F8628);
PPC_FUNC_IMPL(__imp__sub_822F8628) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f8640
	if (!cr6.eq) goto loc_822F8640;
	// lis r11,-32247
	r11.s64 = -2113339392;
	// addi r3,r11,63
	ctx.r3.s64 = r11.s64 + 63;
	// blr 
	return;
loc_822F8640:
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8648"))) PPC_WEAK_FUNC(sub_822F8648);
PPC_FUNC_IMPL(__imp__sub_822F8648) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// rlwinm r11,r4,1,0,30
	r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r4,r11
	r11.u64 = ctx.r4.u64 + r11.u64;
	// rlwinm r11,r11,2,0,29
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r3
	r11.u64 = r11.u64 + ctx.r3.u64;
	// addi r8,r11,296
	ctx.r8.s64 = r11.s64 + 296;
	// lwz r10,300(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 300);
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lbz r9,25(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f86b0
	if (!cr6.eq) goto loc_822F86B0;
loc_822F8674:
	// ld r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// cmpld cr6,r9,r5
	cr6.compare<uint64_t>(ctx.r9.u64, ctx.r5.u64, xer);
	// li r9,1
	ctx.r9.s64 = 1;
	// blt cr6,0x822f8688
	if (cr6.lt) goto loc_822F8688;
	// li r9,0
	ctx.r9.s64 = 0;
loc_822F8688:
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f869c
	if (cr6.eq) goto loc_822F869C;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// b 0x822f86a4
	goto loc_822F86A4;
loc_822F869C:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
loc_822F86A4:
	// lbz r9,25(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 25);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822f8674
	if (cr6.eq) goto loc_822F8674;
loc_822F86B0:
	// lwz r9,4(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r10,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r10.u32);
	// stw r8,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r8.u32);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f86ec
	if (cr6.eq) goto loc_822F86EC;
	// ld r11,16(r10)
	r11.u64 = PPC_LOAD_U64(ctx.r10.u32 + 16);
	// cmpld cr6,r5,r11
	cr6.compare<uint64_t>(ctx.r5.u64, r11.u64, xer);
	// li r11,1
	r11.s64 = 1;
	// blt cr6,0x822f86d8
	if (cr6.lt) goto loc_822F86D8;
	// li r11,0
	r11.s64 = 0;
loc_822F86D8:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f86ec
	if (!cr6.eq) goto loc_822F86EC;
	// addi r11,r1,-16
	r11.s64 = ctx.r1.s64 + -16;
	// b 0x822f86f8
	goto loc_822F86F8;
loc_822F86EC:
	// stw r9,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, ctx.r9.u32);
	// addi r11,r1,-8
	r11.s64 = ctx.r1.s64 + -8;
	// stw r8,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r8.u32);
loc_822F86F8:
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r11.u64);
	// lwz r11,-8(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8714
	if (cr6.eq) goto loc_822F8714;
	// cmplw cr6,r11,r8
	cr6.compare<uint32_t>(r11.u32, ctx.r8.u32, xer);
	// beq cr6,0x822f8718
	if (cr6.eq) goto loc_822F8718;
loc_822F8714:
	// twi 31,r0,22
loc_822F8718:
	// lwz r11,-4(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + -4);
	// subf r10,r11,r7
	ctx.r10.s64 = ctx.r7.s64 - r11.s64;
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r3,r8,1
	ctx.r3.u64 = ctx.r8.u64 ^ 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8730"))) PPC_WEAK_FUNC(sub_822F8730);
PPC_FUNC_IMPL(__imp__sub_822F8730) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// clrlwi r10,r4,31
	ctx.r10.u64 = ctx.r4.u32 & 0x1;
	// addi r9,r11,11204
	ctx.r9.s64 = r11.s64 + 11204;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r9,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r9.u32);
	// beq cr6,0x822f8764
	if (cr6.eq) goto loc_822F8764;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
loc_822F8764:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8778"))) PPC_WEAK_FUNC(sub_822F8778);
PPC_FUNC_IMPL(__imp__sub_822F8778) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r9,-32245
	ctx.r9.s64 = -2113208320;
	// li r30,0
	r30.s64 = 0;
	// li r10,-1
	ctx.r10.s64 = -1;
	// li r11,-1
	r11.s64 = -1;
	// li r6,-1
	ctx.r6.s64 = -1;
	// stw r30,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r30.u32);
	// addi r5,r9,-31740
	ctx.r5.s64 = ctx.r9.s64 + -31740;
	// stb r30,104(r31)
	PPC_STORE_U8(r31.u32 + 104, r30.u8);
	// lis r4,-31927
	ctx.r4.s64 = -2092367872;
	// stw r10,108(r31)
	PPC_STORE_U32(r31.u32 + 108, ctx.r10.u32);
	// stw r10,112(r31)
	PPC_STORE_U32(r31.u32 + 112, ctx.r10.u32);
	// addi r29,r31,160
	r29.s64 = r31.s64 + 160;
	// stw r10,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r10.u32);
	// addi r3,r4,28344
	ctx.r3.s64 = ctx.r4.s64 + 28344;
	// stb r30,144(r31)
	PPC_STORE_U8(r31.u32 + 144, r30.u8);
	// stw r11,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r11.u32);
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// sth r6,156(r31)
	PPC_STORE_U16(r31.u32 + 156, ctx.r6.u16);
	// stw r5,160(r31)
	PPC_STORE_U32(r31.u32 + 160, ctx.r5.u32);
	// stw r30,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r30.u32);
loc_822F87D8:
	// mfmsr r7
	// mtmsrd r13,1
	// lwarx r8,0,r3
	reserved.u32 = *(uint32_t*)(base + ctx.r3.u32);
	ctx.r8.u64 = __builtin_bswap32(reserved.u32);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// stwcx. r8,0,r3
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r3.u32), reserved.s32, __builtin_bswap32(ctx.r8.s32));
	cr0.so = xer.so;
	// mtmsrd r7,1
	// bne 0x822f87d8
	if (!cr0.eq) goto loc_822F87D8;
	// li r5,496
	ctx.r5.s64 = 496;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r29,8
	ctx.r3.s64 = r29.s64 + 8;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// li r5,72
	ctx.r5.s64 = 72;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r29,504
	ctx.r3.s64 = r29.s64 + 504;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r11,-32256
	r11.s64 = -2113929216;
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r30,45(r31)
	PPC_STORE_U8(r31.u32 + 45, r30.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r10,748(r31)
	PPC_STORE_U32(r31.u32 + 748, ctx.r10.u32);
	// lfd f0,3376(r11)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(r11.u32 + 3376);
	// stfd f0,736(r31)
	PPC_STORE_U64(r31.u32 + 736, f0.u64);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F8838"))) PPC_WEAK_FUNC(sub_822F8838);
PPC_FUNC_IMPL(__imp__sub_822F8838) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r29,r6
	r29.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// mr r27,r8
	r27.u64 = ctx.r8.u64;
	// bl 0x82cf9ad0
	sub_82CF9AD0(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x822f8884
	if (cr6.eq) goto loc_822F8884;
	// li r4,25
	ctx.r4.s64 = 25;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
loc_822F8884:
	// addi r3,r31,16
	ctx.r3.s64 = r31.s64 + 16;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// li r5,36
	ctx.r5.s64 = 36;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r30,332
	ctx.r4.s64 = r30.s64 + 332;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822fe898
	sub_822FE898(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// std r29,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r29.u64);
	// ld r11,540(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 540);
	// std r11,8(r31)
	PPC_STORE_U64(r31.u32 + 8, r11.u64);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r3,52(r31)
	PPC_STORE_U32(r31.u32 + 52, ctx.r3.u32);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// stb r10,56(r31)
	PPC_STORE_U8(r31.u32 + 56, ctx.r10.u8);
	// li r4,16
	ctx.r4.s64 = 16;
	// stb r27,57(r31)
	PPC_STORE_U8(r31.u32 + 57, r27.u8);
	// addi r3,r31,58
	ctx.r3.s64 = r31.s64 + 58;
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822F88E8"))) PPC_WEAK_FUNC(sub_822F88E8);
PPC_FUNC_IMPL(__imp__sub_822F88E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister temp{};
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// mr r11,r4
	r11.u64 = ctx.r4.u64;
	// addi r8,r4,4
	ctx.r8.s64 = ctx.r4.s64 + 4;
loc_822F88F4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822f8914
	if (!cr0.eq) goto loc_822F8914;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822f88f4
	if (!cr6.eq) goto loc_822F88F4;
loc_822F8914:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x822f892c
	if (cr6.eq) goto loc_822F892C;
loc_822F891C:
	// cntlzw r11,r9
	r11.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r3,r10,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// blr 
	return;
loc_822F892C:
	// addi r11,r4,4
	r11.s64 = ctx.r4.s64 + 4;
	// addi r10,r5,4
	ctx.r10.s64 = ctx.r5.s64 + 4;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822F8938:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822f8958
	if (!cr0.eq) goto loc_822F8958;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822f8938
	if (!cr6.eq) goto loc_822F8938;
loc_822F8958:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822f891c
	if (!cr6.eq) goto loc_822F891C;
	// addi r11,r4,10
	r11.s64 = ctx.r4.s64 + 10;
	// addi r10,r5,10
	ctx.r10.s64 = ctx.r5.s64 + 10;
	// addi r8,r11,6
	ctx.r8.s64 = r11.s64 + 6;
loc_822F896C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822f898c
	if (!cr0.eq) goto loc_822F898C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822f896c
	if (!cr6.eq) goto loc_822F896C;
loc_822F898C:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822f891c
	if (!cr6.eq) goto loc_822F891C;
	// addi r11,r4,16
	r11.s64 = ctx.r4.s64 + 16;
	// addi r10,r5,16
	ctx.r10.s64 = ctx.r5.s64 + 16;
	// addi r8,r11,20
	ctx.r8.s64 = r11.s64 + 20;
loc_822F89A0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822f89c0
	if (!cr0.eq) goto loc_822F89C0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822f89a0
	if (!cr6.eq) goto loc_822F89A0;
loc_822F89C0:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822f891c
	if (!cr6.eq) goto loc_822F891C;
	// lhz r11,8(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 8);
	// lhz r10,8(r5)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r5.u32 + 8);
	// subfc r9,r10,r11
	xer.ca = r11.u32 >= ctx.r10.u32;
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// subfe r8,r9,r9
	temp.u8 = (~ctx.r9.u32 + ctx.r9.u32 < ~ctx.r9.u32) | (~ctx.r9.u32 + ctx.r9.u32 + xer.ca < xer.ca);
	ctx.r8.u64 = ~ctx.r9.u64 + ctx.r9.u64 + xer.ca;
	xer.ca = temp.u8;
	// clrlwi r3,r8,31
	ctx.r3.u64 = ctx.r8.u32 & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F89E0"))) PPC_WEAK_FUNC(sub_822F89E0);
PPC_FUNC_IMPL(__imp__sub_822F89E0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// addi r3,r30,168
	ctx.r3.s64 = r30.s64 + 168;
	// addi r4,r31,9
	ctx.r4.s64 = r31.s64 + 9;
	// li r5,496
	ctx.r5.s64 = 496;
	// bl 0x82ca30e8
	sub_82CA30E8(ctx, base);
	// addi r10,r30,664
	ctx.r10.s64 = r30.s64 + 664;
	// addi r11,r31,505
	r11.s64 = r31.s64 + 505;
	// li r9,18
	ctx.r9.s64 = 18;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822F8A1C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x822f8a1c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822F8A1C;
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r30,128
	ctx.r4.s64 = r30.s64 + 128;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8222cf18
	sub_8222CF18(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r30,164
	ctx.r3.s64 = r30.s64 + 164;
	// bl 0x82265160
	sub_82265160(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x821c67d8
	sub_821C67D8(ctx, base);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r8,r11,28344
	ctx.r8.s64 = r11.s64 + 28344;
loc_822F8A5C:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822f8a5c
	if (!cr0.eq) goto loc_822F8A5C;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8221eb58
	sub_8221EB58(ctx, base);
	// lfd f12,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// lis r6,-31927
	ctx.r6.s64 = -2092367872;
	// lfd f0,28352(r7)
	f0.u64 = PPC_LOAD_U64(ctx.r7.u32 + 28352);
	// lfd f13,28360(r6)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r6.u32 + 28360);
	// fsub f10,f11,f0
	ctx.f10.f64 = ctx.f11.f64 - f0.f64;
	// fdiv f9,f10,f13
	ctx.f9.f64 = ctx.f10.f64 / ctx.f13.f64;
	// stfd f9,736(r30)
	PPC_STORE_U64(r30.u32 + 736, ctx.f9.u64);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8AC0"))) PPC_WEAK_FUNC(sub_822F8AC0);
PPC_FUNC_IMPL(__imp__sub_822F8AC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// std r6,224(r1)
	PPC_STORE_U64(ctx.r1.u32 + 224, ctx.r6.u64);
	// li r30,0
	r30.s64 = 0;
	// stw r4,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r4.u32);
	// li r11,-1
	r11.s64 = -1;
	// stw r31,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r31.u32);
	// li r3,52
	ctx.r3.s64 = 52;
	// stw r5,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r5.u32);
	// std r7,232(r1)
	PPC_STORE_U64(ctx.r1.u32 + 232, ctx.r7.u64);
	// stw r30,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r30.u32);
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
	// stw r30,12(r31)
	PPC_STORE_U32(r31.u32 + 12, r30.u32);
	// stw r30,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r30.u32);
	// stw r11,256(r31)
	PPC_STORE_U32(r31.u32 + 256, r11.u32);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8b18
	if (cr6.eq) goto loc_822F8B18;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822F8B18:
	// addic. r10,r11,4
	xer.ca = r11.u32 > 4294967291;
	ctx.r10.s64 = r11.s64 + 4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822f8b24
	if (cr0.eq) goto loc_822F8B24;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
loc_822F8B24:
	// addic. r10,r11,8
	xer.ca = r11.u32 > 4294967287;
	ctx.r10.s64 = r11.s64 + 8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822f8b30
	if (cr0.eq) goto loc_822F8B30;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r30.u32);
loc_822F8B30:
	// li r29,1
	r29.s64 = 1;
	// stb r30,49(r11)
	PPC_STORE_U8(r11.u32 + 49, r30.u8);
	// addi r3,r31,344
	ctx.r3.s64 = r31.s64 + 344;
	// stb r29,48(r11)
	PPC_STORE_U8(r11.u32 + 48, r29.u8);
	// stw r11,336(r31)
	PPC_STORE_U32(r31.u32 + 336, r11.u32);
	// stb r29,49(r11)
	PPC_STORE_U8(r11.u32 + 49, r29.u8);
	// lwz r11,336(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// lwz r10,336(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
	// lwz r9,336(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// stw r9,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r9.u32);
	// stw r30,340(r31)
	PPC_STORE_U32(r31.u32 + 340, r30.u32);
	// bl 0x822fff78
	sub_822FFF78(ctx, base);
	// addi r3,r31,356
	ctx.r3.s64 = r31.s64 + 356;
	// bl 0x822fff78
	sub_822FFF78(ctx, base);
	// li r3,40
	ctx.r3.s64 = 40;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f8b84
	if (cr6.eq) goto loc_822F8B84;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
loc_822F8B84:
	// addic. r11,r3,4
	xer.ca = ctx.r3.u32 > 4294967291;
	r11.s64 = ctx.r3.s64 + 4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822f8b90
	if (cr0.eq) goto loc_822F8B90;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822F8B90:
	// addic. r11,r3,8
	xer.ca = ctx.r3.u32 > 4294967287;
	r11.s64 = ctx.r3.s64 + 8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822f8b9c
	if (cr0.eq) goto loc_822F8B9C;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822F8B9C:
	// stb r29,32(r3)
	PPC_STORE_U8(ctx.r3.u32 + 32, r29.u8);
	// addi r11,r31,108
	r11.s64 = r31.s64 + 108;
	// stb r30,33(r3)
	PPC_STORE_U8(ctx.r3.u32 + 33, r30.u8);
	// lis r11,16484
	r11.s64 = 1080295424;
	// stw r3,372(r31)
	PPC_STORE_U32(r31.u32 + 372, ctx.r3.u32);
	// li r28,1
	r28.s64 = 1;
	// stb r29,33(r3)
	PPC_STORE_U8(ctx.r3.u32 + 33, r29.u8);
	// ori r10,r11,15
	ctx.r10.u64 = r11.u64 | 15;
	// lwz r31,372(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 372);
	// li r8,1
	ctx.r8.s64 = 1;
	// stw r31,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r31.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// li r30,0
	r30.s64 = 0;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,1000
	ctx.r3.s64 = 1000;
	// lwz r7,196(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// li r29,-1
	r29.s64 = -1;
	// lwz r11,228(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// li r27,10000
	r27.s64 = 10000;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// lwz r4,224(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
	// li r11,250
	r11.s64 = 250;
	// lwz r25,232(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 232);
	// lwz r7,372(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 372);
	// stw r7,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r7.u32);
	// lwz r31,196(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// stw r4,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r4.u32);
	// li r4,200
	ctx.r4.s64 = 200;
	// lwz r5,372(r5)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r5.u32 + 372);
	// stw r5,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r5.u32);
	// stw r9,376(r31)
	PPC_STORE_U32(r31.u32 + 376, ctx.r9.u32);
	// stb r8,381(r31)
	PPC_STORE_U8(r31.u32 + 381, ctx.r8.u8);
	// stw r10,432(r31)
	PPC_STORE_U32(r31.u32 + 432, ctx.r10.u32);
	// stb r6,382(r31)
	PPC_STORE_U8(r31.u32 + 382, ctx.r6.u8);
	// stw r28,412(r31)
	PPC_STORE_U32(r31.u32 + 412, r28.u32);
	// stb r30,417(r31)
	PPC_STORE_U8(r31.u32 + 417, r30.u8);
	// stw r30,420(r31)
	PPC_STORE_U32(r31.u32 + 420, r30.u32);
	// std r30,424(r31)
	PPC_STORE_U64(r31.u32 + 424, r30.u64);
	// stw r30,440(r31)
	PPC_STORE_U32(r31.u32 + 440, r30.u32);
	// stw r30,444(r31)
	PPC_STORE_U32(r31.u32 + 444, r30.u32);
	// stw r30,448(r31)
	PPC_STORE_U32(r31.u32 + 448, r30.u32);
	// stw r30,452(r31)
	PPC_STORE_U32(r31.u32 + 452, r30.u32);
	// stb r30,456(r31)
	PPC_STORE_U8(r31.u32 + 456, r30.u8);
	// stb r30,457(r31)
	PPC_STORE_U8(r31.u32 + 457, r30.u8);
	// stb r30,458(r31)
	PPC_STORE_U8(r31.u32 + 458, r30.u8);
	// stw r30,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r30.u32);
	// stb r11,109(r31)
	PPC_STORE_U8(r31.u32 + 109, r11.u8);
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r8,r11,-32
	ctx.r8.s64 = r11.s64 + -32;
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r7,r11,224
	ctx.r7.s64 = r11.s64 + 224;
	// lwz r24,204(r1)
	r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// srawi r5,r8,31
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 31;
	// stb r4,108(r31)
	PPC_STORE_U8(r31.u32 + 108, ctx.r4.u8);
	// stw r3,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r3.u32);
	// and r4,r5,r7
	ctx.r4.u64 = ctx.r5.u64 & ctx.r7.u64;
	// stw r10,84(r31)
	PPC_STORE_U32(r31.u32 + 84, ctx.r10.u32);
	// stw r9,88(r31)
	PPC_STORE_U32(r31.u32 + 88, ctx.r9.u32);
	// subf r3,r4,r11
	ctx.r3.s64 = r11.s64 - ctx.r4.s64;
	// stw r27,104(r31)
	PPC_STORE_U32(r31.u32 + 104, r27.u32);
	// stw r30,472(r31)
	PPC_STORE_U32(r31.u32 + 472, r30.u32);
	// stw r29,476(r31)
	PPC_STORE_U32(r31.u32 + 476, r29.u32);
	// stb r30,480(r31)
	PPC_STORE_U8(r31.u32 + 480, r30.u8);
	// stb r28,545(r31)
	PPC_STORE_U8(r31.u32 + 545, r28.u8);
	// stw r24,32(r31)
	PPC_STORE_U32(r31.u32 + 32, r24.u32);
	// stw r25,92(r31)
	PPC_STORE_U32(r31.u32 + 92, r25.u32);
	// stw r29,100(r31)
	PPC_STORE_U32(r31.u32 + 100, r29.u32);
	// stb r3,108(r31)
	PPC_STORE_U8(r31.u32 + 108, ctx.r3.u8);
	// lbz r11,109(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 109);
	// addi r10,r11,-32
	ctx.r10.s64 = r11.s64 + -32;
	// lwz r26,196(r1)
	r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// addi r9,r11,224
	ctx.r9.s64 = r11.s64 + 224;
	// srawi r8,r10,31
	xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 31;
	// lwz r27,212(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// addi r26,r26,109
	r26.s64 = r26.s64 + 109;
	// and r7,r8,r9
	ctx.r7.u64 = ctx.r8.u64 & ctx.r9.u64;
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r26.u32);
	// rotlwi r6,r26,0
	ctx.r6.u64 = __builtin_rotateleft32(r26.u32, 0);
	// subf r6,r7,r11
	ctx.r6.s64 = r11.s64 - ctx.r7.s64;
	// stb r6,109(r31)
	PPC_STORE_U8(r31.u32 + 109, ctx.r6.u8);
	// lwz r11,0(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8d18
	if (cr6.eq) goto loc_822F8D18;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmpwi cr6,r10,48
	cr6.compare<int32_t>(ctx.r10.s32, 48, xer);
	// bge cr6,0x822f8d34
	if (!cr6.lt) goto loc_822F8D34;
	// li r6,48
	ctx.r6.s64 = 48;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// li r4,48
	ctx.r4.s64 = 48;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x82ca39c0
	sub_82CA39C0(ctx, base);
	// b 0x822f8d38
	goto loc_822F8D38;
loc_822F8D18:
	// lis r11,-32247
	r11.s64 = -2113339392;
	// li r6,48
	ctx.r6.s64 = 48;
	// addi r5,r11,63
	ctx.r5.s64 = r11.s64 + 63;
	// li r4,48
	ctx.r4.s64 = 48;
	// addi r3,r31,36
	ctx.r3.s64 = r31.s64 + 36;
	// bl 0x82ca39c0
	sub_82CA39C0(ctx, base);
	// b 0x822f8d38
	goto loc_822F8D38;
loc_822F8D34:
	// stb r30,36(r31)
	PPC_STORE_U8(r31.u32 + 36, r30.u8);
loc_822F8D38:
	// stb r28,297(r31)
	PPC_STORE_U8(r31.u32 + 297, r28.u8);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,6448
	ctx.r3.s64 = r11.s64 + 6448;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// std r30,128(r31)
	PPC_STORE_U64(r31.u32 + 128, r30.u64);
	// std r30,136(r31)
	PPC_STORE_U64(r31.u32 + 136, r30.u64);
	// li r11,-1
	r11.s64 = -1;
	// std r30,144(r31)
	PPC_STORE_U64(r31.u32 + 144, r30.u64);
	// stw r30,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r30.u32);
	// stw r11,172(r31)
	PPC_STORE_U32(r31.u32 + 172, r11.u32);
	// stw r11,552(r31)
	PPC_STORE_U32(r31.u32 + 552, r11.u32);
	// stw r29,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r29.u32);
	// stw r30,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r30.u32);
	// lbz r10,297(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 297);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f8d9c
	if (cr6.eq) goto loc_822F8D9C;
	// mr r11,r30
	r11.u64 = r30.u64;
loc_822F8D7C:
	// addi r10,r11,76
	ctx.r10.s64 = r11.s64 + 76;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r11,r9,24
	r11.u64 = ctx.r9.u32 & 0xFF;
	// stwx r30,r8,r31
	PPC_STORE_U32(ctx.r8.u32 + r31.u32, r30.u32);
	// lbz r7,297(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 297);
	// cmplw cr6,r11,r7
	cr6.compare<uint32_t>(r11.u32, ctx.r7.u32, xer);
	// blt cr6,0x822f8d7c
	if (cr6.lt) goto loc_822F8D7C;
loc_822F8D9C:
	// lis r11,-32048
	r11.s64 = -2100297728;
	// stb r30,328(r31)
	PPC_STORE_U8(r31.u32 + 328, r30.u8);
	// stb r28,110(r31)
	PPC_STORE_U8(r31.u32 + 110, r28.u8);
	// addi r10,r11,-21584
	ctx.r10.s64 = r11.s64 + -21584;
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// stw r30,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r30.u32);
	// stw r10,20(r31)
	PPC_STORE_U32(r31.u32 + 20, ctx.r10.u32);
	// stw r30,548(r31)
	PPC_STORE_U32(r31.u32 + 548, r30.u32);
	// stb r30,380(r31)
	PPC_STORE_U8(r31.u32 + 380, r30.u8);
	// stw r30,288(r31)
	PPC_STORE_U32(r31.u32 + 288, r30.u32);
	// stw r30,124(r31)
	PPC_STORE_U32(r31.u32 + 124, r30.u32);
	// stw r30,384(r31)
	PPC_STORE_U32(r31.u32 + 384, r30.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lis r9,4
	ctx.r9.s64 = 262144;
	// stw r3,404(r31)
	PPC_STORE_U32(r31.u32 + 404, ctx.r3.u32);
	// li r8,20
	ctx.r8.s64 = 20;
	// ori r11,r9,37856
	r11.u64 = ctx.r9.u64 | 37856;
	// stw r30,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r30.u32);
	// li r7,10
	ctx.r7.s64 = 10;
	// stw r8,392(r31)
	PPC_STORE_U32(r31.u32 + 392, ctx.r8.u32);
	// li r5,64
	ctx.r5.s64 = 64;
	// stw r11,408(r31)
	PPC_STORE_U32(r31.u32 + 408, r11.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r7,396(r31)
	PPC_STORE_U32(r31.u32 + 396, ctx.r7.u32);
	// addi r3,r31,481
	ctx.r3.s64 = r31.s64 + 481;
	// stw r11,400(r31)
	PPC_STORE_U32(r31.u32 + 400, r11.u32);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x821c67d8
	sub_821C67D8(ctx, base);
	// lis r6,-31927
	ctx.r6.s64 = -2092367872;
	// addi r3,r6,28344
	ctx.r3.s64 = ctx.r6.s64 + 28344;
loc_822F8E18:
	// mfmsr r4
	// mtmsrd r13,1
	// lwarx r5,0,r3
	reserved.u32 = *(uint32_t*)(base + ctx.r3.u32);
	ctx.r5.u64 = __builtin_bswap32(reserved.u32);
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// stwcx. r5,0,r3
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r3.u32), reserved.s32, __builtin_bswap32(ctx.r5.s32));
	cr0.so = xer.so;
	// mtmsrd r4,1
	// bne 0x822f8e18
	if (!cr0.eq) goto loc_822F8E18;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822F8E48"))) PPC_WEAK_FUNC(sub_822F8E48);
PPC_FUNC_IMPL(__imp__sub_822F8E48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f8ea8
	if (cr6.eq) goto loc_822F8EA8;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822F8E6C:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822f8e6c
	if (!cr0.eq) goto loc_822F8E6C;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822f8ea0
	if (!cr6.eq) goto loc_822F8EA0;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822F8EA0:
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
loc_822F8EA8:
	// lis r11,-32240
	r11.s64 = -2112880640;
	// addi r10,r11,-7728
	ctx.r10.s64 = r11.s64 + -7728;
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F8EC8"))) PPC_WEAK_FUNC(sub_822F8EC8);
PPC_FUNC_IMPL(__imp__sub_822F8EC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r30,128
	ctx.r3.s64 = r30.s64 + 128;
	// bl 0x82cbcf80
	sub_82CBCF80(ctx, base);
	// lwz r3,476(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 476);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x822f8efc
	if (cr6.eq) goto loc_822F8EFC;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x821756e0
	sub_821756E0(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// stw r11,476(r30)
	PPC_STORE_U32(r30.u32 + 476, r11.u32);
loc_822F8EFC:
	// lwz r10,372(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 372);
	// addi r31,r30,368
	r31.s64 = r30.s64 + 368;
	// li r29,0
	r29.s64 = 0;
	// mr r11,r31
	r11.u64 = r31.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822F8F18:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f8f2c
	if (cr6.eq) goto loc_822F8F2C;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f8f30
	if (cr6.eq) goto loc_822F8F30;
loc_822F8F2C:
	// twi 31,r0,22
loc_822F8F30:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f8fa0
	if (cr6.eq) goto loc_822F8FA0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f8f44
	if (!cr6.eq) goto loc_822F8F44;
	// twi 31,r0,22
loc_822F8F44:
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f8f54
	if (!cr6.eq) goto loc_822F8F54;
	// twi 31,r0,22
loc_822F8F54:
	// lwz r9,24(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// stb r29,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r29.u8);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bne cr6,0x822f8f6c
	if (!cr6.eq) goto loc_822F8F6C;
	// twi 31,r0,22
loc_822F8F6C:
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// lwz r11,112(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 112);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x822f8f8c
	if (cr6.lt) goto loc_822F8F8C;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// bge cr6,0x822f8f8c
	if (!cr6.lt) goto loc_822F8F8C;
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// stb r29,481(r11)
	PPC_STORE_U8(r11.u32 + 481, r29.u8);
loc_822F8F8C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822f8f18
	goto loc_822F8F18;
loc_822F8FA0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F8FA8"))) PPC_WEAK_FUNC(sub_822F8FA8);
PPC_FUNC_IMPL(__imp__sub_822F8FA8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r3,r31,128
	ctx.r3.s64 = r31.s64 + 128;
	// bl 0x82cbcf80
	sub_82CBCF80(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r11,6596
	ctx.r3.s64 = r11.s64 + 6596;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,412(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 412);
	// cmpw cr6,r29,r10
	cr6.compare<int32_t>(r29.s32, ctx.r10.s32, xer);
	// beq cr6,0x822f91f0
	if (cr6.eq) goto loc_822F91F0;
	// addi r30,r31,344
	r30.s64 = r31.s64 + 344;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x822feac8
	sub_822FEAC8(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_822F9004:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f9018
	if (cr6.eq) goto loc_822F9018;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f901c
	if (cr6.eq) goto loc_822F901C;
loc_822F9018:
	// twi 31,r0,22
loc_822F901C:
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f9068
	if (cr6.eq) goto loc_822F9068;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f9034
	if (!cr6.eq) goto loc_822F9034;
	// twi 31,r0,22
loc_822F9034:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9044
	if (!cr6.eq) goto loc_822F9044;
	// twi 31,r0,22
loc_822F9044:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,16(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822f9004
	goto loc_822F9004;
loc_822F9068:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822fff10
	sub_822FFF10(ctx, base);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// li r28,0
	r28.s64 = 0;
	// cmpwi cr6,r29,2
	cr6.compare<int32_t>(r29.s32, 2, xer);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r28,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r28.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// stw r29,412(r31)
	PPC_STORE_U32(r31.u32 + 412, r29.u32);
	// bne cr6,0x822f90b0
	if (!cr6.eq) goto loc_822F90B0;
	// li r11,1
	r11.s64 = 1;
	// stw r11,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r11.u32);
	// b 0x822f90b4
	goto loc_822F90B4;
loc_822F90B0:
	// stw r28,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r28.u32);
loc_822F90B4:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fc320
	sub_822FC320(ctx, base);
	// cmpwi cr6,r29,0
	cr6.compare<int32_t>(r29.s32, 0, xer);
	// bne cr6,0x822f9188
	if (!cr6.eq) goto loc_822F9188;
	// addi r30,r31,356
	r30.s64 = r31.s64 + 356;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x822feac8
	sub_822FEAC8(ctx, base);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_822F90E8:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f90fc
	if (cr6.eq) goto loc_822F90FC;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f9100
	if (cr6.eq) goto loc_822F9100;
loc_822F90FC:
	// twi 31,r0,22
loc_822F9100:
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f914c
	if (cr6.eq) goto loc_822F914C;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f9118
	if (!cr6.eq) goto loc_822F9118;
	// twi 31,r0,22
loc_822F9118:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9128
	if (!cr6.eq) goto loc_822F9128;
	// twi 31,r0,22
loc_822F9128:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,16(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822f90e8
	goto loc_822F90E8;
loc_822F914C:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822fff10
	sub_822FFF10(ctx, base);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r28,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r28.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// stb r28,417(r31)
	PPC_STORE_U8(r31.u32 + 417, r28.u8);
	// bl 0x82300360
	sub_82300360(ctx, base);
	// b 0x822f91b4
	goto loc_822F91B4;
loc_822F9188:
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822f91b4
	if (cr6.eq) goto loc_822F91B4;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822f91b0
	if (!cr6.eq) goto loc_822F91B0;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f91b4
	goto loc_822F91B4;
loc_822F91B0:
	// stw r28,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r28.u32);
loc_822F91B4:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x823010a8
	sub_823010A8(ctx, base);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822F91F0:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822F91F8"))) PPC_WEAK_FUNC(sub_822F91F8);
PPC_FUNC_IMPL(__imp__sub_822F91F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,1
	r30.s64 = 1;
	// lwz r11,552(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 552);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x822f92b8
	if (!cr6.eq) goto loc_822F92B8;
	// li r5,254
	ctx.r5.s64 = 254;
	// li r4,2
	ctx.r4.s64 = 2;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x82cfaa30
	sub_82CFAA30(ctx, base);
	// stw r3,552(r31)
	PPC_STORE_U32(r31.u32 + 552, ctx.r3.u32);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x822f9248
	if (!cr6.eq) goto loc_822F9248;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,6624
	ctx.r3.s64 = r11.s64 + 6624;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822F9248:
	// li r11,2
	r11.s64 = 2;
	// lwz r3,552(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 552);
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,1003
	ctx.r9.s64 = 1003;
	// sth r11,96(r1)
	PPC_STORE_U16(ctx.r1.u32 + 96, r11.u16);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// li r5,16
	ctx.r5.s64 = 16;
	// sth r9,98(r1)
	PPC_STORE_U16(ctx.r1.u32 + 98, ctx.r9.u16);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82cfaa90
	sub_82CFAA90(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f928c
	if (cr6.eq) goto loc_822F928C;
	// bl 0x82cfac30
	sub_82CFAC30(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// addi r3,r11,6672
	ctx.r3.s64 = r11.s64 + 6672;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822F928C:
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// lis r4,-32764
	ctx.r4.s64 = -2147221504;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,552(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 552);
	// ori r4,r4,26238
	ctx.r4.u64 = ctx.r4.u64 | 26238;
	// bl 0x82cfaa58
	sub_82CFAA58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f92b8
	if (cr6.eq) goto loc_822F92B8;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,6724
	ctx.r3.s64 = r11.s64 + 6724;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822F92B8:
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// addi r11,r3,8
	r11.s64 = ctx.r3.s64 + 8;
	// stw r11,548(r31)
	PPC_STORE_U32(r31.u32 + 548, r11.u32);
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// stb r30,1329(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1329, r30.u8);
	// lwz r3,548(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 548);
	// bl 0x823040e0
	sub_823040E0(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F92F0"))) PPC_WEAK_FUNC(sub_822F92F0);
PPC_FUNC_IMPL(__imp__sub_822F92F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x822f93ec
	if (!cr6.eq) goto loc_822F93EC;
	// li r5,17
	ctx.r5.s64 = 17;
	// li r4,2
	ctx.r4.s64 = 2;
	// li r3,2
	ctx.r3.s64 = 2;
	// bl 0x82cfaa30
	sub_82CFAA30(ctx, base);
	// stw r3,172(r31)
	PPC_STORE_U32(r31.u32 + 172, ctx.r3.u32);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x822f9350
	if (!cr6.eq) goto loc_822F9350;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,2
	ctx.r4.s64 = 2;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822F9350:
	// li r11,2
	r11.s64 = 2;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,1002
	ctx.r9.s64 = 1002;
	// sth r11,96(r1)
	PPC_STORE_U16(ctx.r1.u32 + 96, r11.u16);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// li r5,16
	ctx.r5.s64 = 16;
	// sth r9,98(r1)
	PPC_STORE_U16(ctx.r1.u32 + 98, ctx.r9.u16);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82cfaa90
	sub_82CFAA90(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f93a4
	if (cr6.eq) goto loc_822F93A4;
	// bl 0x82cfac30
	sub_82CFAC30(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822F93A4:
	// li r11,1
	r11.s64 = 1;
	// lwz r3,172(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// lis r4,-32764
	ctx.r4.s64 = -2147221504;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// ori r4,r4,26238
	ctx.r4.u64 = ctx.r4.u64 | 26238;
	// bl 0x82cfaa58
	sub_82CFAA58(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f93ec
	if (cr6.eq) goto loc_822F93EC;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822F93EC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F9408"))) PPC_WEAK_FUNC(sub_822F9408);
PPC_FUNC_IMPL(__imp__sub_822F9408) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x822f9434
	if (!cr6.eq) goto loc_822F9434;
	// li r11,-1
	r11.s64 = -1;
	// stw r11,256(r31)
	PPC_STORE_U32(r31.u32 + 256, r11.u32);
loc_822F9434:
	// lwz r11,256(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// cmplw cr6,r11,r3
	cr6.compare<uint32_t>(r11.u32, ctx.r3.u32, xer);
	// beq cr6,0x822f95e8
	if (cr6.eq) goto loc_822F95E8;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r9,r10,r3
	ctx.r9.u64 = ctx.r3.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r3.u8 & 0x3F));
	// lwz r11,26848(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26848);
	// and r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 & r11.u64;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822f94b0
	if (!cr6.eq) goto loc_822F94B0;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822f949c
	if (cr6.eq) goto loc_822F949C;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822f9494
	if (!cr6.eq) goto loc_822F9494;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F9494:
	// li r11,0
	r11.s64 = 0;
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
loc_822F949C:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F94B0:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r30,0
	r30.s64 = 0;
	// cmpwi cr6,r11,8
	cr6.compare<int32_t>(r11.s32, 8, xer);
	// bne cr6,0x822f94c8
	if (!cr6.eq) goto loc_822F94C8;
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// stw r30,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r30.u32);
loc_822F94C8:
	// stw r3,256(r31)
	PPC_STORE_U32(r31.u32 + 256, ctx.r3.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,254
	ctx.r4.s64 = 254;
	// bl 0x82cbc160
	sub_82CBC160(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f94f4
	if (cr6.eq) goto loc_822F94F4;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F94F4:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822f9514
	if (!cr6.eq) goto loc_822F9514;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F9514:
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822f9540
	if (cr6.eq) goto loc_822F9540;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822f953c
	if (!cr6.eq) goto loc_822F953C;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f9540
	goto loc_822F9540;
loc_822F953C:
	// stw r30,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r30.u32);
loc_822F9540:
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82cf9b58
	sub_82CF9B58(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822f9568
	if (!cr6.eq) goto loc_822F9568;
loc_822F9550:
	// li r3,5
	ctx.r3.s64 = 5;
	// bl 0x82cbc6b0
	sub_82CBC6B0(ctx, base);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82cf9b58
	sub_82CF9B58(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f9550
	if (cr6.eq) goto loc_822F9550;
loc_822F9568:
	// addi r4,r31,176
	ctx.r4.s64 = r31.s64 + 176;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82cf9af8
	sub_82CF9AF8(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822f9590
	if (cr6.eq) goto loc_822F9590;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F9590:
	// addi r4,r31,264
	ctx.r4.s64 = r31.s64 + 264;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// bl 0x82cbc440
	sub_82CBC440(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822f95b8
	if (cr6.eq) goto loc_822F95B8;
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F95B8:
	// li r5,16
	ctx.r5.s64 = 16;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// addi r4,r31,272
	ctx.r4.s64 = r31.s64 + 272;
	// bl 0x82cbc148
	sub_82CBC148(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822f95e4
	if (cr6.eq) goto loc_822F95E4;
	// li r4,11
	ctx.r4.s64 = 11;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822f95ec
	goto loc_822F95EC;
loc_822F95E4:
	// bl 0x822f91f8
	sub_822F91F8(ctx, base);
loc_822F95E8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_822F95EC:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822F9608"))) PPC_WEAK_FUNC(sub_822F9608);
PPC_FUNC_IMPL(__imp__sub_822F9608) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-1408(r1)
	ea = -1408 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r3,548(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 548);
	// bl 0x822d1568
	sub_822D1568(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822f9700
	if (cr6.eq) goto loc_822F9700;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,6784
	ctx.r3.s64 = r11.s64 + 6784;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r10,3
	ctx.r10.s64 = 3;
	// li r9,5
	ctx.r9.s64 = 5;
	// lwz r3,548(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 548);
	// sth r10,96(r1)
	PPC_STORE_U16(ctx.r1.u32 + 96, ctx.r10.u16);
	// li r5,1261
	ctx.r5.s64 = 1261;
	// stb r9,98(r1)
	PPC_STORE_U8(ctx.r1.u32 + 98, ctx.r9.u8);
	// addi r4,r1,101
	ctx.r4.s64 = ctx.r1.s64 + 101;
	// bl 0x82304408
	sub_82304408(ctx, base);
	// lis r8,-31927
	ctx.r8.s64 = -2092367872;
	// sth r3,99(r1)
	PPC_STORE_U16(ctx.r1.u32 + 99, ctx.r3.u16);
	// li r29,1
	r29.s64 = 1;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r30,r29
	r30.u64 = r29.u64;
	// slw r7,r29,r3
	ctx.r7.u64 = ctx.r3.u8 & 0x20 ? 0 : (r29.u32 << (ctx.r3.u8 & 0x3F));
	// lwz r11,26848(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 26848);
	// and r6,r7,r11
	ctx.r6.u64 = ctx.r7.u64 & r11.u64;
	// cmplwi cr6,r6,0
	cr6.compare<uint32_t>(ctx.r6.u32, 0, xer);
	// beq cr6,0x822f96c8
	if (cr6.eq) goto loc_822F96C8;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,252
	ctx.r4.s64 = 252;
	// bl 0x82cbc160
	sub_82CBC160(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822f9708
	if (!cr6.eq) goto loc_822F9708;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822f96a0
	if (!cr6.eq) goto loc_822F96A0;
	// li r30,0
	r30.s64 = 0;
loc_822F96A0:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r4,251
	ctx.r4.s64 = 251;
	// bl 0x82cbc160
	sub_82CBC160(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822f9708
	if (!cr6.eq) goto loc_822F9708;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822f96c8
	if (!cr6.eq) goto loc_822F96C8;
	// li r29,0
	r29.s64 = 0;
loc_822F96C8:
	// clrlwi r11,r30,24
	r11.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f96e4
	if (cr6.eq) goto loc_822F96E4;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r31,344
	ctx.r4.s64 = r31.s64 + 344;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f9710
	sub_822F9710(ctx, base);
loc_822F96E4:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f9700
	if (cr6.eq) goto loc_822F9700;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r31,356
	ctx.r4.s64 = r31.s64 + 356;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f9710
	sub_822F9710(ctx, base);
loc_822F9700:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fbcd8
	sub_822FBCD8(ctx, base);
loc_822F9708:
	// addi r1,r1,1408
	ctx.r1.s64 = ctx.r1.s64 + 1408;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F9710"))) PPC_WEAK_FUNC(sub_822F9710);
PPC_FUNC_IMPL(__imp__sub_822F9710) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCVRegister v127{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// li r12,-176
	r12.s64 = -176;
	// stvx128 v127,r1,r12
	_mm_store_si128((__m128i*)(base + ((ctx.r1.u32 + r12.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// stwu r1,-352(r1)
	ea = -352 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// mr r15,r5
	r15.u64 = ctx.r5.u64;
	// lwz r11,26912(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26912);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f9750
	if (cr6.eq) goto loc_822F9750;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lbz r10,26821(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 26821);
	// b 0x822f9754
	goto loc_822F9754;
loc_822F9750:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822F9754:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f9ac8
	if (cr6.eq) goto loc_822F9AC8;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lbz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 256);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f9ac8
	if (!cr6.eq) goto loc_822F9AC8;
	// lhz r11,3(r15)
	r11.u64 = PPC_LOAD_U16(r15.u32 + 3);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r19,r11,5
	r19.s64 = r11.s64 + 5;
	// bl 0x822feac8
	sub_822FEAC8(ctx, base);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r28,r1,112
	r28.s64 = ctx.r1.s64 + 112;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// li r16,4
	r16.s64 = 4;
	// li r14,8
	r14.s64 = 8;
	// li r17,2
	r17.s64 = 2;
	// li r18,1003
	r18.s64 = 1003;
	// addi r22,r10,9596
	r22.s64 = ctx.r10.s64 + 9596;
	// addi r20,r9,-28176
	r20.s64 = ctx.r9.s64 + -28176;
	// addi r24,r8,-28192
	r24.s64 = ctx.r8.s64 + -28192;
	// addi r23,r7,-28208
	r23.s64 = ctx.r7.s64 + -28208;
	// lwz r27,0(r11)
	r27.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r25,r11,-27468
	r25.s64 = r11.s64 + -27468;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// addi r21,r11,9512
	r21.s64 = r11.s64 + 9512;
	// lfs f31,0(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r25.u32 + 0);
	f31.f64 = double(temp.f32);
loc_822F97D8:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822f97ec
	if (cr6.eq) goto loc_822F97EC;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// beq cr6,0x822f97f0
	if (cr6.eq) goto loc_822F97F0;
loc_822F97EC:
	// twi 31,r0,22
loc_822F97F0:
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x822f9ac0
	if (cr6.eq) goto loc_822F9AC0;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822f9808
	if (!cr6.eq) goto loc_822F9808;
	// twi 31,r0,22
loc_822F9808:
	// lwz r29,4(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r27,r29
	cr6.compare<uint32_t>(r27.u32, r29.u32, xer);
	// bne cr6,0x822f9818
	if (!cr6.eq) goto loc_822F9818;
	// twi 31,r0,22
loc_822F9818:
	// addi r30,r27,24
	r30.s64 = r27.s64 + 24;
	// addi r31,r26,368
	r31.s64 = r26.s64 + 368;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r10,372(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f9848
	if (cr6.eq) goto loc_822F9848;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822f984c
	if (cr6.eq) goto loc_822F984C;
loc_822F9848:
	// twi 31,r0,22
loc_822F984C:
	// lwz r9,100(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f9888
	if (cr6.eq) goto loc_822F9888;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f9864
	if (!cr6.eq) goto loc_822F9864;
	// twi 31,r0,22
loc_822F9864:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x822f9874
	if (!cr6.eq) goto loc_822F9874;
	// twi 31,r0,22
loc_822F9874:
	// lwz r11,24(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// lwz r10,32(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 32);
	// lwz r9,96(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 96);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9aac
	if (!cr6.eq) goto loc_822F9AAC;
loc_822F9888:
	// cmplw cr6,r27,r29
	cr6.compare<uint32_t>(r27.u32, r29.u32, xer);
	// bne cr6,0x822f9894
	if (!cr6.eq) goto loc_822F9894;
	// twi 31,r0,22
loc_822F9894:
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// ld r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// bl 0x82304740
	sub_82304740(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x822f9aac
	if (!cr6.eq) goto loc_822F9AAC;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bne cr6,0x822f98bc
	if (!cr6.eq) goto loc_822F98BC;
	// twi 31,r0,22
loc_822F98BC:
	// lwz r11,548(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 548);
	// li r4,0
	ctx.r4.s64 = 0;
	// ld r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// lwz r3,148(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 148);
	// bl 0x822f8648
	sub_822F8648(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x822f9aac
	if (!cr6.eq) goto loc_822F9AAC;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r11,26912(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26912);
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r3,156(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 156);
	// bl 0x822641f0
	sub_822641F0(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f9ac0
	if (cr6.eq) goto loc_822F9AC0;
	// bl 0x82455f20
	sub_82455F20(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822f9ac0
	if (cr6.eq) goto loc_822F9AC0;
	// lwz r4,124(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 124);
	// addi r3,r1,144
	ctx.r3.s64 = ctx.r1.s64 + 144;
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,64(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 64);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r9,r1,84
	ctx.r9.s64 = ctx.r1.s64 + 84;
	// stfs f31,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v0,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw128 v127,v0,0
	_mm_store_si128((__m128i*)v127.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v0.u32), 0xFF));
	// lwz r29,4(r28)
	r29.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r27,r29
	cr6.compare<uint32_t>(r27.u32, r29.u32, xer);
	// bne cr6,0x822f993c
	if (!cr6.eq) goto loc_822F993C;
	// twi 31,r0,22
loc_822F993C:
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822f9964
	if (cr6.eq) goto loc_822F9964;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// beq cr6,0x822f9968
	if (cr6.eq) goto loc_822F9968;
loc_822F9964:
	// twi 31,r0,22
loc_822F9968:
	// lwz r11,108(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822f99ec
	if (cr6.eq) goto loc_822F99EC;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f9980
	if (!cr6.eq) goto loc_822F9980;
	// twi 31,r0,22
loc_822F9980:
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f9990
	if (!cr6.eq) goto loc_822F9990;
	// twi 31,r0,22
loc_822F9990:
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// lwz r8,32(r26)
	ctx.r8.u64 = PPC_LOAD_U32(r26.u32 + 32);
	// lwz r7,96(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 96);
	// cmplw cr6,r7,r8
	cr6.compare<uint32_t>(ctx.r7.u32, ctx.r8.u32, xer);
	// bne cr6,0x822f9aac
	if (!cr6.eq) goto loc_822F9AAC;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f99b0
	if (!cr6.eq) goto loc_822F99B0;
	// twi 31,r0,22
loc_822F99B0:
	// lvlx v13,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// lvx128 v0,r0,r23
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r23.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm128 v13,v127,v13,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)v127.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// bne cr6,0x822f99c8
	if (!cr6.eq) goto loc_822F99C8;
	// twi 31,r0,22
loc_822F99C8:
	// lvx128 v0,r0,r24
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r24.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// lvlx v12,r10,r16
	temp.u32 = ctx.r10.u32 + r16.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v13,v13,v12,v0
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// bne cr6,0x822f99e0
	if (!cr6.eq) goto loc_822F99E0;
	// twi 31,r0,22
loc_822F99E0:
	// lvx128 v0,r0,r20
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r20.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v12,r10,r14
	temp.u32 = ctx.r10.u32 + r14.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm128 v127,v13,v12,v0
	_mm_store_si128((__m128i*)v127.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
loc_822F99EC:
	// addi r11,r1,144
	r11.s64 = ctx.r1.s64 + 144;
	// lfs f0,21396(r25)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r25.u32 + 21396);
	f0.f64 = double(temp.f32);
	// addi r10,r1,160
	ctx.r10.s64 = ctx.r1.s64 + 160;
	// lvx128 v0,r0,r11
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((r11.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vsubfp128 v13,v0,v127
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(ctx.v13.f32, _mm_sub_ps(_mm_load_ps(ctx.v0.f32), _mm_load_ps(v127.f32)));
	// vmsum3fp128 v12,v13,v13
	_mm_store_ps(ctx.v12.f32, _mm_dp_ps(_mm_load_ps(ctx.v13.f32), _mm_load_ps(ctx.v13.f32), 0xEF));
	// stvx128 v12,r0,r10
	_mm_store_si128((__m128i*)(base + ((ctx.r10.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v12.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f13,160(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f13.f64 = double(temp.f32);
	// fsqrts f12,f13
	ctx.f12.f64 = double(float(sqrt(ctx.f13.f64)));
	// fcmpu cr6,f12,f0
	cr6.compare(ctx.f12.f64, f0.f64);
	// bge cr6,0x822f9aac
	if (!cr6.lt) goto loc_822F9AAC;
	// cmplw cr6,r27,r29
	cr6.compare<uint32_t>(r27.u32, r29.u32, xer);
	// bne cr6,0x822f9a24
	if (!cr6.eq) goto loc_822F9A24;
	// twi 31,r0,22
loc_822F9A24:
	// lwz r31,16(r27)
	r31.u64 = PPC_LOAD_U32(r27.u32 + 16);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cf9b18
	sub_82CF9B18(ctx, base);
	// cmplwi cr6,r3,3
	cr6.compare<uint32_t>(ctx.r3.u32, 3, xer);
	// bne cr6,0x822f9a48
	if (!cr6.eq) goto loc_822F9A48;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// b 0x822f9a9c
	goto loc_822F9A9C;
loc_822F9A48:
	// addi r11,r1,130
	r11.s64 = ctx.r1.s64 + 130;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_822F9A58:
	// sth r9,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x822f9a58
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822F9A58;
	// stw r31,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, r31.u32);
	// li r8,16
	ctx.r8.s64 = 16;
	// sth r17,128(r1)
	PPC_STORE_U16(ctx.r1.u32 + 128, r17.u16);
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// sth r18,130(r1)
	PPC_STORE_U16(ctx.r1.u32 + 130, r18.u16);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r19
	ctx.r5.u64 = r19.u64;
	// lwz r3,552(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 552);
	// mr r4,r15
	ctx.r4.u64 = r15.u64;
	// bl 0x82cfabb0
	sub_82CFABB0(ctx, base);
	// cmpw cr6,r3,r19
	cr6.compare<int32_t>(ctx.r3.s32, r19.s32, xer);
	// beq cr6,0x822f9aac
	if (cr6.eq) goto loc_822F9AAC;
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822F9A9C:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
loc_822F9AAC:
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r27,92(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r28,88(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// b 0x822f97d8
	goto loc_822F97D8;
loc_822F9AC0:
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82300360
	sub_82300360(ctx, base);
loc_822F9AC8:
	// addi r1,r1,352
	ctx.r1.s64 = ctx.r1.s64 + 352;
	// li r0,-176
	r0.s64 = -176;
	// lvx128 v127,r1,r0
	_mm_store_si128((__m128i*)v127.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r1.u32 + r0.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	return;
}

__attribute__((alias("__imp__sub_822F9AE0"))) PPC_WEAK_FUNC(sub_822F9AE0);
PPC_FUNC_IMPL(__imp__sub_822F9AE0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x822fad38
	sub_822FAD38(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// addi r4,r31,356
	ctx.r4.s64 = r31.s64 + 356;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f9c30
	sub_822F9C30(ctx, base);
	// addi r4,r31,344
	ctx.r4.s64 = r31.s64 + 344;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822f9c30
	sub_822F9C30(ctx, base);
	// lwz r11,96(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x822f9b3c
	if (cr6.lt) goto loc_822F9B3C;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,116(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 116);
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// subf r9,r11,r3
	ctx.r9.s64 = ctx.r3.s64 - r11.s64;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x822f9b3c
	if (cr6.lt) goto loc_822F9B3C;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fdc60
	sub_822FDC60(ctx, base);
loc_822F9B3C:
	// lwz r30,352(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// li r29,0
	r29.s64 = 0;
	// lbz r11,109(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 109);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822f9b58
	if (cr6.lt) goto loc_822F9B58;
	// stw r29,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r29.u32);
	// b 0x822f9b78
	goto loc_822F9B78;
loc_822F9B58:
	// lwz r11,396(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 396);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x822f9b80
	if (cr6.gt) goto loc_822F9B80;
	// lwz r11,412(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 412);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822f9b78
	if (!cr6.eq) goto loc_822F9B78;
	// li r11,1
	r11.s64 = 1;
	// stw r11,388(r31)
	PPC_STORE_U32(r31.u32 + 388, r11.u32);
loc_822F9B78:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fc320
	sub_822FC320(ctx, base);
loc_822F9B80:
	// lwz r11,100(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x822f9bac
	if (cr6.lt) goto loc_822F9BAC;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,120(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 120);
	// lwz r10,100(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 100);
	// subf r9,r11,r3
	ctx.r9.s64 = ctx.r3.s64 - r11.s64;
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// blt cr6,0x822f9bac
	if (cr6.lt) goto loc_822F9BAC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fc320
	sub_822FC320(ctx, base);
loc_822F9BAC:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,408(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 408);
	// lwz r10,404(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 404);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x822f9bd8
	if (!cr6.gt) goto loc_822F9BD8;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fa2a0
	sub_822FA2A0(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822F9BD8:
	// lwz r11,392(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 392);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bgt cr6,0x822f9c20
	if (cr6.gt) goto loc_822F9C20;
	// lwz r11,400(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 400);
	// lwz r10,384(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 384);
	// add r11,r11,r10
	r11.u64 = r11.u64 + ctx.r10.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// ble cr6,0x822f9c20
	if (!cr6.gt) goto loc_822F9C20;
	// lbz r11,456(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 456);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f9c20
	if (!cr6.eq) goto loc_822F9C20;
	// lbz r11,457(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 457);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822f9c20
	if (!cr6.eq) goto loc_822F9C20;
	// stb r29,296(r31)
	PPC_STORE_U8(r31.u32 + 296, r29.u8);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fa5f8
	sub_822FA5F8(ctx, base);
loc_822F9C20:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822F9C30"))) PPC_WEAK_FUNC(sub_822F9C30);
PPC_FUNC_IMPL(__imp__sub_822F9C30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// addi r23,r3,-30000
	r23.s64 = ctx.r3.s64 + -30000;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822feac8
	sub_822FEAC8(ctx, base);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r27,r1,112
	r27.s64 = ctx.r1.s64 + 112;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// li r24,8
	r24.s64 = 8;
	// addi r25,r11,6840
	r25.s64 = r11.s64 + 6840;
	// lwz r30,0(r10)
	r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
loc_822F9C80:
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822f9c94
	if (cr6.eq) goto loc_822F9C94;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x822f9c98
	if (cr6.eq) goto loc_822F9C98;
loc_822F9C94:
	// twi 31,r0,22
loc_822F9C98:
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f9e7c
	if (cr6.eq) goto loc_822F9E7C;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x822f9cac
	if (!cr6.eq) goto loc_822F9CAC;
	// twi 31,r0,22
loc_822F9CAC:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822f9cbc
	if (!cr6.eq) goto loc_822F9CBC;
	// twi 31,r0,22
loc_822F9CBC:
	// lwz r11,76(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 76);
	// cmplw cr6,r11,r23
	cr6.compare<uint32_t>(r11.u32, r23.u32, xer);
	// bgt cr6,0x822f9e64
	if (cr6.gt) goto loc_822F9E64;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822f9cd8
	if (!cr6.eq) goto loc_822F9CD8;
	// twi 31,r0,22
loc_822F9CD8:
	// addi r29,r30,16
	r29.s64 = r30.s64 + 16;
	// addi r31,r26,368
	r31.s64 = r26.s64 + 368;
	// addi r5,r29,8
	ctx.r5.s64 = r29.s64 + 8;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,372(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 372);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822f9d10
	if (cr6.eq) goto loc_822F9D10;
	// cmplw cr6,r5,r31
	cr6.compare<uint32_t>(ctx.r5.u32, r31.u32, xer);
	// beq cr6,0x822f9d14
	if (cr6.eq) goto loc_822F9D14;
loc_822F9D10:
	// twi 31,r0,22
loc_822F9D14:
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x822f9e64
	if (cr6.eq) goto loc_822F9E64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// bne cr6,0x822f9d2c
	if (!cr6.eq) goto loc_822F9D2C;
	// twi 31,r0,22
loc_822F9D2C:
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9d3c
	if (!cr6.eq) goto loc_822F9D3C;
	// twi 31,r0,22
loc_822F9D3C:
	// lwz r8,24(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// lwz r9,148(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 148);
	// subf r7,r9,r28
	ctx.r7.s64 = r28.s64 - ctx.r9.s64;
	// bne cr6,0x822f9d54
	if (!cr6.eq) goto loc_822F9D54;
	// twi 31,r0,22
loc_822F9D54:
	// cmpwi cr6,r9,-1
	cr6.compare<int32_t>(ctx.r9.s32, -1, xer);
	// bne cr6,0x822f9d60
	if (!cr6.eq) goto loc_822F9D60;
	// li r7,10000
	ctx.r7.s64 = 10000;
loc_822F9D60:
	// li r6,0
	ctx.r6.s64 = 0;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9d70
	if (!cr6.eq) goto loc_822F9D70;
	// twi 31,r0,22
loc_822F9D70:
	// lbz r9,144(r8)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r8.u32 + 144);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822f9dc8
	if (!cr6.eq) goto loc_822F9DC8;
	// cmplwi cr6,r7,10000
	cr6.compare<uint32_t>(ctx.r7.u32, 10000, xer);
	// blt cr6,0x822f9dc8
	if (cr6.lt) goto loc_822F9DC8;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9d90
	if (!cr6.eq) goto loc_822F9D90;
	// twi 31,r0,22
loc_822F9D90:
	// lwz r9,152(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 152);
	// cmpwi cr6,r9,-1
	cr6.compare<int32_t>(ctx.r9.s32, -1, xer);
	// bne cr6,0x822f9dac
	if (!cr6.eq) goto loc_822F9DAC;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9da8
	if (!cr6.eq) goto loc_822F9DA8;
	// twi 31,r0,22
loc_822F9DA8:
	// stw r28,152(r8)
	PPC_STORE_U32(ctx.r8.u32 + 152, r28.u32);
loc_822F9DAC:
	// lwz r10,4(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822f9dbc
	if (!cr6.eq) goto loc_822F9DBC;
	// twi 31,r0,22
loc_822F9DBC:
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// li r6,1
	ctx.r6.s64 = 1;
	// stw r28,148(r10)
	PPC_STORE_U32(ctx.r10.u32 + 148, r28.u32);
loc_822F9DC8:
	// lwz r9,4(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 4);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822f9dd8
	if (!cr6.eq) goto loc_822F9DD8;
	// twi 31,r0,22
loc_822F9DD8:
	// lwz r10,24(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 24);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// lwz r11,152(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 152);
	// subf r11,r11,r28
	r11.s64 = r28.s64 - r11.s64;
	// bne cr6,0x822f9df0
	if (!cr6.eq) goto loc_822F9DF0;
	// twi 31,r0,22
loc_822F9DF0:
	// lbz r10,144(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 144);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822f9e30
	if (!cr6.eq) goto loc_822F9E30;
	// cmplwi cr6,r11,35000
	cr6.compare<uint32_t>(r11.u32, 35000, xer);
	// blt cr6,0x822f9e30
	if (cr6.lt) goto loc_822F9E30;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822f9e1c
	if (!cr6.eq) goto loc_822F9E1C;
	// twi 31,r0,22
loc_822F9E1C:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,0(r29)
	ctx.r4.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// b 0x822f9e64
	goto loc_822F9E64;
loc_822F9E30:
	// clrlwi r11,r6,24
	r11.u64 = ctx.r6.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f9e64
	if (cr6.eq) goto loc_822F9E64;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822f9e4c
	if (!cr6.eq) goto loc_822F9E4C;
	// twi 31,r0,22
loc_822F9E4C:
	// stb r24,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r24.u8);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r6,0(r29)
	ctx.r6.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
loc_822F9E64:
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r30,92(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r27,88(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// b 0x822f9c80
	goto loc_822F9C80;
loc_822F9E7C:
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// addi r8,r1,112
	ctx.r8.s64 = ctx.r1.s64 + 112;
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r11.u32);
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// stw r8,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r8.u32);
	// ld r6,96(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x823010a8
	sub_823010A8(ctx, base);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c24
	return;
}

__attribute__((alias("__imp__sub_822F9EC0"))) PPC_WEAK_FUNC(sub_822F9EC0);
PPC_FUNC_IMPL(__imp__sub_822F9EC0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc8
	// stwu r1,-304(r1)
	ea = -304 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// lwz r11,412(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 412);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822fa0a4
	if (cr6.eq) goto loc_822FA0A4;
	// lwz r11,156(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822f9efc
	if (cr6.eq) goto loc_822F9EFC;
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// beq cr6,0x822f9efc
	if (cr6.eq) goto loc_822F9EFC;
	// bl 0x822fc840
	sub_822FC840(ctx, base);
loc_822F9EFC:
	// stw r31,32(r30)
	PPC_STORE_U32(r30.u32 + 32, r31.u32);
	// addi r23,r30,36
	r23.s64 = r30.s64 + 36;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// li r4,48
	ctx.r4.s64 = 48;
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// li r21,0
	r21.s64 = 0;
	// li r20,1
	r20.s64 = 1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822f9f30
	if (cr6.eq) goto loc_822F9F30;
	// stb r20,381(r30)
	PPC_STORE_U8(r30.u32 + 381, r20.u8);
	// b 0x822f9f34
	goto loc_822F9F34;
loc_822F9F30:
	// stb r21,417(r30)
	PPC_STORE_U8(r30.u32 + 417, r21.u8);
loc_822F9F34:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fc320
	sub_822FC320(ctx, base);
	// lwz r11,372(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 372);
	// addi r25,r30,368
	r25.s64 = r30.s64 + 368;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r27,r25
	r27.u64 = r25.u64;
	// li r22,-1
	r22.s64 = -1;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// li r24,12
	r24.s64 = 12;
	// addi r26,r10,28928
	r26.s64 = ctx.r10.s64 + 28928;
	// lwz r29,0(r11)
	r29.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r28,r11,28940
	r28.s64 = r11.s64 + 28940;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
loc_822F9F6C:
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822f9f80
	if (cr6.eq) goto loc_822F9F80;
	// cmplw cr6,r27,r25
	cr6.compare<uint32_t>(r27.u32, r25.u32, xer);
	// beq cr6,0x822f9f84
	if (cr6.eq) goto loc_822F9F84;
loc_822F9F80:
	// twi 31,r0,22
loc_822F9F84:
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// beq cr6,0x822fa0a4
	if (cr6.eq) goto loc_822FA0A4;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x822f9f98
	if (!cr6.eq) goto loc_822F9F98;
	// twi 31,r0,22
loc_822F9F98:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x822f9fa8
	if (!cr6.eq) goto loc_822F9FA8;
	// twi 31,r0,22
loc_822F9FA8:
	// lwz r31,24(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// lwz r11,32(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fa000
	if (!cr6.eq) goto loc_822FA000;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fa000
	if (cr6.eq) goto loc_822FA000;
	// ld r11,120(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 120);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// stw r21,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r21.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r31,r31,120
	r31.s64 = r31.s64 + 120;
	// std r11,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, r11.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stw r28,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r28.u32);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// ld r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x823006e0
	sub_823006E0(ctx, base);
	// b 0x822fa048
	goto loc_822FA048;
loc_822FA000:
	// ld r11,120(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 120);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// stw r20,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r20.u32);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, r11.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// addi r31,r31,108
	r31.s64 = r31.s64 + 108;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r10,-1
	cr6.compare<int32_t>(ctx.r10.s32, -1, xer);
	// beq cr6,0x822fa048
	if (cr6.eq) goto loc_822FA048;
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// stw r26,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r26.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r11.u32);
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x825f7b10
	sub_825F7B10(ctx, base);
	// stw r22,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r22.u32);
loc_822FA048:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// cmplw cr6,r29,r11
	cr6.compare<uint32_t>(r29.u32, r11.u32, xer);
	// bne cr6,0x822fa058
	if (!cr6.eq) goto loc_822FA058;
	// twi 31,r0,22
loc_822FA058:
	// lwz r11,24(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 24);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r31,100(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 100);
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r10,32(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 32);
	// mr r5,r23
	ctx.r5.u64 = r23.u64;
	// stb r24,144(r1)
	PPC_STORE_U8(ctx.r1.u32 + 144, r24.u8);
	// li r4,48
	ctx.r4.s64 = 48;
	// addi r3,r1,145
	ctx.r3.s64 = ctx.r1.s64 + 145;
	// stw r10,193(r1)
	PPC_STORE_U32(ctx.r1.u32 + 193, ctx.r10.u32);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// li r5,53
	ctx.r5.s64 = 53;
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
	// lwz r29,84(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r27,80(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822f9f6c
	goto loc_822F9F6C;
loc_822FA0A4:
	// addi r1,r1,304
	ctx.r1.s64 = ctx.r1.s64 + 304;
	// b 0x82ca2c18
	return;
}

__attribute__((alias("__imp__sub_822FA0B0"))) PPC_WEAK_FUNC(sub_822FA0B0);
PPC_FUNC_IMPL(__imp__sub_822FA0B0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r4,0
	ctx.r4.s64 = 0;
	// li r5,1
	ctx.r5.s64 = 1;
	// ori r4,r4,32778
	ctx.r4.u64 = ctx.r4.u64 | 32778;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// lis r4,0
	ctx.r4.s64 = 0;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r5,1
	ctx.r5.s64 = 1;
	// ori r4,r4,32779
	ctx.r4.u64 = ctx.r4.u64 | 32779;
	// bl 0x82cbc0a8
	sub_82CBC0A8(ctx, base);
	// lis r4,4096
	ctx.r4.s64 = 268435456;
	// addi r6,r31,32
	ctx.r6.s64 = r31.s64 + 32;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r5,4
	ctx.r5.s64 = 4;
	// ori r4,r4,10
	ctx.r4.u64 = ctx.r4.u64 | 10;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lis r4,20480
	ctx.r4.s64 = 1342177280;
	// addi r6,r31,84
	ctx.r6.s64 = r31.s64 + 84;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r5,4
	ctx.r5.s64 = 4;
	// ori r4,r4,11
	ctx.r4.u64 = ctx.r4.u64 | 11;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lis r4,20480
	ctx.r4.s64 = 1342177280;
	// addi r6,r31,88
	ctx.r6.s64 = r31.s64 + 88;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r5,4
	ctx.r5.s64 = 4;
	// ori r4,r4,12
	ctx.r4.u64 = ctx.r4.u64 | 12;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lis r4,8192
	ctx.r4.s64 = 536870912;
	// addi r6,r31,264
	ctx.r6.s64 = r31.s64 + 264;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r5,8
	ctx.r5.s64 = 8;
	// ori r4,r4,17
	ctx.r4.u64 = ctx.r4.u64 | 17;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lis r4,4096
	ctx.r4.s64 = 268435456;
	// addi r6,r31,388
	ctx.r6.s64 = r31.s64 + 388;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r5,4
	ctx.r5.s64 = 4;
	// ori r4,r4,24
	ctx.r4.u64 = ctx.r4.u64 | 24;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82cf9a88
	sub_82CF9A88(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	cr6.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq cr6,0x822fa18c
	if (cr6.eq) goto loc_822FA18C;
	// li r4,13
	ctx.r4.s64 = 13;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
loc_822FA18C:
	// lbz r11,297(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 297);
	// lis r4,4096
	ctx.r4.s64 = 268435456;
	// lbz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// addi r6,r31,168
	ctx.r6.s64 = r31.s64 + 168;
	// li r5,4
	ctx.r5.s64 = 4;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// divw r8,r9,r11
	ctx.r8.s32 = ctx.r9.s32 / r11.s32;
	// ori r4,r4,18
	ctx.r4.u64 = ctx.r4.u64 | 18;
	// mullw r7,r8,r11
	ctx.r7.s64 = int64_t(ctx.r8.s32) * int64_t(r11.s32);
	// subf r11,r7,r9
	r11.s64 = ctx.r9.s64 - ctx.r7.s64;
	// stw r11,168(r31)
	PPC_STORE_U32(r31.u32 + 168, r11.u32);
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lwz r3,252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 252);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x822fa1d4
	if (cr6.eq) goto loc_822FA1D4;
	// bl 0x82cbbf60
	sub_82CBBF60(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// stw r11,252(r31)
	PPC_STORE_U32(r31.u32 + 252, r11.u32);
loc_822FA1D4:
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// lwz r11,8(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x822fa28c
	if (!cr6.eq) goto loc_822FA28C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7488
	ctx.r3.s64 = r11.s64 + 7488;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// std r30,128(r31)
	PPC_STORE_U64(r31.u32 + 128, r30.u64);
	// std r30,136(r31)
	PPC_STORE_U64(r31.u32 + 136, r30.u64);
	// std r30,144(r31)
	PPC_STORE_U64(r31.u32 + 144, r30.u64);
	// stw r30,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r30.u32);
	// lwz r29,256(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// li r4,815
	ctx.r4.s64 = 815;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82339c20
	sub_82339C20(ctx, base);
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// lwz r11,26912(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26912);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fa234
	if (cr6.eq) goto loc_822FA234;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lbz r30,26821(r10)
	r30.u64 = PPC_LOAD_U8(ctx.r10.u32 + 26821);
loc_822FA234:
	// clrlwi r10,r30,24
	ctx.r10.u64 = r30.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fa25c
	if (cr6.eq) goto loc_822FA25C;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lwz r11,140(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 140);
	// lbz r10,243(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 243);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fa25c
	if (cr6.eq) goto loc_822FA25C;
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r10,245(r11)
	PPC_STORE_U8(r11.u32 + 245, ctx.r10.u8);
loc_822FA25C:
	// cmplwi cr6,r5,997
	cr6.compare<uint32_t>(ctx.r5.u32, 997, xer);
	// beq cr6,0x822fa278
	if (cr6.eq) goto loc_822FA278;
	// li r4,14
	ctx.r4.s64 = 14;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
loc_822FA278:
	// li r11,15
	r11.s64 = 15;
	// stw r11,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r11.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r3.u32);
	// stw r3,324(r31)
	PPC_STORE_U32(r31.u32 + 324, ctx.r3.u32);
loc_822FA28C:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fc320
	sub_822FC320(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FA2A0"))) PPC_WEAK_FUNC(sub_822FA2A0);
PPC_FUNC_IMPL(__imp__sub_822FA2A0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,412(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 412);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822fa2d4
	if (!cr6.eq) goto loc_822FA2D4;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7580
	ctx.r3.s64 = r11.s64 + 7580;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// b 0x822fa408
	goto loc_822FA408;
loc_822FA2D4:
	// lbz r11,417(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 417);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fa2ec
	if (cr6.eq) goto loc_822FA2EC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7640
	ctx.r3.s64 = r11.s64 + 7640;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FA2EC:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fd920
	sub_822FD920(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fa408
	if (cr6.eq) goto loc_822FA408;
	// lis r10,-31950
	ctx.r10.s64 = -2093875200;
	// li r11,-1
	r11.s64 = -1;
	// addi r30,r10,-27328
	r30.s64 = ctx.r10.s64 + -27328;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// li r5,100
	ctx.r5.s64 = 100;
	// stw r11,-27328(r10)
	PPC_STORE_U32(ctx.r10.u32 + -27328, r11.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// bl 0x82cf9f20
	sub_82CF9F20(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822fa348
	if (cr6.eq) goto loc_822FA348;
	// li r4,16
	ctx.r4.s64 = 16;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822fa408
	goto loc_822FA408;
loc_822FA348:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// stw r3,288(r31)
	PPC_STORE_U32(r31.u32 + 288, ctx.r3.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7712
	ctx.r3.s64 = r11.s64 + 7712;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// addi r7,r31,128
	ctx.r7.s64 = r31.s64 + 128;
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// std r11,128(r31)
	PPC_STORE_U64(r31.u32 + 128, r11.u64);
	// li r6,0
	ctx.r6.s64 = 0;
	// std r11,136(r31)
	PPC_STORE_U64(r31.u32 + 136, r11.u64);
	// std r11,144(r31)
	PPC_STORE_U64(r31.u32 + 144, r11.u64);
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// lwz r4,288(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x82cbcfa8
	sub_82CBCFA8(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r10,-1
	ctx.r10.s64 = -1;
	// cmplwi cr6,r5,997
	cr6.compare<uint32_t>(ctx.r5.u32, 997, xer);
	// stw r10,292(r31)
	PPC_STORE_U32(r31.u32 + 292, ctx.r10.u32);
	// beq cr6,0x822fa3c8
	if (cr6.eq) goto loc_822FA3C8;
	// cmplwi cr6,r5,0
	cr6.compare<uint32_t>(ctx.r5.u32, 0, xer);
	// beq cr6,0x822fa3c8
	if (cr6.eq) goto loc_822FA3C8;
	// li r4,17
	ctx.r4.s64 = 17;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// lwz r3,0(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// bl 0x82cbbf60
	sub_82CBBF60(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// stw r11,0(r30)
	PPC_STORE_U32(r30.u32 + 0, r11.u32);
	// b 0x822fa408
	goto loc_822FA408;
loc_822FA3C8:
	// li r11,18
	r11.s64 = 18;
	// stw r11,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r11.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// stw r3,404(r31)
	PPC_STORE_U32(r31.u32 + 404, ctx.r3.u32);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x822fa408
	if (cr6.eq) goto loc_822FA408;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822fa400
	if (!cr6.eq) goto loc_822FA400;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822fa408
	goto loc_822FA408;
loc_822FA400:
	// li r11,4
	r11.s64 = 4;
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
loc_822FA408:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FA420"))) PPC_WEAK_FUNC(sub_822FA420);
PPC_FUNC_IMPL(__imp__sub_822FA420) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,7804
	ctx.r3.s64 = r11.s64 + 7804;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fa540
	if (cr6.eq) goto loc_822FA540;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7840
	ctx.r3.s64 = r11.s64 + 7840;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// li r29,0
	r29.s64 = 0;
	// lwz r4,0(r10)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r4,1
	cr6.compare<uint32_t>(ctx.r4.u32, 1, xer);
	// blt cr6,0x822fa534
	if (cr6.lt) goto loc_822FA534;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7872
	ctx.r3.s64 = r11.s64 + 7872;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r5,4(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// bl 0x822fd338
	sub_822FD338(ctx, base);
	// stw r29,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r29.u32);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r30,80(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// addi r10,r10,536
	ctx.r10.s64 = ctx.r10.s64 + 536;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
loc_822FA4B4:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x822fa4d4
	if (!cr0.eq) goto loc_822FA4D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bne cr6,0x822fa4b4
	if (!cr6.eq) goto loc_822FA4B4;
loc_822FA4D4:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x822fa534
	if (cr6.eq) goto loc_822FA534;
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fa534
	if (!cr6.eq) goto loc_822FA534;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7924
	ctx.r3.s64 = r11.s64 + 7924;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lwz r9,292(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 292);
	// addi r3,r10,7956
	ctx.r3.s64 = ctx.r10.s64 + 7956;
	// lwz r11,288(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// mulli r10,r9,196
	ctx.r10.s64 = ctx.r9.s64 * 196;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// addi r4,r11,8
	ctx.r4.s64 = r11.s64 + 8;
	// ld r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fce40
	sub_822FCE40(ctx, base);
	// lbz r11,328(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 328);
	// addi r8,r11,1
	ctx.r8.s64 = r11.s64 + 1;
	// stb r8,328(r31)
	PPC_STORE_U8(r31.u32 + 328, ctx.r8.u8);
loc_822FA534:
	// lwz r3,300(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 300);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// stw r29,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r29.u32);
loc_822FA540:
	// lwz r11,292(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 292);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x822fa5ec
	if (!cr6.gt) goto loc_822FA5EC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,7988
	ctx.r3.s64 = r11.s64 + 7988;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,292(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 292);
	// lwz r10,288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// lis r8,19795
	ctx.r8.s64 = 1297285120;
	// mulli r9,r11,196
	ctx.r9.s64 = r11.s64 * 196;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r7,r11,-1
	ctx.r7.s64 = r11.s64 + -1;
	// ori r6,r8,2033
	ctx.r6.u64 = ctx.r8.u64 | 2033;
	// stw r7,292(r31)
	PPC_STORE_U32(r31.u32 + 292, ctx.r7.u32);
	// addi r30,r10,-196
	r30.s64 = ctx.r10.s64 + -196;
	// lwz r5,-160(r10)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r10.u32 + -160);
	// cmplw cr6,r5,r6
	cr6.compare<uint32_t>(ctx.r5.u32, ctx.r6.u32, xer);
	// bne cr6,0x822fa5ec
	if (!cr6.eq) goto loc_822FA5EC;
	// lwz r11,24(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// clrlwi r10,r11,31
	ctx.r10.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fa5ec
	if (cr6.eq) goto loc_822FA5EC;
	// rlwinm r11,r11,0,27,27
	r11.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fa5ec
	if (cr6.eq) goto loc_822FA5EC;
	// addi r29,r31,368
	r29.s64 = r31.s64 + 368;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,372(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fa5d0
	if (cr6.eq) goto loc_822FA5D0;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// beq cr6,0x822fa5d4
	if (cr6.eq) goto loc_822FA5D4;
loc_822FA5D0:
	// twi 31,r0,22
loc_822FA5D4:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fa5ec
	if (!cr6.eq) goto loc_822FA5EC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// ld r4,28(r30)
	ctx.r4.u64 = PPC_LOAD_U64(r30.u32 + 28);
	// bl 0x822fa920
	sub_822FA920(ctx, base);
loc_822FA5EC:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FA5F8"))) PPC_WEAK_FUNC(sub_822FA5F8);
PPC_FUNC_IMPL(__imp__sub_822FA5F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bcc
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,384(r31)
	PPC_STORE_U32(r31.u32 + 384, ctx.r3.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lbz r4,380(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 380);
	// addi r3,r11,8008
	ctx.r3.s64 = r11.s64 + 8008;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r30,r31,128
	r30.s64 = r31.s64 + 128;
	// addi r24,r11,8028
	r24.s64 = r11.s64 + 8028;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// lwz r11,128(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// addi r10,r11,-997
	ctx.r10.s64 = r11.s64 + -997;
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// xori r7,r8,1
	ctx.r7.u64 = ctx.r8.u64 ^ 1;
	// clrlwi r29,r7,24
	r29.u64 = ctx.r7.u32 & 0xFF;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822fa914
	if (cr6.eq) goto loc_822FA914;
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fa678
	if (!cr6.eq) goto loc_822FA678;
loc_822FA664:
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcd40
	sub_822FCD40(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c1c
	return;
loc_822FA678:
	// lwz r11,412(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 412);
	// cmpwi cr6,r11,2
	cr6.compare<int32_t>(r11.s32, 2, xer);
	// bne cr6,0x822fa664
	if (!cr6.eq) goto loc_822FA664;
	// li r29,0
	r29.s64 = 0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r29,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r29.u32);
	// addi r3,r11,8056
	ctx.r3.s64 = r11.s64 + 8056;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r4,256(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r29.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,50
	ctx.r5.s64 = 50;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82cf96a0
	sub_82CF96A0(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// addi r25,r11,8084
	r25.s64 = r11.s64 + 8084;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmplwi cr6,r28,122
	cr6.compare<uint32_t>(r28.u32, 122, xer);
	// bne cr6,0x822fa904
	if (!cr6.eq) goto loc_822FA904;
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fa904
	if (cr6.eq) goto loc_822FA904;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822fa714
	if (!cr6.eq) goto loc_822FA714;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,22
	ctx.r4.s64 = 22;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c1c
	return;
loc_822FA714:
	// lbz r11,296(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 296);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// addi r11,r11,76
	r11.s64 = r11.s64 + 76;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r28,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + r31.u32, r28.u32);
	// lwz r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lis r8,-31924
	ctx.r8.s64 = -2092171264;
	// lis r11,4096
	r11.s64 = 268435456;
	// addi r23,r8,8512
	r23.s64 = ctx.r8.s64 + 8512;
	// ori r11,r11,10
	r11.u64 = r11.u64 | 10;
	// li r6,5
	ctx.r6.s64 = 5;
	// li r4,5
	ctx.r4.s64 = 5;
	// stw r11,8512(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8512, r11.u32);
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// lis r9,20480
	ctx.r9.s64 = 1342177280;
	// stb r4,80(r23)
	PPC_STORE_U8(r23.u32 + 80, ctx.r4.u8);
	// lis r4,-32256
	ctx.r4.s64 = -2113929216;
	// stb r6,56(r23)
	PPC_STORE_U8(r23.u32 + 56, ctx.r6.u8);
	// lis r6,-32256
	ctx.r6.s64 = -2113929216;
	// ori r9,r9,15
	ctx.r9.u64 = ctx.r9.u64 | 15;
	// stw r11,16(r23)
	PPC_STORE_U32(r23.u32 + 16, r11.u32);
	// lis r7,20480
	ctx.r7.s64 = 1342177280;
	// lfs f13,3040(r4)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r4.u32 + 3040);
	ctx.f13.f64 = double(temp.f32);
	// lis r5,20480
	ctx.r5.s64 = 1342177280;
	// lfs f0,3088(r6)
	temp.u32 = PPC_LOAD_U32(ctx.r6.u32 + 3088);
	f0.f64 = double(temp.f32);
	// lis r22,-31924
	r22.s64 = -2092171264;
	// stw r9,24(r23)
	PPC_STORE_U32(r23.u32 + 24, ctx.r9.u32);
	// lis r3,20480
	ctx.r3.s64 = 1342177280;
	// stfs f0,40(r23)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r23.u32 + 40, temp.u32);
	// lis r26,4096
	r26.s64 = 268435456;
	// stfs f0,88(r23)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(r23.u32 + 88, temp.u32);
	// ori r7,r7,13
	ctx.r7.u64 = ctx.r7.u64 | 13;
	// stfs f13,64(r23)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r23.u32 + 64, temp.u32);
	// ori r5,r5,16
	ctx.r5.u64 = ctx.r5.u64 | 16;
	// stfs f13,112(r23)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(r23.u32 + 112, temp.u32);
	// li r8,5
	ctx.r8.s64 = 5;
	// stw r7,48(r23)
	PPC_STORE_U32(r23.u32 + 48, ctx.r7.u32);
	// li r27,5
	r27.s64 = 5;
	// stw r5,72(r23)
	PPC_STORE_U32(r23.u32 + 72, ctx.r5.u32);
	// addi r21,r22,8492
	r21.s64 = r22.s64 + 8492;
	// stb r8,32(r23)
	PPC_STORE_U8(r23.u32 + 32, ctx.r8.u8);
	// lis r9,4096
	ctx.r9.s64 = 268435456;
	// stb r27,104(r23)
	PPC_STORE_U8(r23.u32 + 104, r27.u8);
	// lis r6,0
	ctx.r6.s64 = 0;
	// lis r4,0
	ctx.r4.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// ori r3,r3,14
	ctx.r3.u64 = ctx.r3.u64 | 14;
	// ori r26,r26,18
	r26.u64 = r26.u64 | 18;
	// stb r10,8(r23)
	PPC_STORE_U8(r23.u32 + 8, ctx.r10.u8);
	// ori r4,r4,32779
	ctx.r4.u64 = ctx.r4.u64 | 32779;
	// stw r3,96(r23)
	PPC_STORE_U32(r23.u32 + 96, ctx.r3.u32);
	// ori r9,r9,24
	ctx.r9.u64 = ctx.r9.u64 | 24;
	// stw r26,120(r23)
	PPC_STORE_U32(r23.u32 + 120, r26.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// lbz r11,296(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 296);
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r4,8(r21)
	PPC_STORE_U32(r21.u32 + 8, ctx.r4.u32);
	// ori r6,r6,32778
	ctx.r6.u64 = ctx.r6.u64 | 32778;
	// stb r10,128(r23)
	PPC_STORE_U8(r23.u32 + 128, ctx.r10.u8);
	// li r5,1
	ctx.r5.s64 = 1;
	// stw r9,144(r23)
	PPC_STORE_U32(r23.u32 + 144, ctx.r9.u32);
	// li r27,1
	r27.s64 = 1;
	// stw r8,160(r23)
	PPC_STORE_U32(r23.u32 + 160, ctx.r8.u32);
	// stw r11,136(r23)
	PPC_STORE_U32(r23.u32 + 136, r11.u32);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stb r7,152(r23)
	PPC_STORE_U8(r23.u32 + 152, ctx.r7.u8);
	// stw r6,8492(r22)
	PPC_STORE_U32(r22.u32 + 8492, ctx.r6.u32);
	// stw r5,4(r21)
	PPC_STORE_U32(r21.u32 + 4, ctx.r5.u32);
	// stw r27,12(r21)
	PPC_STORE_U32(r21.u32 + 12, r27.u32);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// addi r11,r11,-997
	r11.s64 = r11.s64 + -997;
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 ^ 1;
	// clrlwi r4,r8,24
	ctx.r4.u64 = ctx.r8.u32 & 0xFF;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// addi r3,r7,8096
	ctx.r3.s64 = ctx.r7.s64 + 8096;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// std r29,0(r30)
	PPC_STORE_U64(r30.u32 + 0, r29.u64);
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// std r29,8(r30)
	PPC_STORE_U64(r30.u32 + 8, r29.u64);
	// std r29,16(r30)
	PPC_STORE_U64(r30.u32 + 16, r29.u64);
	// stw r29,24(r30)
	PPC_STORE_U32(r30.u32 + 24, r29.u32);
	// addi r3,r6,8192
	ctx.r3.s64 = ctx.r6.s64 + 8192;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r9,r21
	ctx.r9.u64 = r21.u64;
	// mr r8,r23
	ctx.r8.u64 = r23.u64;
	// lwz r4,256(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r30.u32);
	// li r7,2
	ctx.r7.s64 = 2;
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r28.u32);
	// li r6,7
	ctx.r6.s64 = 7;
	// li r5,50
	ctx.r5.s64 = 50;
	// li r3,1
	ctx.r3.s64 = 1;
	// bl 0x82cf96a0
	sub_82CF96A0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmplwi cr6,r30,997
	cr6.compare<uint32_t>(r30.u32, 997, xer);
	// beq cr6,0x822fa8d4
	if (cr6.eq) goto loc_822FA8D4;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822fa8d4
	if (cr6.eq) goto loc_822FA8D4;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// li r4,23
	ctx.r4.s64 = 23;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c1c
	return;
loc_822FA8D4:
	// li r11,24
	r11.s64 = 24;
	// li r4,5
	ctx.r4.s64 = 5;
	// stw r11,164(r31)
	PPC_STORE_U32(r31.u32 + 164, r11.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcd40
	sub_822FCD40(ctx, base);
	// lbz r11,296(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 296);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// stb r10,380(r31)
	PPC_STORE_U8(r31.u32 + 380, ctx.r10.u8);
	// stb r9,296(r31)
	PPC_STORE_U8(r31.u32 + 296, ctx.r9.u8);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c1c
	return;
loc_822FA904:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// li r4,21
	ctx.r4.s64 = 21;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
loc_822FA914:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c1c
	return;
}

__attribute__((alias("__imp__sub_822FA920"))) PPC_WEAK_FUNC(sub_822FA920);
PPC_FUNC_IMPL(__imp__sub_822FA920) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r3,r11,8212
	ctx.r3.s64 = r11.s64 + 8212;
	// std r30,152(r1)
	PPC_STORE_U64(ctx.r1.u32 + 152, r30.u64);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lbz r4,380(r31)
	ctx.r4.u64 = PPC_LOAD_U8(r31.u32 + 380);
	// addi r3,r10,8008
	ctx.r3.s64 = ctx.r10.s64 + 8008;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,128(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r8,r11,-997
	ctx.r8.s64 = r11.s64 + -997;
	// addi r3,r9,8028
	ctx.r3.s64 = ctx.r9.s64 + 8028;
	// cntlzw r7,r8
	ctx.r7.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// addi r29,r31,128
	r29.s64 = r31.s64 + 128;
	// rlwinm r6,r7,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// xori r5,r6,1
	ctx.r5.u64 = ctx.r6.u64 ^ 1;
	// clrlwi r28,r5,24
	r28.u64 = ctx.r5.u32 & 0xFF;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822faa28
	if (cr6.eq) goto loc_822FAA28;
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r11,r1,152
	r11.s64 = ctx.r1.s64 + 152;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// addi r8,r11,8
	ctx.r8.s64 = r11.s64 + 8;
	// std r7,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r7.u64);
loc_822FA9A0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822fa9c0
	if (!cr0.eq) goto loc_822FA9C0;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822fa9a0
	if (!cr6.eq) goto loc_822FA9A0;
loc_822FA9C0:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x822faa28
	if (cr6.eq) goto loc_822FAA28;
	// stw r7,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r7.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r4,256(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cf9780
	sub_82CF9780(ctx, base);
	// cmplwi cr6,r3,122
	cr6.compare<uint32_t>(ctx.r3.u32, 122, xer);
	// bne cr6,0x822faa28
	if (!cr6.eq) goto loc_822FAA28;
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822faa28
	if (cr6.eq) goto loc_822FAA28;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// stw r28,300(r31)
	PPC_STORE_U32(r31.u32 + 300, r28.u32);
	// mr r7,r29
	ctx.r7.u64 = r29.u64;
	// lwz r4,256(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cf9780
	sub_82CF9780(ctx, base);
loc_822FAA28:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822FAA30"))) PPC_WEAK_FUNC(sub_822FAA30);
PPC_FUNC_IMPL(__imp__sub_822FAA30) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bc0
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r11,8240
	ctx.r3.s64 = r11.s64 + 8240;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r27,0
	r27.s64 = 0;
	// lbz r10,297(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 297);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r27,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, r27.u32);
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// stb r27,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r27.u8);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// beq cr6,0x822faaa4
	if (cr6.eq) goto loc_822FAAA4;
loc_822FAA74:
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x822ff858
	sub_822FF858(ctx, base);
	// lbz r11,80(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// lbz r10,297(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 297);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// clrlwi r11,r9,24
	r11.u64 = ctx.r9.u32 & 0xFF;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822faa74
	if (cr6.lt) goto loc_822FAA74;
	// lwz r4,120(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_822FAAA4:
	// lis r11,20971
	r11.s64 = 1374355456;
	// lbz r10,108(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 108);
	// ori r9,r11,34079
	ctx.r9.u64 = r11.u64 | 34079;
	// mulhwu r8,r10,r9
	ctx.r8.u64 = (uint64_t(ctx.r10.u32) * uint64_t(ctx.r9.u32)) >> 32;
	// rlwinm r7,r8,28,4,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xFFFFFFF;
	// mulli r6,r7,50
	ctx.r6.s64 = ctx.r7.s64 * 50;
	// subf. r5,r6,r10
	ctx.r5.s64 = ctx.r10.s64 - ctx.r6.s64;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// beq 0x822faaec
	if (cr0.eq) goto loc_822FAAEC;
	// cmplw cr6,r3,r4
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, xer);
	// ble cr6,0x822faad8
	if (!cr6.gt) goto loc_822FAAD8;
	// twi 31,r0,22
	// twi 31,r0,22
	// cmplw cr6,r3,r4
	cr6.compare<uint32_t>(ctx.r3.u32, ctx.r4.u32, xer);
loc_822FAAD8:
	// beq cr6,0x822faaec
	if (cr6.eq) goto loc_822FAAEC;
	// li r5,0
	ctx.r5.s64 = 0;
	// bl 0x82301c88
	sub_82301C88(ctx, base);
	// lwz r4,120(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_822FAAEC:
	// lbz r11,297(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 297);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822facdc
	if (cr6.eq) goto loc_822FACDC;
	// lis r6,8192
	ctx.r6.s64 = 536870912;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lis r7,-32246
	ctx.r7.s64 = -2113273856;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r22,r27
	r22.u64 = r27.u64;
	// ori r23,r6,17
	r23.u64 = ctx.r6.u64 | 17;
	// addi r19,r10,8392
	r19.s64 = ctx.r10.s64 + 8392;
	// addi r21,r9,8332
	r21.s64 = ctx.r9.s64 + 8332;
	// addi r25,r8,536
	r25.s64 = ctx.r8.s64 + 536;
	// addi r18,r7,8296
	r18.s64 = ctx.r7.s64 + 8296;
	// addi r20,r11,8276
	r20.s64 = r11.s64 + 8276;
loc_822FAB2C:
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fab40
	if (cr6.eq) goto loc_822FAB40;
	// subf r11,r3,r4
	r11.s64 = ctx.r4.s64 - ctx.r3.s64;
	// cmplw cr6,r22,r11
	cr6.compare<uint32_t>(r22.u32, r11.u32, xer);
	// blt cr6,0x822fab44
	if (cr6.lt) goto loc_822FAB44;
loc_822FAB40:
	// twi 31,r0,22
loc_822FAB44:
	// lbzx r11,r3,r22
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + r22.u32);
	// addi r10,r11,76
	ctx.r10.s64 = r11.s64 + 76;
	// rlwinm r24,r10,2,0,29
	r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r28,r24,r30
	r28.u64 = PPC_LOAD_U32(r24.u32 + r30.u32);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822facc4
	if (cr6.eq) goto loc_822FACC4;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lwz r4,0(r28)
	ctx.r4.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r26,r27
	r26.u64 = r27.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x822facb0
	if (!cr6.gt) goto loc_822FACB0;
	// mr r29,r27
	r29.u64 = r27.u64;
loc_822FAB7C:
	// lbz r11,328(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 328);
	// lbz r10,108(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 108);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x822facb0
	if (!cr6.lt) goto loc_822FACB0;
	// lbz r11,109(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 109);
	// lwz r10,352(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 352);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x822facb0
	if (!cr6.lt) goto loc_822FACB0;
	// lwz r8,4(r28)
	ctx.r8.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r9,r27
	ctx.r9.u64 = r27.u64;
	// addi r10,r8,76
	ctx.r10.s64 = ctx.r8.s64 + 76;
	// lwz r11,76(r8)
	r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 76);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// ble cr6,0x822fabe0
	if (!cr6.gt) goto loc_822FABE0;
	// add r11,r29,r8
	r11.u64 = r29.u64 + ctx.r8.u64;
	// lwz r11,84(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 84);
loc_822FABBC:
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r7,r23
	cr6.compare<uint32_t>(ctx.r7.u32, r23.u32, xer);
	// beq cr6,0x822fabe0
	if (cr6.eq) goto loc_822FABE0;
	// addi r10,r10,92
	ctx.r10.s64 = ctx.r10.s64 + 92;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r11,r11,24
	r11.s64 = r11.s64 + 24;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r7
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r7.u32, xer);
	// blt cr6,0x822fabbc
	if (cr6.lt) goto loc_822FABBC;
loc_822FABE0:
	// add r31,r29,r8
	r31.u64 = r29.u64 + ctx.r8.u64;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r31,8
	ctx.r3.s64 = r31.s64 + 8;
	// bl 0x82cf9af8
	sub_82CF9AF8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bne cr6,0x822fac94
	if (!cr6.eq) goto loc_822FAC94;
	// ld r11,176(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 176);
	// ld r10,96(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// cmpld cr6,r10,r11
	cr6.compare<uint64_t>(ctx.r10.u64, r11.u64, xer);
	// beq cr6,0x822fac94
	if (cr6.eq) goto loc_822FAC94;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,84
	ctx.r3.s64 = ctx.r1.s64 + 84;
	// bl 0x822fd338
	sub_822FD338(ctx, base);
	// stw r27,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r27.u32);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r27.u32);
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r31,84(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// addi r9,r11,4
	ctx.r9.s64 = r11.s64 + 4;
loc_822FAC40:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r8,r7,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x822fac60
	if (!cr0.eq) goto loc_822FAC60;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r9
	cr6.compare<int32_t>(r11.s32, ctx.r9.s32, xer);
	// bne cr6,0x822fac40
	if (!cr6.eq) goto loc_822FAC40;
loc_822FAC60:
	// cmpwi cr6,r8,0
	cr6.compare<int32_t>(ctx.r8.s32, 0, xer);
	// beq cr6,0x822fac8c
	if (cr6.eq) goto loc_822FAC8C;
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fac8c
	if (!cr6.eq) goto loc_822FAC8C;
	// li r5,0
	ctx.r5.s64 = 0;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fce40
	sub_822FCE40(ctx, base);
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// b 0x822fac98
	goto loc_822FAC98;
loc_822FAC8C:
	// mr r3,r19
	ctx.r3.u64 = r19.u64;
	// b 0x822fac98
	goto loc_822FAC98;
loc_822FAC94:
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
loc_822FAC98:
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// addi r26,r26,1
	r26.s64 = r26.s64 + 1;
	// addi r29,r29,92
	r29.s64 = r29.s64 + 92;
	// cmplw cr6,r26,r11
	cr6.compare<uint32_t>(r26.u32, r11.u32, xer);
	// blt cr6,0x822fab7c
	if (cr6.lt) goto loc_822FAB7C;
loc_822FACB0:
	// lwzx r3,r24,r30
	ctx.r3.u64 = PPC_LOAD_U32(r24.u32 + r30.u32);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r4,120(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stwx r27,r24,r30
	PPC_STORE_U32(r24.u32 + r30.u32, r27.u32);
loc_822FACC4:
	// addi r11,r22,1
	r11.s64 = r22.s64 + 1;
	// lbz r10,297(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 297);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r22,r11
	r22.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822fab2c
	if (cr6.lt) goto loc_822FAB2C;
loc_822FACDC:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,8412
	ctx.r3.s64 = r11.s64 + 8412;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r11,156(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// stw r3,116(r30)
	PPC_STORE_U32(r30.u32 + 116, ctx.r3.u32);
	// cmpwi cr6,r11,6
	cr6.compare<int32_t>(r11.s32, 6, xer);
	// beq cr6,0x822fad20
	if (cr6.eq) goto loc_822FAD20;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822fad18
	if (!cr6.eq) goto loc_822FAD18;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// b 0x822fad20
	goto loc_822FAD20;
loc_822FAD18:
	// li r11,6
	r11.s64 = 6;
	// stw r11,156(r30)
	PPC_STORE_U32(r30.u32 + 156, r11.u32);
loc_822FAD20:
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fad30
	if (cr6.eq) goto loc_822FAD30;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822FAD30:
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x82ca2c10
	return;
}

__attribute__((alias("__imp__sub_822FAD38"))) PPC_WEAK_FUNC(sub_822FAD38);
PPC_FUNC_IMPL(__imp__sub_822FAD38) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r14{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister f31{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb0
	// stfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -160, f31.u64);
	// stwu r1,-3488(r1)
	ea = -3488 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,16
	r11.s64 = 16;
	// lis r24,-32246
	r24.s64 = -2113273856;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, r11.u32);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lis r23,-32246
	r23.s64 = -2113273856;
	// lis r21,-32246
	r21.s64 = -2113273856;
	// lis r30,-32246
	r30.s64 = -2113273856;
	// lis r6,-32246
	ctx.r6.s64 = -2113273856;
	// lfs f31,-27468(r11)
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27468);
	f31.f64 = double(temp.f32);
	// addi r9,r24,8744
	ctx.r9.s64 = r24.s64 + 8744;
	// addi r5,r23,8656
	ctx.r5.s64 = r23.s64 + 8656;
	// addi r11,r21,8604
	r11.s64 = r21.s64 + 8604;
	// stw r9,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r9.u32);
	// addi r30,r30,9004
	r30.s64 = r30.s64 + 9004;
	// stw r5,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r5.u32);
	// addi r6,r6,5664
	ctx.r6.s64 = ctx.r6.s64 + 5664;
	// stw r11,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, r11.u32);
	// lis r29,-32246
	r29.s64 = -2113273856;
	// stw r30,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, r30.u32);
	// lis r28,-32246
	r28.s64 = -2113273856;
	// stw r6,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r6.u32);
	// lis r27,-32246
	r27.s64 = -2113273856;
	// lis r7,-32256
	ctx.r7.s64 = -2113929216;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r22,-32246
	r22.s64 = -2113273856;
	// mr r20,r3
	r20.u64 = ctx.r3.u64;
	// lis r4,-32246
	ctx.r4.s64 = -2113273856;
	// lis r3,-32246
	ctx.r3.s64 = -2113273856;
	// lis r31,-31927
	r31.s64 = -2092367872;
	// lis r26,-31927
	r26.s64 = -2092367872;
	// lis r25,-32246
	r25.s64 = -2113273856;
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r29,r29,8920
	r29.s64 = r29.s64 + 8920;
	// addi r28,r28,8848
	r28.s64 = r28.s64 + 8848;
	// addi r30,r27,8800
	r30.s64 = r27.s64 + 8800;
	// stw r29,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, r29.u32);
	// addi r7,r7,3900
	ctx.r7.s64 = ctx.r7.s64 + 3900;
	// stw r28,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, r28.u32);
	// addi r8,r8,8520
	ctx.r8.s64 = ctx.r8.s64 + 8520;
	// stw r30,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, r30.u32);
	// addi r6,r10,8464
	ctx.r6.s64 = ctx.r10.s64 + 8464;
	// stw r7,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r7.u32);
	// addi r23,r22,8620
	r23.s64 = r22.s64 + 8620;
	// stw r8,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r8.u32);
	// li r19,0
	r19.s64 = 0;
	// stw r6,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r6.u32);
	// li r18,1
	r18.s64 = 1;
	// lis r15,-31927
	r15.s64 = -2092367872;
	// addi r14,r4,11180
	r14.s64 = ctx.r4.s64 + 11180;
	// addi r17,r3,11192
	r17.s64 = ctx.r3.s64 + 11192;
	// addi r16,r31,28928
	r16.s64 = r31.s64 + 28928;
	// addi r26,r26,28940
	r26.s64 = r26.s64 + 28940;
	// addi r25,r25,8776
	r25.s64 = r25.s64 + 8776;
	// addi r22,r5,8556
	r22.s64 = ctx.r5.s64 + 8556;
	// addi r21,r9,8488
	r21.s64 = ctx.r9.s64 + 8488;
	// addi r24,r11,8432
	r24.s64 = r11.s64 + 8432;
loc_822FAE34:
	// addi r8,r1,196
	ctx.r8.s64 = ctx.r1.s64 + 196;
	// lwz r3,172(r20)
	ctx.r3.u64 = PPC_LOAD_U32(r20.u32 + 172);
	// addi r7,r1,128
	ctx.r7.s64 = ctx.r1.s64 + 128;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,1269
	ctx.r5.s64 = 1269;
	// addi r4,r1,2048
	ctx.r4.s64 = ctx.r1.s64 + 2048;
	// bl 0x82cfab20
	sub_82CFAB20(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,768
	ctx.r3.s64 = ctx.r1.s64 + 768;
	// addi r4,r1,2048
	ctx.r4.s64 = ctx.r1.s64 + 2048;
	// li r5,1269
	ctx.r5.s64 = 1269;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// cmpwi cr6,r31,-1
	cr6.compare<int32_t>(r31.s32, -1, xer);
	// beq cr6,0x822fbcc8
	if (cr6.eq) goto loc_822FBCC8;
	// cmpwi cr6,r31,0
	cr6.compare<int32_t>(r31.s32, 0, xer);
	// ble cr6,0x822fbcc8
	if (!cr6.gt) goto loc_822FBCC8;
	// lhz r10,130(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 130);
	// cmplwi cr6,r10,1002
	cr6.compare<uint32_t>(ctx.r10.u32, 1002, xer);
	// bne cr6,0x822fbcbc
	if (!cr6.eq) goto loc_822FBCBC;
	// lbz r11,2048(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 2048);
	// stw r19,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r19.u32);
	// stw r19,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r19.u32);
	// cmplwi cr6,r11,12
	cr6.compare<uint32_t>(r11.u32, 12, xer);
	// bgt cr6,0x822fbcbc
	if (cr6.gt) goto loc_822FBCBC;
	// lis r12,-32208
	r12.s64 = -2110783488;
	// addi r12,r12,-20820
	r12.s64 = r12.s64 + -20820;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822FAEE0;
	case 1:
		goto loc_822FB2BC;
	case 2:
		goto loc_822FB4E8;
	case 3:
		goto loc_822FBCBC;
	case 4:
		goto loc_822FBCBC;
	case 5:
		goto loc_822FBCBC;
	case 6:
		goto loc_822FB4FC;
	case 7:
		goto loc_822FB8AC;
	case 8:
		goto loc_822FBA50;
	case 9:
		goto loc_822FBA7C;
	case 10:
		goto loc_822FBAE0;
	case 11:
		goto loc_822FBAF0;
	case 12:
		goto loc_822FBB68;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-20768(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -20768);
	// lwz r17,-19780(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -19780);
	// lwz r17,-19224(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -19224);
	// lwz r17,-17220(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17220);
	// lwz r17,-17220(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17220);
	// lwz r17,-17220(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17220);
	// lwz r17,-19204(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -19204);
	// lwz r17,-18260(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -18260);
	// lwz r17,-17840(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17840);
	// lwz r17,-17796(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17796);
	// lwz r17,-17696(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17696);
	// lwz r17,-17680(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17680);
	// lwz r17,-17560(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -17560);
loc_822FAEE0:
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r11,781
	r11.s64 = 781;
	// ldx r11,r1,r11
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + r11.u32);
	// ld r10,264(r20)
	ctx.r10.u64 = PPC_LOAD_U64(r20.u32 + 264);
	// mr r29,r18
	r29.u64 = r18.u64;
	// cmpld cr6,r11,r10
	cr6.compare<uint64_t>(r11.u64, ctx.r10.u64, xer);
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// bne cr6,0x822faf10
	if (!cr6.eq) goto loc_822FAF10;
	// lwz r3,148(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FAF10:
	// stw r19,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, r19.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,144
	ctx.r6.s64 = ctx.r1.s64 + 144;
	// lwz r3,256(r20)
	ctx.r3.u64 = PPC_LOAD_U32(r20.u32 + 256);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82cbc158
	sub_82CBC158(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822faf48
	if (!cr6.eq) goto loc_822FAF48;
	// lwz r11,144(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 ^ 1;
	// stb r8,861(r1)
	PPC_STORE_U8(ctx.r1.u32 + 861, ctx.r8.u8);
loc_822FAF48:
	// lwz r11,793(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 793);
	// lwz r10,32(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 32);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x822faf80
	if (cr6.eq) goto loc_822FAF80;
	// lbz r10,861(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822faf80
	if (!cr6.eq) goto loc_822FAF80;
	// mr r3,r21
	ctx.r3.u64 = r21.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FAF80:
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fb01c
	if (!cr6.eq) goto loc_822FB01C;
	// lwz r3,212(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 212);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lbz r10,861(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// lwz r7,216(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 216);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fafb8
	if (!cr6.eq) goto loc_822FAFB8;
	// lwz r7,224(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 224);
loc_822FAFB8:
	// mr r3,r22
	ctx.r3.u64 = r22.u64;
	// ld r5,128(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// addi r4,r1,797
	ctx.r4.s64 = ctx.r1.s64 + 797;
	// ld r6,136(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 136);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r7,r1,797
	ctx.r7.s64 = ctx.r1.s64 + 797;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lbz r6,861(r1)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// ld r5,96(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// bl 0x822fda20
	sub_822FDA20(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822faffc
	if (cr6.eq) goto loc_822FAFFC;
	// lwz r3,220(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 220);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// b 0x822fb08c
	goto loc_822FB08C;
loc_822FAFFC:
	// mr r3,r23
	ctx.r3.u64 = r23.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// mr r29,r19
	r29.u64 = r19.u64;
	// b 0x822fb09c
	goto loc_822FB09C;
loc_822FB01C:
	// lwz r3,168(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r30,112(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x822fb034
	if (!cr6.eq) goto loc_822FB034;
	// twi 31,r0,22
loc_822FB034:
	// lwz r31,116(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fb048
	if (!cr6.eq) goto loc_822FB048;
	// twi 31,r0,22
loc_822FB048:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,76(r31)
	PPC_STORE_U32(r31.u32 + 76, ctx.r3.u32);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fb060
	if (!cr6.eq) goto loc_822FB060;
	// twi 31,r0,22
loc_822FB060:
	// ld r11,24(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 24);
	// cmpldi cr6,r11,0
	cr6.compare<uint64_t>(r11.u64, 0, xer);
	// bne cr6,0x822fb09c
	if (!cr6.eq) goto loc_822FB09C;
	// lwz r3,160(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fb084
	if (!cr6.eq) goto loc_822FB084;
	// twi 31,r0,22
loc_822FB084:
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// std r11,24(r31)
	PPC_STORE_U64(r31.u32 + 24, r11.u64);
loc_822FB08C:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lbz r5,861(r1)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// bl 0x822fce40
	sub_822FCE40(ctx, base);
loc_822FB09C:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// addi r29,r20,368
	r29.s64 = r20.s64 + 368;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,384
	ctx.r3.s64 = ctx.r1.s64 + 384;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r11,372(r20)
	r11.u64 = PPC_LOAD_U32(r20.u32 + 372);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lwz r31,88(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822fb0dc
	if (cr6.eq) goto loc_822FB0DC;
	// cmplw cr6,r31,r29
	cr6.compare<uint32_t>(r31.u32, r29.u32, xer);
	// beq cr6,0x822fb0e0
	if (cr6.eq) goto loc_822FB0E0;
loc_822FB0DC:
	// twi 31,r0,22
loc_822FB0E0:
	// lwz r30,92(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fb220
	if (!cr6.eq) goto loc_822FB220;
	// li r3,752
	ctx.r3.s64 = 752;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fb108
	if (cr6.eq) goto loc_822FB108;
	// bl 0x822f8778
	sub_822F8778(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822fb10c
	goto loc_822FB10C;
loc_822FB108:
	// mr r31,r19
	r31.u64 = r19.u64;
loc_822FB10C:
	// addi r5,r1,797
	ctx.r5.s64 = ctx.r1.s64 + 797;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,128
	ctx.r3.s64 = r31.s64 + 128;
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// lbz r11,861(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// stb r11,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r11.u8);
	// lbz r9,861(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822fb140
	if (cr6.eq) goto loc_822FB140;
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// sth r19,156(r31)
	PPC_STORE_U16(r31.u32 + 156, r19.u16);
	// b 0x822fb14c
	goto loc_822FB14C;
loc_822FB140:
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lhz r11,-27304(r11)
	r11.u64 = PPC_LOAD_U16(r11.u32 + -27304);
	// sth r11,156(r31)
	PPC_STORE_U16(r31.u32 + 156, r11.u16);
loc_822FB14C:
	// addi r4,r1,769
	ctx.r4.s64 = ctx.r1.s64 + 769;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,789(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 789);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,744(r31)
	PPC_STORE_U32(r31.u32 + 744, ctx.r3.u32);
	// stw r19,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r19.u32);
	// addi r5,r1,813
	ctx.r5.s64 = ctx.r1.s64 + 813;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// li r4,48
	ctx.r4.s64 = 48;
	// stw r10,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r10.u32);
	// addi r3,r31,45
	ctx.r3.s64 = r31.s64 + 45;
	// ld r9,96(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// addi r30,r31,120
	r30.s64 = r31.s64 + 120;
	// std r9,120(r31)
	PPC_STORE_U64(r31.u32 + 120, ctx.r9.u64);
	// lwz r8,793(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 793);
	// stw r8,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r8.u32);
	// lwz r7,862(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 862);
	// stw r7,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r7.u32);
	// lwz r6,866(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 866);
	// stw r6,748(r31)
	PPC_STORE_U32(r31.u32 + 748, ctx.r6.u32);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// stw r31,328(r1)
	PPC_STORE_U32(ctx.r1.u32 + 328, r31.u32);
	// addi r5,r1,320
	ctx.r5.s64 = ctx.r1.s64 + 320;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,608
	ctx.r3.s64 = ctx.r1.s64 + 608;
	// std r11,320(r1)
	PPC_STORE_U64(ctx.r1.u32 + 320, r11.u64);
	// bl 0x822ff270
	sub_822FF270(ctx, base);
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// lwz r9,32(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 32);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fb20c
	if (!cr6.eq) goto loc_822FB20C;
	// ld r11,96(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// addi r4,r1,368
	ctx.r4.s64 = ctx.r1.s64 + 368;
	// stw r19,368(r1)
	PPC_STORE_U32(ctx.r1.u32 + 368, r19.u32);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// std r11,376(r1)
	PPC_STORE_U64(ctx.r1.u32 + 376, r11.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// stw r26,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, r26.u32);
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// stw r11,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, r11.u32);
	// ld r4,256(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// bl 0x823006e0
	sub_823006E0(ctx, base);
loc_822FB20C:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x822fc078
	sub_822FC078(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FB220:
	// lwz r3,244(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x822fb234
	if (!cr6.eq) goto loc_822FB234;
	// twi 31,r0,22
loc_822FB234:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fb244
	if (!cr6.eq) goto loc_822FB244;
	// twi 31,r0,22
loc_822FB244:
	// lwz r31,24(r30)
	r31.u64 = PPC_LOAD_U32(r30.u32 + 24);
	// addi r4,r1,769
	ctx.r4.s64 = ctx.r1.s64 + 769;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// stw r8,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r8.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r7,789(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 789);
	// stw r7,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r7.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,744(r31)
	PPC_STORE_U32(r31.u32 + 744, ctx.r3.u32);
	// lwz r6,793(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 793);
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r6,96(r31)
	PPC_STORE_U32(r31.u32 + 96, ctx.r6.u32);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lbz r4,861(r1)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
	// stb r4,44(r31)
	PPC_STORE_U8(r31.u32 + 44, ctx.r4.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// lwz r11,862(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 862);
	// stw r11,116(r31)
	PPC_STORE_U32(r31.u32 + 116, r11.u32);
	// lwz r10,866(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 866);
	// stw r10,748(r31)
	PPC_STORE_U32(r31.u32 + 748, ctx.r10.u32);
	// bl 0x822fc078
	sub_822FC078(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FB2BC:
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// mr r29,r19
	r29.u64 = r19.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// clrlwi r27,r3,24
	r27.u64 = ctx.r3.u32 & 0xFF;
	// lwz r30,116(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r28,112(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822fb408
	if (cr6.eq) goto loc_822FB408;
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822fb2f0
	if (!cr6.eq) goto loc_822FB2F0;
	// twi 31,r0,22
loc_822FB2F0:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fb300
	if (!cr6.eq) goto loc_822FB300;
	// twi 31,r0,22
loc_822FB300:
	// ld r10,24(r30)
	ctx.r10.u64 = PPC_LOAD_U64(r30.u32 + 24);
	// cmpldi cr6,r10,0
	cr6.compare<uint64_t>(ctx.r10.u64, 0, xer);
	// beq cr6,0x822fb408
	if (cr6.eq) goto loc_822FB408;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fb318
	if (!cr6.eq) goto loc_822FB318;
	// twi 31,r0,22
loc_822FB318:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,76(r30)
	PPC_STORE_U32(r30.u32 + 76, ctx.r3.u32);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fb330
	if (!cr6.eq) goto loc_822FB330;
	// twi 31,r0,22
loc_822FB330:
	// ld r11,24(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 24);
	// addi r31,r20,368
	r31.s64 = r20.s64 + 368;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,416
	ctx.r3.s64 = ctx.r1.s64 + 416;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// bl 0x82323168
	sub_82323168(ctx, base);
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r9,372(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 372);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fb36c
	if (cr6.eq) goto loc_822FB36C;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fb370
	if (cr6.eq) goto loc_822FB370;
loc_822FB36C:
	// twi 31,r0,22
loc_822FB370:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fb400
	if (cr6.eq) goto loc_822FB400;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fb388
	if (!cr6.eq) goto loc_822FB388;
	// twi 31,r0,22
loc_822FB388:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fb398
	if (!cr6.eq) goto loc_822FB398;
	// twi 31,r0,22
loc_822FB398:
	// lwz r31,24(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// addi r4,r1,769
	ctx.r4.s64 = ctx.r1.s64 + 769;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// stw r8,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r8.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r7,781(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 781);
	// stw r7,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r7.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// stw r3,744(r31)
	PPC_STORE_U32(r31.u32 + 744, ctx.r3.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// stb r19,144(r31)
	PPC_STORE_U8(r31.u32 + 144, r19.u8);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// stw r11,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r11.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r11,148(r31)
	PPC_STORE_U32(r31.u32 + 148, r11.u32);
	// bl 0x822fc078
	sub_822FC078(ctx, base);
	// b 0x822fb40c
	goto loc_822FB40C;
loc_822FB400:
	// lwz r3,252(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FB408:
	// mr r29,r18
	r29.u64 = r18.u64;
loc_822FB40C:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// bne cr6,0x822fb4a4
	if (!cr6.eq) goto loc_822FB4A4;
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,256(r20)
	ctx.r3.u64 = PPC_LOAD_U32(r20.u32 + 256);
	// addi r6,r1,208
	ctx.r6.s64 = ctx.r1.s64 + 208;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82cbc158
	sub_82CBC158(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822fb458
	if (!cr6.eq) goto loc_822FB458;
	// lwz r11,208(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 208);
	// addi r10,r11,-1
	ctx.r10.s64 = r11.s64 + -1;
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// rlwinm r5,r9,27,31,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// stb r5,861(r1)
	PPC_STORE_U8(ctx.r1.u32 + 861, ctx.r5.u8);
	// b 0x822fb45c
	goto loc_822FB45C;
loc_822FB458:
	// lbz r5,861(r1)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r1.u32 + 861);
loc_822FB45C:
	// lbz r11,109(r20)
	r11.u64 = PPC_LOAD_U8(r20.u32 + 109);
	// lwz r10,352(r20)
	ctx.r10.u64 = PPC_LOAD_U32(r20.u32 + 352);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// blt cr6,0x822fb494
	if (cr6.lt) goto loc_822FB494;
	// clrlwi r11,r5,24
	r11.u64 = ctx.r5.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fb494
	if (!cr6.eq) goto loc_822FB494;
	// lwz r3,176(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// b 0x822fb4c8
	goto loc_822FB4C8;
loc_822FB494:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// bl 0x822fce40
	sub_822FCE40(ctx, base);
	// b 0x822fb4c8
	goto loc_822FB4C8;
loc_822FB4A4:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822fb4b0
	if (!cr6.eq) goto loc_822FB4B0;
	// twi 31,r0,22
loc_822FB4B0:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fb4c0
	if (!cr6.eq) goto loc_822FB4C0;
	// twi 31,r0,22
loc_822FB4C0:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,76(r30)
	PPC_STORE_U32(r30.u32 + 76, ctx.r3.u32);
loc_822FB4C8:
	// li r11,2
	r11.s64 = 2;
	// lwz r6,132(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// li r5,1
	ctx.r5.s64 = 1;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FB4E8:
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x822fce40
	sub_822FCE40(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FB4FC:
	// bl 0x82226ed8
	sub_82226ED8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,26912(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 26912);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lbz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 256);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fbcbc
	if (!cr6.eq) goto loc_822FBCBC;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,156(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 156);
	// bl 0x822641f0
	sub_822641F0(ctx, base);
	// bl 0x82455f20
	sub_82455F20(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fb54c
	if (cr6.eq) goto loc_822FB54C;
	// lbz r11,144(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// mr r11,r18
	r11.u64 = r18.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fb550
	if (!cr6.eq) goto loc_822FB550;
loc_822FB54C:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB550:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,60(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// mr r31,r19
	r31.u64 = r19.u64;
	// rlwinm r10,r11,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fb65c
	if (cr6.eq) goto loc_822FB65C;
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fb59c
	if (cr6.eq) goto loc_822FB59C;
	// lbz r11,218(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 218);
	// rotlwi r11,r11,3
	r11.u64 = __builtin_rotateleft32(r11.u32, 3);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fb660
	goto loc_822FB660;
loc_822FB59C:
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r19,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, r19.u32);
	// subf r11,r10,r6
	r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r10.u32);
	// srawi. r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822fb604
	if (!cr0.gt) goto loc_822FB604;
loc_822FB5B4:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,218
	cr6.compare<int32_t>(ctx.r7.s32, 218, xer);
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// blt cr6,0x822fb5d4
	if (cr6.lt) goto loc_822FB5D4;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
loc_822FB5D4:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822fb5f0
	if (cr6.eq) goto loc_822FB5F0;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822fb5f8
	goto loc_822FB5F8;
loc_822FB5F0:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822FB5F8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822fb5b4
	if (cr6.gt) goto loc_822FB5B4;
	// stw r10,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r10.u32);
loc_822FB604:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x822fb648
	if (cr6.eq) goto loc_822FB648;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,218
	cr6.compare<int32_t>(r11.s32, 218, xer);
	// mr r11,r18
	r11.u64 = r18.u64;
	// bgt cr6,0x822fb620
	if (cr6.gt) goto loc_822FB620;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB620:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fb648
	if (!cr6.eq) goto loc_822FB648;
	// ld r11,232(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 232);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fb660
	goto loc_822FB660;
loc_822FB648:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fb660
	goto loc_822FB660;
loc_822FB65C:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB660:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,44(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 44);
	// rlwinm r10,r11,5,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 5) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fb6f0
	if (cr6.eq) goto loc_822FB6F0;
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fb6e8
	if (!cr6.eq) goto loc_822FB6E8;
	// lwz r11,76(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// srawi. r11,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	r11.s64 = ctx.r9.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822fb6e8
	if (!cr0.gt) goto loc_822FB6E8;
loc_822FB69C:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,91
	cr6.compare<int32_t>(ctx.r7.s32, 91, xer);
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// blt cr6,0x822fb6bc
	if (cr6.lt) goto loc_822FB6BC;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
loc_822FB6BC:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822fb6d8
	if (cr6.eq) goto loc_822FB6D8;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822fb6e0
	goto loc_822FB6E0;
loc_822FB6D8:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822FB6E0:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822fb69c
	if (cr6.gt) goto loc_822FB69C;
loc_822FB6E8:
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fb6f4
	goto loc_822FB6F4;
loc_822FB6F0:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB6F4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// bl 0x82396ed8
	sub_82396ED8(ctx, base);
	// lwz r11,785(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 785);
	// lwz r10,789(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 789);
	// addi r4,r1,769
	ctx.r4.s64 = ctx.r1.s64 + 769;
	// addi r3,r1,476
	ctx.r3.s64 = ctx.r1.s64 + 476;
	// stw r11,468(r1)
	PPC_STORE_U32(ctx.r1.u32 + 468, r11.u32);
	// stw r10,472(r1)
	PPC_STORE_U32(ctx.r1.u32 + 472, ctx.r10.u32);
	// bl 0x82275368
	sub_82275368(ctx, base);
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB728:
	// addi r10,r1,793
	ctx.r10.s64 = ctx.r1.s64 + 793;
	// addi r9,r1,797
	ctx.r9.s64 = ctx.r1.s64 + 797;
	// addi r8,r1,480
	ctx.r8.s64 = ctx.r1.s64 + 480;
	// addi r7,r1,484
	ctx.r7.s64 = ctx.r1.s64 + 484;
	// lwzx r6,r11,r10
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + ctx.r10.u32);
	// lwzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + ctx.r9.u32);
	// stwx r6,r11,r8
	PPC_STORE_U32(r11.u32 + ctx.r8.u32, ctx.r6.u32);
	// stwx r5,r11,r7
	PPC_STORE_U32(r11.u32 + ctx.r7.u32, ctx.r5.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x822fb728
	if (cr6.lt) goto loc_822FB728;
	// lwz r7,877(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 877);
	// stfs f31,84(r1)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lwz r11,865(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 865);
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lwz r5,861(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 861);
	// addi r10,r1,104
	ctx.r10.s64 = ctx.r1.s64 + 104;
	// lwz r3,857(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 857);
	// addi r8,r1,104
	ctx.r8.s64 = ctx.r1.s64 + 104;
	// lwz r9,873(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 873);
	// addi r4,r1,84
	ctx.r4.s64 = ctx.r1.s64 + 84;
	// stw r7,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r7.u32);
	// addi r7,r1,104
	ctx.r7.s64 = ctx.r1.s64 + 104;
	// stw r11,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, r11.u32);
	// lfs f0,248(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 248);
	f0.f64 = double(temp.f32);
	// stw r5,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r5.u32);
	// lfs f9,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f9.f64 = double(temp.f32);
	// stw r3,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r3.u32);
	// lfs f12,192(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 192);
	ctx.f12.f64 = double(temp.f32);
	// stw r9,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r9.u32);
	// addi r3,r1,400
	ctx.r3.s64 = ctx.r1.s64 + 400;
	// lwz r9,881(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 881);
	// addi r30,r1,84
	r30.s64 = ctx.r1.s64 + 84;
	// stw r3,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r3.u32);
	// addi r29,r1,84
	r29.s64 = ctx.r1.s64 + 84;
	// lwz r11,132(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r28,r1,432
	r28.s64 = ctx.r1.s64 + 432;
	// lwz r5,869(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 869);
	// addi r27,r1,400
	r27.s64 = ctx.r1.s64 + 400;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r9,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r9.u32);
	// addi r9,r1,432
	ctx.r9.s64 = ctx.r1.s64 + 432;
	// stw r5,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r5.u32);
	// lfs f8,180(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	ctx.f8.f64 = double(temp.f32);
	// addi r5,r1,576
	ctx.r5.s64 = ctx.r1.s64 + 576;
	// lvlx v0,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r6,r1,172
	ctx.r6.s64 = ctx.r1.s64 + 172;
	// stfs f31,84(r1)
	temp.f32 = float(f31.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lwz r4,885(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 885);
	// stw r4,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r4.u32);
	// addi r4,r1,464
	ctx.r4.s64 = ctx.r1.s64 + 464;
	// lfs f10,200(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 200);
	ctx.f10.f64 = double(temp.f32);
	// stfs f10,104(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lfs f13,184(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	ctx.f13.f64 = double(temp.f32);
	// lfs f11,240(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 240);
	ctx.f11.f64 = double(temp.f32);
	// stfs f11,84(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// stfs f0,104(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// lvlx v11,0,r8
	temp.u32 = ctx.r8.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v11,v0,4,3
	_mm_store_ps(ctx.v11.f32, _mm_blend_ps(_mm_load_ps(ctx.v11.f32), _mm_permute_ps(_mm_load_ps(ctx.v0.f32), 57), 4));
	// stfs f9,104(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 104, temp.u32);
	// stw r11,28600(r10)
	PPC_STORE_U32(ctx.r10.u32 + 28600, r11.u32);
	// stfs f12,172(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 172, temp.u32);
	// addi r8,r1,544
	ctx.r8.s64 = ctx.r1.s64 + 544;
	// lvlx v9,0,r6
	temp.u32 = ctx.r6.u32;
	_mm_store_si128((__m128i*)ctx.v9.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v8,0,r7
	temp.u32 = ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v10,0,r30
	temp.u32 = r30.u32;
	_mm_store_si128((__m128i*)ctx.v10.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v10,v13,4,3
	_mm_store_ps(ctx.v10.f32, _mm_blend_ps(_mm_load_ps(ctx.v10.f32), _mm_permute_ps(_mm_load_ps(ctx.v13.f32), 57), 4));
	// vrlimi128 v9,v8,4,3
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v8.f32), 57), 4));
	// stfs f13,84(r1)
	temp.f32 = float(ctx.f13.f64);
	PPC_STORE_U32(ctx.r1.u32 + 84, temp.u32);
	// lvlx v7,0,r29
	temp.u32 = r29.u32;
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vrlimi128 v7,v12,4,3
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v12.f32), 57), 4));
	// vrlimi128 v9,v11,3,2
	_mm_store_ps(ctx.v9.f32, _mm_blend_ps(_mm_load_ps(ctx.v9.f32), _mm_permute_ps(_mm_load_ps(ctx.v11.f32), 78), 3));
	// lfs f7,188(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	ctx.f7.f64 = double(temp.f32);
	// vrlimi128 v7,v10,3,2
	_mm_store_ps(ctx.v7.f32, _mm_blend_ps(_mm_load_ps(ctx.v7.f32), _mm_permute_ps(_mm_load_ps(ctx.v10.f32), 78), 3));
	// stvx128 v9,r0,r28
	_mm_store_si128((__m128i*)(base + ((r28.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// ld r11,0(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// ld r10,8(r9)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// stvx128 v7,r0,r27
	_mm_store_si128((__m128i*)(base + ((r27.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lwz r9,88(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// std r11,0(r8)
	PPC_STORE_U64(ctx.r8.u32 + 0, r11.u64);
	// ld r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r9.u32 + 8);
	// lwz r11,889(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 889);
	// std r10,8(r8)
	PPC_STORE_U64(ctx.r8.u32 + 8, ctx.r10.u64);
	// stfs f8,560(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 560, temp.u32);
	// ld r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r9.u32 + 0);
	// std r7,0(r5)
	PPC_STORE_U64(ctx.r5.u32 + 0, ctx.r7.u64);
	// std r6,8(r5)
	PPC_STORE_U64(ctx.r5.u32 + 8, ctx.r6.u64);
	// stfs f7,592(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 592, temp.u32);
	// stw r11,596(r1)
	PPC_STORE_U32(ctx.r1.u32 + 596, r11.u32);
	// bl 0x8238c488
	sub_8238C488(ctx, base);
	// addi r3,r1,476
	ctx.r3.s64 = ctx.r1.s64 + 476;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// stw r17,464(r1)
	PPC_STORE_U32(ctx.r1.u32 + 464, r17.u32);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FB8AC:
	// bl 0x82226ed8
	sub_82226ED8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,26912(r15)
	r11.u64 = PPC_LOAD_U32(r15.u32 + 26912);
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lbz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 256);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fbcbc
	if (!cr6.eq) goto loc_822FBCBC;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,156(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 156);
	// bl 0x822641f0
	sub_822641F0(ctx, base);
	// bl 0x82455f20
	sub_82455F20(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fb8fc
	if (cr6.eq) goto loc_822FB8FC;
	// lbz r11,144(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// mr r11,r18
	r11.u64 = r18.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fb900
	if (!cr6.eq) goto loc_822FB900;
loc_822FB8FC:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB900:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,60(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 60);
	// mr r31,r19
	r31.u64 = r19.u64;
	// rlwinm r10,r11,6,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 6) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fba0c
	if (cr6.eq) goto loc_822FBA0C;
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fb94c
	if (cr6.eq) goto loc_822FB94C;
	// lbz r11,218(r11)
	r11.u64 = PPC_LOAD_U8(r11.u32 + 218);
	// rotlwi r11,r11,3
	r11.u64 = __builtin_rotateleft32(r11.u32, 3);
	// add r10,r11,r10
	ctx.r10.u64 = r11.u64 + ctx.r10.u64;
	// lwz r11,4(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fba10
	goto loc_822FBA10;
loc_822FB94C:
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r19,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, r19.u32);
	// subf r11,r10,r6
	r11.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// srawi. r11,r11,3
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x7) != 0);
	r11.s64 = r11.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822fb9b4
	if (!cr0.gt) goto loc_822FB9B4;
loc_822FB964:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,218
	cr6.compare<int32_t>(ctx.r7.s32, 218, xer);
	// mr r7,r18
	ctx.r7.u64 = r18.u64;
	// blt cr6,0x822fb984
	if (cr6.lt) goto loc_822FB984;
	// mr r7,r19
	ctx.r7.u64 = r19.u64;
loc_822FB984:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822fb9a0
	if (cr6.eq) goto loc_822FB9A0;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822fb9a8
	goto loc_822FB9A8;
loc_822FB9A0:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822FB9A8:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822fb964
	if (cr6.gt) goto loc_822FB964;
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
loc_822FB9B4:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x822fb9f8
	if (cr6.eq) goto loc_822FB9F8;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,218
	cr6.compare<int32_t>(r11.s32, 218, xer);
	// mr r11,r18
	r11.u64 = r18.u64;
	// bgt cr6,0x822fb9d0
	if (cr6.gt) goto loc_822FB9D0;
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FB9D0:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fb9f8
	if (!cr6.eq) goto loc_822FB9F8;
	// ld r11,152(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 152);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fba10
	goto loc_822FBA10;
loc_822FB9F8:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r31,r11
	r31.u64 = r11.u64;
	// mr r11,r18
	r11.u64 = r18.u64;
	// b 0x822fba10
	goto loc_822FBA10;
loc_822FBA0C:
	// mr r11,r19
	r11.u64 = r19.u64;
loc_822FBA10:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,785(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 785);
	// addi r5,r1,769
	ctx.r5.s64 = ctx.r1.s64 + 769;
	// stw r14,624(r1)
	PPC_STORE_U32(ctx.r1.u32 + 624, r14.u32);
	// li r4,16
	ctx.r4.s64 = 16;
	// stb r18,628(r1)
	PPC_STORE_U8(ctx.r1.u32 + 628, r18.u8);
	// addi r3,r1,636
	ctx.r3.s64 = ctx.r1.s64 + 636;
	// stw r11,632(r1)
	PPC_STORE_U32(ctx.r1.u32 + 632, r11.u32);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// addi r4,r1,624
	ctx.r4.s64 = ctx.r1.s64 + 624;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8238ced0
	sub_8238CED0(ctx, base);
	// stw r17,624(r1)
	PPC_STORE_U32(ctx.r1.u32 + 624, r17.u32);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FBA50:
	// li r10,9
	ctx.r10.s64 = 9;
	// ld r11,264(r20)
	r11.u64 = PPC_LOAD_U64(r20.u32 + 264);
	// li r5,9
	ctx.r5.s64 = 9;
	// lwz r6,132(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// stb r10,296(r1)
	PPC_STORE_U8(ctx.r1.u32 + 296, ctx.r10.u8);
	// addi r4,r1,296
	ctx.r4.s64 = ctx.r1.s64 + 296;
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// li r12,297
	r12.s64 = 297;
	// stdx r11,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, r11.u64);
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FBA7C:
	// addi r31,r20,368
	r31.s64 = r20.s64 + 368;
	// addi r5,r1,769
	ctx.r5.s64 = ctx.r1.s64 + 769;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,280
	ctx.r3.s64 = ctx.r1.s64 + 280;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r11,280(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 280);
	// lwz r9,372(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbaa8
	if (cr6.eq) goto loc_822FBAA8;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fbaac
	if (cr6.eq) goto loc_822FBAAC;
loc_822FBAA8:
	// twi 31,r0,22
loc_822FBAAC:
	// lwz r10,284(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fbac4
	if (!cr6.eq) goto loc_822FBAC4;
	// twi 31,r0,22
loc_822FBAC4:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fbad4
	if (!cr6.eq) goto loc_822FBAD4;
	// twi 31,r0,22
loc_822FBAD4:
	// lwz r11,24(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// stb r18,144(r11)
	PPC_STORE_U8(r11.u32 + 144, r18.u8);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FBAE0:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// bl 0x822fd0e8
	sub_822FD0E8(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FBAF0:
	// addi r31,r20,368
	r31.s64 = r20.s64 + 368;
	// addi r5,r1,769
	ctx.r5.s64 = ctx.r1.s64 + 769;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r3,204(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 204);
	// li r4,2049
	ctx.r4.s64 = 2049;
	// ldx r4,r1,r4
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + ctx.r4.u32);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,272(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 272);
	// lwz r9,372(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbb2c
	if (cr6.eq) goto loc_822FBB2C;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fbb30
	if (cr6.eq) goto loc_822FBB30;
loc_822FBB2C:
	// twi 31,r0,22
loc_822FBB30:
	// lwz r10,276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fbb48
	if (!cr6.eq) goto loc_822FBB48;
	// twi 31,r0,22
loc_822FBB48:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fbb58
	if (!cr6.eq) goto loc_822FBB58;
	// twi 31,r0,22
loc_822FBB58:
	// addi r4,r1,768
	ctx.r4.s64 = ctx.r1.s64 + 768;
	// lwz r3,24(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// bl 0x822f89e0
	sub_822F89E0(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FBB68:
	// addi r5,r1,112
	ctx.r5.s64 = ctx.r1.s64 + 112;
	// lwz r4,132(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// lwz r11,112(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fbb94
	if (!cr6.eq) goto loc_822FBB94;
	// twi 31,r0,22
loc_822FBB94:
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fbba8
	if (!cr6.eq) goto loc_822FBBA8;
	// twi 31,r0,22
loc_822FBBA8:
	// ld r11,24(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 24);
	// addi r31,r20,368
	r31.s64 = r20.s64 + 368;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,448
	ctx.r3.s64 = ctx.r1.s64 + 448;
	// std r11,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, r11.u64);
	// bl 0x82323168
	sub_82323168(ctx, base);
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r9,372(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 372);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbbe4
	if (cr6.eq) goto loc_822FBBE4;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fbbe8
	if (cr6.eq) goto loc_822FBBE8;
loc_822FBBE4:
	// twi 31,r0,22
loc_822FBBE8:
	// lwz r10,92(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fbc00
	if (!cr6.eq) goto loc_822FBC00;
	// twi 31,r0,22
loc_822FBC00:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fbc10
	if (!cr6.eq) goto loc_822FBC10;
	// twi 31,r0,22
loc_822FBC10:
	// lwz r31,24(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// addi r5,r1,769
	ctx.r5.s64 = ctx.r1.s64 + 769;
	// lwz r11,817(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 817);
	// li r4,48
	ctx.r4.s64 = 48;
	// addi r3,r31,45
	ctx.r3.s64 = r31.s64 + 45;
	// stw r11,96(r31)
	PPC_STORE_U32(r31.u32 + 96, r11.u32);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// lwz r10,96(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 96);
	// lwz r9,32(r20)
	ctx.r9.u64 = PPC_LOAD_U32(r20.u32 + 32);
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// ld r11,120(r31)
	r11.u64 = PPC_LOAD_U64(r31.u32 + 120);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fbc78
	if (!cr6.eq) goto loc_822FBC78;
	// stw r19,352(r1)
	PPC_STORE_U32(ctx.r1.u32 + 352, r19.u32);
	// addi r4,r1,352
	ctx.r4.s64 = ctx.r1.s64 + 352;
	// std r11,360(r1)
	PPC_STORE_U64(ctx.r1.u32 + 360, r11.u64);
	// addi r31,r31,120
	r31.s64 = r31.s64 + 120;
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// lwz r11,4(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// stw r26,288(r1)
	PPC_STORE_U32(ctx.r1.u32 + 288, r26.u32);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// stw r11,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, r11.u32);
	// ld r4,288(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 288);
	// bl 0x823006e0
	sub_823006E0(ctx, base);
	// b 0x822fbcbc
	goto loc_822FBCBC;
loc_822FBC78:
	// stw r18,336(r1)
	PPC_STORE_U32(ctx.r1.u32 + 336, r18.u32);
	// addi r4,r1,336
	ctx.r4.s64 = ctx.r1.s64 + 336;
	// std r11,344(r1)
	PPC_STORE_U64(ctx.r1.u32 + 344, r11.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// addi r31,r31,108
	r31.s64 = r31.s64 + 108;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmpwi cr6,r10,-1
	cr6.compare<int32_t>(ctx.r10.s32, -1, xer);
	// beq cr6,0x822fbcbc
	if (cr6.eq) goto loc_822FBCBC;
	// stw r16,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, r16.u32);
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
	// lwz r11,4(r16)
	r11.u64 = PPC_LOAD_U32(r16.u32 + 4);
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// stw r11,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, r11.u32);
	// ld r4,264(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 264);
	// bl 0x825f7b10
	sub_825F7B10(ctx, base);
	// li r11,-1
	r11.s64 = -1;
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
loc_822FBCBC:
	// lhz r10,130(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 130);
	// cmplwi cr6,r10,1002
	cr6.compare<uint32_t>(ctx.r10.u32, 1002, xer);
	// beq cr6,0x822fae34
	if (cr6.eq) goto loc_822FAE34;
loc_822FBCC8:
	// addi r1,r1,3488
	ctx.r1.s64 = ctx.r1.s64 + 3488;
	// lfd f31,-160(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -160);
	// b 0x82ca2c00
	return;
}

__attribute__((alias("__imp__sub_822FBCD8"))) PPC_WEAK_FUNC(sub_822FBCD8);
PPC_FUNC_IMPL(__imp__sub_822FBCD8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r15{};
	PPCRegister r16{};
	PPCRegister r17{};
	PPCRegister r18{};
	PPCRegister r19{};
	PPCRegister r20{};
	PPCRegister r21{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bb4
	// ld r12,-4096(r1)
	r12.u64 = PPC_LOAD_U64(ctx.r1.u32 + -4096);
	// stwu r1,-4432(r1)
	ea = -4432 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,16
	r11.s64 = 16;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r11.u32);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// lis r22,-31927
	r22.s64 = -2092367872;
	// li r21,0
	r21.s64 = 0;
	// li r23,1
	r23.s64 = 1;
	// li r15,3
	r15.s64 = 3;
	// addi r18,r8,9168
	r18.s64 = ctx.r8.s64 + 9168;
	// addi r17,r9,9108
	r17.s64 = ctx.r9.s64 + 9108;
	// addi r20,r10,9068
	r20.s64 = ctx.r10.s64 + 9068;
	// addi r16,r11,9044
	r16.s64 = r11.s64 + 9044;
loc_822FBD24:
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// lwz r3,552(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 552);
	// addi r7,r1,144
	ctx.r7.s64 = ctx.r1.s64 + 144;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,2533
	ctx.r5.s64 = 2533;
	// addi r4,r1,1744
	ctx.r4.s64 = ctx.r1.s64 + 1744;
	// bl 0x82cfab20
	sub_82CFAB20(ctx, base);
	// mr r19,r3
	r19.u64 = ctx.r3.u64;
	// addi r3,r1,464
	ctx.r3.s64 = ctx.r1.s64 + 464;
	// addi r4,r1,1744
	ctx.r4.s64 = ctx.r1.s64 + 1744;
	// li r5,1269
	ctx.r5.s64 = 1269;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// cmpwi cr6,r19,-1
	cr6.compare<int32_t>(r19.s32, -1, xer);
	// beq cr6,0x822fbfbc
	if (cr6.eq) goto loc_822FBFBC;
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// ble cr6,0x822fbfc0
	if (!cr6.gt) goto loc_822FBFC0;
	// lbz r10,1746(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 1746);
	// stw r21,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r21.u32);
	// stw r21,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r21.u32);
	// cmplwi cr6,r10,5
	cr6.compare<uint32_t>(ctx.r10.u32, 5, xer);
	// beq cr6,0x822fbd80
	if (cr6.eq) goto loc_822FBD80;
	// mr r3,r18
	ctx.r3.u64 = r18.u64;
	// b 0x822fbfa8
	goto loc_822FBFA8;
loc_822FBD80:
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// lwz r4,148(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbfa4
	if (cr6.eq) goto loc_822FBFA4;
	// lwz r25,88(r1)
	r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// bne cr6,0x822fbdac
	if (!cr6.eq) goto loc_822FBDAC;
	// twi 31,r0,22
loc_822FBDAC:
	// lwz r30,92(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fbdc0
	if (!cr6.eq) goto loc_822FBDC0;
	// twi 31,r0,22
loc_822FBDC0:
	// ld r4,24(r30)
	ctx.r4.u64 = PPC_LOAD_U64(r30.u32 + 24);
	// addi r24,r30,24
	r24.s64 = r30.s64 + 24;
	// cmpldi cr6,r4,0
	cr6.compare<uint64_t>(ctx.r4.u64, 0, xer);
	// beq cr6,0x822fbfa4
	if (cr6.eq) goto loc_822FBFA4;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fbddc
	if (!cr6.eq) goto loc_822FBDDC;
	// twi 31,r0,22
loc_822FBDDC:
	// mr r3,r20
	ctx.r3.u64 = r20.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fbdf4
	if (!cr6.eq) goto loc_822FBDF4;
	// twi 31,r0,22
loc_822FBDF4:
	// addi r31,r26,368
	r31.s64 = r26.s64 + 368;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r11,372(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 372);
	// lwz r28,104(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// beq cr6,0x822fbe20
	if (cr6.eq) goto loc_822FBE20;
	// cmplw cr6,r28,r31
	cr6.compare<uint32_t>(r28.u32, r31.u32, xer);
	// beq cr6,0x822fbe24
	if (cr6.eq) goto loc_822FBE24;
loc_822FBE20:
	// twi 31,r0,22
loc_822FBE24:
	// lwz r27,108(r1)
	r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 108);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// beq cr6,0x822fbf9c
	if (cr6.eq) goto loc_822FBF9C;
	// lwz r3,256(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 256);
	// mr r31,r23
	r31.u64 = r23.u64;
	// lwz r11,26848(r22)
	r11.u64 = PPC_LOAD_U32(r22.u32 + 26848);
	// mr r29,r23
	r29.u64 = r23.u64;
	// slw r10,r23,r3
	ctx.r10.u64 = ctx.r3.u8 & 0x20 ? 0 : (r23.u32 << (ctx.r3.u8 & 0x3F));
	// and r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 & r11.u64;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822fbe9c
	if (cr6.eq) goto loc_822FBE9C;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,252
	ctx.r4.s64 = 252;
	// bl 0x82cbc160
	sub_82CBC160(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822fbfc0
	if (!cr6.eq) goto loc_822FBFC0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822fbe74
	if (!cr6.eq) goto loc_822FBE74;
	// mr r31,r21
	r31.u64 = r21.u64;
loc_822FBE74:
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r3,256(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 256);
	// li r4,251
	ctx.r4.s64 = 251;
	// bl 0x82cbc160
	sub_82CBC160(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822fbfc0
	if (!cr6.eq) goto loc_822FBFC0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822fbe9c
	if (!cr6.eq) goto loc_822FBE9C;
	// mr r29,r21
	r29.u64 = r21.u64;
loc_822FBE9C:
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822fbea8
	if (!cr6.eq) goto loc_822FBEA8;
	// twi 31,r0,22
loc_822FBEA8:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bne cr6,0x822fbeb8
	if (!cr6.eq) goto loc_822FBEB8;
	// twi 31,r0,22
loc_822FBEB8:
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fbee0
	if (!cr6.eq) goto loc_822FBEE0;
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fbfac
	if (cr6.eq) goto loc_822FBFAC;
	// lwz r11,24(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 24);
	// lbz r10,44(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 44);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fbfac
	if (cr6.eq) goto loc_822FBFAC;
loc_822FBEE0:
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// stw r23,312(r1)
	PPC_STORE_U32(ctx.r1.u32 + 312, r23.u32);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fbf04
	if (!cr6.eq) goto loc_822FBF04;
	// twi 31,r0,22
loc_822FBF04:
	// ld r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// std r10,280(r1)
	PPC_STORE_U64(ctx.r1.u32 + 280, ctx.r10.u64);
	// bne cr6,0x822fbf18
	if (!cr6.eq) goto loc_822FBF18;
	// twi 31,r0,22
loc_822FBF18:
	// std r10,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r10.u64);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// stw r21,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r21.u32);
	// bne cr6,0x822fbf2c
	if (!cr6.eq) goto loc_822FBF2C;
	// twi 31,r0,22
loc_822FBF2C:
	// lbz r11,80(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fbf58
	if (!cr6.eq) goto loc_822FBF58;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// lwz r3,548(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 548);
	// bl 0x823041c8
	sub_823041C8(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fbf54
	if (!cr6.eq) goto loc_822FBF54;
	// twi 31,r0,22
loc_822FBF54:
	// stb r23,80(r30)
	PPC_STORE_U8(r30.u32 + 80, r23.u8);
loc_822FBF58:
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r3,548(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 548);
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// lhz r5,467(r1)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r1.u32 + 467);
	// addi r4,r1,469
	ctx.r4.s64 = ctx.r1.s64 + 469;
	// bl 0x823044a0
	sub_823044A0(ctx, base);
	// lwz r11,4(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 4);
	// stw r15,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r15.u32);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fbf84
	if (!cr6.eq) goto loc_822FBF84;
	// twi 31,r0,22
loc_822FBF84:
	// ld r11,0(r24)
	r11.u64 = PPC_LOAD_U64(r24.u32 + 0);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// std r11,136(r1)
	PPC_STORE_U64(ctx.r1.u32 + 136, r11.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// b 0x822fbfac
	goto loc_822FBFAC;
loc_822FBF9C:
	// mr r3,r17
	ctx.r3.u64 = r17.u64;
	// b 0x822fbfa8
	goto loc_822FBFA8;
loc_822FBFA4:
	// mr r3,r16
	ctx.r3.u64 = r16.u64;
loc_822FBFA8:
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FBFAC:
	// cmpwi cr6,r19,0
	cr6.compare<int32_t>(r19.s32, 0, xer);
	// bgt cr6,0x822fbd24
	if (cr6.gt) goto loc_822FBD24;
	// addi r1,r1,4432
	ctx.r1.s64 = ctx.r1.s64 + 4432;
	// b 0x82ca2c04
	return;
loc_822FBFBC:
	// bl 0x82cfac30
	sub_82CFAC30(ctx, base);
loc_822FBFC0:
	// addi r1,r1,4432
	ctx.r1.s64 = ctx.r1.s64 + 4432;
	// b 0x82ca2c04
	return;
}

__attribute__((alias("__imp__sub_822FBFC8"))) PPC_WEAK_FUNC(sub_822FBFC8);
PPC_FUNC_IMPL(__imp__sub_822FBFC8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister f0{};
	PPCVRegister v27{};
	PPCVRegister v28{};
	PPCVRegister v29{};
	PPCVRegister v30{};
	PPCVRegister v31{};
	PPCRegister temp{};
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lvlx v13,0,r4
	temp.u32 = ctx.r4.u32;
	_mm_store_si128((__m128i*)ctx.v13.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r10,r1,-32
	ctx.r10.s64 = ctx.r1.s64 + -32;
	// addi r9,r1,-32
	ctx.r9.s64 = ctx.r1.s64 + -32;
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// li r7,84
	ctx.r7.s64 = 84;
	// lfs f0,-27468(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r11.u32 + -27468);
	f0.f64 = double(temp.f32);
	// addi r6,r8,-28208
	ctx.r6.s64 = ctx.r8.s64 + -28208;
	// stfs f0,-32(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// lis r5,-32246
	ctx.r5.s64 = -2113273856;
	// lvlx v12,0,r10
	temp.u32 = ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v12.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v9,v12,0
	_mm_store_si128((__m128i*)ctx.v9.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v12.u32), 0xFF));
	// stfs f0,-32(r1)
	temp.f32 = float(f0.f64);
	PPC_STORE_U32(ctx.r1.u32 + -32, temp.u32);
	// li r11,88
	r11.s64 = 88;
	// lvlx v11,0,r9
	temp.u32 = ctx.r9.u32;
	_mm_store_si128((__m128i*)ctx.v11.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vspltw v10,v11,0
	_mm_store_si128((__m128i*)ctx.v10.u32, _mm_shuffle_epi32(_mm_load_si128((__m128i*)ctx.v11.u32), 0xFF));
	// addi r9,r5,-28192
	ctx.r9.s64 = ctx.r5.s64 + -28192;
	// lvx128 v0,r0,r6
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r6.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// li r10,4
	ctx.r10.s64 = 4;
	// lvlx v8,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v8.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lis r8,-32246
	ctx.r8.s64 = -2113273856;
	// vperm v7,v10,v8,v0
	_mm_store_si128((__m128i*)ctx.v7.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v10.u8), _mm_load_si128((__m128i*)ctx.v8.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v6,v9,v13,v0
	_mm_store_si128((__m128i*)ctx.v6.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v9.u8), _mm_load_si128((__m128i*)ctx.v13.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// li r7,92
	ctx.r7.s64 = 92;
	// li r6,8
	ctx.r6.s64 = 8;
	// lvlx v5,r3,r11
	temp.u32 = ctx.r3.u32 + r11.u32;
	_mm_store_si128((__m128i*)ctx.v5.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// addi r5,r8,-28176
	ctx.r5.s64 = ctx.r8.s64 + -28176;
	// lvx128 v0,r0,r9
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// lvlx v4,r4,r10
	temp.u32 = ctx.r4.u32 + ctx.r10.u32;
	_mm_store_si128((__m128i*)ctx.v4.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// vperm v3,v7,v5,v0
	_mm_store_si128((__m128i*)ctx.v3.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v7.u8), _mm_load_si128((__m128i*)ctx.v5.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v2,v6,v4,v0
	_mm_store_si128((__m128i*)ctx.v2.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v6.u8), _mm_load_si128((__m128i*)ctx.v4.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// addi r9,r1,-16
	ctx.r9.s64 = ctx.r1.s64 + -16;
	// lvlx v1,r3,r7
	temp.u32 = ctx.r3.u32 + ctx.r7.u32;
	_mm_store_si128((__m128i*)ctx.v1.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvlx v31,r4,r6
	temp.u32 = ctx.r4.u32 + ctx.r6.u32;
	_mm_store_si128((__m128i*)v31.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + (temp.u32 & ~0xF))), _mm_load_si128((__m128i*)&VectorMaskL[(temp.u32 & 0xF) * 16])));
	// lvx128 v0,r0,r5
	_mm_store_si128((__m128i*)ctx.v0.u8, _mm_shuffle_epi8(_mm_load_si128((__m128i*)(base + ((ctx.r5.u32) & ~0xF))), _mm_load_si128((__m128i*)VectorMaskL)));
	// vperm v30,v3,v1,v0
	_mm_store_si128((__m128i*)v30.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v3.u8), _mm_load_si128((__m128i*)ctx.v1.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vperm v29,v2,v31,v0
	_mm_store_si128((__m128i*)v29.u8, _mm_perm_epi8_(_mm_load_si128((__m128i*)ctx.v2.u8), _mm_load_si128((__m128i*)v31.u8), _mm_load_si128((__m128i*)ctx.v0.u8)));
	// vsubfp v28,v29,v30
	ctx.fpscr.enableFlushModeUnconditional();
	_mm_store_ps(v28.f32, _mm_sub_ps(_mm_load_ps(v29.f32), _mm_load_ps(v30.f32)));
	// vmsum3fp128 v27,v28,v28
	_mm_store_ps(v27.f32, _mm_dp_ps(_mm_load_ps(v28.f32), _mm_load_ps(v28.f32), 0xEF));
	// stvx128 v27,r0,r9
	_mm_store_si128((__m128i*)(base + ((ctx.r9.u32) & ~0xF)), _mm_shuffle_epi8(_mm_load_si128((__m128i*)v27.u8), _mm_load_si128((__m128i*)VectorMaskL)));
	// lfs f0,-16(r1)
	ctx.fpscr.disableFlushModeUnconditional();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + -16);
	f0.f64 = double(temp.f32);
	// fsqrts f1,f0
	ctx.f1.f64 = double(float(sqrt(f0.f64)));
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FC078"))) PPC_WEAK_FUNC(sub_822FC078);
PPC_FUNC_IMPL(__imp__sub_822FC078) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd4
	// stfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -88, f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r25,r4
	r25.u64 = ctx.r4.u64;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// bl 0x822fbfc8
	sub_822FBFC8(ctx, base);
	// lbz r11,44(r25)
	r11.u64 = PPC_LOAD_U8(r25.u32 + 44);
	// fmr f31,f1
	ctx.fpscr.disableFlushMode();
	f31.f64 = ctx.f1.f64;
	// lhz r27,156(r25)
	r27.u64 = PPC_LOAD_U16(r25.u32 + 156);
	// addi r24,r28,356
	r24.s64 = r28.s64 + 356;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fc0b4
	if (!cr6.eq) goto loc_822FC0B4;
	// addi r24,r28,344
	r24.s64 = r28.s64 + 344;
loc_822FC0B4:
	// lwz r10,4(r24)
	ctx.r10.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// mr r11,r24
	r11.u64 = r24.u64;
	// lis r9,0
	ctx.r9.s64 = 0;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// ori r26,r9,65535
	r26.u64 = ctx.r9.u64 | 65535;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822FC0D0:
	// lwz r9,4(r24)
	ctx.r9.u64 = PPC_LOAD_U32(r24.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fc0e4
	if (cr6.eq) goto loc_822FC0E4;
	// cmplw cr6,r11,r24
	cr6.compare<uint32_t>(r11.u32, r24.u32, xer);
	// beq cr6,0x822fc0e8
	if (cr6.eq) goto loc_822FC0E8;
loc_822FC0E4:
	// twi 31,r0,22
loc_822FC0E8:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fc2e0
	if (cr6.eq) goto loc_822FC2E0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fc0fc
	if (!cr6.eq) goto loc_822FC0FC;
	// twi 31,r0,22
loc_822FC0FC:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fc10c
	if (!cr6.eq) goto loc_822FC10C;
	// twi 31,r0,22
loc_822FC10C:
	// addi r11,r10,16
	r11.s64 = ctx.r10.s64 + 16;
	// addi r31,r28,368
	r31.s64 = r28.s64 + 368;
	// addi r5,r11,8
	ctx.r5.s64 = r11.s64 + 8;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r11,372(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 372);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lwz r29,88(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822fc144
	if (cr6.eq) goto loc_822FC144;
	// cmplw cr6,r29,r31
	cr6.compare<uint32_t>(r29.u32, r31.u32, xer);
	// beq cr6,0x822fc148
	if (cr6.eq) goto loc_822FC148;
loc_822FC144:
	// twi 31,r0,22
loc_822FC148:
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822fc2cc
	if (cr6.eq) goto loc_822FC2CC;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// bne cr6,0x822fc160
	if (!cr6.eq) goto loc_822FC160;
	// twi 31,r0,22
loc_822FC160:
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc170
	if (!cr6.eq) goto loc_822FC170;
	// twi 31,r0,22
loc_822FC170:
	// lwz r4,24(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// ld r11,120(r25)
	r11.u64 = PPC_LOAD_U64(r25.u32 + 120);
	// ld r10,120(r4)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r4.u32 + 120);
	// cmpld cr6,r10,r11
	cr6.compare<uint64_t>(ctx.r10.u64, r11.u64, xer);
	// beq cr6,0x822fc2cc
	if (cr6.eq) goto loc_822FC2CC;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc190
	if (!cr6.eq) goto loc_822FC190;
	// twi 31,r0,22
loc_822FC190:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822fbfc8
	sub_822FBFC8(ctx, base);
	// clrlwi r11,r23,24
	r11.u64 = r23.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fc1f0
	if (cr6.eq) goto loc_822FC1F0;
	// fcmpu cr6,f31,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, ctx.f1.f64);
	// blt cr6,0x822fc278
	if (cr6.lt) goto loc_822FC278;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc1b8
	if (!cr6.eq) goto loc_822FC1B8;
	// twi 31,r0,22
loc_822FC1B8:
	// lhz r11,156(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 156);
	// clrlwi r10,r27,16
	ctx.r10.u64 = r27.u32 & 0xFFFF;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x822fc2cc
	if (cr6.gt) goto loc_822FC2CC;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc1d4
	if (!cr6.eq) goto loc_822FC1D4;
	// twi 31,r0,22
loc_822FC1D4:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// clrlwi r27,r11,16
	r27.u64 = r11.u32 & 0xFFFF;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822fc0d0
	goto loc_822FC0D0;
loc_822FC1F0:
	// fcmpu cr6,f31,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, ctx.f1.f64);
	// ble cr6,0x822fc248
	if (!cr6.gt) goto loc_822FC248;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc204
	if (!cr6.eq) goto loc_822FC204;
	// twi 31,r0,22
loc_822FC204:
	// lhz r11,156(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 156);
	// lhz r10,156(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 156);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x822fc248
	if (cr6.gt) goto loc_822FC248;
	// clrlwi r11,r27,16
	r11.u64 = r27.u32 & 0xFFFF;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r27,r11,16
	r27.u64 = r11.u32 & 0xFFFF;
	// bne cr6,0x822fc22c
	if (!cr6.eq) goto loc_822FC22C;
	// twi 31,r0,22
loc_822FC22C:
	// lhz r11,156(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 156);
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// clrlwi r11,r10,16
	r11.u64 = ctx.r10.u32 & 0xFFFF;
	// sth r11,156(r4)
	PPC_STORE_U16(ctx.r4.u32 + 156, r11.u16);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r31,r9
	cr6.compare<uint32_t>(r31.u32, ctx.r9.u32, xer);
	// b 0x822fc29c
	goto loc_822FC29C;
loc_822FC248:
	// fcmpu cr6,f31,f1
	ctx.fpscr.disableFlushMode();
	cr6.compare(f31.f64, ctx.f1.f64);
	// bge cr6,0x822fc2cc
	if (!cr6.lt) goto loc_822FC2CC;
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc25c
	if (!cr6.eq) goto loc_822FC25C;
	// twi 31,r0,22
loc_822FC25C:
	// lhz r11,156(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 156);
	// lhz r10,156(r4)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r4.u32 + 156);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822fc2cc
	if (cr6.lt) goto loc_822FC2CC;
	// clrlwi r11,r27,16
	r11.u64 = r27.u32 & 0xFFFF;
	// add r10,r11,r26
	ctx.r10.u64 = r11.u64 + r26.u64;
	// clrlwi r27,r10,16
	r27.u64 = ctx.r10.u32 & 0xFFFF;
loc_822FC278:
	// cmplw cr6,r31,r30
	cr6.compare<uint32_t>(r31.u32, r30.u32, xer);
	// bne cr6,0x822fc284
	if (!cr6.eq) goto loc_822FC284;
	// twi 31,r0,22
loc_822FC284:
	// lhz r11,156(r4)
	r11.u64 = PPC_LOAD_U16(ctx.r4.u32 + 156);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// clrlwi r11,r11,16
	r11.u64 = r11.u32 & 0xFFFF;
	// sth r11,156(r4)
	PPC_STORE_U16(ctx.r4.u32 + 156, r11.u16);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
loc_822FC29C:
	// bne cr6,0x822fc2a4
	if (!cr6.eq) goto loc_822FC2A4;
	// twi 31,r0,22
loc_822FC2A4:
	// lwz r10,548(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 548);
	// clrlwi r6,r11,16
	ctx.r6.u64 = r11.u32 & 0xFFFF;
	// lwz r9,24(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r3,0(r10)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// ld r4,120(r9)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r9.u32 + 120);
	// lwz r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r7,80(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 80);
	// mtctr r7
	ctr.u64 = ctx.r7.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822FC2CC:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822fc0d0
	goto loc_822FC0D0;
loc_822FC2E0:
	// lhz r11,156(r25)
	r11.u64 = PPC_LOAD_U16(r25.u32 + 156);
	// clrlwi r6,r27,16
	ctx.r6.u64 = r27.u32 & 0xFFFF;
	// cmplw cr6,r11,r6
	cr6.compare<uint32_t>(r11.u32, ctx.r6.u32, xer);
	// beq cr6,0x822fc314
	if (cr6.eq) goto loc_822FC314;
	// sth r27,156(r25)
	PPC_STORE_U16(r25.u32 + 156, r27.u16);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r11,548(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 548);
	// ld r4,120(r25)
	ctx.r4.u64 = PPC_LOAD_U64(r25.u32 + 120);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,80(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 80);
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822FC314:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f31,-88(r1)
	ctx.fpscr.disableFlushMode();
	f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x82ca2c24
	return;
}

__attribute__((alias("__imp__sub_822FC320"))) PPC_WEAK_FUNC(sub_822FC320);
PPC_FUNC_IMPL(__imp__sub_822FC320) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r10,-31924
	ctx.r10.s64 = -2092171264;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lis r28,-31924
	r28.s64 = -2092171264;
	// lwz r11,7956(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 7956);
	// clrlwi r9,r11,31
	ctx.r9.u64 = r11.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822fc35c
	if (!cr6.eq) goto loc_822FC35C;
	// lwz r9,32(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// ori r11,r11,1
	r11.u64 = r11.u64 | 1;
	// stw r11,7956(r10)
	PPC_STORE_U32(ctx.r10.u32 + 7956, r11.u32);
	// stw r9,7952(r28)
	PPC_STORE_U32(r28.u32 + 7952, ctx.r9.u32);
	// b 0x822fc360
	goto loc_822FC360;
loc_822FC35C:
	// lwz r9,7952(r28)
	ctx.r9.u64 = PPC_LOAD_U32(r28.u32 + 7952);
loc_822FC360:
	// rlwinm r8,r11,0,30,30
	ctx.r8.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x2;
	// lis r29,-31924
	r29.s64 = -2092171264;
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822fc380
	if (!cr6.eq) goto loc_822FC380;
	// lwz r8,388(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 388);
	// ori r11,r11,2
	r11.u64 = r11.u64 | 2;
	// stw r11,7956(r10)
	PPC_STORE_U32(ctx.r10.u32 + 7956, r11.u32);
	// stw r8,7948(r29)
	PPC_STORE_U32(r29.u32 + 7948, ctx.r8.u32);
loc_822FC380:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r30,r31,32
	r30.s64 = r31.s64 + 32;
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fc3b0
	if (cr6.eq) goto loc_822FC3B0;
	// lis r4,4096
	ctx.r4.s64 = 268435456;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,4
	ctx.r5.s64 = 4;
	// ori r4,r4,10
	ctx.r4.u64 = ctx.r4.u64 | 10;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r11,7952(r28)
	PPC_STORE_U32(r28.u32 + 7952, r11.u32);
loc_822FC3B0:
	// lwz r11,7948(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 7948);
	// addi r30,r31,388
	r30.s64 = r31.s64 + 388;
	// lwz r10,388(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 388);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// beq cr6,0x822fc3e4
	if (cr6.eq) goto loc_822FC3E4;
	// lis r4,4096
	ctx.r4.s64 = 268435456;
	// lwz r3,256(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,4
	ctx.r5.s64 = 4;
	// ori r4,r4,24
	ctx.r4.u64 = ctx.r4.u64 | 24;
	// bl 0x82cbc140
	sub_82CBC140(ctx, base);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r11,7948(r29)
	PPC_STORE_U32(r29.u32 + 7948, r11.u32);
loc_822FC3E4:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,120(r31)
	PPC_STORE_U32(r31.u32 + 120, ctx.r3.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822FC3F8"))) PPC_WEAK_FUNC(sub_822FC3F8);
PPC_FUNC_IMPL(__imp__sub_822FC3F8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lwz r11,156(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 156);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x822fc6e0
	if (!cr6.eq) goto loc_822FC6E0;
	// lwz r11,172(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 172);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x822fc434
	if (!cr6.eq) goto loc_822FC434;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,27
	ctx.r4.s64 = 27;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c34
	return;
loc_822FC434:
	// li r11,16
	r11.s64 = 16;
	// li r28,0
	r28.s64 = 0;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// li r27,1
	r27.s64 = 1;
loc_822FC444:
	// addi r8,r1,92
	ctx.r8.s64 = ctx.r1.s64 + 92;
	// lwz r3,172(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 172);
	// addi r7,r1,176
	ctx.r7.s64 = ctx.r1.s64 + 176;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,45
	ctx.r5.s64 = 45;
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// bl 0x82cfab20
	sub_82CFAB20(ctx, base);
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// beq cr6,0x822fc6c4
	if (cr6.eq) goto loc_822FC6C4;
	// cmplwi cr6,r3,45
	cr6.compare<uint32_t>(ctx.r3.u32, 45, xer);
	// bne cr6,0x822fc6a8
	if (!cr6.eq) goto loc_822FC6A8;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r4,r1,224
	ctx.r4.s64 = ctx.r1.s64 + 224;
	// li r5,45
	ctx.r5.s64 = 45;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lbz r11,224(r1)
	r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 224);
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// beq cr6,0x822fc494
	if (cr6.eq) goto loc_822FC494;
	// cmplwi cr6,r11,4
	cr6.compare<uint32_t>(r11.u32, 4, xer);
	// bne cr6,0x822fc6b0
	if (!cr6.eq) goto loc_822FC6B0;
loc_822FC494:
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// lwz r10,249(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 249);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fc4ac
	if (!cr6.eq) goto loc_822FC4AC;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fc4b8
	if (!cr6.eq) goto loc_822FC4B8;
loc_822FC4AC:
	// li r31,4
	r31.s64 = 4;
	// stb r31,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, r31.u8);
	// b 0x822fc4bc
	goto loc_822FC4BC;
loc_822FC4B8:
	// lbz r31,112(r1)
	r31.u64 = PPC_LOAD_U8(ctx.r1.u32 + 112);
loc_822FC4BC:
	// li r11,125
	r11.s64 = 125;
	// ldx r11,r1,r11
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + r11.u32);
	// ld r10,264(r29)
	ctx.r10.u64 = PPC_LOAD_U64(r29.u32 + 264);
	// cmpld cr6,r11,r10
	cr6.compare<uint64_t>(r11.u64, ctx.r10.u64, xer);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// beq cr6,0x822fc6b0
	if (cr6.eq) goto loc_822FC6B0;
	// addi r30,r29,368
	r30.s64 = r29.s64 + 368;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r11,96(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r9,372(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fc500
	if (cr6.eq) goto loc_822FC500;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x822fc504
	if (cr6.eq) goto loc_822FC504;
loc_822FC500:
	// twi 31,r0,22
loc_822FC504:
	// lwz r10,100(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fc5f4
	if (!cr6.eq) goto loc_822FC5F4;
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r11,3
	cr6.compare<uint32_t>(r11.u32, 3, xer);
	// bne cr6,0x822fc6b0
	if (!cr6.eq) goto loc_822FC6B0;
	// li r3,752
	ctx.r3.s64 = 752;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fc538
	if (cr6.eq) goto loc_822FC538;
	// bl 0x822f8778
	sub_822F8778(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// b 0x822fc53c
	goto loc_822FC53C;
loc_822FC538:
	// mr r31,r28
	r31.u64 = r28.u64;
loc_822FC53C:
	// addi r5,r1,141
	ctx.r5.s64 = ctx.r1.s64 + 141;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r31,128
	ctx.r3.s64 = r31.s64 + 128;
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// li r7,0
	ctx.r7.s64 = 0;
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// lwz r3,256(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 256);
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x82cbc158
	sub_82CBC158(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822fc584
	if (!cr6.eq) goto loc_822FC584;
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// xori r8,r9,1
	ctx.r8.u64 = ctx.r9.u64 ^ 1;
	// stb r8,44(r31)
	PPC_STORE_U8(r31.u32 + 44, ctx.r8.u8);
	// b 0x822fc588
	goto loc_822FC588;
loc_822FC584:
	// stb r28,44(r31)
	PPC_STORE_U8(r31.u32 + 44, r28.u8);
loc_822FC588:
	// addi r4,r1,113
	ctx.r4.s64 = ctx.r1.s64 + 113;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r11,133(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 133);
	// stw r11,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r11.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,744(r31)
	PPC_STORE_U32(r31.u32 + 744, ctx.r3.u32);
	// stw r28,40(r31)
	PPC_STORE_U32(r31.u32 + 40, r28.u32);
	// addi r5,r1,160
	ctx.r5.s64 = ctx.r1.s64 + 160;
	// lwz r10,180(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r10,100(r31)
	PPC_STORE_U32(r31.u32 + 100, ctx.r10.u32);
	// addi r3,r1,272
	ctx.r3.s64 = ctx.r1.s64 + 272;
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r9,120(r31)
	PPC_STORE_U64(r31.u32 + 120, ctx.r9.u64);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r31,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, r31.u32);
	// std r8,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r8.u64);
	// bl 0x822ff270
	sub_822FF270(ctx, base);
	// ld r7,80(r1)
	ctx.r7.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r28,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, r28.u32);
	// addi r4,r1,192
	ctx.r4.s64 = ctx.r1.s64 + 192;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// std r7,200(r1)
	PPC_STORE_U64(ctx.r1.u32 + 200, ctx.r7.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// b 0x822fc6b0
	goto loc_822FC6B0;
loc_822FC5F4:
	// clrlwi r9,r31,24
	ctx.r9.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r9,3
	cr6.compare<uint32_t>(ctx.r9.u32, 3, xer);
	// bne cr6,0x822fc664
	if (!cr6.eq) goto loc_822FC664;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fc60c
	if (!cr6.eq) goto loc_822FC60C;
	// twi 31,r0,22
loc_822FC60C:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fc61c
	if (!cr6.eq) goto loc_822FC61C;
	// twi 31,r0,22
loc_822FC61C:
	// lwz r31,24(r10)
	r31.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// addi r4,r1,113
	ctx.r4.s64 = ctx.r1.s64 + 113;
	// li r5,12
	ctx.r5.s64 = 12;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lwz r8,24(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// stw r11,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r11.u32);
	// stw r10,32(r31)
	PPC_STORE_U32(r31.u32 + 32, ctx.r10.u32);
	// stw r9,36(r31)
	PPC_STORE_U32(r31.u32 + 36, ctx.r9.u32);
	// stw r8,40(r31)
	PPC_STORE_U32(r31.u32 + 40, ctx.r8.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r7,133(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 133);
	// stw r7,24(r31)
	PPC_STORE_U32(r31.u32 + 24, ctx.r7.u32);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,744(r31)
	PPC_STORE_U32(r31.u32 + 744, ctx.r3.u32);
	// b 0x822fc6b0
	goto loc_822FC6B0;
loc_822FC664:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r27,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, r27.u32);
	// bne cr6,0x822fc674
	if (!cr6.eq) goto loc_822FC674;
	// twi 31,r0,22
loc_822FC674:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bne cr6,0x822fc684
	if (!cr6.eq) goto loc_822FC684;
	// twi 31,r0,22
loc_822FC684:
	// ld r11,16(r10)
	r11.u64 = PPC_LOAD_U64(ctx.r10.u32 + 16);
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// std r11,216(r1)
	PPC_STORE_U64(ctx.r1.u32 + 216, r11.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x822fe640
	sub_822FE640(ctx, base);
	// b 0x822fc6b0
	goto loc_822FC6B0;
loc_822FC6A8:
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x822fc6e0
	if (!cr6.gt) goto loc_822FC6E0;
loc_822FC6B0:
	// lhz r10,178(r1)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r1.u32 + 178);
	// cmplwi cr6,r10,1002
	cr6.compare<uint32_t>(ctx.r10.u32, 1002, xer);
	// beq cr6,0x822fc444
	if (cr6.eq) goto loc_822FC444;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c34
	return;
loc_822FC6C4:
	// bl 0x82cfac30
	sub_82CFAC30(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,10035
	cr6.compare<int32_t>(ctx.r5.s32, 10035, xer);
	// beq cr6,0x822fc6e0
	if (cr6.eq) goto loc_822FC6E0;
	// li r4,28
	ctx.r4.s64 = 28;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
loc_822FC6E0:
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822FC6E8"))) PPC_WEAK_FUNC(sub_822FC6E8);
PPC_FUNC_IMPL(__imp__sub_822FC6E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,7
	cr6.compare<int32_t>(r11.s32, 7, xer);
	// bne cr6,0x822fc828
	if (!cr6.eq) goto loc_822FC828;
	// lwz r11,172(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x822fc734
	if (!cr6.eq) goto loc_822FC734;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,29
	ctx.r4.s64 = 29;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822FC734:
	// lwz r11,32(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fc828
	if (cr6.eq) goto loc_822FC828;
	// clrlwi r10,r4,24
	ctx.r10.u64 = ctx.r4.u32 & 0xFF;
	// stw r11,121(r1)
	PPC_STORE_U32(ctx.r1.u32 + 121, r11.u32);
	// addi r4,r31,84
	ctx.r4.s64 = r31.s64 + 84;
	// cntlzw r9,r10
	ctx.r9.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// addi r3,r1,97
	ctx.r3.s64 = ctx.r1.s64 + 97;
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// li r5,12
	ctx.r5.s64 = 12;
	// xori r11,r8,1
	r11.u64 = ctx.r8.u64 ^ 1;
	// addi r7,r11,3
	ctx.r7.s64 = r11.s64 + 3;
	// stb r7,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r7.u8);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,117(r1)
	PPC_STORE_U32(ctx.r1.u32 + 117, ctx.r3.u32);
	// ld r6,264(r31)
	ctx.r6.u64 = PPC_LOAD_U64(r31.u32 + 264);
	// addi r5,r31,272
	ctx.r5.s64 = r31.s64 + 272;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,125
	ctx.r3.s64 = ctx.r1.s64 + 125;
	// li r12,109
	r12.s64 = 109;
	// stdx r6,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r6.u64);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// li r5,2
	ctx.r5.s64 = 2;
	// li r4,-1
	ctx.r4.s64 = -1;
	// lwz r3,172(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 172);
	// li r11,1002
	r11.s64 = 1002;
	// sth r5,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, ctx.r5.u16);
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// li r8,16
	ctx.r8.s64 = 16;
	// sth r11,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, r11.u16);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,45
	ctx.r5.s64 = 45;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x82cfabb0
	sub_82CFABB0(ctx, base);
	// cmpwi cr6,r3,45
	cr6.compare<int32_t>(ctx.r3.s32, 45, xer);
	// beq cr6,0x822fc820
	if (cr6.eq) goto loc_822FC820;
	// cmpwi cr6,r3,-1
	cr6.compare<int32_t>(ctx.r3.s32, -1, xer);
	// bne cr6,0x822fc7fc
	if (!cr6.eq) goto loc_822FC7FC;
	// bl 0x82cfac30
	sub_82CFAC30(ctx, base);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r4,30
	ctx.r4.s64 = 30;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822FC7FC:
	// subfic r5,r3,45
	xer.ca = ctx.r3.u32 <= 45;
	ctx.r5.s64 = 45 - ctx.r3.s64;
	// li r4,31
	ctx.r4.s64 = 31;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_822FC820:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,116(r31)
	PPC_STORE_U32(r31.u32 + 116, ctx.r3.u32);
loc_822FC828:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FC840"))) PPC_WEAK_FUNC(sub_822FC840);
PPC_FUNC_IMPL(__imp__sub_822FC840) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r3,r11,9216
	ctx.r3.s64 = r11.s64 + 9216;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// cmpwi cr6,r10,7
	cr6.compare<int32_t>(ctx.r10.s32, 7, xer);
	// bne cr6,0x822fc87c
	if (!cr6.eq) goto loc_822FC87C;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fc6e8
	sub_822FC6E8(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
loc_822FC87C:
	// addi r31,r30,344
	r31.s64 = r30.s64 + 344;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822feac8
	sub_822FEAC8(ctx, base);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_822FC8A0:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fc8b4
	if (cr6.eq) goto loc_822FC8B4;
	// addi r9,r1,112
	ctx.r9.s64 = ctx.r1.s64 + 112;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fc8b8
	if (cr6.eq) goto loc_822FC8B8;
loc_822FC8B4:
	// twi 31,r0,22
loc_822FC8B8:
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fc904
	if (cr6.eq) goto loc_822FC904;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fc8d0
	if (!cr6.eq) goto loc_822FC8D0;
	// twi 31,r0,22
loc_822FC8D0:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fc8e0
	if (!cr6.eq) goto loc_822FC8E0;
	// twi 31,r0,22
loc_822FC8E0:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,16(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822fc8a0
	goto loc_822FC8A0;
loc_822FC904:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822fff10
	sub_822FFF10(ctx, base);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r28,0
	r28.s64 = 0;
	// addi r4,r30,356
	ctx.r4.s64 = r30.s64 + 356;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// bl 0x822feac8
	sub_822FEAC8(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r29,r1,96
	r29.s64 = ctx.r1.s64 + 96;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// stw r29,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r29.u32);
	// addi r27,r10,9248
	r27.s64 = ctx.r10.s64 + 9248;
	// lwz r31,0(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r31.u32);
loc_822FC95C:
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822fc970
	if (cr6.eq) goto loc_822FC970;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// beq cr6,0x822fc974
	if (cr6.eq) goto loc_822FC974;
loc_822FC970:
	// twi 31,r0,22
loc_822FC974:
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822fca08
	if (cr6.eq) goto loc_822FCA08;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// bne cr6,0x822fc98c
	if (!cr6.eq) goto loc_822FC98C;
	// twi 31,r0,22
loc_822FC98C:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fc99c
	if (!cr6.eq) goto loc_822FC99C;
	// twi 31,r0,22
loc_822FC99C:
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r3,256(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 256);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r4,r31,24
	ctx.r4.s64 = r31.s64 + 24;
	// bl 0x82cbc158
	sub_82CBC158(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// bne cr6,0x822fc9f0
	if (!cr6.eq) goto loc_822FC9F0;
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822fc9f0
	if (!cr6.eq) goto loc_822FC9F0;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fc9e0
	if (!cr6.eq) goto loc_822FC9E0;
	// twi 31,r0,22
loc_822FC9E0:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,16(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
loc_822FC9F0:
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// lwz r31,92(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lwz r29,88(r1)
	r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// b 0x822fc95c
	goto loc_822FC95C;
loc_822FCA08:
	// lwz r10,156(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 156);
	// cmpwi cr6,r10,10
	cr6.compare<int32_t>(ctx.r10.s32, 10, xer);
	// beq cr6,0x822fca8c
	if (cr6.eq) goto loc_822FCA8C;
	// cmpwi cr6,r10,5
	cr6.compare<int32_t>(ctx.r10.s32, 5, xer);
	// ble cr6,0x822fca8c
	if (!cr6.gt) goto loc_822FCA8C;
	// lwz r3,300(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 300);
	// stb r28,328(r30)
	PPC_STORE_U8(r30.u32 + 328, r28.u8);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fca34
	if (cr6.eq) goto loc_822FCA34;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// stw r28,300(r30)
	PPC_STORE_U32(r30.u32 + 300, r28.u32);
loc_822FCA34:
	// lbz r11,297(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 297);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fca78
	if (cr6.eq) goto loc_822FCA78;
	// mr r29,r28
	r29.u64 = r28.u64;
loc_822FCA44:
	// addi r11,r29,76
	r11.s64 = r29.s64 + 76;
	// rlwinm r31,r11,2,0,29
	r31.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r3,r31,r30
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + r30.u32);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fca60
	if (cr6.eq) goto loc_822FCA60;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// stwx r28,r31,r30
	PPC_STORE_U32(r31.u32 + r30.u32, r28.u32);
loc_822FCA60:
	// addi r11,r29,1
	r11.s64 = r29.s64 + 1;
	// lbz r10,297(r30)
	ctx.r10.u64 = PPC_LOAD_U8(r30.u32 + 297);
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// mr r29,r11
	r29.u64 = r11.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// blt cr6,0x822fca44
	if (cr6.lt) goto loc_822FCA44;
loc_822FCA78:
	// stb r28,296(r30)
	PPC_STORE_U8(r30.u32 + 296, r28.u8);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fa5f8
	sub_822FA5F8(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
loc_822FCA8C:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// addi r8,r1,96
	ctx.r8.s64 = ctx.r1.s64 + 96;
	// addi r9,r1,96
	ctx.r9.s64 = ctx.r1.s64 + 96;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// ld r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x823010a8
	sub_823010A8(ctx, base);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,116(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r28.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stw r28,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, r28.u32);
	// addi r6,r1,112
	ctx.r6.s64 = ctx.r1.s64 + 112;
	// stw r6,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r6.u32);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r11.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r7,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r7.u32);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// ld r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x823010a8
	sub_823010A8(ctx, base);
	// lwz r3,116(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822FCB10"))) PPC_WEAK_FUNC(sub_822FCB10);
PPC_FUNC_IMPL(__imp__sub_822FCB10) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r0{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r15{};
	PPCRegister r17{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r11,9268
	ctx.r3.s64 = r11.s64 + 9268;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r29,r31,128
	r29.s64 = r31.s64 + 128;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x82cbcf80
	sub_82CBCF80(ctx, base);
	// cmpwi cr6,r30,32
	cr6.compare<int32_t>(r30.s32, 32, xer);
	// beq cr6,0x822fcb50
	if (cr6.eq) goto loc_822FCB50;
	// stw r30,24(r31)
	PPC_STORE_U32(r31.u32 + 24, r30.u32);
	// stw r28,28(r31)
	PPC_STORE_U32(r31.u32 + 28, r28.u32);
loc_822FCB50:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r11,r11,-12
	r11.s64 = r11.s64 + -12;
	// cmplwi cr6,r11,22
	cr6.compare<uint32_t>(r11.u32, 22, xer);
	// bgt cr6,0x822fcd14
	if (cr6.gt) goto loc_822FCD14;
	// li r30,0
	r30.s64 = 0;
	// lis r12,-32208
	r12.s64 = -2110783488;
	// addi r12,r12,-13444
	r12.s64 = r12.s64 + -13444;
	// rlwinm r0,r11,2,0,29
	r0.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	r0.u64 = PPC_LOAD_U32(r12.u32 + r0.u32);
	// mtctr r0
	ctr.u64 = r0.u64;
	// bctr 
	switch (r11.u64) {
	case 0:
		goto loc_822FCBD8;
	case 1:
		goto loc_822FCC04;
	case 2:
		goto loc_822FCBE4;
	case 3:
		goto loc_822FCBE4;
	case 4:
		goto loc_822FCC9C;
	case 5:
		goto loc_822FCC88;
	case 6:
		goto loc_822FCC88;
	case 7:
		goto loc_822FCC88;
	case 8:
		goto loc_822FCD14;
	case 9:
		goto loc_822FCD14;
	case 10:
		goto loc_822FCD14;
	case 11:
		goto loc_822FCD14;
	case 12:
		goto loc_822FCC44;
	case 13:
		goto loc_822FCBD8;
	case 14:
		goto loc_822FCD14;
	case 15:
		goto loc_822FCD14;
	case 16:
		goto loc_822FCD14;
	case 17:
		goto loc_822FCD14;
	case 18:
		goto loc_822FCD14;
	case 19:
		goto loc_822FCD14;
	case 20:
		goto loc_822FCD14;
	case 21:
		goto loc_822FCBD8;
	case 22:
		goto loc_822FCCFC;
	default:
		__builtin_unreachable();
	}
	// lwz r17,-13352(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13352);
	// lwz r17,-13308(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13308);
	// lwz r17,-13340(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13340);
	// lwz r17,-13340(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13340);
	// lwz r17,-13156(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13156);
	// lwz r17,-13176(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13176);
	// lwz r17,-13176(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13176);
	// lwz r17,-13176(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13176);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13244(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13244);
	// lwz r17,-13352(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13352);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13036(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13036);
	// lwz r17,-13352(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13352);
	// lwz r17,-13060(r15)
	r17.u64 = PPC_LOAD_U32(r15.u32 + -13060);
loc_822FCBD8:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCBE4:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,9312
	ctx.r3.s64 = r11.s64 + 9312;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// std r30,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r30.u64);
	// std r30,8(r29)
	PPC_STORE_U64(r29.u32 + 8, r30.u64);
	// std r30,16(r29)
	PPC_STORE_U64(r29.u32 + 16, r30.u64);
	// stw r30,24(r29)
	PPC_STORE_U32(r29.u32 + 24, r30.u32);
loc_822FCC04:
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// beq cr6,0x822fcc38
	if (cr6.eq) goto loc_822FCC38;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822fcc34
	if (!cr6.eq) goto loc_822FCC34;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCC34:
	// stw r30,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r30.u32);
loc_822FCC38:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCC44:
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,4
	cr6.compare<int32_t>(r11.s32, 4, xer);
	// beq cr6,0x822fcc7c
	if (cr6.eq) goto loc_822FCC7C;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822fcc74
	if (!cr6.eq) goto loc_822FCC74;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCC74:
	// li r11,4
	r11.s64 = 4;
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
loc_822FCC7C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCC88:
	// lwz r3,288(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 288);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fcc9c
	if (cr6.eq) goto loc_822FCC9C;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// stw r30,288(r31)
	PPC_STORE_U32(r31.u32 + 288, r30.u32);
loc_822FCC9C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,9408
	ctx.r3.s64 = r11.s64 + 9408;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// std r30,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r30.u64);
	// std r30,8(r29)
	PPC_STORE_U64(r29.u32 + 8, r30.u64);
	// std r30,16(r29)
	PPC_STORE_U64(r29.u32 + 16, r30.u64);
	// stw r30,24(r29)
	PPC_STORE_U32(r29.u32 + 24, r30.u32);
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,1
	cr6.compare<int32_t>(r11.s32, 1, xer);
	// beq cr6,0x822fccf0
	if (cr6.eq) goto loc_822FCCF0;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822fcce8
	if (!cr6.eq) goto loc_822FCCE8;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCCE8:
	// li r11,1
	r11.s64 = 1;
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
loc_822FCCF0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCCFC:
	// li r30,0
	r30.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r30,456(r31)
	PPC_STORE_U8(r31.u32 + 456, r30.u8);
	// stb r30,457(r31)
	PPC_STORE_U8(r31.u32 + 457, r30.u8);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
loc_822FCD14:
	// lwz r11,156(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 156);
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// beq cr6,0x822fcd30
	if (cr6.eq) goto loc_822FCD30;
	// li r11,10
	r11.s64 = 10;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stw r11,156(r31)
	PPC_STORE_U32(r31.u32 + 156, r11.u32);
	// bl 0x822fc840
	sub_822FC840(ctx, base);
loc_822FCD30:
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822FCD40"))) PPC_WEAK_FUNC(sub_822FCD40);
PPC_FUNC_IMPL(__imp__sub_822FCD40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// cmpw cr6,r4,r11
	cr6.compare<int32_t>(ctx.r4.s32, r11.s32, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// cmpwi cr6,r11,10
	cr6.compare<int32_t>(r11.s32, 10, xer);
	// bne cr6,0x822fcd60
	if (!cr6.eq) goto loc_822FCD60;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,32
	ctx.r4.s64 = 32;
	// b 0x822fcb10
	sub_822FCB10(ctx, base);
	return;
loc_822FCD60:
	// stw r4,156(r3)
	PPC_STORE_U32(ctx.r3.u32 + 156, ctx.r4.u32);
	// cmpwi cr6,r4,10
	cr6.compare<int32_t>(ctx.r4.s32, 10, xer);
	// bnelr cr6
	if (!cr6.eq) return;
	// cmpwi cr6,r4,0
	cr6.compare<int32_t>(ctx.r4.s32, 0, xer);
	// beqlr cr6
	if (cr6.eq) return;
	// b 0x822fc840
	sub_822FC840(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_822FCD78"))) PPC_WEAK_FUNC(sub_822FCD78);
PPC_FUNC_IMPL(__imp__sub_822FCD78) {
	PPC_FUNC_PROLOGUE();
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FCD80"))) PPC_WEAK_FUNC(sub_822FCD80);
PPC_FUNC_IMPL(__imp__sub_822FCD80) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// bl 0x82cf9b18
	sub_82CF9B18(ctx, base);
	// cmplwi cr6,r3,3
	cr6.compare<uint32_t>(ctx.r3.u32, 3, xer);
	// bne cr6,0x822fcdc0
	if (!cr6.eq) goto loc_822FCDC0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r11,9512
	ctx.r3.s64 = r11.s64 + 9512;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// b 0x822fce28
	goto loc_822FCE28;
loc_822FCDC0:
	// addi r11,r1,82
	r11.s64 = ctx.r1.s64 + 82;
	// li r10,0
	ctx.r10.s64 = 0;
	// li r9,7
	ctx.r9.s64 = 7;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822FCDD0:
	// sth r10,0(r11)
	PPC_STORE_U16(r11.u32 + 0, ctx.r10.u16);
	// addi r11,r11,2
	r11.s64 = r11.s64 + 2;
	// bdnz 0x822fcdd0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822FCDD0;
	// li r11,2
	r11.s64 = 2;
	// lwz r3,172(r30)
	ctx.r3.u64 = PPC_LOAD_U32(r30.u32 + 172);
	// li r10,1002
	ctx.r10.s64 = 1002;
	// lwz r9,20(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 20);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// li r8,16
	ctx.r8.s64 = 16;
	// sth r11,80(r1)
	PPC_STORE_U16(ctx.r1.u32 + 80, r11.u16);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// sth r10,82(r1)
	PPC_STORE_U16(ctx.r1.u32 + 82, ctx.r10.u16);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// cmpw cr6,r3,r29
	cr6.compare<int32_t>(ctx.r3.s32, r29.s32, xer);
	// beq cr6,0x822fce38
	if (cr6.eq) goto loc_822FCE38;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,9568
	ctx.r3.s64 = r11.s64 + 9568;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FCE28:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
loc_822FCE38:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822FCE40"))) PPC_WEAK_FUNC(sub_822FCE40);
PPC_FUNC_IMPL(__imp__sub_822FCE40) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fce70
	if (cr6.eq) goto loc_822FCE70;
	// lis r11,-32256
	r11.s64 = -2113929216;
	// addi r5,r11,3900
	ctx.r5.s64 = r11.s64 + 3900;
	// b 0x822fce78
	goto loc_822FCE78;
loc_822FCE70:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r5,r11,5664
	ctx.r5.s64 = r11.s64 + 5664;
loc_822FCE78:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r11,9628
	ctx.r3.s64 = r11.s64 + 9628;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r10,0
	ctx.r10.s64 = 0;
	// ld r9,264(r31)
	ctx.r9.u64 = PPC_LOAD_U64(r31.u32 + 264);
	// addi r5,r31,272
	ctx.r5.s64 = r31.s64 + 272;
	// stb r10,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r10.u8);
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,109
	ctx.r3.s64 = ctx.r1.s64 + 109;
	// li r12,93
	r12.s64 = 93;
	// stdx r9,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r9.u64);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// lwz r8,32(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 32);
	// addi r5,r31,36
	ctx.r5.s64 = r31.s64 + 36;
	// li r4,48
	ctx.r4.s64 = 48;
	// addi r3,r1,125
	ctx.r3.s64 = ctx.r1.s64 + 125;
	// stw r8,105(r1)
	PPC_STORE_U32(ctx.r1.u32 + 105, ctx.r8.u32);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// addi r4,r31,84
	ctx.r4.s64 = r31.s64 + 84;
	// addi r3,r1,81
	ctx.r3.s64 = ctx.r1.s64 + 81;
	// li r5,12
	ctx.r5.s64 = 12;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lis r7,-31927
	ctx.r7.s64 = -2092367872;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// stw r3,101(r1)
	PPC_STORE_U32(ctx.r1.u32 + 101, ctx.r3.u32);
	// stb r29,173(r1)
	PPC_STORE_U8(ctx.r1.u32 + 173, r29.u8);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,102
	ctx.r5.s64 = 102;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r11,26932(r7)
	r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + 26932);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,26808(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 26808);
	// stw r11,174(r1)
	PPC_STORE_U32(ctx.r1.u32 + 174, r11.u32);
	// stw r10,178(r1)
	PPC_STORE_U32(ctx.r1.u32 + 178, ctx.r10.u32);
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FCF18"))) PPC_WEAK_FUNC(sub_822FCF18);
PPC_FUNC_IMPL(__imp__sub_822FCF18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// stw r6,133(r1)
	PPC_STORE_U32(ctx.r1.u32 + 133, ctx.r6.u32);
	// li r10,6
	ctx.r10.s64 = 6;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// stb r10,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, ctx.r10.u8);
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// addi r5,r30,272
	ctx.r5.s64 = r30.s64 + 272;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,113
	ctx.r3.s64 = ctx.r1.s64 + 113;
	// stw r11,129(r1)
	PPC_STORE_U32(ctx.r1.u32 + 129, r11.u32);
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// addi r9,r1,113
	ctx.r9.s64 = ctx.r1.s64 + 113;
	// addi r8,r1,117
	ctx.r8.s64 = ctx.r1.s64 + 117;
	// addi r11,r31,24
	r11.s64 = r31.s64 + 24;
	// subf r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - r31.s64;
	// li r10,8
	ctx.r10.s64 = 8;
loc_822FCF6C:
	// lwz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(r11.u32 + -4);
	// addic. r10,r10,-1
	xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stwx r7,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + r11.u32, ctx.r7.u32);
	// stwx r6,r8,r11
	PPC_STORE_U32(ctx.r8.u32 + r11.u32, ctx.r6.u32);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// bne 0x822fcf6c
	if (!cr0.eq) goto loc_822FCF6C;
	// lbz r9,86(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 86);
	// addi r3,r1,201
	ctx.r3.s64 = ctx.r1.s64 + 201;
	// lbz r7,85(r31)
	ctx.r7.u64 = PPC_LOAD_U8(r31.u32 + 85);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// lbz r6,84(r31)
	ctx.r6.u64 = PPC_LOAD_U8(r31.u32 + 84);
	// li r5,12
	ctx.r5.s64 = 12;
	// std r9,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r9.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r7,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r7.u64);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r6,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r6.u64);
	// lfd f12,80(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f11,f12
	ctx.f11.f64 = double(ctx.f12.s64);
	// fcfid f10,f13
	ctx.f10.f64 = double(ctx.f13.s64);
	// fcfid f9,f0
	ctx.f9.f64 = double(f0.s64);
	// frsp f8,f11
	ctx.f8.f64 = double(float(ctx.f11.f64));
	// stfs f8,96(r1)
	temp.f32 = float(ctx.f8.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// frsp f7,f10
	ctx.f7.f64 = double(float(ctx.f10.f64));
	// stfs f7,92(r1)
	temp.f32 = float(ctx.f7.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// frsp f6,f9
	ctx.f6.f64 = double(float(ctx.f9.f64));
	// stfs f6,88(r1)
	temp.f32 = float(ctx.f6.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lbz r11,93(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 93);
	// lbz r10,92(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 92);
	// lfs f5,88(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 88);
	ctx.f5.f64 = double(temp.f32);
	// lbz r9,94(r31)
	ctx.r9.u64 = PPC_LOAD_U8(r31.u32 + 94);
	// stfs f5,80(r1)
	temp.f32 = float(ctx.f5.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// addi r3,r1,217
	ctx.r3.s64 = ctx.r1.s64 + 217;
	// lwz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r4,r1,88
	ctx.r4.s64 = ctx.r1.s64 + 88;
	// stw r8,213(r1)
	PPC_STORE_U32(ctx.r1.u32 + 213, ctx.r8.u32);
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r11.u64);
	// lfd f4,104(r1)
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// lfd f3,88(r1)
	ctx.f3.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r9,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r9.u64);
	// lfd f2,104(r1)
	ctx.f2.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f4
	f0.f64 = double(ctx.f4.s64);
	// li r5,12
	ctx.r5.s64 = 12;
	// fcfid f1,f2
	ctx.f1.f64 = double(ctx.f2.s64);
	// fcfid f13,f3
	ctx.f13.f64 = double(ctx.f3.s64);
	// frsp f11,f0
	ctx.f11.f64 = double(float(f0.f64));
	// stfs f11,92(r1)
	temp.f32 = float(ctx.f11.f64);
	PPC_STORE_U32(ctx.r1.u32 + 92, temp.u32);
	// frsp f12,f1
	ctx.f12.f64 = double(float(ctx.f1.f64));
	// stfs f12,88(r1)
	temp.f32 = float(ctx.f12.f64);
	PPC_STORE_U32(ctx.r1.u32 + 88, temp.u32);
	// frsp f10,f13
	ctx.f10.f64 = double(float(ctx.f13.f64));
	// stfs f10,96(r1)
	temp.f32 = float(ctx.f10.f64);
	PPC_STORE_U32(ctx.r1.u32 + 96, temp.u32);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// lwz r7,112(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 112);
	// lfs f9,96(r31)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(r31.u32 + 96);
	ctx.f9.f64 = double(temp.f32);
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// stfs f9,80(r1)
	temp.f32 = float(ctx.f9.f64);
	PPC_STORE_U32(ctx.r1.u32 + 80, temp.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r5,229(r1)
	PPC_STORE_U32(ctx.r1.u32 + 229, ctx.r5.u32);
	// li r5,125
	ctx.r5.s64 = 125;
	// stw r7,233(r1)
	PPC_STORE_U32(ctx.r1.u32 + 233, ctx.r7.u32);
	// addi r4,r1,112
	ctx.r4.s64 = ctx.r1.s64 + 112;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FD080"))) PPC_WEAK_FUNC(sub_822FD080);
PPC_FUNC_IMPL(__imp__sub_822FD080) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r11,7
	r11.s64 = 7;
	// stw r5,97(r1)
	PPC_STORE_U32(ctx.r1.u32 + 97, ctx.r5.u32);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// stb r11,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, r11.u8);
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// addi r5,r31,272
	ctx.r5.s64 = r31.s64 + 272;
	// li r4,16
	ctx.r4.s64 = 16;
	// addi r3,r1,81
	ctx.r3.s64 = ctx.r1.s64 + 81;
	// bl 0x82170cc8
	sub_82170CC8(ctx, base);
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,21
	ctx.r5.s64 = 21;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FD0E8"))) PPC_WEAK_FUNC(sub_822FD0E8);
PPC_FUNC_IMPL(__imp__sub_822FD0E8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-736(r1)
	ea = -736 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lwz r11,26912(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 26912);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd118
	if (cr6.eq) goto loc_822FD118;
	// lis r10,-31927
	ctx.r10.s64 = -2092367872;
	// lbz r10,26821(r10)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + 26821);
	// b 0x822fd11c
	goto loc_822FD11C;
loc_822FD118:
	// li r10,0
	ctx.r10.s64 = 0;
loc_822FD11C:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fd330
	if (cr6.eq) goto loc_822FD330;
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// lbz r10,256(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 256);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fd330
	if (!cr6.eq) goto loc_822FD330;
	// li r4,1
	ctx.r4.s64 = 1;
	// lwz r3,156(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 156);
	// bl 0x822641f0
	sub_822641F0(ctx, base);
	// bl 0x82455f20
	sub_82455F20(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fd164
	if (cr6.eq) goto loc_822FD164;
	// lbz r11,144(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 144);
	// rlwinm r10,r11,0,25,25
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 0) & 0x40;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fd168
	if (!cr6.eq) goto loc_822FD168;
loc_822FD164:
	// li r11,0
	r11.s64 = 0;
loc_822FD168:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd330
	if (cr6.eq) goto loc_822FD330;
	// lwz r11,40(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// li r30,0
	r30.s64 = 0;
	// rlwinm r10,r11,7,31,31
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 7) & 0x1;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fd27c
	if (cr6.eq) goto loc_822FD27C;
	// lwz r11,140(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd1b4
	if (cr6.eq) goto loc_822FD1B4;
	// lbz r10,57(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 57);
	// lwz r11,72(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// rotlwi r10,r10,3
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + r11.u64;
	// lwz r11,4(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822fd280
	goto loc_822FD280;
loc_822FD1B4:
	// lwz r10,72(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 72);
	// li r11,0
	r11.s64 = 0;
	// lwz r6,76(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 76);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// subf r9,r10,r6
	ctx.r9.s64 = ctx.r6.s64 - ctx.r10.s64;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// srawi. r11,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	r11.s64 = ctx.r9.s32 >> 3;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// ble 0x822fd224
	if (!cr0.gt) goto loc_822FD224;
loc_822FD1D4:
	// srawi r9,r11,1
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x1) != 0);
	ctx.r9.s64 = r11.s32 >> 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r7,57
	cr6.compare<int32_t>(ctx.r7.s32, 57, xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// blt cr6,0x822fd1f4
	if (cr6.lt) goto loc_822FD1F4;
	// li r7,0
	ctx.r7.s64 = 0;
loc_822FD1F4:
	// clrlwi r7,r7,24
	ctx.r7.u64 = ctx.r7.u32 & 0xFF;
	// cmplwi cr6,r7,0
	cr6.compare<uint32_t>(ctx.r7.u32, 0, xer);
	// beq cr6,0x822fd210
	if (cr6.eq) goto loc_822FD210;
	// subf r11,r9,r11
	r11.s64 = r11.s64 - ctx.r9.s64;
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// b 0x822fd218
	goto loc_822FD218;
loc_822FD210:
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// mr r11,r9
	r11.u64 = ctx.r9.u64;
loc_822FD218:
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bgt cr6,0x822fd1d4
	if (cr6.gt) goto loc_822FD1D4;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822FD224:
	// cmplw cr6,r10,r6
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r6.u32, xer);
	// beq cr6,0x822fd268
	if (cr6.eq) goto loc_822FD268;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmpwi cr6,r11,57
	cr6.compare<int32_t>(r11.s32, 57, xer);
	// li r11,1
	r11.s64 = 1;
	// bgt cr6,0x822fd240
	if (cr6.gt) goto loc_822FD240;
	// li r11,0
	r11.s64 = 0;
loc_822FD240:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fd268
	if (!cr6.eq) goto loc_822FD268;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822fd280
	goto loc_822FD280;
loc_822FD268:
	// mr r11,r6
	r11.u64 = ctx.r6.u64;
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mr r30,r11
	r30.u64 = r11.u64;
	// li r11,1
	r11.s64 = 1;
	// b 0x822fd280
	goto loc_822FD280;
loc_822FD27C:
	// li r11,0
	r11.s64 = 0;
loc_822FD280:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd330
	if (cr6.eq) goto loc_822FD330;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// ld r4,264(r28)
	ctx.r4.u64 = PPC_LOAD_U64(r28.u32 + 264);
	// addi r3,r11,9672
	ctx.r3.s64 = r11.s64 + 9672;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// ld r9,264(r28)
	ctx.r9.u64 = PPC_LOAD_U64(r28.u32 + 264);
	// li r10,11
	ctx.r10.s64 = 11;
	// li r31,0
	r31.s64 = 0;
	// stb r10,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, ctx.r10.u8);
	// addi r30,r30,12
	r30.s64 = r30.s64 + 12;
	// addi r29,r1,105
	r29.s64 = ctx.r1.s64 + 105;
	// li r12,97
	r12.s64 = 97;
	// stdx r9,r1,r12
	PPC_STORE_U64(ctx.r1.u32 + r12.u32, ctx.r9.u64);
loc_822FD2BC:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplwi cr6,r31,124
	cr6.compare<uint32_t>(r31.u32, 124, xer);
	// blt cr6,0x822fd2bc
	if (cr6.lt) goto loc_822FD2BC;
	// li r31,0
	r31.s64 = 0;
	// addi r29,r1,601
	r29.s64 = ctx.r1.s64 + 601;
loc_822FD2F0:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// stw r3,0(r29)
	PPC_STORE_U32(r29.u32 + 0, ctx.r3.u32);
	// addi r29,r29,4
	r29.s64 = r29.s64 + 4;
	// cmplwi cr6,r31,18
	cr6.compare<uint32_t>(r31.u32, 18, xer);
	// blt cr6,0x822fd2f0
	if (cr6.lt) goto loc_822FD2F0;
	// mr r6,r27
	ctx.r6.u64 = r27.u64;
	// li r5,577
	ctx.r5.s64 = 577;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
loc_822FD330:
	// addi r1,r1,736
	ctx.r1.s64 = ctx.r1.s64 + 736;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822FD338"))) PPC_WEAK_FUNC(sub_822FD338);
PPC_FUNC_IMPL(__imp__sub_822FD338) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// addi r29,r31,332
	r29.s64 = r31.s64 + 332;
	// addi r28,r30,8
	r28.s64 = r30.s64 + 8;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// lwz r26,336(r31)
	r26.u64 = PPC_LOAD_U32(r31.u32 + 336);
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822ffc28
	sub_822FFC28(ctx, base);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// rotlwi r31,r26,0
	r31.u64 = __builtin_rotateleft32(r26.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd384
	if (cr6.eq) goto loc_822FD384;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// beq cr6,0x822fd388
	if (cr6.eq) goto loc_822FD388;
loc_822FD384:
	// twi 31,r0,22
loc_822FD388:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fd3b8
	if (cr6.eq) goto loc_822FD3B8;
	// addi r5,r11,12
	ctx.r5.s64 = r11.s64 + 12;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822f88e8
	sub_822F88E8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fd3b8
	if (!cr6.eq) goto loc_822FD3B8;
	// addi r11,r1,88
	r11.s64 = ctx.r1.s64 + 88;
	// b 0x822fd3c4
	goto loc_822FD3C4;
loc_822FD3B8:
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r31.u32);
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
loc_822FD3C4:
	// ld r11,0(r11)
	r11.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, r11.u64);
	// lwz r11,88(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd3e0
	if (cr6.eq) goto loc_822FD3E0;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// beq cr6,0x822fd3e4
	if (cr6.eq) goto loc_822FD3E4;
loc_822FD3E0:
	// twi 31,r0,22
loc_822FD3E4:
	// lwz r11,92(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// beq cr6,0x822fd408
	if (cr6.eq) goto loc_822FD408;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r11,536(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 536);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c30
	return;
loc_822FD408:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// ld r7,52(r30)
	ctx.r7.u64 = PPC_LOAD_U64(r30.u32 + 52);
	// li r4,3411
	ctx.r4.s64 = 3411;
	// ld r6,44(r30)
	ctx.r6.u64 = PPC_LOAD_U64(r30.u32 + 44);
	// addi r3,r11,9728
	ctx.r3.s64 = r11.s64 + 9728;
	// ld r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// addi r31,r30,44
	r31.s64 = r30.s64 + 44;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cf9a98
	sub_82CF9A98(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// li r4,3415
	ctx.r4.s64 = 3415;
	// ld r7,0(r30)
	ctx.r7.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// addi r3,r10,9856
	ctx.r3.s64 = ctx.r10.s64 + 9856;
	// lhz r6,16(r30)
	ctx.r6.u64 = PPC_LOAD_U16(r30.u32 + 16);
	// lwz r5,0(r28)
	ctx.r5.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82cf9ab8
	sub_82CF9AB8(ctx, base);
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// beq cr6,0x822fd49c
	if (cr6.eq) goto loc_822FD49C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// ld r5,0(r30)
	ctx.r5.u64 = PPC_LOAD_U64(r30.u32 + 0);
	// li r4,3418
	ctx.r4.s64 = 3418;
	// addi r3,r11,9992
	ctx.r3.s64 = r11.s64 + 9992;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cf9aa8
	sub_82CF9AA8(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// lwz r11,536(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 536);
	// stw r11,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c30
	return;
loc_822FD49C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r4,3422
	ctx.r4.s64 = 3422;
	// addi r3,r11,10112
	ctx.r3.s64 = r11.s64 + 10112;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r10,0(r27)
	PPC_STORE_U32(r27.u32 + 0, ctx.r10.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822FD4C8"))) PPC_WEAK_FUNC(sub_822FD4C8);
PPC_FUNC_IMPL(__imp__sub_822FD4C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-576(r1)
	ea = -576 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// addi r3,r11,10236
	ctx.r3.s64 = r11.s64 + 10236;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lbz r10,382(r26)
	ctx.r10.u64 = PPC_LOAD_U8(r26.u32 + 382);
	// li r25,1
	r25.s64 = 1;
	// li r29,0
	r29.s64 = 0;
	// mr r31,r25
	r31.u64 = r25.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fd60c
	if (cr6.eq) goto loc_822FD60C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// addi r3,r11,10264
	ctx.r3.s64 = r11.s64 + 10264;
	// li r4,3437
	ctx.r4.s64 = 3437;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// addi r4,r1,160
	ctx.r4.s64 = ctx.r1.s64 + 160;
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82cf9ad0
	sub_82CF9AD0(ctx, base);
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// lwz r10,236(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 236);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq cr6,0x822fd588
	if (cr6.eq) goto loc_822FD588;
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// lwz r11,1156(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1156);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd5a0
	if (cr6.eq) goto loc_822FD5A0;
	// lwz r9,1160(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1160);
	// li r10,296
	ctx.r10.s64 = 296;
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// divw. r7,r8,r10
	ctx.r7.s32 = ctx.r8.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// beq 0x822fd5a0
	if (cr0.eq) goto loc_822FD5A0;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd574
	if (cr6.eq) goto loc_822FD574;
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// divw. r7,r8,r10
	ctx.r7.s32 = ctx.r8.s32 / ctx.r10.s32;
	cr0.compare<int32_t>(ctx.r7.s32, 0, xer);
	// bne 0x822fd578
	if (!cr0.eq) goto loc_822FD578;
loc_822FD574:
	// twi 31,r0,22
loc_822FD578:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd5a0
	if (cr6.eq) goto loc_822FD5A0;
	// addi r4,r11,12
	ctx.r4.s64 = r11.s64 + 12;
	// b 0x822fd594
	goto loc_822FD594;
loc_822FD588:
	// bl 0x8217e3f8
	sub_8217E3F8(ctx, base);
	// addi r11,r3,244
	r11.s64 = ctx.r3.s64 + 244;
	// addi r4,r11,8
	ctx.r4.s64 = r11.s64 + 8;
loc_822FD594:
	// li r5,36
	ctx.r5.s64 = 36;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
loc_822FD5A0:
	// addi r11,r1,160
	r11.s64 = ctx.r1.s64 + 160;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822FD5AC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822fd5cc
	if (!cr0.eq) goto loc_822FD5CC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822fd5ac
	if (!cr6.eq) goto loc_822FD5AC;
loc_822FD5CC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822fd60c
	if (!cr6.eq) goto loc_822FD60C;
	// addi r11,r1,164
	r11.s64 = ctx.r1.s64 + 164;
	// addi r10,r1,116
	ctx.r10.s64 = ctx.r1.s64 + 116;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822FD5E0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822fd600
	if (!cr0.eq) goto loc_822FD600;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822fd5e0
	if (!cr6.eq) goto loc_822FD5E0;
loc_822FD600:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne cr6,0x822fd60c
	if (!cr6.eq) goto loc_822FD60C;
	// mr r31,r29
	r31.u64 = r29.u64;
loc_822FD60C:
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fe490
	sub_822FE490(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fd66c
	if (!cr6.eq) goto loc_822FD66C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10392
	ctx.r3.s64 = r11.s64 + 10392;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// clrlwi r10,r31,24
	ctx.r10.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fd918
	if (cr6.eq) goto loc_822FD918;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// addi r3,r11,10448
	ctx.r3.s64 = r11.s64 + 10448;
	// li r4,3472
	ctx.r4.s64 = 3472;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82cf9ae8
	sub_82CF9AE8(ctx, base);
	// addi r1,r1,576
	ctx.r1.s64 = ctx.r1.s64 + 576;
	// b 0x82ca2c28
	return;
loc_822FD66C:
	// clrlwi r27,r31,24
	r27.u64 = r31.u32 & 0xFF;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822fd684
	if (cr6.eq) goto loc_822FD684;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r4,r11,10568
	ctx.r4.s64 = r11.s64 + 10568;
	// b 0x822fd68c
	goto loc_822FD68C;
loc_822FD684:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r4,r11,10572
	ctx.r4.s64 = r11.s64 + 10572;
loc_822FD68C:
	// lwz r28,80(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r28,0
	cr6.compare<uint32_t>(r28.u32, 0, xer);
	// bne cr6,0x822fd69c
	if (!cr6.eq) goto loc_822FD69C;
	// twi 31,r0,22
loc_822FD69C:
	// lwz r30,84(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fd6b0
	if (!cr6.eq) goto loc_822FD6B0;
	// twi 31,r0,22
loc_822FD6B0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r5,r30,82
	ctx.r5.s64 = r30.s64 + 82;
	// addi r3,r11,10580
	ctx.r3.s64 = r11.s64 + 10580;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822fd7d0
	if (cr6.eq) goto loc_822FD7D0;
	// lwz r31,4(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// bne cr6,0x822fd6d8
	if (!cr6.eq) goto loc_822FD6D8;
	// twi 31,r0,22
loc_822FD6D8:
	// lbz r11,80(r30)
	r11.u64 = PPC_LOAD_U8(r30.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd72c
	if (cr6.eq) goto loc_822FD72C;
	// li r5,296
	ctx.r5.s64 = 296;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,208
	ctx.r3.s64 = ctx.r1.s64 + 208;
	// bl 0x82ca3190
	sub_82CA3190(ctx, base);
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// stw r25,360(r1)
	PPC_STORE_U32(ctx.r1.u32 + 360, r25.u32);
	// bne cr6,0x822fd704
	if (!cr6.eq) goto loc_822FD704;
	// twi 31,r0,22
loc_822FD704:
	// ld r11,24(r30)
	r11.u64 = PPC_LOAD_U64(r30.u32 + 24);
	// addi r4,r1,208
	ctx.r4.s64 = ctx.r1.s64 + 208;
	// lwz r3,548(r26)
	ctx.r3.u64 = PPC_LOAD_U32(r26.u32 + 548);
	// std r11,328(r1)
	PPC_STORE_U64(ctx.r1.u32 + 328, r11.u64);
	// bl 0x82304368
	sub_82304368(ctx, base);
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fd728
	if (!cr6.eq) goto loc_822FD728;
	// twi 31,r0,22
loc_822FD728:
	// stb r29,80(r30)
	PPC_STORE_U8(r30.u32 + 80, r29.u8);
loc_822FD72C:
	// lwz r7,4(r28)
	ctx.r7.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r7
	cr6.compare<uint32_t>(r30.u32, ctx.r7.u32, xer);
	// bne cr6,0x822fd73c
	if (!cr6.eq) goto loc_822FD73C;
	// twi 31,r0,22
loc_822FD73C:
	// addi r31,r30,32
	r31.s64 = r30.s64 + 32;
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// mr r11,r31
	r11.u64 = r31.u64;
	// addi r10,r10,540
	ctx.r10.s64 = ctx.r10.s64 + 540;
	// addi r8,r31,8
	ctx.r8.s64 = r31.s64 + 8;
loc_822FD750:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822fd770
	if (!cr0.eq) goto loc_822FD770;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822fd750
	if (!cr6.eq) goto loc_822FD750;
loc_822FD770:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// beq cr6,0x822fd7b4
	if (cr6.eq) goto loc_822FD7B4;
	// cmplw cr6,r30,r7
	cr6.compare<uint32_t>(r30.u32, ctx.r7.u32, xer);
	// bne cr6,0x822fd784
	if (!cr6.eq) goto loc_822FD784;
	// twi 31,r0,22
loc_822FD784:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// ld r5,0(r31)
	ctx.r5.u64 = PPC_LOAD_U64(r31.u32 + 0);
	// li r4,3494
	ctx.r4.s64 = 3494;
	// addi r3,r11,9992
	ctx.r3.s64 = r11.s64 + 9992;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,4(r28)
	ctx.r10.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fd7a8
	if (!cr6.eq) goto loc_822FD7A8;
	// twi 31,r0,22
loc_822FD7A8:
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82cf9aa8
	sub_82CF9AA8(ctx, base);
	// b 0x822fd7d0
	goto loc_822FD7D0;
loc_822FD7B4:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r5,r24
	ctx.r5.u64 = r24.u64;
	// addi r3,r11,10448
	ctx.r3.s64 = r11.s64 + 10448;
	// li r4,3499
	ctx.r4.s64 = 3499;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// bl 0x82cf9ae8
	sub_82CF9AE8(ctx, base);
loc_822FD7D0:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fd7e0
	if (!cr6.eq) goto loc_822FD7E0;
	// twi 31,r0,22
loc_822FD7E0:
	// ld r31,24(r30)
	r31.u64 = PPC_LOAD_U64(r30.u32 + 24);
	// addi r29,r30,24
	r29.s64 = r30.s64 + 24;
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// beq cr6,0x822fd834
	if (cr6.eq) goto loc_822FD834;
	// std r31,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r31.u64);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r26,368
	ctx.r4.s64 = r26.s64 + 368;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// ld r4,96(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// bl 0x822fe640
	sub_822FE640(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd834
	if (cr6.eq) goto loc_822FD834;
	// std r31,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r31.u64);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// stw r25,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r25.u32);
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// b 0x822fd8e8
	goto loc_822FD8E8;
loc_822FD834:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10608
	ctx.r3.s64 = r11.s64 + 10608;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// cmpldi cr6,r31,0
	cr6.compare<uint64_t>(r31.u64, 0, xer);
	// beq cr6,0x822fd86c
	if (cr6.eq) goto loc_822FD86C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10660
	ctx.r3.s64 = r11.s64 + 10660;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// std r31,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, r31.u64);
	// stw r25,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r25.u32);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// b 0x822fd878
	goto loc_822FD878;
loc_822FD86C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10684
	ctx.r3.s64 = r11.s64 + 10684;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FD878:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fd888
	if (!cr6.eq) goto loc_822FD888;
	// twi 31,r0,22
loc_822FD888:
	// addi r31,r26,368
	r31.s64 = r26.s64 + 368;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82323168
	sub_82323168(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,372(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 372);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd8b4
	if (cr6.eq) goto loc_822FD8B4;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fd8b8
	if (cr6.eq) goto loc_822FD8B8;
loc_822FD8B4:
	// twi 31,r0,22
loc_822FD8B8:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x822fd8dc
	if (cr6.eq) goto loc_822FD8DC;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x822fe640
	sub_822FE640(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10708
	ctx.r3.s64 = r11.s64 + 10708;
	// b 0x822fd8e4
	goto loc_822FD8E4;
loc_822FD8DC:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10724
	ctx.r3.s64 = r11.s64 + 10724;
loc_822FD8E4:
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FD8E8:
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822fd8f8
	if (!cr6.eq) goto loc_822FD8F8;
	// twi 31,r0,22
loc_822FD8F8:
	// addi r4,r30,40
	ctx.r4.s64 = r30.s64 + 40;
	// addi r3,r26,332
	ctx.r3.s64 = r26.s64 + 332;
	// bl 0x822fe9e8
	sub_822FE9E8(ctx, base);
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822fd918
	if (cr6.eq) goto loc_822FD918;
	// mr r4,r24
	ctx.r4.u64 = r24.u64;
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x822fe558
	sub_822FE558(ctx, base);
loc_822FD918:
	// addi r1,r1,576
	ctx.r1.s64 = ctx.r1.s64 + 576;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822FD920"))) PPC_WEAK_FUNC(sub_822FD920);
PPC_FUNC_IMPL(__imp__sub_822FD920) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// clrlwi r11,r4,24
	r11.u64 = ctx.r4.u32 & 0xFF;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd95c
	if (cr6.eq) goto loc_822FD95C;
	// bl 0x822d3fa0
	sub_822D3FA0(ctx, base);
	// lbz r11,5(r3)
	r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
	// lbz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// or r9,r11,r10
	ctx.r9.u64 = r11.u64 | ctx.r10.u64;
	// cntlzw r8,r9
	ctx.r8.u64 = ctx.r9.u32 == 0 ? 32 : __builtin_clz(ctx.r9.u32);
	// rlwinm r3,r8,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 27) & 0x1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822FD95C:
	// lwz r11,128(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 128);
	// addi r30,r31,128
	r30.s64 = r31.s64 + 128;
	// cmplwi cr6,r11,997
	cr6.compare<uint32_t>(r11.u32, 997, xer);
	// beq cr6,0x822fda0c
	if (cr6.eq) goto loc_822FDA0C;
	// lbz r11,458(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 458);
	// li r29,0
	r29.s64 = 0;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fd9fc
	if (cr6.eq) goto loc_822FD9FC;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10744
	ctx.r3.s64 = r11.s64 + 10744;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lwz r10,164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// stb r29,380(r31)
	PPC_STORE_U8(r31.u32 + 380, r29.u8);
	// cmpwi cr6,r10,0
	cr6.compare<int32_t>(ctx.r10.s32, 0, xer);
	// bne cr6,0x822fd9b0
	if (!cr6.eq) goto loc_822FD9B0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10768
	ctx.r3.s64 = r11.s64 + 10768;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822FD9B0:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82cbc490
	sub_82CBC490(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	cr6.compare<int32_t>(r30.s32, 0, xer);
	// bge cr6,0x822fd9f0
	if (!cr6.lt) goto loc_822FD9F0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10800
	ctx.r3.s64 = r11.s64 + 10800;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,164(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 164);
	// bl 0x822fcb10
	sub_822FCB10(ctx, base);
	// stb r29,458(r31)
	PPC_STORE_U8(r31.u32 + 458, r29.u8);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822FD9F0:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10832
	ctx.r3.s64 = r11.s64 + 10832;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FD9FC:
	// li r3,1
	ctx.r3.s64 = 1;
	// stb r29,458(r31)
	PPC_STORE_U8(r31.u32 + 458, r29.u8);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822FDA0C:
	// li r11,1
	r11.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,458(r31)
	PPC_STORE_U8(r31.u32 + 458, r11.u8);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FDA20"))) PPC_WEAK_FUNC(sub_822FDA20);
PPC_FUNC_IMPL(__imp__sub_822FDA20) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-336(r1)
	ea = -336 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// addi r3,r11,10844
	ctx.r3.s64 = r11.s64 + 10844;
	// stw r27,364(r1)
	PPC_STORE_U32(ctx.r1.u32 + 364, r27.u32);
	// mr r29,r5
	r29.u64 = ctx.r5.u64;
	// mr r28,r6
	r28.u64 = ctx.r6.u64;
	// mr r26,r7
	r26.u64 = ctx.r7.u64;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// clrlwi r10,r28,24
	ctx.r10.u64 = r28.u32 & 0xFF;
	// lwz r11,364(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 364);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// lwz r25,352(r31)
	r25.u64 = PPC_LOAD_U32(r31.u32 + 352);
	// beq cr6,0x822fdb4c
	if (cr6.eq) goto loc_822FDB4C;
	// add r30,r25,r11
	r30.u64 = r25.u64 + r11.u64;
	// addi r11,r30,1
	r11.s64 = r30.s64 + 1;
	// cmplwi cr6,r11,32
	cr6.compare<uint32_t>(r11.u32, 32, xer);
	// ble cr6,0x822fdaa8
	if (!cr6.gt) goto loc_822FDAA8;
	// cmplwi cr6,r25,0
	cr6.compare<uint32_t>(r25.u32, 0, xer);
	// beq cr6,0x822fdb1c
	if (cr6.eq) goto loc_822FDB1C;
	// lwz r10,348(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// addi r11,r31,344
	r11.s64 = r31.s64 + 344;
	// rotlwi r9,r10,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fda98
	if (!cr6.eq) goto loc_822FDA98;
	// twi 31,r0,22
loc_822FDA98:
	// li r5,1
	ctx.r5.s64 = 1;
	// lwz r4,16(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 16);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x822fd4c8
	sub_822FD4C8(ctx, base);
loc_822FDAA8:
	// cmplwi cr6,r30,32
	cr6.compare<uint32_t>(r30.u32, 32, xer);
	// bge cr6,0x822fdb34
	if (!cr6.lt) goto loc_822FDB34;
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r1,364
	ctx.r5.s64 = ctx.r1.s64 + 364;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x822f8838
	sub_822F8838(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r27,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r27.u32);
	// addi r10,r1,104
	ctx.r10.s64 = ctx.r1.s64 + 104;
	// li r9,10
	ctx.r9.s64 = 10;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822FDAE0:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822fdae0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822FDAE0;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// addi r4,r31,356
	ctx.r4.s64 = r31.s64 + 356;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822feb98
	sub_822FEB98(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10948
	ctx.r3.s64 = r11.s64 + 10948;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c2c
	return;
loc_822FDB1C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10884
	ctx.r3.s64 = r11.s64 + 10884;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c2c
	return;
loc_822FDB34:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10964
	ctx.r3.s64 = r11.s64 + 10964;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c2c
	return;
loc_822FDB4C:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,10988
	ctx.r3.s64 = r11.s64 + 10988;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// lbz r10,109(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 109);
	// cmplw cr6,r25,r10
	cr6.compare<uint32_t>(r25.u32, ctx.r10.u32, xer);
	// bge cr6,0x822fdc48
	if (!cr6.lt) goto loc_822FDC48;
	// lwz r10,348(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 348);
	// addi r30,r31,344
	r30.s64 = r31.s64 + 344;
	// mr r11,r30
	r11.u64 = r30.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r9.u32);
loc_822FDB7C:
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fdb90
	if (cr6.eq) goto loc_822FDB90;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x822fdb94
	if (cr6.eq) goto loc_822FDB94;
loc_822FDB90:
	// twi 31,r0,22
loc_822FDB94:
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x822fdbd8
	if (cr6.eq) goto loc_822FDBD8;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fdba8
	if (!cr6.eq) goto loc_822FDBA8;
	// twi 31,r0,22
loc_822FDBA8:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x822fdbb8
	if (!cr6.eq) goto loc_822FDBB8;
	// twi 31,r0,22
loc_822FDBB8:
	// ld r11,24(r9)
	r11.u64 = PPC_LOAD_U64(ctx.r9.u32 + 24);
	// cmpld cr6,r11,r29
	cr6.compare<uint64_t>(r11.u64, r29.u64, xer);
	// beq cr6,0x822fdc54
	if (cr6.eq) goto loc_822FDC54;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822fdb7c
	goto loc_822FDB7C;
loc_822FDBD8:
	// mr r8,r28
	ctx.r8.u64 = r28.u64;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// addi r5,r1,364
	ctx.r5.s64 = ctx.r1.s64 + 364;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,192
	ctx.r3.s64 = ctx.r1.s64 + 192;
	// bl 0x822f8838
	sub_822F8838(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stw r27,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, r27.u32);
	// addi r10,r1,104
	ctx.r10.s64 = ctx.r1.s64 + 104;
	// li r9,10
	ctx.r9.s64 = 10;
	// mtctr r9
	ctr.u64 = ctx.r9.u64;
loc_822FDC08:
	// ld r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 0);
	// addi r11,r11,8
	r11.s64 = r11.s64 + 8;
	// std r9,0(r10)
	PPC_STORE_U64(ctx.r10.u32 + 0, ctx.r9.u64);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bdnz 0x822fdc08
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822FDC08;
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822feb98
	sub_822FEB98(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r4,r25
	ctx.r4.u64 = r25.u64;
	// addi r3,r11,11016
	ctx.r3.s64 = r11.s64 + 11016;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c2c
	return;
loc_822FDC48:
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r11,11076
	ctx.r3.s64 = r11.s64 + 11076;
	// bl 0x82172ee8
	sub_82172EE8(ctx, base);
loc_822FDC54:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,336
	ctx.r1.s64 = ctx.r1.s64 + 336;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822FDC60"))) PPC_WEAK_FUNC(sub_822FDC60);
PPC_FUNC_IMPL(__imp__sub_822FDC60) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// li r10,5
	ctx.r10.s64 = 5;
	// li r28,1
	r28.s64 = 1;
	// li r3,20
	ctx.r3.s64 = 20;
	// mr r27,r28
	r27.u64 = r28.u64;
	// lbz r11,109(r29)
	r11.u64 = PPC_LOAD_U8(r29.u32 + 109);
	// divwu r26,r11,r10
	r26.u32 = r11.u32 / ctx.r10.u32;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// li r30,0
	r30.s64 = 0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fdc9c
	if (cr6.eq) goto loc_822FDC9C;
	// stw r30,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r30.u32);
loc_822FDC9C:
	// addic. r11,r3,4
	xer.ca = ctx.r3.u32 > 4294967291;
	r11.s64 = ctx.r3.s64 + 4;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822fdca8
	if (cr0.eq) goto loc_822FDCA8;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822FDCA8:
	// addic. r11,r3,8
	xer.ca = ctx.r3.u32 > 4294967287;
	r11.s64 = ctx.r3.s64 + 8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x822fdcb4
	if (cr0.eq) goto loc_822FDCB4;
	// stw r30,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r30.u32);
loc_822FDCB4:
	// stb r28,16(r3)
	PPC_STORE_U8(ctx.r3.u32 + 16, r28.u8);
	// addi r31,r29,368
	r31.s64 = r29.s64 + 368;
	// stb r30,17(r3)
	PPC_STORE_U8(ctx.r3.u32 + 17, r30.u8);
	// stw r3,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r3.u32);
	// mr r11,r31
	r11.u64 = r31.u64;
	// stb r28,17(r3)
	PPC_STORE_U8(ctx.r3.u32 + 17, r28.u8);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r10,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r10.u32);
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r10,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r10.u32);
	// lwz r10,372(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 372);
	// stw r30,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r30.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
loc_822FDCF8:
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fdd0c
	if (cr6.eq) goto loc_822FDD0C;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fdd10
	if (cr6.eq) goto loc_822FDD10;
loc_822FDD0C:
	// twi 31,r0,22
loc_822FDD10:
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fdd80
	if (cr6.eq) goto loc_822FDD80;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fdd24
	if (!cr6.eq) goto loc_822FDD24;
	// twi 31,r0,22
loc_822FDD24:
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fdd34
	if (!cr6.eq) goto loc_822FDD34;
	// twi 31,r0,22
loc_822FDD34:
	// lwz r8,24(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 24);
	// addi r5,r10,24
	ctx.r5.s64 = ctx.r10.s64 + 24;
	// lwz r11,32(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 32);
	// lwz r7,96(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 96);
	// cmplw cr6,r7,r11
	cr6.compare<uint32_t>(ctx.r7.u32, r11.u32, xer);
	// bne cr6,0x822fdd6c
	if (!cr6.eq) goto loc_822FDD6C;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fdd6c
	if (cr6.eq) goto loc_822FDD6C;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fdd60
	if (!cr6.eq) goto loc_822FDD60;
	// twi 31,r0,22
loc_822FDD60:
	// addi r4,r1,120
	ctx.r4.s64 = ctx.r1.s64 + 120;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// bl 0x822ff960
	sub_822FF960(ctx, base);
loc_822FDD6C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822fdcf8
	goto loc_822FDCF8;
loc_822FDD80:
	// lwz r10,124(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 124);
	// clrlwi r11,r10,31
	r11.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fdd94
	if (!cr6.eq) goto loc_822FDD94;
	// li r27,17
	r27.s64 = 17;
loc_822FDD94:
	// lis r11,-21846
	r11.s64 = -1431699456;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// ori r7,r11,43691
	ctx.r7.u64 = r11.u64 | 43691;
	// mulhwu r6,r8,r7
	ctx.r6.u64 = (uint64_t(ctx.r8.u32) * uint64_t(ctx.r7.u32)) >> 32;
	// rlwinm r11,r6,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r5,r11,r9
	ctx.r5.u64 = r11.u64 + ctx.r9.u64;
	// subf. r4,r5,r8
	ctx.r4.s64 = ctx.r8.s64 - ctx.r5.s64;
	cr0.compare<int32_t>(ctx.r4.s32, 0, xer);
	// bne 0x822fddbc
	if (!cr0.eq) goto loc_822FDDBC;
	// ori r27,r27,256
	r27.u64 = r27.u64 | 256;
loc_822FDDBC:
	// clrlwi r11,r10,30
	r11.u64 = ctx.r10.u32 & 0x3;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fddcc
	if (!cr6.eq) goto loc_822FDDCC;
	// ori r27,r27,4096
	r27.u64 = r27.u64 | 4096;
loc_822FDDCC:
	// lis r11,-13108
	r11.s64 = -859045888;
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// ori r8,r11,52429
	ctx.r8.u64 = r11.u64 | 52429;
	// mulhwu r7,r9,r8
	ctx.r7.u64 = (uint64_t(ctx.r9.u32) * uint64_t(ctx.r8.u32)) >> 32;
	// rlwinm r11,r7,30,2,31
	r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 30) & 0x3FFFFFFF;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r6,r11,r10
	ctx.r6.u64 = r11.u64 + ctx.r10.u64;
	// subf. r5,r6,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r6.s64;
	cr0.compare<int32_t>(ctx.r5.s32, 0, xer);
	// bne 0x822fddf4
	if (!cr0.eq) goto loc_822FDDF4;
	// oris r27,r27,1
	r27.u64 = r27.u64 | 65536;
loc_822FDDF4:
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r10,r1,120
	ctx.r10.s64 = ctx.r1.s64 + 120;
	// mr r31,r28
	r31.u64 = r28.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
loc_822FDE0C:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fde20
	if (cr6.eq) goto loc_822FDE20;
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fde24
	if (cr6.eq) goto loc_822FDE24;
loc_822FDE20:
	// twi 31,r0,22
loc_822FDE24:
	// lwz r9,124(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// beq cr6,0x822fdf20
	if (cr6.eq) goto loc_822FDF20;
	// cmplwi cr6,r31,1
	cr6.compare<uint32_t>(r31.u32, 1, xer);
	// bne cr6,0x822fde48
	if (!cr6.eq) goto loc_822FDE48;
	// clrlwi r9,r27,31
	ctx.r9.u64 = r27.u32 & 0x1;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822fdea4
	if (!cr6.eq) goto loc_822FDEA4;
	// b 0x822fdef8
	goto loc_822FDEF8;
loc_822FDE48:
	// cmplwi cr6,r31,2
	cr6.compare<uint32_t>(r31.u32, 2, xer);
	// bne cr6,0x822fde60
	if (!cr6.eq) goto loc_822FDE60;
	// rlwinm r9,r27,0,27,27
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x10;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822fdea4
	if (!cr6.eq) goto loc_822FDEA4;
	// b 0x822fdef8
	goto loc_822FDEF8;
loc_822FDE60:
	// cmplwi cr6,r31,3
	cr6.compare<uint32_t>(r31.u32, 3, xer);
	// bne cr6,0x822fde78
	if (!cr6.eq) goto loc_822FDE78;
	// rlwinm r9,r27,0,23,23
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x100;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822fdea4
	if (!cr6.eq) goto loc_822FDEA4;
	// b 0x822fdef8
	goto loc_822FDEF8;
loc_822FDE78:
	// cmplwi cr6,r31,4
	cr6.compare<uint32_t>(r31.u32, 4, xer);
	// bne cr6,0x822fde90
	if (!cr6.eq) goto loc_822FDE90;
	// rlwinm r9,r27,0,19,19
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x1000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822fdea4
	if (!cr6.eq) goto loc_822FDEA4;
	// b 0x822fdef8
	goto loc_822FDEF8;
loc_822FDE90:
	// cmplwi cr6,r31,5
	cr6.compare<uint32_t>(r31.u32, 5, xer);
	// bne cr6,0x822fdef8
	if (!cr6.eq) goto loc_822FDEF8;
	// rlwinm r9,r27,0,15,15
	ctx.r9.u64 = __builtin_rotateleft64(r27.u32 | (r27.u64 << 32), 0) & 0x10000;
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822fdef8
	if (cr6.eq) goto loc_822FDEF8;
loc_822FDEA4:
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fdeb0
	if (!cr6.eq) goto loc_822FDEB0;
	// twi 31,r0,22
loc_822FDEB0:
	// lwz r10,4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fdec0
	if (!cr6.eq) goto loc_822FDEC0;
	// twi 31,r0,22
loc_822FDEC0:
	// lwz r11,12(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// addi r4,r29,84
	ctx.r4.s64 = r29.s64 + 84;
	// addi r3,r1,97
	ctx.r3.s64 = ctx.r1.s64 + 97;
	// stb r28,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r28.u8);
	// li r5,12
	ctx.r5.s64 = 12;
	// lwz r25,100(r11)
	r25.u64 = PPC_LOAD_U32(r11.u32 + 100);
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// bl 0x82266070
	sub_82266070(ctx, base);
	// stw r3,109(r1)
	PPC_STORE_U32(ctx.r1.u32 + 109, ctx.r3.u32);
	// mr r6,r25
	ctx.r6.u64 = r25.u64;
	// li r5,17
	ctx.r5.s64 = 17;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// bl 0x822fcd80
	sub_822FCD80(ctx, base);
loc_822FDEF8:
	// addi r30,r30,1
	r30.s64 = r30.s64 + 1;
	// mullw r11,r31,r26
	r11.s64 = int64_t(r31.s32) * int64_t(r26.s32);
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// blt cr6,0x822fdf0c
	if (cr6.lt) goto loc_822FDF0C;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
loc_822FDF0C:
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82a596f0
	sub_82A596F0(ctx, base);
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// b 0x822fde0c
	goto loc_822FDE0C;
loc_822FDF20:
	// bl 0x82266070
	sub_82266070(ctx, base);
	// lwz r10,124(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 124);
	// lwz r11,124(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// addi r7,r10,1
	ctx.r7.s64 = ctx.r10.s64 + 1;
	// stw r3,116(r29)
	PPC_STORE_U32(r29.u32 + 116, ctx.r3.u32);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// stw r7,124(r29)
	PPC_STORE_U32(r29.u32 + 124, ctx.r7.u32);
	// addi r4,r1,120
	ctx.r4.s64 = ctx.r1.s64 + 120;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// stw r5,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r5.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x826a4a20
	sub_826A4A20(ctx, base);
	// lwz r3,124(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822FDF78"))) PPC_WEAK_FUNC(sub_822FDF78);
PPC_FUNC_IMPL(__imp__sub_822FDF78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r30,0
	r30.s64 = 0;
	// addi r28,r31,424
	r28.s64 = r31.s64 + 424;
	// addi r27,r31,432
	r27.s64 = r31.s64 + 432;
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, r30.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// stw r4,460(r31)
	PPC_STORE_U32(r31.u32 + 460, ctx.r4.u32);
	// addi r9,r1,100
	ctx.r9.s64 = ctx.r1.s64 + 100;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// ld r11,120(r4)
	r11.u64 = PPC_LOAD_U64(ctx.r4.u32 + 120);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r4,256(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// std r11,424(r31)
	PPC_STORE_U64(r31.u32 + 424, r11.u64);
	// li r5,1
	ctx.r5.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r26,r30
	r26.u64 = r30.u64;
	// bl 0x82cfaca8
	sub_82CFACA8(ctx, base);
	// lis r25,-32761
	r25.s64 = -2147024896;
	// cmplwi cr6,r3,122
	cr6.compare<uint32_t>(ctx.r3.u32, 122, xer);
	// beq cr6,0x822fdffc
	if (cr6.eq) goto loc_822FDFFC;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bgt cr6,0x822fdfec
	if (cr6.gt) goto loc_822FDFEC;
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// b 0x822fdff4
	goto loc_822FDFF4;
loc_822FDFEC:
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// or r26,r11,r25
	r26.u64 = r11.u64 | r25.u64;
loc_822FDFF4:
	// cmpwi cr6,r26,0
	cr6.compare<int32_t>(r26.s32, 0, xer);
	// blt cr6,0x822fe088
	if (cr6.lt) goto loc_822FE088;
loc_822FDFFC:
	// lwz r4,100(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// addi r29,r31,452
	r29.s64 = r31.s64 + 452;
	// lwz r11,452(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 452);
	// cmplw cr6,r4,r11
	cr6.compare<uint32_t>(ctx.r4.u32, r11.u32, xer);
	// ble cr6,0x822fe02c
	if (!cr6.gt) goto loc_822FE02C;
	// stw r30,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r30.u32);
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// stb r30,96(r1)
	PPC_STORE_U8(ctx.r1.u32 + 96, r30.u8);
	// addi r3,r31,436
	ctx.r3.s64 = r31.s64 + 436;
	// bl 0x82300648
	sub_82300648(ctx, base);
	// lwz r11,100(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// stw r11,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r11.u32);
loc_822FE02C:
	// std r30,128(r31)
	PPC_STORE_U64(r31.u32 + 128, r30.u64);
	// addi r11,r31,128
	r11.s64 = r31.s64 + 128;
	// std r30,136(r31)
	PPC_STORE_U64(r31.u32 + 136, r30.u64);
	// mr r9,r29
	ctx.r9.u64 = r29.u64;
	// std r30,144(r31)
	PPC_STORE_U64(r31.u32 + 144, r30.u64);
	// stw r30,152(r31)
	PPC_STORE_U32(r31.u32 + 152, r30.u32);
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// li r7,1
	ctx.r7.s64 = 1;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// mr r6,r28
	ctx.r6.u64 = r28.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r4,256(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 256);
	// lwz r10,440(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 440);
	// bl 0x82cfaca8
	sub_82CFACA8(ctx, base);
	// cmplwi cr6,r3,997
	cr6.compare<uint32_t>(ctx.r3.u32, 997, xer);
	// beq cr6,0x822fe088
	if (cr6.eq) goto loc_822FE088;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// ble cr6,0x822fe08c
	if (!cr6.gt) goto loc_822FE08C;
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// or r3,r11,r25
	ctx.r3.u64 = r11.u64 | r25.u64;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c2c
	return;
loc_822FE088:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
loc_822FE08C:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822FE098"))) PPC_WEAK_FUNC(sub_822FE098);
PPC_FUNC_IMPL(__imp__sub_822FE098) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCRegister reserved{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31950
	r11.s64 = -2093875200;
	// lis r9,6184
	ctx.r9.s64 = 405274624;
	// addi r10,r11,-9848
	ctx.r10.s64 = r11.s64 + -9848;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r29,0
	r29.s64 = 0;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// ori r9,r9,134
	ctx.r9.u64 = ctx.r9.u64 | 134;
loc_822FE0C0:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmpw cr6,r8,r9
	cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, xer);
	// beq cr6,0x822fe0dc
	if (cr6.eq) goto loc_822FE0DC;
	// addi r11,r11,112
	r11.s64 = r11.s64 + 112;
	// addi r8,r10,11872
	ctx.r8.s64 = ctx.r10.s64 + 11872;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// blt cr6,0x822fe0c0
	if (cr6.lt) goto loc_822FE0C0;
loc_822FE0DC:
	// lwz r11,476(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 476);
	// cmpwi cr6,r11,-1
	cr6.compare<int32_t>(r11.s32, -1, xer);
	// bne cr6,0x822fe0f8
	if (!cr6.eq) goto loc_822FE0F8;
loc_822FE0E8:
	// lis r3,-32768
	ctx.r3.s64 = -2147483648;
	// ori r3,r3,16389
	ctx.r3.u64 = ctx.r3.u64 | 16389;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FE0F8:
	// mr r11,r29
	r11.u64 = r29.u64;
	// addi r10,r31,481
	ctx.r10.s64 = r31.s64 + 481;
loc_822FE100:
	// lbzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + r11.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822fe29c
	if (cr6.eq) goto loc_822FE29C;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// blt cr6,0x822fe100
	if (cr6.lt) goto loc_822FE100;
	// li r11,-1
	r11.s64 = -1;
loc_822FE11C:
	// lwz r10,460(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// stw r11,112(r10)
	PPC_STORE_U32(ctx.r10.u32 + 112, r11.u32);
	// lwz r9,460(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// lwz r4,112(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 112);
	// cmpwi cr6,r4,-1
	cr6.compare<int32_t>(ctx.r4.s32, -1, xer);
	// beq cr6,0x822fe0e8
	if (cr6.eq) goto loc_822FE0E8;
	// addi r11,r1,84
	r11.s64 = ctx.r1.s64 + 84;
	// li r10,3
	ctx.r10.s64 = 3;
	// li r5,64
	ctx.r5.s64 = 64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// bl 0x82a396a8
	sub_82A396A8(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lwz r3,464(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 464);
	// lwz r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmpw cr6,r3,r9
	cr6.compare<int32_t>(ctx.r3.s32, ctx.r9.s32, xer);
	// beq cr6,0x822fe1d8
	if (cr6.eq) goto loc_822FE1D8;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fe1a8
	if (cr6.eq) goto loc_822FE1A8;
	// addi r9,r3,4
	ctx.r9.s64 = ctx.r3.s64 + 4;
loc_822FE170:
	// mfmsr r10
	// mtmsrd r13,1
	// lwarx r11,0,r9
	reserved.u32 = *(uint32_t*)(base + ctx.r9.u32);
	r11.u64 = __builtin_bswap32(reserved.u32);
	// addi r11,r11,-1
	r11.s64 = r11.s64 + -1;
	// stwcx. r11,0,r9
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r9.u32), reserved.s32, __builtin_bswap32(r11.s32));
	cr0.so = xer.so;
	// mtmsrd r10,1
	// bne 0x822fe170
	if (!cr0.eq) goto loc_822FE170;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822fe1a4
	if (!cr6.eq) goto loc_822FE1A4;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822FE1A4:
	// stw r29,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r29.u32);
loc_822FE1A8:
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r11,464(r31)
	PPC_STORE_U32(r31.u32 + 464, r11.u32);
	// beq cr6,0x822fe1d8
	if (cr6.eq) goto loc_822FE1D8;
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
loc_822FE1BC:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r11
	reserved.u32 = *(uint32_t*)(base + r11.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwcx. r10,0,r11
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + r11.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822fe1bc
	if (!cr0.eq) goto loc_822FE1BC;
loc_822FE1D8:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe228
	if (cr6.eq) goto loc_822FE228;
	// mr r3,r11
	ctx.r3.u64 = r11.u64;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822FE1EC:
	// mfmsr r9
	// mtmsrd r13,1
	// lwarx r10,0,r8
	reserved.u32 = *(uint32_t*)(base + ctx.r8.u32);
	ctx.r10.u64 = __builtin_bswap32(reserved.u32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stwcx. r10,0,r8
	cr0.lt = 0;
	cr0.gt = 0;
	cr0.eq = __sync_bool_compare_and_swap(reinterpret_cast<uint32_t*>(base + ctx.r8.u32), reserved.s32, __builtin_bswap32(ctx.r10.s32));
	cr0.so = xer.so;
	// mtmsrd r9,1
	// bne 0x822fe1ec
	if (!cr0.eq) goto loc_822FE1EC;
	// mr r11,r10
	r11.u64 = ctx.r10.u64;
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// bne cr6,0x822fe224
	if (!cr6.eq) goto loc_822FE224;
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822FE224:
	// stw r29,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r29.u32);
loc_822FE228:
	// lwz r30,464(r31)
	r30.u64 = PPC_LOAD_U32(r31.u32 + 464);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822fe0e8
	if (cr6.eq) goto loc_822FE0E8;
	// lwz r11,460(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 460);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r28,440(r31)
	r28.u64 = PPC_LOAD_U32(r31.u32 + 440);
	// addi r27,r31,128
	r27.s64 = r31.s64 + 128;
	// lwz r10,112(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 112);
	// stw r10,468(r31)
	PPC_STORE_U32(r31.u32 + 468, ctx.r10.u32);
	// bl 0x82a398e0
	sub_82A398E0(ctx, base);
	// mr r26,r3
	r26.u64 = ctx.r3.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82a39888
	sub_82A39888(ctx, base);
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// mr r5,r3
	ctx.r5.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r11,24
	ctx.r3.s64 = r11.s64 + 24;
	// mr r6,r26
	ctx.r6.u64 = r26.u64;
	// li r7,64
	ctx.r7.s64 = 64;
	// mr r8,r27
	ctx.r8.u64 = r27.u64;
	// bl 0x82cbc220
	sub_82CBC220(ctx, base);
	// stb r29,480(r31)
	PPC_STORE_U8(r31.u32 + 480, r29.u8);
	// cmplwi cr6,r3,997
	cr6.compare<uint32_t>(ctx.r3.u32, 997, xer);
	// beq cr6,0x822fe2b4
	if (cr6.eq) goto loc_822FE2B4;
	// cmpwi cr6,r3,0
	cr6.compare<int32_t>(ctx.r3.s32, 0, xer);
	// bgt cr6,0x822fe2ac
	if (cr6.gt) goto loc_822FE2AC;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FE29C:
	// add r10,r11,r31
	ctx.r10.u64 = r11.u64 + r31.u64;
	// li r9,1
	ctx.r9.s64 = 1;
	// stb r9,481(r10)
	PPC_STORE_U8(ctx.r10.u32 + 481, ctx.r9.u8);
	// b 0x822fe11c
	goto loc_822FE11C;
loc_822FE2AC:
	// clrlwi r11,r3,16
	r11.u64 = ctx.r3.u32 & 0xFFFF;
	// oris r29,r11,32775
	r29.u64 = r11.u64 | 2147942400;
loc_822FE2B4:
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822FE2C0"))) PPC_WEAK_FUNC(sub_822FE2C0);
PPC_FUNC_IMPL(__imp__sub_822FE2C0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r22{};
	PPCRegister r23{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd0
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r23,r5
	r23.u64 = ctx.r5.u64;
	// mr r22,r3
	r22.u64 = ctx.r3.u64;
	// mr r24,r4
	r24.u64 = ctx.r4.u64;
	// cmplwi cr6,r23,0
	cr6.compare<uint32_t>(r23.u32, 0, xer);
	// beq cr6,0x822fe488
	if (cr6.eq) goto loc_822FE488;
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_822FE2F0:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x822fe2f0
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822FE2F0;
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_822FE30C:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x822fe30c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_822FE30C;
	// mr r27,r24
	r27.u64 = r24.u64;
	// mr r30,r24
	r30.u64 = r24.u64;
	// cmplwi cr6,r24,0
	cr6.compare<uint32_t>(r24.u32, 0, xer);
	// beq cr6,0x822fe42c
	if (cr6.eq) goto loc_822FE42C;
	// li r26,0
	r26.s64 = 0;
	// lis r25,-31927
	r25.s64 = -2092367872;
loc_822FE330:
	// cmplwi cr6,r26,28
	cr6.compare<uint32_t>(r26.u32, 28, xer);
	// bge cr6,0x822fe42c
	if (!cr6.lt) goto loc_822FE42C;
	// add r11,r24,r23
	r11.u64 = r24.u64 + r23.u64;
	// cmplw cr6,r27,r11
	cr6.compare<uint32_t>(r27.u32, r11.u32, xer);
	// bgt cr6,0x822fe42c
	if (cr6.gt) goto loc_822FE42C;
	// li r4,10
	ctx.r4.s64 = 10;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca3980
	sub_82CA3980(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822fe42c
	if (cr6.eq) goto loc_822FE42C;
	// subf r29,r30,r27
	r29.s64 = r27.s64 - r30.s64;
	// addi r28,r29,1
	r28.s64 = r29.s64 + 1;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x8221f3f0
	sub_8221F3F0(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// bne cr6,0x822fe398
	if (!cr6.eq) goto loc_822FE398;
	// lwz r11,28060(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28060);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe394
	if (cr6.eq) goto loc_822FE394;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822FE394:
	// bl 0x82cbbb58
	sub_82CBBB58(ctx, base);
loc_822FE398:
	// addi r11,r1,112
	r11.s64 = ctx.r1.s64 + 112;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r30
	ctx.r5.u64 = r30.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stwx r31,r26,r11
	PPC_STORE_U32(r26.u32 + r11.u32, r31.u32);
	// bl 0x82ca39c0
	sub_82CA39C0(ctx, base);
	// li r4,61
	ctx.r4.s64 = 61;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x82ca3980
	sub_82CA3980(ctx, base);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fe420
	if (cr6.eq) goto loc_822FE420;
	// addi r29,r3,1
	r29.s64 = ctx.r3.s64 + 1;
	// subf r31,r29,r27
	r31.s64 = r27.s64 - r29.s64;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x8221f3f0
	sub_8221F3F0(ctx, base);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// bne cr6,0x822fe404
	if (!cr6.eq) goto loc_822FE404;
	// lwz r11,28060(r25)
	r11.u64 = PPC_LOAD_U32(r25.u32 + 28060);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe400
	if (cr6.eq) goto loc_822FE400;
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// li r4,0
	ctx.r4.s64 = 0;
	// mtctr r11
	ctr.u64 = r11.u64;
	// bctrl 
	PPC_CALL_INDIRECT_FUNC(ctr.u32);
loc_822FE400:
	// bl 0x82cbbb58
	sub_82CBBB58(ctx, base);
loc_822FE404:
	// addi r11,r1,80
	r11.s64 = ctx.r1.s64 + 80;
	// mr r6,r31
	ctx.r6.u64 = r31.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// addi r4,r31,1
	ctx.r4.s64 = r31.s64 + 1;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// stwx r30,r26,r11
	PPC_STORE_U32(r26.u32 + r11.u32, r30.u32);
	// bl 0x82ca39c0
	sub_82CA39C0(ctx, base);
loc_822FE420:
	// addi r30,r27,1
	r30.s64 = r27.s64 + 1;
	// addi r26,r26,4
	r26.s64 = r26.s64 + 4;
	// b 0x822fe330
	goto loc_822FE330;
loc_822FE42C:
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// stw r3,96(r22)
	PPC_STORE_U32(r22.u32 + 96, ctx.r3.u32);
	// lwz r3,84(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// lwz r3,88(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// stb r11,108(r22)
	PPC_STORE_U8(r22.u32 + 108, r11.u8);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r3,92(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// stb r10,109(r22)
	PPC_STORE_U8(r22.u32 + 109, ctx.r10.u8);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// stw r3,392(r22)
	PPC_STORE_U32(r22.u32 + 392, ctx.r3.u32);
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// stw r3,396(r22)
	PPC_STORE_U32(r22.u32 + 396, ctx.r3.u32);
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// stw r3,400(r22)
	PPC_STORE_U32(r22.u32 + 400, ctx.r3.u32);
	// lwz r3,104(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// bl 0x82ca3b30
	sub_82CA3B30(ctx, base);
	// stw r3,408(r22)
	PPC_STORE_U32(r22.u32 + 408, ctx.r3.u32);
loc_822FE488:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x82ca2c20
	return;
}

__attribute__((alias("__imp__sub_822FE490"))) PPC_WEAK_FUNC(sub_822FE490);
PPC_FUNC_IMPL(__imp__sub_822FE490) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r4,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r4.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r31,r5
	r31.u64 = ctx.r5.u64;
	// addi r29,r30,356
	r29.s64 = r30.s64 + 356;
	// addi r5,r1,156
	ctx.r5.s64 = ctx.r1.s64 + 156;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// li r28,1
	r28.s64 = 1;
	// bl 0x822ff190
	sub_822FF190(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r10,360(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 360);
	// beq cr6,0x822fe4e8
	if (cr6.eq) goto loc_822FE4E8;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// beq cr6,0x822fe4ec
	if (cr6.eq) goto loc_822FE4EC;
loc_822FE4E8:
	// twi 31,r0,22
loc_822FE4EC:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fe548
	if (!cr6.eq) goto loc_822FE548;
	// addi r29,r30,344
	r29.s64 = r30.s64 + 344;
	// addi r5,r1,156
	ctx.r5.s64 = ctx.r1.s64 + 156;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822ff190
	sub_822FF190(ctx, base);
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// stw r11,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r10,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r10.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// lwz r10,348(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 348);
	// beq cr6,0x822fe534
	if (cr6.eq) goto loc_822FE534;
	// cmplw cr6,r11,r29
	cr6.compare<uint32_t>(r11.u32, r29.u32, xer);
	// beq cr6,0x822fe538
	if (cr6.eq) goto loc_822FE538;
loc_822FE534:
	// twi 31,r0,22
loc_822FE538:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r3,0
	ctx.r3.s64 = 0;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// beq cr6,0x822fe54c
	if (cr6.eq) goto loc_822FE54C;
loc_822FE548:
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
loc_822FE54C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822FE558"))) PPC_WEAK_FUNC(sub_822FE558);
PPC_FUNC_IMPL(__imp__sub_822FE558) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// stw r4,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r4.u32);
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// addi r5,r1,156
	ctx.r5.s64 = ctx.r1.s64 + 156;
	// addi r31,r30,356
	r31.s64 = r30.s64 + 356;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822ff190
	sub_822FF190(ctx, base);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r10,360(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 360);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe5a0
	if (cr6.eq) goto loc_822FE5A0;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fe5a4
	if (cr6.eq) goto loc_822FE5A4;
loc_822FE5A0:
	// twi 31,r0,22
loc_822FE5A4:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fe61c
	if (!cr6.eq) goto loc_822FE61C;
	// addi r31,r30,344
	r31.s64 = r30.s64 + 344;
	// addi r5,r1,156
	ctx.r5.s64 = ctx.r1.s64 + 156;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822ff190
	sub_822FF190(ctx, base);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,348(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 348);
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe5e4
	if (cr6.eq) goto loc_822FE5E4;
	// cmplw cr6,r11,r31
	cr6.compare<uint32_t>(r11.u32, r31.u32, xer);
	// beq cr6,0x822fe5e8
	if (cr6.eq) goto loc_822FE5E8;
loc_822FE5E4:
	// twi 31,r0,22
loc_822FE5E8:
	// lwz r9,84(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r9,r10
	cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, xer);
	// beq cr6,0x822fe628
	if (cr6.eq) goto loc_822FE628;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fe600
	if (!cr6.eq) goto loc_822FE600;
	// twi 31,r0,22
loc_822FE600:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r9,r11
	cr6.compare<uint32_t>(ctx.r9.u32, r11.u32, xer);
	// bne cr6,0x822fe610
	if (!cr6.eq) goto loc_822FE610;
	// twi 31,r0,22
loc_822FE610:
	// ld r5,0(r3)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// b 0x822fe620
	goto loc_822FE620;
loc_822FE61C:
	// ld r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_822FE620:
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// bl 0x822fed20
	sub_822FED20(ctx, base);
loc_822FE628:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FE640"))) PPC_WEAK_FUNC(sub_822FE640);
PPC_FUNC_IMPL(__imp__sub_822FE640) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// addi r28,r29,368
	r28.s64 = r29.s64 + 368;
	// std r27,168(r1)
	PPC_STORE_U64(ctx.r1.u32 + 168, r27.u64);
	// lwz r30,168(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 168);
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// lwz r11,372(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 372);
	// beq cr6,0x822fe674
	if (cr6.eq) goto loc_822FE674;
	// cmplw cr6,r30,r28
	cr6.compare<uint32_t>(r30.u32, r28.u32, xer);
	// beq cr6,0x822fe678
	if (cr6.eq) goto loc_822FE678;
loc_822FE674:
	// twi 31,r0,22
loc_822FE678:
	// lwz r31,172(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 172);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// beq cr6,0x822fe7e0
	if (cr6.eq) goto loc_822FE7E0;
	// li r11,1
	r11.s64 = 1;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// bne cr6,0x822fe698
	if (!cr6.eq) goto loc_822FE698;
	// twi 31,r0,22
loc_822FE698:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fe6a8
	if (!cr6.eq) goto loc_822FE6A8;
	// twi 31,r0,22
loc_822FE6A8:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r10,120(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 120);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// bl 0x822fe7f0
	sub_822FE7F0(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fe6d0
	if (!cr6.eq) goto loc_822FE6D0;
	// twi 31,r0,22
loc_822FE6D0:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// addi r5,r10,108
	ctx.r5.s64 = ctx.r10.s64 + 108;
	// lwz r10,108(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 108);
	// cmpwi cr6,r10,-1
	cr6.compare<int32_t>(ctx.r10.s32, -1, xer);
	// beq cr6,0x822fe728
	if (cr6.eq) goto loc_822FE728;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fe6f0
	if (!cr6.eq) goto loc_822FE6F0;
	// twi 31,r0,22
loc_822FE6F0:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// addi r3,r11,28928
	ctx.r3.s64 = r11.s64 + 28928;
	// stw r3,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r3.u32);
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x825f7b10
	sub_825F7B10(ctx, base);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fe71c
	if (!cr6.eq) goto loc_822FE71C;
	// twi 31,r0,22
loc_822FE71C:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r10,-1
	ctx.r10.s64 = -1;
	// stw r10,108(r11)
	PPC_STORE_U32(r11.u32 + 108, ctx.r10.u32);
loc_822FE728:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fe738
	if (!cr6.eq) goto loc_822FE738;
	// twi 31,r0,22
loc_822FE738:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,112(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 112);
	// cmpwi cr6,r11,0
	cr6.compare<int32_t>(r11.s32, 0, xer);
	// blt cr6,0x822fe75c
	if (cr6.lt) goto loc_822FE75C;
	// cmpwi cr6,r11,64
	cr6.compare<int32_t>(r11.s32, 64, xer);
	// bge cr6,0x822fe75c
	if (!cr6.lt) goto loc_822FE75C;
	// add r11,r11,r29
	r11.u64 = r11.u64 + r29.u64;
	// stb r10,481(r11)
	PPC_STORE_U8(r11.u32 + 481, ctx.r10.u8);
loc_822FE75C:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fe76c
	if (!cr6.eq) goto loc_822FE76C;
	// twi 31,r0,22
loc_822FE76C:
	// lwz r11,24(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// lwz r9,460(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 460);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822fe788
	if (!cr6.eq) goto loc_822FE788;
	// stw r10,460(r29)
	PPC_STORE_U32(r29.u32 + 460, ctx.r10.u32);
	// stb r10,456(r29)
	PPC_STORE_U8(r29.u32 + 456, ctx.r10.u8);
	// stb r10,457(r29)
	PPC_STORE_U8(r29.u32 + 457, ctx.r10.u8);
loc_822FE788:
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822fe798
	if (!cr6.eq) goto loc_822FE798;
	// twi 31,r0,22
loc_822FE798:
	// lwz r31,24(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 24);
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822fe7c4
	if (cr6.eq) goto loc_822FE7C4;
	// addi r11,r31,160
	r11.s64 = r31.s64 + 160;
	// addi r3,r11,4
	ctx.r3.s64 = r11.s64 + 4;
	// bl 0x82214f08
	sub_82214F08(ctx, base);
	// lis r11,-32246
	r11.s64 = -2113273856;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r10,r11,11204
	ctx.r10.s64 = r11.s64 + 11204;
	// stw r10,160(r31)
	PPC_STORE_U32(r31.u32 + 160, ctx.r10.u32);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_822FE7C4:
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822ff3c8
	sub_822FF3C8(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
loc_822FE7E0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822FE7F0"))) PPC_WEAK_FUNC(sub_822FE7F0);
PPC_FUNC_IMPL(__imp__sub_822FE7F0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bgt cr6,0x822fe820
	if (cr6.gt) goto loc_822FE820;
	// li r4,1
	ctx.r4.s64 = 1;
	// bl 0x82300940
	sub_82300940(ctx, base);
loc_822FE820:
	// lwz r11,12(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r11,r11,r9
	r11.u64 = r11.u64 + ctx.r9.u64;
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bgt cr6,0x822fe83c
	if (cr6.gt) goto loc_822FE83C;
	// subf r11,r10,r11
	r11.s64 = r11.s64 - ctx.r10.s64;
loc_822FE83C:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// rlwinm r30,r11,2,0,29
	r30.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r30
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + r30.u32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822fe860
	if (!cr6.eq) goto loc_822FE860;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stwx r3,r11,r30
	PPC_STORE_U32(r11.u32 + r30.u32, ctx.r3.u32);
loc_822FE860:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwzx r11,r11,r30
	r11.u64 = PPC_LOAD_U32(r11.u32 + r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe880
	if (cr6.eq) goto loc_822FE880;
	// ld r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U64(r29.u32 + 0);
	// ld r9,8(r29)
	ctx.r9.u64 = PPC_LOAD_U64(r29.u32 + 8);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
	// std r9,8(r11)
	PPC_STORE_U64(r11.u32 + 8, ctx.r9.u64);
loc_822FE880:
	// lwz r11,16(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 16);
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,16(r31)
	PPC_STORE_U32(r31.u32 + 16, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FE898"))) PPC_WEAK_FUNC(sub_822FE898);
PPC_FUNC_IMPL(__imp__sub_822FE898) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// li r25,1
	r25.s64 = 1;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// mr r27,r25
	r27.u64 = r25.u64;
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lbz r11,49(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 49);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fe908
	if (!cr6.eq) goto loc_822FE908;
loc_822FE8CC:
	// addi r5,r31,12
	ctx.r5.s64 = r31.s64 + 12;
	// mr r4,r26
	ctx.r4.u64 = r26.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// mr r29,r31
	r29.u64 = r31.u64;
	// bl 0x822f88e8
	sub_822F88E8(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// clrlwi r11,r27,24
	r11.u64 = r27.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe8f8
	if (cr6.eq) goto loc_822FE8F8;
	// lwz r31,0(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// b 0x822fe8fc
	goto loc_822FE8FC;
loc_822FE8F8:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822FE8FC:
	// lbz r11,49(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 49);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe8cc
	if (cr6.eq) goto loc_822FE8CC;
loc_822FE908:
	// clrlwi r11,r27,24
	r11.u64 = r27.u32 & 0xFF;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fe96c
	if (cr6.eq) goto loc_822FE96C;
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fe968
	if (!cr6.eq) goto loc_822FE968;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// bl 0x822ffca0
	sub_822FFCA0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r25,8(r28)
	PPC_STORE_U8(r28.u32 + 8, r25.u8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// stw r9,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r9.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
loc_822FE968:
	// bl 0x823d4e58
	sub_823D4E58(ctx, base);
loc_822FE96C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r5,r26
	ctx.r5.u64 = r26.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// addi r4,r11,12
	ctx.r4.s64 = r11.s64 + 12;
	// bl 0x822f88e8
	sub_822F88E8(ctx, base);
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822fe9c8
	if (cr6.eq) goto loc_822FE9C8;
	// mr r7,r26
	ctx.r7.u64 = r26.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822ffca0
	sub_822FFCA0(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r25,8(r28)
	PPC_STORE_U8(r28.u32 + 8, r25.u8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// stw r9,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r9.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
loc_822FE9C8:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stb r10,8(r28)
	PPC_STORE_U8(r28.u32 + 8, ctx.r10.u8);
	// std r11,0(r28)
	PPC_STORE_U64(r28.u32 + 0, r11.u64);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822FE9E8"))) PPC_WEAK_FUNC(sub_822FE9E8);
PPC_FUNC_IMPL(__imp__sub_822FE9E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be8
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lwz r28,4(r30)
	r28.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r31,4(r28)
	r31.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lbz r11,49(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 49);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fea48
	if (!cr6.eq) goto loc_822FEA48;
loc_822FEA10:
	// addi r5,r31,12
	ctx.r5.s64 = r31.s64 + 12;
	// mr r4,r29
	ctx.r4.u64 = r29.u64;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f88e8
	sub_822F88E8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fea38
	if (cr6.eq) goto loc_822FEA38;
	// mr r28,r31
	r28.u64 = r31.u64;
	// lwz r31,0(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// b 0x822fea3c
	goto loc_822FEA3C;
loc_822FEA38:
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822FEA3C:
	// lbz r11,49(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 49);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fea10
	if (cr6.eq) goto loc_822FEA10;
loc_822FEA48:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, r28.u32);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r30.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x822ffc28
	sub_822FFC28(ctx, base);
	// ld r11,0(r3)
	r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// li r31,0
	r31.s64 = 0;
	// mr r29,r11
	r29.u64 = r11.u64;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, r11.u64);
loc_822FEA70:
	// lwz r11,80(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fea84
	if (cr6.eq) goto loc_822FEA84;
	// cmplw cr6,r11,r30
	cr6.compare<uint32_t>(r11.u32, r30.u32, xer);
	// beq cr6,0x822fea88
	if (cr6.eq) goto loc_822FEA88;
loc_822FEA84:
	// twi 31,r0,22
loc_822FEA88:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822feaa4
	if (cr6.eq) goto loc_822FEAA4;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r31,r31,1
	r31.s64 = r31.s64 + 1;
	// bl 0x823d4808
	sub_823D4808(ctx, base);
	// b 0x822fea70
	goto loc_822FEA70;
loc_822FEAA4:
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// ld r6,88(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// mr r4,r30
	ctx.r4.u64 = r30.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// bl 0x822ffb18
	sub_822FFB18(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c38
	return;
}

__attribute__((alias("__imp__sub_822FEAC8"))) PPC_WEAK_FUNC(sub_822FEAC8);
PPC_FUNC_IMPL(__imp__sub_822FEAC8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// bl 0x822fff78
	sub_822FFF78(ctx, base);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lwz r29,4(r31)
	r29.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// bl 0x82301190
	sub_82301190(ctx, base);
	// stw r3,4(r29)
	PPC_STORE_U32(r29.u32 + 4, ctx.r3.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lbz r8,105(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822feb7c
	if (!cr6.eq) goto loc_822FEB7C;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822feb3c
	if (!cr6.eq) goto loc_822FEB3C;
loc_822FEB28:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822feb28
	if (cr6.eq) goto loc_822FEB28;
loc_822FEB3C:
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,4(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r11,8(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822feb6c
	if (!cr6.eq) goto loc_822FEB6C;
loc_822FEB58:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822feb58
	if (cr6.eq) goto loc_822FEB58;
loc_822FEB6C:
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
loc_822FEB7C:
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_822FEB98"))) PPC_WEAK_FUNC(sub_822FEB98);
PPC_FUNC_IMPL(__imp__sub_822FEB98) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// li r26,1
	r26.s64 = 1;
	// mr r28,r3
	r28.u64 = ctx.r3.u64;
	// mr r30,r5
	r30.u64 = ctx.r5.u64;
	// mr r31,r26
	r31.u64 = r26.u64;
	// lwz r29,4(r27)
	r29.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r9,4(r29)
	ctx.r9.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lbz r11,105(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fec28
	if (!cr6.eq) goto loc_822FEC28;
loc_822FEBCC:
	// mr r29,r9
	r29.u64 = ctx.r9.u64;
	// addi r10,r9,16
	ctx.r10.s64 = ctx.r9.s64 + 16;
	// mr r11,r30
	r11.u64 = r30.u64;
	// addi r7,r30,4
	ctx.r7.s64 = r30.s64 + 4;
loc_822FEBDC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	cr0.compare<int32_t>(ctx.r8.s32, 0, xer);
	// bne 0x822febfc
	if (!cr0.eq) goto loc_822FEBFC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bne cr6,0x822febdc
	if (!cr6.eq) goto loc_822FEBDC;
loc_822FEBFC:
	// cntlzw r11,r8
	r11.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// cntlzw r10,r11
	ctx.r10.u64 = r11.u32 == 0 ? 32 : __builtin_clz(r11.u32);
	// rlwinm r31,r10,27,31,31
	r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x822fec18
	if (cr6.eq) goto loc_822FEC18;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// b 0x822fec1c
	goto loc_822FEC1C;
loc_822FEC18:
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
loc_822FEC1C:
	// lbz r11,105(r9)
	r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822febcc
	if (cr6.eq) goto loc_822FEBCC;
loc_822FEC28:
	// clrlwi r11,r31,24
	r11.u64 = r31.u32 & 0xFF;
	// stw r27,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r27.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r29.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fec8c
	if (cr6.eq) goto loc_822FEC8C;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r29,r10
	cr6.compare<uint32_t>(r29.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fec88
	if (!cr6.eq) goto loc_822FEC88;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// bl 0x82300000
	sub_82300000(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r28)
	PPC_STORE_U8(r28.u32 + 8, r26.u8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// stw r9,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FEC88:
	// bl 0x82301bc0
	sub_82301BC0(ctx, base);
loc_822FEC8C:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r10,r30
	ctx.r10.u64 = r30.u64;
	// addi r11,r11,16
	r11.s64 = r11.s64 + 16;
	// addi r8,r11,4
	ctx.r8.s64 = r11.s64 + 4;
loc_822FEC9C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822fecbc
	if (!cr0.eq) goto loc_822FECBC;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822fec9c
	if (!cr6.eq) goto loc_822FEC9C;
loc_822FECBC:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x822fed00
	if (!cr6.lt) goto loc_822FED00;
	// mr r7,r30
	ctx.r7.u64 = r30.u64;
	// mr r6,r29
	ctx.r6.u64 = r29.u64;
	// mr r5,r31
	ctx.r5.u64 = r31.u64;
	// mr r4,r27
	ctx.r4.u64 = r27.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82300000
	sub_82300000(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r28)
	PPC_STORE_U8(r28.u32 + 8, r26.u8);
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r28)
	PPC_STORE_U32(r28.u32 + 0, ctx.r10.u32);
	// stw r9,4(r28)
	PPC_STORE_U32(r28.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FED00:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// stb r10,8(r28)
	PPC_STORE_U8(r28.u32 + 8, ctx.r10.u8);
	// std r11,0(r28)
	PPC_STORE_U64(r28.u32 + 0, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822FED20"))) PPC_WEAK_FUNC(sub_822FED20);
PPC_FUNC_IMPL(__imp__sub_822FED20) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r5,256(r1)
	PPC_STORE_U64(ctx.r1.u32 + 256, ctx.r5.u64);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r31,260(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lbz r11,105(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fed8c
	if (cr6.eq) goto loc_822FED8C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5600
	ctx.r4.s64 = r11.s64 + 5600;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822FED8C:
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// mr r26,r31
	r26.u64 = r31.u64;
	// bl 0x823007b8
	sub_823007B8(ctx, base);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lbz r11,105(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fedb0
	if (cr6.eq) goto loc_822FEDB0;
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// b 0x822fedd8
	goto loc_822FEDD8;
loc_822FEDB0:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lbz r9,105(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822fedc8
	if (cr6.eq) goto loc_822FEDC8;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// b 0x822fedd8
	goto loc_822FEDD8;
loc_822FEDC8:
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// lwz r28,8(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bne cr6,0x822feec4
	if (!cr6.eq) goto loc_822FEEC4;
loc_822FEDD8:
	// lbz r11,105(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 105);
	// lwz r31,4(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fedec
	if (!cr6.eq) goto loc_822FEDEC;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822FEDEC:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x822fee04
	if (!cr6.eq) goto loc_822FEE04;
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// b 0x822fee1c
	goto loc_822FEE1C;
loc_822FEE04:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822fee18
	if (!cr6.eq) goto loc_822FEE18;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// b 0x822fee1c
	goto loc_822FEE1C;
loc_822FEE18:
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
loc_822FEE1C:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822fee6c
	if (!cr6.eq) goto loc_822FEE6C;
	// lbz r11,105(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fee40
	if (cr6.eq) goto loc_822FEE40;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// b 0x822fee68
	goto loc_822FEE68;
loc_822FEE40:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822fee68
	if (!cr6.eq) goto loc_822FEE68;
loc_822FEE54:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822fee54
	if (cr6.eq) goto loc_822FEE54;
loc_822FEE68:
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
loc_822FEE6C:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822fef58
	if (!cr6.eq) goto loc_822FEF58;
	// lbz r11,105(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fee94
	if (cr6.eq) goto loc_822FEE94;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822fef58
	goto loc_822FEF58;
loc_822FEE94:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822feebc
	if (!cr6.eq) goto loc_822FEEBC;
loc_822FEEA8:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r8,105(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822feea8
	if (cr6.eq) goto loc_822FEEA8;
loc_822FEEBC:
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822fef58
	goto loc_822FEF58;
loc_822FEEC4:
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822feee4
	if (!cr6.eq) goto loc_822FEEE4;
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x822fef0c
	goto loc_822FEF0C;
loc_822FEEE4:
	// lbz r10,105(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 105);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822feef8
	if (!cr6.eq) goto loc_822FEEF8;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822FEEF8:
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
loc_822FEF0C:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822fef24
	if (!cr6.eq) goto loc_822FEF24;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// b 0x822fef40
	goto loc_822FEF40;
loc_822FEF24:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822fef3c
	if (!cr6.eq) goto loc_822FEF3C;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x822fef40
	goto loc_822FEF40;
loc_822FEF3C:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
loc_822FEF40:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lbz r8,104(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 104);
	// lbz r9,104(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 104);
	// stb r9,104(r11)
	PPC_STORE_U8(r11.u32 + 104, ctx.r9.u8);
	// stb r8,104(r26)
	PPC_STORE_U8(r26.u32 + 104, ctx.r8.u8);
loc_822FEF58:
	// lbz r11,104(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 104);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822ff0f8
	if (!cr6.eq) goto loc_822FF0F8;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// li r29,1
	r29.s64 = 1;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// beq cr6,0x822ff0f4
	if (cr6.eq) goto loc_822FF0F4;
	// li r30,0
	r30.s64 = 0;
loc_822FEF7C:
	// lbz r11,104(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 104);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822ff0f4
	if (!cr6.eq) goto loc_822FF0F4;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822ff038
	if (!cr6.eq) goto loc_822FF038;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r10,104(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 104);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822fefbc
	if (!cr6.eq) goto loc_822FEFBC;
	// stb r29,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,104(r31)
	PPC_STORE_U8(r31.u32 + 104, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82300270
	sub_82300270(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822FEFBC:
	// lbz r10,105(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff08c
	if (!cr6.eq) goto loc_822FF08C;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822fefe8
	if (!cr6.eq) goto loc_822FEFE8;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x822ff088
	if (cr6.eq) goto loc_822FF088;
loc_822FEFE8:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff014
	if (!cr6.eq) goto loc_822FF014;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,104(r10)
	PPC_STORE_U8(ctx.r10.u32 + 104, r29.u8);
	// stb r30,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r30.u8);
	// bl 0x823002e8
	sub_823002E8(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822FF014:
	// lbz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 104);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,104(r11)
	PPC_STORE_U8(r11.u32 + 104, ctx.r10.u8);
	// stb r29,104(r31)
	PPC_STORE_U8(r31.u32 + 104, r29.u8);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stb r29,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r29.u8);
	// bl 0x82300270
	sub_82300270(ctx, base);
	// b 0x822ff0f4
	goto loc_822FF0F4;
loc_822FF038:
	// lbz r10,104(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 104);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff05c
	if (!cr6.eq) goto loc_822FF05C;
	// stb r29,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,104(r31)
	PPC_STORE_U8(r31.u32 + 104, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x823002e8
	sub_823002E8(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822FF05C:
	// lbz r10,105(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff08c
	if (!cr6.eq) goto loc_822FF08C;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff0a8
	if (!cr6.eq) goto loc_822FF0A8;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff0a8
	if (!cr6.eq) goto loc_822FF0A8;
loc_822FF088:
	// stb r30,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r30.u8);
loc_822FF08C:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// mr r28,r31
	r28.u64 = r31.u64;
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// bne cr6,0x822fef7c
	if (!cr6.eq) goto loc_822FEF7C;
	// b 0x822ff0f4
	goto loc_822FF0F4;
loc_822FF0A8:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff0d4
	if (!cr6.eq) goto loc_822FF0D4;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,104(r10)
	PPC_STORE_U8(ctx.r10.u32 + 104, r29.u8);
	// stb r30,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r30.u8);
	// bl 0x82300270
	sub_82300270(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822FF0D4:
	// lbz r10,104(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 104);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,104(r11)
	PPC_STORE_U8(r11.u32 + 104, ctx.r10.u8);
	// stb r29,104(r31)
	PPC_STORE_U8(r31.u32 + 104, r29.u8);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r29,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r29.u8);
	// bl 0x823002e8
	sub_823002E8(ctx, base);
loc_822FF0F4:
	// stb r29,104(r28)
	PPC_STORE_U8(r28.u32 + 104, r29.u8);
loc_822FF0F8:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff128
	if (cr6.eq) goto loc_822FF128;
	// ld r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// stw r9,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r9.u32);
	// std r10,0(r25)
	PPC_STORE_U64(r25.u32 + 0, ctx.r10.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
loc_822FF128:
	// ld r11,256(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// std r11,0(r25)
	PPC_STORE_U64(r25.u32 + 0, r11.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822FF138"))) PPC_WEAK_FUNC(sub_822FF138);
PPC_FUNC_IMPL(__imp__sub_822FF138) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x822fff10
	sub_822FFF10(ctx, base);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r9,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// stw r8,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r8.u32);
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r7,8(r7)
	PPC_STORE_U32(ctx.r7.u32 + 8, ctx.r7.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FF190"))) PPC_WEAK_FUNC(sub_822FF190);
PPC_FUNC_IMPL(__imp__sub_822FF190) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r31{};
	// std r31,-8(r1)
	PPC_STORE_U64(ctx.r1.u32 + -8, r31.u64);
	// lwz r6,4(r4)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r8,4(r6)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// lbz r11,105(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ff1f8
	if (!cr6.eq) goto loc_822FF1F8;
loc_822FF1A8:
	// addi r11,r8,16
	r11.s64 = ctx.r8.s64 + 16;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
	// addi r7,r11,4
	ctx.r7.s64 = r11.s64 + 4;
loc_822FF1B4:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r31,0(r10)
	r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r31,r9
	ctx.r9.s64 = ctx.r9.s64 - r31.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822ff1d4
	if (!cr0.eq) goto loc_822FF1D4;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r7
	cr6.compare<int32_t>(r11.s32, ctx.r7.s32, xer);
	// bne cr6,0x822ff1b4
	if (!cr6.eq) goto loc_822FF1B4;
loc_822FF1D4:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bge cr6,0x822ff1e4
	if (!cr6.lt) goto loc_822FF1E4;
	// lwz r8,8(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// b 0x822ff1ec
	goto loc_822FF1EC;
loc_822FF1E4:
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// lwz r8,0(r8)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
loc_822FF1EC:
	// lbz r11,105(r8)
	r11.u64 = PPC_LOAD_U8(ctx.r8.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff1a8
	if (cr6.eq) goto loc_822FF1A8;
loc_822FF1F8:
	// lwz r7,4(r4)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r6,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, ctx.r6.u32);
	// stw r4,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r4.u32);
	// cmplw cr6,r6,r7
	cr6.compare<uint32_t>(ctx.r6.u32, ctx.r7.u32, xer);
	// beq cr6,0x822ff248
	if (cr6.eq) goto loc_822FF248;
	// addi r10,r6,16
	ctx.r10.s64 = ctx.r6.s64 + 16;
	// mr r11,r5
	r11.u64 = ctx.r5.u64;
	// addi r8,r5,4
	ctx.r8.s64 = ctx.r5.s64 + 4;
loc_822FF218:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 0);
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf. r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822ff238
	if (!cr0.eq) goto loc_822FF238;
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmpw cr6,r11,r8
	cr6.compare<int32_t>(r11.s32, ctx.r8.s32, xer);
	// bne cr6,0x822ff218
	if (!cr6.eq) goto loc_822FF218;
loc_822FF238:
	// cmpwi cr6,r9,0
	cr6.compare<int32_t>(ctx.r9.s32, 0, xer);
	// blt cr6,0x822ff248
	if (cr6.lt) goto loc_822FF248;
	// addi r11,r1,-32
	r11.s64 = ctx.r1.s64 + -32;
	// b 0x822ff254
	goto loc_822FF254;
loc_822FF248:
	// stw r7,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, ctx.r7.u32);
	// addi r11,r1,-24
	r11.s64 = ctx.r1.s64 + -24;
	// stw r4,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, ctx.r4.u32);
loc_822FF254:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r10.u32);
	// stw r9,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r9.u32);
	// ld r31,-8(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FF270"))) PPC_WEAK_FUNC(sub_822FF270);
PPC_FUNC_IMPL(__imp__sub_822FF270) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r26,1
	r26.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r30,4(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff2e4
	if (!cr6.eq) goto loc_822FF2E4;
	// ld r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U64(r27.u32 + 0);
loc_822FF2A8:
	// ld r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// mr r30,r11
	r30.u64 = r11.u64;
	// cmpld cr6,r9,r10
	cr6.compare<uint64_t>(ctx.r9.u64, ctx.r10.u64, xer);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// blt cr6,0x822ff2c0
	if (cr6.lt) goto loc_822FF2C0;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822FF2C0:
	// clrlwi r29,r10,24
	r29.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822ff2d4
	if (cr6.eq) goto loc_822FF2D4;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822ff2d8
	goto loc_822FF2D8;
loc_822FF2D4:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_822FF2D8:
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ff2a8
	if (cr6.eq) goto loc_822FF2A8;
loc_822FF2E4:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff348
	if (cr6.eq) goto loc_822FF348;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ff344
	if (!cr6.eq) goto loc_822FF344;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x823003c8
	sub_823003C8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FF344:
	// bl 0x828836e8
	sub_828836E8(ctx, base);
loc_822FF348:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// ld r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U64(r27.u32 + 0);
	// ld r9,16(r11)
	ctx.r9.u64 = PPC_LOAD_U64(r11.u32 + 16);
	// mr r11,r26
	r11.u64 = r26.u64;
	// cmpld cr6,r9,r10
	cr6.compare<uint64_t>(ctx.r9.u64, ctx.r10.u64, xer);
	// blt cr6,0x822ff364
	if (cr6.lt) goto loc_822FF364;
	// li r11,0
	r11.s64 = 0;
loc_822FF364:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff3ac
	if (cr6.eq) goto loc_822FF3AC;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x823003c8
	sub_823003C8(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FF3AC:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r10,8(r31)
	PPC_STORE_U8(r31.u32 + 8, ctx.r10.u8);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822FF3C8"))) PPC_WEAK_FUNC(sub_822FF3C8);
PPC_FUNC_IMPL(__imp__sub_822FF3C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r5,256(r1)
	PPC_STORE_U64(ctx.r1.u32 + 256, ctx.r5.u64);
	// mr r25,r3
	r25.u64 = ctx.r3.u64;
	// lwz r31,260(r1)
	r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r27,r4
	r27.u64 = ctx.r4.u64;
	// lbz r11,33(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff434
	if (cr6.eq) goto loc_822FF434;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5600
	ctx.r4.s64 = r11.s64 + 5600;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822FF434:
	// addi r3,r1,256
	ctx.r3.s64 = ctx.r1.s64 + 256;
	// mr r26,r31
	r26.u64 = r31.u64;
	// bl 0x82a962b0
	sub_82A962B0(ctx, base);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// lbz r11,33(r10)
	r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff458
	if (cr6.eq) goto loc_822FF458;
	// lwz r28,8(r26)
	r28.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// b 0x822ff480
	goto loc_822FF480;
loc_822FF458:
	// lwz r11,8(r26)
	r11.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// lbz r9,33(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822ff470
	if (cr6.eq) goto loc_822FF470;
	// mr r28,r10
	r28.u64 = ctx.r10.u64;
	// b 0x822ff480
	goto loc_822FF480;
loc_822FF470:
	// lwz r11,260(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// lwz r28,8(r11)
	r28.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// bne cr6,0x822ff56c
	if (!cr6.eq) goto loc_822FF56C;
loc_822FF480:
	// lbz r11,33(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// lwz r31,4(r26)
	r31.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ff494
	if (!cr6.eq) goto loc_822FF494;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822FF494:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r10,r26
	cr6.compare<uint32_t>(ctx.r10.u32, r26.u32, xer);
	// bne cr6,0x822ff4ac
	if (!cr6.eq) goto loc_822FF4AC;
	// stw r28,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r28.u32);
	// b 0x822ff4c4
	goto loc_822FF4C4;
loc_822FF4AC:
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822ff4c0
	if (!cr6.eq) goto loc_822FF4C0;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// b 0x822ff4c4
	goto loc_822FF4C4;
loc_822FF4C0:
	// stw r28,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r28.u32);
loc_822FF4C4:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,0(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822ff514
	if (!cr6.eq) goto loc_822FF514;
	// lbz r11,33(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff4e8
	if (cr6.eq) goto loc_822FF4E8;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// b 0x822ff510
	goto loc_822FF510;
loc_822FF4E8:
	// lwz r11,0(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 0);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822ff510
	if (!cr6.eq) goto loc_822FF510;
loc_822FF4FC:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822ff4fc
	if (cr6.eq) goto loc_822FF4FC;
loc_822FF510:
	// stw r10,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r10.u32);
loc_822FF514:
	// lwz r9,4(r27)
	ctx.r9.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r11,8(r9)
	r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// cmplw cr6,r11,r26
	cr6.compare<uint32_t>(r11.u32, r26.u32, xer);
	// bne cr6,0x822ff600
	if (!cr6.eq) goto loc_822FF600;
	// lbz r11,33(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff53c
	if (cr6.eq) goto loc_822FF53C;
	// mr r10,r31
	ctx.r10.u64 = r31.u64;
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822ff600
	goto loc_822FF600;
loc_822FF53C:
	// lwz r11,8(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 8);
	// mr r10,r28
	ctx.r10.u64 = r28.u64;
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// bne cr6,0x822ff564
	if (!cr6.eq) goto loc_822FF564;
loc_822FF550:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r8,33(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r8,0
	cr6.compare<uint32_t>(ctx.r8.u32, 0, xer);
	// beq cr6,0x822ff550
	if (cr6.eq) goto loc_822FF550;
loc_822FF564:
	// stw r10,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r10.u32);
	// b 0x822ff600
	goto loc_822FF600;
loc_822FF56C:
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// lwz r10,0(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 0);
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// cmplw cr6,r11,r9
	cr6.compare<uint32_t>(r11.u32, ctx.r9.u32, xer);
	// bne cr6,0x822ff58c
	if (!cr6.eq) goto loc_822FF58C;
	// mr r31,r11
	r31.u64 = r11.u64;
	// b 0x822ff5b4
	goto loc_822FF5B4;
loc_822FF58C:
	// lbz r10,33(r28)
	ctx.r10.u64 = PPC_LOAD_U8(r28.u32 + 33);
	// lwz r31,4(r11)
	r31.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff5a0
	if (!cr6.eq) goto loc_822FF5A0;
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
loc_822FF5A0:
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
	// lwz r10,8(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r10,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r10.u32);
	// lwz r9,8(r26)
	ctx.r9.u64 = PPC_LOAD_U32(r26.u32 + 8);
	// stw r11,4(r9)
	PPC_STORE_U32(ctx.r9.u32 + 4, r11.u32);
loc_822FF5B4:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822ff5cc
	if (!cr6.eq) goto loc_822FF5CC;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// b 0x822ff5e8
	goto loc_822FF5E8;
loc_822FF5CC:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r9,r26
	cr6.compare<uint32_t>(ctx.r9.u32, r26.u32, xer);
	// bne cr6,0x822ff5e4
	if (!cr6.eq) goto loc_822FF5E4;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// b 0x822ff5e8
	goto loc_822FF5E8;
loc_822FF5E4:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
loc_822FF5E8:
	// lwz r10,4(r26)
	ctx.r10.u64 = PPC_LOAD_U32(r26.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lbz r8,32(r11)
	ctx.r8.u64 = PPC_LOAD_U8(r11.u32 + 32);
	// lbz r9,32(r26)
	ctx.r9.u64 = PPC_LOAD_U8(r26.u32 + 32);
	// stb r9,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r9.u8);
	// stb r8,32(r26)
	PPC_STORE_U8(r26.u32 + 32, ctx.r8.u8);
loc_822FF600:
	// lbz r11,32(r26)
	r11.u64 = PPC_LOAD_U8(r26.u32 + 32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822ff7a0
	if (!cr6.eq) goto loc_822FF7A0;
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// li r29,1
	r29.s64 = 1;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// beq cr6,0x822ff79c
	if (cr6.eq) goto loc_822FF79C;
	// li r30,0
	r30.s64 = 0;
loc_822FF624:
	// lbz r11,32(r28)
	r11.u64 = PPC_LOAD_U8(r28.u32 + 32);
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// bne cr6,0x822ff79c
	if (!cr6.eq) goto loc_822FF79C;
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822ff6e0
	if (!cr6.eq) goto loc_822FF6E0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff664
	if (!cr6.eq) goto loc_822FF664;
	// stb r29,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822FF664:
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff734
	if (!cr6.eq) goto loc_822FF734;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff690
	if (!cr6.eq) goto loc_822FF690;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// beq cr6,0x822ff730
	if (cr6.eq) goto loc_822FF730;
loc_822FF690:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff6bc
	if (!cr6.eq) goto loc_822FF6BC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r29.u8);
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// bl 0x8234d600
	sub_8234D600(ctx, base);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
loc_822FF6BC:
	// lbz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r10.u8);
	// stb r29,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r29.u8);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stb r29,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r29.u8);
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
	// b 0x822ff79c
	goto loc_822FF79C;
loc_822FF6E0:
	// lbz r10,32(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 32);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff704
	if (!cr6.eq) goto loc_822FF704;
	// stb r29,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r29.u8);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stb r30,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r30.u8);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x8234d600
	sub_8234D600(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822FF704:
	// lbz r10,33(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 33);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ff734
	if (!cr6.eq) goto loc_822FF734;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff750
	if (!cr6.eq) goto loc_822FF750;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff750
	if (!cr6.eq) goto loc_822FF750;
loc_822FF730:
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
loc_822FF734:
	// lwz r11,4(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// mr r28,r31
	r28.u64 = r31.u64;
	// lwz r31,4(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// cmplw cr6,r28,r10
	cr6.compare<uint32_t>(r28.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ff624
	if (!cr6.eq) goto loc_822FF624;
	// b 0x822ff79c
	goto loc_822FF79C;
loc_822FF750:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bne cr6,0x822ff77c
	if (!cr6.eq) goto loc_822FF77C;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// mr r4,r11
	ctx.r4.u64 = r11.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r29,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r29.u8);
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
	// lwz r11,0(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822FF77C:
	// lbz r10,32(r31)
	ctx.r10.u64 = PPC_LOAD_U8(r31.u32 + 32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stb r10,32(r11)
	PPC_STORE_U8(r11.u32 + 32, ctx.r10.u8);
	// stb r29,32(r31)
	PPC_STORE_U8(r31.u32 + 32, r29.u8);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r29,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r29.u8);
	// bl 0x8234d600
	sub_8234D600(ctx, base);
loc_822FF79C:
	// stb r29,32(r28)
	PPC_STORE_U8(r28.u32 + 32, r29.u8);
loc_822FF7A0:
	// mr r3,r26
	ctx.r3.u64 = r26.u64;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,8(r27)
	r11.u64 = PPC_LOAD_U32(r27.u32 + 8);
	// mr r3,r25
	ctx.r3.u64 = r25.u64;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff7d0
	if (cr6.eq) goto loc_822FF7D0;
	// ld r10,256(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// addi r9,r11,-1
	ctx.r9.s64 = r11.s64 + -1;
	// stw r9,8(r27)
	PPC_STORE_U32(r27.u32 + 8, ctx.r9.u32);
	// std r10,0(r25)
	PPC_STORE_U64(r25.u32 + 0, ctx.r10.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
loc_822FF7D0:
	// ld r11,256(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 256);
	// std r11,0(r25)
	PPC_STORE_U64(r25.u32 + 0, r11.u64);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c2c
	return;
}

__attribute__((alias("__imp__sub_822FF7E0"))) PPC_WEAK_FUNC(sub_822FF7E0);
PPC_FUNC_IMPL(__imp__sub_822FF7E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-31927
	r11.s64 = -2092367872;
	// li r10,0
	ctx.r10.s64 = 0;
	// addi r31,r11,28940
	r31.s64 = r11.s64 + 28940;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r3,0(r11)
	ctx.r3.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r11.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// stw r10,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r10.u32);
	// beq cr6,0x822ff83c
	if (cr6.eq) goto loc_822FF83C;
loc_822FF824:
	// lwz r30,0(r3)
	r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// cmplw cr6,r30,r11
	cr6.compare<uint32_t>(r30.u32, r11.u32, xer);
	// bne cr6,0x822ff824
	if (!cr6.eq) goto loc_822FF824;
loc_822FF83C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FF858"))) PPC_WEAK_FUNC(sub_822FF858);
PPC_FUNC_IMPL(__imp__sub_822FF858) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ff888
	if (!cr6.eq) goto loc_822FF888;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x822ff890
	goto loc_822FF890;
loc_822FF888:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
loc_822FF890:
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ff8c0
	if (cr6.eq) goto loc_822FF8C0;
	// lwz r9,12(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 12);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// cmplw cr6,r10,r8
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r8.u32, xer);
	// bge cr6,0x822ff8c0
	if (!cr6.lt) goto loc_822FF8C0;
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// lbz r10,0(r6)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r9,r11,1
	ctx.r9.s64 = r11.s64 + 1;
	// stb r10,0(r11)
	PPC_STORE_U8(r11.u32 + 0, ctx.r10.u8);
	// stw r9,8(r31)
	PPC_STORE_U32(r31.u32 + 8, ctx.r9.u32);
	// b 0x822ff944
	goto loc_822FF944;
loc_822FF8C0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ff8d0
	if (!cr6.gt) goto loc_822FF8D0;
	// twi 31,r0,22
loc_822FF8D0:
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// beq cr6,0x822ff8e8
	if (cr6.eq) goto loc_822FF8E8;
	// subf. r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x822ff8f0
	if (!cr0.eq) goto loc_822FF8F0;
loc_822FF8E8:
	// li r30,0
	r30.s64 = 0;
	// b 0x822ff900
	goto loc_822FF900;
loc_822FF8F0:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ff8fc
	if (!cr6.gt) goto loc_822FF8FC;
	// twi 31,r0,22
loc_822FF8FC:
	// subf r30,r11,r10
	r30.s64 = ctx.r10.s64 - r11.s64;
loc_822FF900:
	// li r5,1
	ctx.r5.s64 = 1;
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82301878
	sub_82301878(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x822ff924
	if (!cr6.gt) goto loc_822FF924;
	// twi 31,r0,22
loc_822FF924:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// add r11,r11,r30
	r11.u64 = r11.u64 + r30.u64;
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bgt cr6,0x822ff940
	if (cr6.gt) goto loc_822FF940;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// bge cr6,0x822ff944
	if (!cr6.lt) goto loc_822FF944;
loc_822FF940:
	// twi 31,r0,22
loc_822FF944:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FF960"))) PPC_WEAK_FUNC(sub_822FF960);
PPC_FUNC_IMPL(__imp__sub_822FF960) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	PPCRegister f0{};
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be0
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	r28.u64 = ctx.r4.u64;
	// li r26,1
	r26.s64 = 1;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// mr r29,r26
	r29.u64 = r26.u64;
	// lwz r30,4(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// lwz r11,4(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lbz r10,17(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 17);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x822ffa04
	if (!cr6.eq) goto loc_822FFA04;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lfs f0,8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 + ctx.f13.f64));
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// fadds f10,f12,f11
	ctx.f10.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// fabs f0,f10
	f0.u64 = ctx.f10.u64 & ~0x8000000000000000;
loc_822FF9B0:
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r30,r11
	r30.u64 = r11.u64;
	// lfs f13,8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	ctx.f13.f64 = double(temp.f32);
	// lfs f12,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f12.f64 = double(temp.f32);
	// fadds f11,f13,f12
	ctx.f11.f64 = double(float(ctx.f13.f64 + ctx.f12.f64));
	// lfs f10,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f10.f64 = double(temp.f32);
	// mr r10,r26
	ctx.r10.u64 = r26.u64;
	// fadds f9,f11,f10
	ctx.f9.f64 = double(float(ctx.f11.f64 + ctx.f10.f64));
	// fabs f8,f9
	ctx.f8.u64 = ctx.f9.u64 & ~0x8000000000000000;
	// fcmpu cr6,f0,f8
	cr6.compare(f0.f64, ctx.f8.f64);
	// blt cr6,0x822ff9e0
	if (cr6.lt) goto loc_822FF9E0;
	// li r10,0
	ctx.r10.s64 = 0;
loc_822FF9E0:
	// clrlwi r29,r10,24
	r29.u64 = ctx.r10.u32 & 0xFF;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x822ff9f4
	if (cr6.eq) goto loc_822FF9F4;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// b 0x822ff9f8
	goto loc_822FF9F8;
loc_822FF9F4:
	// lwz r11,8(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 8);
loc_822FF9F8:
	// lbz r10,17(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 17);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x822ff9b0
	if (cr6.eq) goto loc_822FF9B0;
loc_822FFA04:
	// clrlwi r11,r29,24
	r11.u64 = r29.u32 & 0xFF;
	// stw r28,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r28.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r30.u32);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ffa68
	if (cr6.eq) goto loc_822FFA68;
	// lwz r11,4(r28)
	r11.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r30,r10
	cr6.compare<uint32_t>(r30.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ffa64
	if (!cr6.eq) goto loc_822FFA64;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x8282ed70
	sub_8282ED70(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FFA64:
	// bl 0x824708f8
	sub_824708F8(ctx, base);
loc_822FFA68:
	// lwz r11,84(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 0);
	// lwz r9,12(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 12);
	// mr r11,r26
	r11.u64 = r26.u64;
	// lfs f0,8(r10)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	f0.f64 = double(temp.f32);
	// lfs f13,4(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	ctx.f13.f64 = double(temp.f32);
	// fadds f12,f0,f13
	ctx.f12.f64 = double(float(f0.f64 + ctx.f13.f64));
	// lfs f11,0(r10)
	temp.u32 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	ctx.f11.f64 = double(temp.f32);
	// lfs f10,8(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	ctx.f10.f64 = double(temp.f32);
	// lfs f9,4(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	ctx.f9.f64 = double(temp.f32);
	// fadds f8,f10,f9
	ctx.f8.f64 = double(float(ctx.f10.f64 + ctx.f9.f64));
	// lfs f7,0(r9)
	temp.u32 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	ctx.f7.f64 = double(temp.f32);
	// fadds f6,f12,f11
	ctx.f6.f64 = double(float(ctx.f12.f64 + ctx.f11.f64));
	// fadds f5,f8,f7
	ctx.f5.f64 = double(float(ctx.f8.f64 + ctx.f7.f64));
	// fabs f4,f6
	ctx.f4.u64 = ctx.f6.u64 & ~0x8000000000000000;
	// fabs f3,f5
	ctx.f3.u64 = ctx.f5.u64 & ~0x8000000000000000;
	// fcmpu cr6,f3,f4
	cr6.compare(ctx.f3.f64, ctx.f4.f64);
	// blt cr6,0x822ffab4
	if (cr6.lt) goto loc_822FFAB4;
	// li r11,0
	r11.s64 = 0;
loc_822FFAB4:
	// clrlwi r11,r11,24
	r11.u64 = r11.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ffafc
	if (cr6.eq) goto loc_822FFAFC;
	// mr r7,r27
	ctx.r7.u64 = r27.u64;
	// mr r6,r30
	ctx.r6.u64 = r30.u64;
	// mr r5,r29
	ctx.r5.u64 = r29.u64;
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x8282ed70
	sub_8282ED70(ctx, base);
	// mr r11,r3
	r11.u64 = ctx.r3.u64;
	// stb r26,8(r31)
	PPC_STORE_U8(r31.u32 + 8, r26.u8);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stw r10,0(r31)
	PPC_STORE_U32(r31.u32 + 0, ctx.r10.u32);
	// stw r9,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r9.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
loc_822FFAFC:
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// stb r10,8(r31)
	PPC_STORE_U8(r31.u32 + 8, ctx.r10.u8);
	// std r11,0(r31)
	PPC_STORE_U64(r31.u32 + 0, r11.u64);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c30
	return;
}

__attribute__((alias("__imp__sub_822FFB18"))) PPC_WEAK_FUNC(sub_822FFB18);
PPC_FUNC_IMPL(__imp__sub_822FFB18) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// std r5,176(r1)
	PPC_STORE_U64(ctx.r1.u32 + 176, ctx.r5.u64);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r6,184(r1)
	PPC_STORE_U64(ctx.r1.u32 + 184, ctx.r6.u64);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// beq cr6,0x822ffb50
	if (cr6.eq) goto loc_822FFB50;
	// cmplw cr6,r9,r31
	cr6.compare<uint32_t>(ctx.r9.u32, r31.u32, xer);
	// beq cr6,0x822ffb54
	if (cr6.eq) goto loc_822FFB54;
loc_822FFB50:
	// twi 31,r0,22
loc_822FFB54:
	// lwz r8,180(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// lwz r28,188(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 188);
	// lwz r30,184(r1)
	r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 184);
	// cmplw cr6,r8,r10
	cr6.compare<uint32_t>(ctx.r8.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ffbcc
	if (!cr6.eq) goto loc_822FFBCC;
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x822ffb78
	if (cr6.eq) goto loc_822FFB78;
	// cmplw cr6,r30,r31
	cr6.compare<uint32_t>(r30.u32, r31.u32, xer);
	// beq cr6,0x822ffb7c
	if (cr6.eq) goto loc_822FFB7C;
loc_822FFB78:
	// twi 31,r0,22
loc_822FFB7C:
	// cmplw cr6,r28,r11
	cr6.compare<uint32_t>(r28.u32, r11.u32, xer);
	// bne cr6,0x822ffbcc
	if (!cr6.eq) goto loc_822FFBCC;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// bl 0x82301b58
	sub_82301B58(ctx, base);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// li r11,0
	r11.s64 = 0;
	// stw r31,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r31.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// lwz r7,4(r31)
	ctx.r7.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r6,0(r7)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// stw r6,4(r29)
	PPC_STORE_U32(r29.u32 + 4, ctx.r6.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
loc_822FFBCC:
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822ffbdc
	if (cr6.eq) goto loc_822FFBDC;
	// cmplw cr6,r9,r30
	cr6.compare<uint32_t>(ctx.r9.u32, r30.u32, xer);
	// beq cr6,0x822ffbe0
	if (cr6.eq) goto loc_822FFBE0;
loc_822FFBDC:
	// twi 31,r0,22
loc_822FFBE0:
	// lwz r11,180(r1)
	r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 180);
	// cmplw cr6,r11,r28
	cr6.compare<uint32_t>(r11.u32, r28.u32, xer);
	// beq cr6,0x822ffc14
	if (cr6.eq) goto loc_822FFC14;
	// addi r3,r1,176
	ctx.r3.s64 = ctx.r1.s64 + 176;
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// bl 0x823d4808
	sub_823D4808(ctx, base);
	// mr r5,r27
	ctx.r5.u64 = r27.u64;
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82300ba0
	sub_82300BA0(ctx, base);
	// ld r5,176(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 176);
	// lwz r9,176(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 176);
	// b 0x822ffbcc
	goto loc_822FFBCC;
loc_822FFC14:
	// std r5,0(r29)
	PPC_STORE_U64(r29.u32 + 0, ctx.r5.u64);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822FFC28"))) PPC_WEAK_FUNC(sub_822FFC28);
PPC_FUNC_IMPL(__imp__sub_822FFC28) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	r30.u64 = ctx.r4.u64;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// mr r28,r5
	r28.u64 = ctx.r5.u64;
	// lwz r29,4(r30)
	r29.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// lwz r31,4(r29)
	r31.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lbz r11,49(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 49);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822ffc8c
	if (!cr6.eq) goto loc_822FFC8C;
loc_822FFC54:
	// mr r5,r28
	ctx.r5.u64 = r28.u64;
	// addi r4,r31,12
	ctx.r4.s64 = r31.s64 + 12;
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// bl 0x822f88e8
	sub_822F88E8(ctx, base);
	// clrlwi r11,r3,24
	r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ffc78
	if (cr6.eq) goto loc_822FFC78;
	// lwz r31,8(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// b 0x822ffc80
	goto loc_822FFC80;
loc_822FFC78:
	// mr r29,r31
	r29.u64 = r31.u64;
	// lwz r31,0(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 0);
loc_822FFC80:
	// lbz r11,49(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 49);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ffc54
	if (cr6.eq) goto loc_822FFC54;
loc_822FFC8C:
	// stw r29,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r29.u32);
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_822FFCA0"))) PPC_WEAK_FUNC(sub_822FFCA0);
PPC_FUNC_IMPL(__imp__sub_822FFCA0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r11,1820
	r11.s64 = 119275520;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// ori r9,r11,29126
	ctx.r9.u64 = r11.u64 | 29126;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x822ffd18
	if (cr6.lt) goto loc_822FFD18;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5552
	ctx.r4.s64 = r11.s64 + 5552;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_822FFD18:
	// li r3,52
	ctx.r3.s64 = 52;
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x822ffd58
	if (cr6.eq) goto loc_822FFD58;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// addi r3,r27,12
	ctx.r3.s64 = r27.s64 + 12;
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
	// li r5,36
	ctx.r5.s64 = 36;
	// stw r30,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r30.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// stb r25,48(r27)
	PPC_STORE_U8(r27.u32 + 48, r25.u8);
	// stb r25,49(r27)
	PPC_STORE_U8(r27.u32 + 49, r25.u8);
loc_822FFD58:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// bne cr6,0x822ffd88
	if (!cr6.eq) goto loc_822FFD88;
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r27.u32);
	// b 0x822ffdc8
	goto loc_822FFDC8;
loc_822FFD88:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822ffdb0
	if (cr6.eq) goto loc_822FFDB0;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ffdc8
	if (!cr6.eq) goto loc_822FFDC8;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// b 0x822ffdc8
	goto loc_822FFDC8;
loc_822FFDB0:
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x822ffdc8
	if (!cr6.eq) goto loc_822FFDC8;
	// stw r27,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r27.u32);
loc_822FFDC8:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// li r30,1
	r30.s64 = 1;
	// mr r31,r27
	r31.u64 = r27.u64;
	// lbz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 48);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822ffef0
	if (!cr6.eq) goto loc_822FFEF0;
loc_822FFDE4:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x822ffe6c
	if (!cr6.eq) goto loc_822FFE6C;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 48);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822ffe2c
	if (!cr6.eq) goto loc_822FFE2C;
	// rotlwi r9,r4,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// stb r30,48(r9)
	PPC_STORE_U8(ctx.r9.u32 + 48, r30.u8);
	// stb r30,48(r10)
	PPC_STORE_U8(ctx.r10.u32 + 48, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,48(r7)
	PPC_STORE_U8(ctx.r7.u32 + 48, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822ffedc
	goto loc_822FFEDC;
loc_822FFE2C:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822ffe44
	if (!cr6.eq) goto loc_822FFE44;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82300fb8
	sub_82300FB8(ctx, base);
loc_822FFE44:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,48(r11)
	PPC_STORE_U8(r11.u32 + 48, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,48(r9)
	PPC_STORE_U8(ctx.r9.u32 + 48, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x82301030
	sub_82301030(ctx, base);
	// b 0x822ffedc
	goto loc_822FFEDC;
loc_822FFE6C:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 48);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x822ffea0
	if (!cr6.eq) goto loc_822FFEA0;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r30,48(r9)
	PPC_STORE_U8(ctx.r9.u32 + 48, r30.u8);
	// stb r30,48(r10)
	PPC_STORE_U8(ctx.r10.u32 + 48, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,48(r7)
	PPC_STORE_U8(ctx.r7.u32 + 48, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x822ffedc
	goto loc_822FFEDC;
loc_822FFEA0:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x822ffeb8
	if (!cr6.eq) goto loc_822FFEB8;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82301030
	sub_82301030(ctx, base);
loc_822FFEB8:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,48(r11)
	PPC_STORE_U8(r11.u32 + 48, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,48(r9)
	PPC_STORE_U8(ctx.r9.u32 + 48, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x82300fb8
	sub_82300FB8(ctx, base);
loc_822FFEDC:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lbz r9,48(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 48);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x822ffde4
	if (cr6.eq) goto loc_822FFDE4;
loc_822FFEF0:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r27,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r27.u32);
	// stw r29,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r29.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stb r30,48(r10)
	PPC_STORE_U8(ctx.r10.u32 + 48, r30.u8);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_822FFF10"))) PPC_WEAK_FUNC(sub_822FFF10);
PPC_FUNC_IMPL(__imp__sub_822FFF10) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r30,r3
	r30.u64 = ctx.r3.u64;
	// lbz r11,105(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x822fff5c
	if (!cr6.eq) goto loc_822FFF5C;
loc_822FFF38:
	// mr r3,r30
	ctx.r3.u64 = r30.u64;
	// lwz r4,8(r31)
	ctx.r4.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// bl 0x822fff10
	sub_822FFF10(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// lwz r31,0(r31)
	r31.u64 = PPC_LOAD_U32(r31.u32 + 0);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// lbz r11,105(r31)
	r11.u64 = PPC_LOAD_U8(r31.u32 + 105);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x822fff38
	if (cr6.eq) goto loc_822FFF38;
loc_822FFF5C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_822FFF78"))) PPC_WEAK_FUNC(sub_822FFF78);
PPC_FUNC_IMPL(__imp__sub_822FFF78) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// li r3,112
	ctx.r3.s64 = 112;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x822fffa4
	if (cr6.eq) goto loc_822FFFA4;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, r11.u32);
loc_822FFFA4:
	// addic. r10,r3,4
	xer.ca = ctx.r3.u32 > 4294967291;
	ctx.r10.s64 = ctx.r3.s64 + 4;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822fffb0
	if (cr0.eq) goto loc_822FFFB0;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_822FFFB0:
	// addic. r10,r3,8
	xer.ca = ctx.r3.u32 > 4294967287;
	ctx.r10.s64 = ctx.r3.s64 + 8;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x822fffbc
	if (cr0.eq) goto loc_822FFFBC;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
loc_822FFFBC:
	// li r10,1
	ctx.r10.s64 = 1;
	// stb r11,105(r3)
	PPC_STORE_U8(ctx.r3.u32 + 105, r11.u8);
	// stb r10,104(r3)
	PPC_STORE_U8(ctx.r3.u32 + 104, ctx.r10.u8);
	// stw r3,4(r31)
	PPC_STORE_U32(r31.u32 + 4, ctx.r3.u32);
	// stb r10,105(r3)
	PPC_STORE_U8(ctx.r3.u32 + 105, ctx.r10.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r10,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r10.u32);
	// lwz r9,4(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r9,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r9.u32);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// stw r8,8(r8)
	PPC_STORE_U32(ctx.r8.u32 + 8, ctx.r8.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82300000"))) PPC_WEAK_FUNC(sub_82300000);
PPC_FUNC_IMPL(__imp__sub_82300000) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r11,744
	r11.s64 = 48758784;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// ori r9,r11,47661
	ctx.r9.u64 = r11.u64 | 47661;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r28,r7
	r28.u64 = ctx.r7.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82300078
	if (cr6.lt) goto loc_82300078;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5552
	ctx.r4.s64 = r11.s64 + 5552;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r30.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_82300078:
	// li r3,112
	ctx.r3.s64 = 112;
	// lwz r30,4(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x823000b8
	if (cr6.eq) goto loc_823000B8;
	// stw r30,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r30.u32);
	// addi r3,r27,16
	ctx.r3.s64 = r27.s64 + 16;
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
	// li r5,88
	ctx.r5.s64 = 88;
	// stw r30,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r30.u32);
	// mr r4,r28
	ctx.r4.u64 = r28.u64;
	// bl 0x82ca2c60
	sub_82CA2C60(ctx, base);
	// stb r25,104(r27)
	PPC_STORE_U8(r27.u32 + 104, r25.u8);
	// stb r25,105(r27)
	PPC_STORE_U8(r27.u32 + 105, r25.u8);
loc_823000B8:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// bne cr6,0x823000e8
	if (!cr6.eq) goto loc_823000E8;
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r27.u32);
	// b 0x82300128
	goto loc_82300128;
loc_823000E8:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82300110
	if (cr6.eq) goto loc_82300110;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x82300128
	if (!cr6.eq) goto loc_82300128;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// b 0x82300128
	goto loc_82300128;
loc_82300110:
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x82300128
	if (!cr6.eq) goto loc_82300128;
	// stw r27,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r27.u32);
loc_82300128:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// li r30,1
	r30.s64 = 1;
	// mr r31,r27
	r31.u64 = r27.u64;
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300250
	if (!cr6.eq) goto loc_82300250;
loc_82300144:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x823001cc
	if (!cr6.eq) goto loc_823001CC;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8230018c
	if (!cr6.eq) goto loc_8230018C;
	// rotlwi r9,r4,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// stb r30,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r30.u8);
	// stb r30,104(r10)
	PPC_STORE_U8(ctx.r10.u32 + 104, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,104(r7)
	PPC_STORE_U8(ctx.r7.u32 + 104, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x8230023c
	goto loc_8230023C;
loc_8230018C:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x823001a4
	if (!cr6.eq) goto loc_823001A4;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x82300270
	sub_82300270(ctx, base);
loc_823001A4:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x823002e8
	sub_823002E8(ctx, base);
	// b 0x8230023c
	goto loc_8230023C;
loc_823001CC:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300200
	if (!cr6.eq) goto loc_82300200;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r30,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r30.u8);
	// stb r30,104(r10)
	PPC_STORE_U8(ctx.r10.u32 + 104, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,104(r7)
	PPC_STORE_U8(ctx.r7.u32 + 104, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x8230023c
	goto loc_8230023C;
loc_82300200:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x82300218
	if (!cr6.eq) goto loc_82300218;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x823002e8
	sub_823002E8(ctx, base);
loc_82300218:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,104(r11)
	PPC_STORE_U8(r11.u32 + 104, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,104(r9)
	PPC_STORE_U8(ctx.r9.u32 + 104, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x82300270
	sub_82300270(ctx, base);
loc_8230023C:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lbz r9,104(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 104);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82300144
	if (cr6.eq) goto loc_82300144;
loc_82300250:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r27,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r27.u32);
	// stw r29,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r29.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stb r30,104(r10)
	PPC_STORE_U8(ctx.r10.u32 + 104, r30.u8);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_82300270"))) PPC_WEAK_FUNC(sub_82300270);
PPC_FUNC_IMPL(__imp__sub_82300270) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r10,8(r4)
	PPC_STORE_U32(ctx.r4.u32 + 8, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,105(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 105);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300290
	if (!cr6.eq) goto loc_82300290;
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
loc_82300290:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x823002b8
	if (!cr6.eq) goto loc_823002B8;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_823002B8:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x823002d8
	if (!cr6.eq) goto loc_823002D8;
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_823002D8:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// stw r4,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823002E8"))) PPC_WEAK_FUNC(sub_823002E8);
PPC_FUNC_IMPL(__imp__sub_823002E8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,105(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 105);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300308
	if (!cr6.eq) goto loc_82300308;
	// stw r4,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r4.u32);
loc_82300308:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// stw r10,4(r11)
	PPC_STORE_U32(r11.u32 + 4, ctx.r10.u32);
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x82300330
	if (!cr6.eq) goto loc_82300330;
	// stw r11,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, r11.u32);
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_82300330:
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,8(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x82300350
	if (!cr6.eq) goto loc_82300350;
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r11.u32);
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
loc_82300350:
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, r11.u32);
	// stw r4,8(r11)
	PPC_STORE_U32(r11.u32 + 8, ctx.r4.u32);
	// stw r11,4(r4)
	PPC_STORE_U32(ctx.r4.u32 + 4, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82300360"))) PPC_WEAK_FUNC(sub_82300360);
PPC_FUNC_IMPL(__imp__sub_82300360) {
	PPC_FUNC_PROLOGUE();
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// addi r3,r1,88
	ctx.r3.s64 = ctx.r1.s64 + 88;
	// stw r31,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, r31.u32);
	// mr r4,r31
	ctx.r4.u64 = r31.u64;
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r6,80(r1)
	ctx.r6.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// ld r5,88(r1)
	ctx.r5.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// bl 0x823010a8
	sub_823010A8(ctx, base);
	// lwz r3,4(r31)
	ctx.r3.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
	// li r11,0
	r11.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(r31.u32 + 4, r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823003C8"))) PPC_WEAK_FUNC(sub_823003C8);
PPC_FUNC_IMPL(__imp__sub_823003C8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r24{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bd8
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r4
	r29.u64 = ctx.r4.u64;
	// lis r11,4095
	r11.s64 = 268369920;
	// mr r24,r3
	r24.u64 = ctx.r3.u64;
	// ori r9,r11,65534
	ctx.r9.u64 = r11.u64 | 65534;
	// mr r26,r5
	r26.u64 = ctx.r5.u64;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// mr r31,r6
	r31.u64 = ctx.r6.u64;
	// mr r30,r7
	r30.u64 = ctx.r7.u64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// blt cr6,0x82300440
	if (cr6.lt) goto loc_82300440;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,5552
	ctx.r4.s64 = r11.s64 + 5552;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r28,r10,5696
	r28.s64 = ctx.r10.s64 + 5696;
	// stw r28,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r28.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r28,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r28.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
loc_82300440:
	// li r3,40
	ctx.r3.s64 = 40;
	// lwz r28,4(r29)
	r28.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// li r25,0
	r25.s64 = 0;
	// cmplwi cr6,r27,0
	cr6.compare<uint32_t>(r27.u32, 0, xer);
	// beq cr6,0x82300490
	if (cr6.eq) goto loc_82300490;
	// stw r28,0(r27)
	PPC_STORE_U32(r27.u32 + 0, r28.u32);
	// stw r31,4(r27)
	PPC_STORE_U32(r27.u32 + 4, r31.u32);
	// stw r28,8(r27)
	PPC_STORE_U32(r27.u32 + 8, r28.u32);
	// lwz r11,0(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 0);
	// stw r11,16(r27)
	PPC_STORE_U32(r27.u32 + 16, r11.u32);
	// lwz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U32(r30.u32 + 4);
	// stw r10,20(r27)
	PPC_STORE_U32(r27.u32 + 20, ctx.r10.u32);
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// stw r9,24(r27)
	PPC_STORE_U32(r27.u32 + 24, ctx.r9.u32);
	// lwz r8,12(r30)
	ctx.r8.u64 = PPC_LOAD_U32(r30.u32 + 12);
	// stw r8,28(r27)
	PPC_STORE_U32(r27.u32 + 28, ctx.r8.u32);
	// stb r25,32(r27)
	PPC_STORE_U8(r27.u32 + 32, r25.u8);
	// stb r25,33(r27)
	PPC_STORE_U8(r27.u32 + 33, r25.u8);
loc_82300490:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// stw r10,8(r29)
	PPC_STORE_U32(r29.u32 + 8, ctx.r10.u32);
	// bne cr6,0x823004c0
	if (!cr6.eq) goto loc_823004C0;
	// stw r27,4(r11)
	PPC_STORE_U32(r11.u32 + 4, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// lwz r10,4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// stw r27,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, r27.u32);
	// b 0x82300500
	goto loc_82300500;
loc_823004C0:
	// clrlwi r11,r26,24
	r11.u64 = r26.u32 & 0xFF;
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x823004e8
	if (cr6.eq) goto loc_823004E8;
	// stw r27,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x82300500
	if (!cr6.eq) goto loc_82300500;
	// stw r27,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r27.u32);
	// b 0x82300500
	goto loc_82300500;
loc_823004E8:
	// stw r27,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r27.u32);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r31,r10
	cr6.compare<uint32_t>(r31.u32, ctx.r10.u32, xer);
	// bne cr6,0x82300500
	if (!cr6.eq) goto loc_82300500;
	// stw r27,8(r11)
	PPC_STORE_U32(r11.u32 + 8, r27.u32);
loc_82300500:
	// lwz r10,4(r27)
	ctx.r10.u64 = PPC_LOAD_U32(r27.u32 + 4);
	// addi r11,r27,4
	r11.s64 = r27.s64 + 4;
	// li r30,1
	r30.s64 = 1;
	// mr r31,r27
	r31.u64 = r27.u64;
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300628
	if (!cr6.eq) goto loc_82300628;
loc_8230051C:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r10,4(r4)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r4.u32 + 4);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// bne cr6,0x823005a4
	if (!cr6.eq) goto loc_823005A4;
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300564
	if (!cr6.eq) goto loc_82300564;
	// rotlwi r9,r4,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r4.u32, 0);
	// stb r30,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r30.u8);
	// stb r30,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,32(r7)
	PPC_STORE_U8(ctx.r7.u32 + 32, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x82300614
	goto loc_82300614;
loc_82300564:
	// lwz r11,8(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 8);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x8230057c
	if (!cr6.eq) goto loc_8230057C;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
loc_8230057C:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x8234d600
	sub_8234D600(ctx, base);
	// b 0x82300614
	goto loc_82300614;
loc_823005A4:
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x823005d8
	if (!cr6.eq) goto loc_823005D8;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// stb r30,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r30.u8);
	// stb r30,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r30.u8);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r7,4(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stb r25,32(r7)
	PPC_STORE_U8(ctx.r7.u32 + 32, r25.u8);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lwz r31,4(r6)
	r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// b 0x82300614
	goto loc_82300614;
loc_823005D8:
	// lwz r11,0(r4)
	r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmplw cr6,r31,r11
	cr6.compare<uint32_t>(r31.u32, r11.u32, xer);
	// bne cr6,0x823005f0
	if (!cr6.eq) goto loc_823005F0;
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// bl 0x8234d600
	sub_8234D600(ctx, base);
loc_823005F0:
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// stb r30,32(r11)
	PPC_STORE_U8(r11.u32 + 32, r30.u8);
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stb r25,32(r9)
	PPC_STORE_U8(ctx.r9.u32 + 32, r25.u8);
	// lwz r8,4(r31)
	ctx.r8.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r4,4(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// bl 0x8292cc88
	sub_8292CC88(ctx, base);
loc_82300614:
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// addi r11,r31,4
	r11.s64 = r31.s64 + 4;
	// lbz r9,32(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 32);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x8230051c
	if (cr6.eq) goto loc_8230051C;
loc_82300628:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// mr r3,r24
	ctx.r3.u64 = r24.u64;
	// stw r27,4(r24)
	PPC_STORE_U32(r24.u32 + 4, r27.u32);
	// stw r29,0(r24)
	PPC_STORE_U32(r24.u32 + 0, r29.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// stb r30,32(r10)
	PPC_STORE_U8(ctx.r10.u32 + 32, r30.u8);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x82ca2c28
	return;
}

__attribute__((alias("__imp__sub_82300648"))) PPC_WEAK_FUNC(sub_82300648);
PPC_FUNC_IMPL(__imp__sub_82300648) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r12{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r11,8(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r10,r11
	ctx.r9.s64 = r11.s64 - ctx.r10.s64;
	// cmplw cr6,r4,r9
	cr6.compare<uint32_t>(ctx.r4.u32, ctx.r9.u32, xer);
	// ble cr6,0x82300698
	if (!cr6.gt) goto loc_82300698;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - r11.s64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// li r11,0
	r11.s64 = 0;
	// add r5,r10,r4
	ctx.r5.u64 = ctx.r10.u64 + ctx.r4.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r11.u32);
	// ld r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// bl 0x823012b0
	sub_823012B0(ctx, base);
	// b 0x823006c4
	goto loc_823006C4;
loc_82300698:
	// bge cr6,0x823006c4
	if (!cr6.lt) goto loc_823006C4;
	// add r3,r10,r4
	ctx.r3.u64 = ctx.r10.u64 + ctx.r4.u64;
	// cmplw cr6,r3,r11
	cr6.compare<uint32_t>(ctx.r3.u32, r11.u32, xer);
	// beq cr6,0x823006c4
	if (cr6.eq) goto loc_823006C4;
	// subf. r6,r11,r11
	ctx.r6.s64 = r11.s64 - r11.s64;
	cr0.compare<int32_t>(ctx.r6.s32, 0, xer);
	// add r30,r6,r3
	r30.u64 = ctx.r6.u64 + ctx.r3.u64;
	// ble 0x823006c0
	if (!cr0.gt) goto loc_823006C0;
	// mr r5,r11
	ctx.r5.u64 = r11.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_823006C0:
	// stw r30,8(r31)
	PPC_STORE_U32(r31.u32 + 8, r30.u32);
loc_823006C4:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	// ld r30,-24(r1)
	r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_823006E0"))) PPC_WEAK_FUNC(sub_823006E0);
PPC_FUNC_IMPL(__imp__sub_823006E0) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2be4
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// std r4,232(r1)
	PPC_STORE_U64(ctx.r1.u32 + 232, ctx.r4.u64);
	// li r3,16
	ctx.r3.s64 = 16;
	// lwz r28,236(r1)
	r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r27,r5
	r27.u64 = ctx.r5.u64;
	// lwz r30,4(r28)
	r30.u64 = PPC_LOAD_U32(r28.u32 + 4);
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// mr r31,r3
	r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	cr6.compare<uint32_t>(r31.u32, 0, xer);
	// beq cr6,0x82300714
	if (cr6.eq) goto loc_82300714;
	// stw r28,0(r31)
	PPC_STORE_U32(r31.u32 + 0, r28.u32);
loc_82300714:
	// addi r29,r31,4
	r29.s64 = r31.s64 + 4;
	// cmplwi cr6,r29,0
	cr6.compare<uint32_t>(r29.u32, 0, xer);
	// beq cr6,0x82300724
	if (cr6.eq) goto loc_82300724;
	// stw r30,0(r29)
	PPC_STORE_U32(r29.u32 + 0, r30.u32);
loc_82300724:
	// addic. r11,r31,8
	xer.ca = r31.u32 > 4294967287;
	r11.s64 = r31.s64 + 8;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82300734
	if (cr0.eq) goto loc_82300734;
	// ld r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U64(r27.u32 + 0);
	// std r10,0(r11)
	PPC_STORE_U64(r11.u32 + 0, ctx.r10.u64);
loc_82300734:
	// lis r11,-31927
	r11.s64 = -2092367872;
	// lis r10,8191
	ctx.r10.s64 = 536805376;
	// addi r30,r11,28940
	r30.s64 = r11.s64 + 28940;
	// ori r9,r10,65535
	ctx.r9.u64 = ctx.r10.u64 | 65535;
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
	// subf r8,r11,r9
	ctx.r8.s64 = ctx.r9.s64 - r11.s64;
	// cmplwi cr6,r8,1
	cr6.compare<uint32_t>(ctx.r8.u32, 1, xer);
	// bge cr6,0x8230079c
	if (!cr6.lt) goto loc_8230079C;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r4,r11,11140
	ctx.r4.s64 = r11.s64 + 11140;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// addi r27,r10,5696
	r27.s64 = ctx.r10.s64 + 5696;
	// stw r27,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r27.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r27,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, r27.u32);
	// addi r3,r1,112
	ctx.r3.s64 = ctx.r1.s64 + 112;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x82171810
	sub_82171810(ctx, base);
	// lwz r11,8(r30)
	r11.u64 = PPC_LOAD_U32(r30.u32 + 8);
loc_8230079C:
	// addi r11,r11,1
	r11.s64 = r11.s64 + 1;
	// stw r11,8(r30)
	PPC_STORE_U32(r30.u32 + 8, r11.u32);
	// stw r31,4(r28)
	PPC_STORE_U32(r28.u32 + 4, r31.u32);
	// lwz r11,0(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 0);
	// stw r31,0(r11)
	PPC_STORE_U32(r11.u32 + 0, r31.u32);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x82ca2c34
	return;
}

__attribute__((alias("__imp__sub_823007B8"))) PPC_WEAK_FUNC(sub_823007B8);
PPC_FUNC_IMPL(__imp__sub_823007B8) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	// lwz r11,0(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// bne cr6,0x823007c8
	if (!cr6.eq) goto loc_823007C8;
	// twi 31,r0,22
loc_823007C8:
	// lwz r11,4(r3)
	r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lbz r10,105(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x823007e0
	if (cr6.eq) goto loc_823007E0;
	// twi 31,r0,22
	// blr 
	return;
loc_823007E0:
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// lbz r9,105(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 105);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x8230081c
	if (!cr6.eq) goto loc_8230081C;
	// lwz r11,0(r10)
	r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// lbz r9,105(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// bne cr6,0x82300814
	if (!cr6.eq) goto loc_82300814;
loc_82300800:
	// mr r10,r11
	ctx.r10.u64 = r11.u64;
	// lwz r11,0(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 0);
	// lbz r9,105(r11)
	ctx.r9.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r9,0
	cr6.compare<uint32_t>(ctx.r9.u32, 0, xer);
	// beq cr6,0x82300800
	if (cr6.eq) goto loc_82300800;
loc_82300814:
	// stw r10,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, ctx.r10.u32);
	// blr 
	return;
loc_8230081C:
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lbz r10,105(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// bne cr6,0x82300854
	if (!cr6.eq) goto loc_82300854;
loc_8230082C:
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(r11.u32 + 8);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bne cr6,0x82300854
	if (!cr6.eq) goto loc_82300854;
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// rotlwi r11,r11,0
	r11.u64 = __builtin_rotateleft32(r11.u32, 0);
	// lwz r11,4(r11)
	r11.u64 = PPC_LOAD_U32(r11.u32 + 4);
	// lbz r10,105(r11)
	ctx.r10.u64 = PPC_LOAD_U8(r11.u32 + 105);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x8230082c
	if (cr6.eq) goto loc_8230082C;
loc_82300854:
	// stw r11,4(r3)
	PPC_STORE_U32(ctx.r3.u32 + 4, r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_82300860"))) PPC_WEAK_FUNC(sub_82300860);
PPC_FUNC_IMPL(__imp__sub_82300860) {
	PPC_FUNC_PROLOGUE();
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bec
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	r31.u64 = ctx.r4.u64;
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// std r4,160(r1)
	PPC_STORE_U64(ctx.r1.u32 + 160, ctx.r4.u64);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplwi cr6,r11,0
	cr6.compare<uint32_t>(r11.u32, 0, xer);
	// beq cr6,0x82300898
	if (cr6.eq) goto loc_82300898;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi. r9,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	cr0.compare<int32_t>(ctx.r9.s32, 0, xer);
	// bne 0x823008a0
	if (!cr0.eq) goto loc_823008A0;
loc_82300898:
	// li r30,0
	r30.s64 = 0;
	// b 0x823008d0
	goto loc_823008D0;
loc_823008A0:
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x823008ac
	if (!cr6.gt) goto loc_823008AC;
	// twi 31,r0,22
loc_823008AC:
	// lwz r10,160(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x823008c0
	if (cr6.eq) goto loc_823008C0;
	// cmplw cr6,r10,r31
	cr6.compare<uint32_t>(ctx.r10.u32, r31.u32, xer);
	// beq cr6,0x823008c4
	if (cr6.eq) goto loc_823008C4;
loc_823008C0:
	// twi 31,r0,22
loc_823008C4:
	// lwz r10,164(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	// subf r9,r11,r10
	ctx.r9.s64 = ctx.r10.s64 - r11.s64;
	// srawi r30,r9,3
	xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	r30.s64 = ctx.r9.s32 >> 3;
loc_823008D0:
	// li r5,1
	ctx.r5.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = r31.u64;
	// bl 0x82301540
	sub_82301540(ctx, base);
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// cmplw cr6,r11,r10
	cr6.compare<uint32_t>(r11.u32, ctx.r10.u32, xer);
	// ble cr6,0x823008f0
	if (!cr6.gt) goto loc_823008F0;
	// twi 31,r0,22
loc_823008F0:
	// rlwinm r10,r30,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(r31.u32 + 8);
	// stw r31,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, r31.u32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + r11.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r11.u32);
	// ld r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// std r8,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r8.u64);
	// bgt cr6,0x82300920
	if (cr6.gt) goto loc_82300920;
	// lwz r11,4(r31)
	r11.u64 = PPC_LOAD_U32(r31.u32 + 4);
	// cmplw cr6,r10,r11
	cr6.compare<uint32_t>(ctx.r10.u32, r11.u32, xer);
	// bge cr6,0x82300924
	if (!cr6.lt) goto loc_82300924;
loc_82300920:
	// twi 31,r0,22
loc_82300924:
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// mr r3,r29
	ctx.r3.u64 = r29.u64;
	// ld r11,80(r1)
	r11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// std r11,0(r29)
	PPC_STORE_U64(r29.u32 + 0, r11.u64);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x82ca2c3c
	return;
}

__attribute__((alias("__imp__sub_82300940"))) PPC_WEAK_FUNC(sub_82300940);
PPC_FUNC_IMPL(__imp__sub_82300940) {
	PPC_FUNC_PROLOGUE();
	PPCRegister ctr{};
	PPCXERRegister xer{};
	PPCCRRegister cr0{};
	PPCCRRegister cr6{};
	PPCRegister r11{};
	PPCRegister r25{};
	PPCRegister r26{};
	PPCRegister r27{};
	PPCRegister r28{};
	PPCRegister r29{};
	PPCRegister r30{};
	PPCRegister r31{};
	uint32_t ea{};
	// mflr r12
	// bl 0x82ca2bdc
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	r29.u64 = ctx.r3.u64;
	// lis r11,4095
	r11.s64 = 268369920;
	// li r25,0
	r25.s64 = 0;
	// ori r31,r11,65535
	r31.u64 = r11.u64 | 65535;
	// li r26,1
	r26.s64 = 1;
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// subf r9,r10,r31
	ctx.r9.s64 = r31.s64 - ctx.r10.s64;
	// cmplwi cr6,r9,1
	cr6.compare<uint32_t>(ctx.r9.u32, 1, xer);
	// bge cr6,0x823009d0
	if (!cr6.lt) goto loc_823009D0;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// addi r3,r1,96
	ctx.r3.s64 = ctx.r1.s64 + 96;
	// addi r4,r11,11160
	ctx.r4.s64 = r11.s64 + 11160;
	// bl 0x822f2020
	sub_822F2020(ctx, base);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x826c3ef0
	sub_826C3EF0(ctx, base);
	// lis r10,-32246
	ctx.r10.s64 = -2113273856;
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// addi r30,r10,5696
	r30.s64 = ctx.r10.s64 + 5696;
	// stw r30,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r30.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// stw r30,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, r30.u32);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// bl 0x826c3fa8
	sub_826C3FA8(ctx, base);
	// lwz r9,120(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmplwi cr6,r9,16
	cr6.compare<uint32_t>(ctx.r9.u32, 16, xer);
	// blt cr6,0x823009c0
	if (cr6.lt) goto loc_823009C0;
	// lwz r3,100(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_823009C0:
	// li r11,15
	r11.s64 = 15;
	// stw r25,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, r25.u32);
	// stb r25,100(r1)
	PPC_STORE_U8(ctx.r1.u32 + 100, r25.u8);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, r11.u32);
loc_823009D0:
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r11,r10,31,1,31
	r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// cmplwi cr6,r11,8
	cr6.compare<uint32_t>(r11.u32, 8, xer);
	// bge cr6,0x823009e8
	if (!cr6.lt) goto loc_823009E8;
	// li r11,8
	r11.s64 = 8;
	// b 0x823009f0
	goto loc_823009F0;
loc_823009E8:
	// cmplwi cr6,r11,1
	cr6.compare<uint32_t>(r11.u32, 1, xer);
	// ble cr6,0x82300a00
	if (!cr6.gt) goto loc_82300A00;
loc_823009F0:
	// subf r9,r11,r31
	ctx.r9.s64 = r31.s64 - r11.s64;
	// cmplw cr6,r10,r9
	cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, xer);
	// bgt cr6,0x82300a00
	if (cr6.gt) goto loc_82300A00;
	// mr r26,r11
	r26.u64 = r11.u64;
loc_82300A00:
	// lwz r30,12(r29)
	r30.u64 = PPC_LOAD_U32(r29.u32 + 12);
	// add. r31,r10,r26
	r31.u64 = ctx.r10.u64 + r26.u64;
	cr0.compare<int32_t>(r31.s32, 0, xer);
	// bne 0x82300a14
	if (!cr0.eq) goto loc_82300A14;
	// mr r31,r25
	r31.u64 = r25.u64;
	// b 0x82300a48
	goto loc_82300A48;
loc_82300A14:
	// li r11,-1
	r11.s64 = -1;
	// divwu r10,r11,r31
	ctx.r10.u32 = r11.u32 / r31.u32;
	// cmplwi cr6,r10,4
	cr6.compare<uint32_t>(ctx.r10.u32, 4, xer);
	// bge cr6,0x82300a48
	if (!cr6.lt) goto loc_82300A48;
	// lis r11,-32246
	r11.s64 = -2113273856;
	// stw r25,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, r25.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r10,r11,5684
	ctx.r10.s64 = r11.s64 + 5684;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// bl 0x822f1f00
	sub_822F1F00(ctx, base);
	// lis r9,-32246
	ctx.r9.s64 = -2113273856;
	// addi r8,r9,5672
	ctx.r8.s64 = ctx.r9.s64 + 5672;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
loc_82300A48:
	// rlwinm r3,r31,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(r31.u32 | (r31.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8221f388
	sub_8221F388(ctx, base);
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// lwz r10,8(r29)
	ctx.r10.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// rlwinm r31,r30,2,0,29
	r31.u64 = __builtin_rotateleft64(r30.u32 | (r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r27,r3
	r27.u64 = ctx.r3.u64;
	// add r5,r31,r11
	ctx.r5.u64 = r31.u64 + r11.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r31,r27
	ctx.r3.u64 = r31.u64 + r27.u64;
	// subf r10,r5,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r5.s64;
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + r11.u64;
	// srawi. r11,r8,2
	xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	r11.s64 = ctx.r8.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r6,r3
	r28.u64 = ctx.r6.u64 + ctx.r3.u64;
	// beq 0x82300a8c
	if (cr0.eq) goto loc_82300A8C;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_82300A8C:
	// lwz r5,4(r29)
	ctx.r5.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplw cr6,r30,r26
	cr6.compare<uint32_t>(r30.u32, r26.u32, xer);
	// bgt cr6,0x82300b08
	if (cr6.gt) goto loc_82300B08;
	// subf r11,r5,r31
	r11.s64 = r31.s64 - ctx.r5.s64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r6,r28
	r31.u64 = ctx.r6.u64 + r28.u64;
	// beq 0x82300abc
	if (cr0.eq) goto loc_82300ABC;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_82300ABC:
	// subf. r10,r30,r26
	ctx.r10.s64 = r26.s64 - r30.s64;
	cr0.compare<int32_t>(ctx.r10.s32, 0, xer);
	// beq 0x82300ae4
	if (cr0.eq) goto loc_82300AE4;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r9,r25
	ctx.r9.u64 = r25.u64;
	// cmplwi cr6,r10,0
	cr6.compare<uint32_t>(ctx.r10.u32, 0, xer);
	// beq cr6,0x82300ae4
	if (cr6.eq) goto loc_82300AE4;
	// mtctr r10
	ctr.u64 = ctx.r10.u64;
loc_82300AD8:
	// stw r9,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82300ad8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82300AD8;
loc_82300AE4:
	// cmplwi cr6,r30,0
	cr6.compare<uint32_t>(r30.u32, 0, xer);
	// beq cr6,0x82300b78
	if (cr6.eq) goto loc_82300B78;
	// mr r11,r27
	r11.u64 = r27.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mtctr r30
	ctr.u64 = r30.u64;
loc_82300AF8:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82300af8
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82300AF8;
	// b 0x82300b78
	goto loc_82300B78;
loc_82300B08:
	// rlwinm r30,r26,2,0,29
	r30.u64 = __builtin_rotateleft64(r26.u32 | (r26.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r5,r30
	r11.s64 = r30.s64 - ctx.r5.s64;
	// add r11,r11,r5
	r11.u64 = r11.u64 + ctx.r5.u64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// beq 0x82300b2c
	if (cr0.eq) goto loc_82300B2C;
	// rlwinm r4,r11,2,0,29
	ctx.r4.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r28
	ctx.r3.u64 = r28.u64;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_82300B2C:
	// lwz r11,4(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// add r5,r30,r11
	ctx.r5.u64 = r30.u64 + r11.u64;
	// subf r10,r5,r31
	ctx.r10.s64 = r31.s64 - ctx.r5.s64;
	// add r11,r10,r11
	r11.u64 = ctx.r10.u64 + r11.u64;
	// srawi. r11,r11,2
	xer.ca = (r11.s32 < 0) & ((r11.u32 & 0x3) != 0);
	r11.s64 = r11.s32 >> 2;
	cr0.compare<int32_t>(r11.s32, 0, xer);
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(r11.u32 | (r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r6,r27
	r31.u64 = ctx.r6.u64 + r27.u64;
	// beq 0x82300b58
	if (cr0.eq) goto loc_82300B58;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r3,r27
	ctx.r3.u64 = r27.u64;
	// bl 0x82ca3808
	sub_82CA3808(ctx, base);
loc_82300B58:
	// cmplwi cr6,r26,0
	cr6.compare<uint32_t>(r26.u32, 0, xer);
	// beq cr6,0x82300b78
	if (cr6.eq) goto loc_82300B78;
	// mr r11,r31
	r11.u64 = r31.u64;
	// mr r10,r25
	ctx.r10.u64 = r25.u64;
	// mtctr r26
	ctr.u64 = r26.u64;
loc_82300B6C:
	// stw r10,0(r11)
	PPC_STORE_U32(r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,4
	r11.s64 = r11.s64 + 4;
	// bdnz 0x82300b6c
	--ctr.u64;
	if (ctr.u32 != 0) goto loc_82300B6C;
loc_82300B78:
	// lwz r3,4(r29)
	ctx.r3.u64 = PPC_LOAD_U32(r29.u32 + 4);
	// cmplwi cr6,r3,0
	cr6.compare<uint32_t>(ctx.r3.u32, 0, xer);
	// beq cr6,0x82300b88
	if (cr6.eq) goto loc_82300B88;
	// bl 0x8221be68
	sub_8221BE68(ctx, base);
loc_82300B88:
	// lwz r11,8(r29)
	r11.u64 = PPC_LOAD_U32(r29.u32 + 8);
	// stw r27,4(r29)
	PPC_STORE_U32(r29.u32 + 4, r27.u32);
	// add r11,r11,r26
	r11.u64 = r11.u64 + r26.u64;
	// stw r11,8(r29)
	PPC_STORE_U32(r29.u32 + 8, r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x82ca2c2c
	return;
}

